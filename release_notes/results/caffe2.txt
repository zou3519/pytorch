0c18de2623  # Add inferBoundShapeOp
8c6f0c0587  # Detect TorchScript archives in torch.load (#29339)
6c9b188262  # Support in-place update in IndexHashOp (#30275)
ee20e66c48  # replace the SLSRQ for their right emulations in the replayer test (#30367)
45880f4246  # Change logging to remove the word "error" from info log
53785771a7  # Don't build test_cpp_rpc if torch is built without distributed support (#30587)
d32f261f16  # make the order btw div and mul in adagrad update consistent (#30449)
a5b1f6e7d7  # Add missing _API definitions. (#30310)
ca072951d5  # move MaskedAdagrad to caffe2/operators/experimental/optimizers (#30714)
e09c415387  # Back out "make the order btw div and mul in adagrad update consistent" (#30737)
7a2889b014  # Stop producing op_version_set version numbers.
e123d90a93  # Back out "Back out "Back out "Revert D18542342: Boxed variable dispatch""" (#30650)
6848f9abb8  # call fp16<->fp32 routines in fbgemm from Half2Float and Float2Half operators (#30715)
42324cb6e8  # Change interface from map of TensorShape to shapeInfoMap (#30802)
c34ef1aa2e  # Automatic update of fbcode/onnx to c08a7b76cf7c1555ae37186f12be4d62b2c39b3b (#30619)
a42d093db2  # FCTransposed to FbFCPacked (#29766)
bb7befb12c  # Support loading by blob in predictor
313c211f3f  # Calling JITed 8 Bit Fused SLS in FBGEMM from C2 (#30926)
7f5f2e8871  # add ZERO_COLLISION_HASH to caffe2 data type (#30912)
4a751dfc20  # optimize MulGradient for common shapes (#19705)
9047d4df45  # Remove all remaining usages of BUILD_NAMEDTENSOR (#31116)
199e1fb348  # Use AVX2 to increase frequency for FP16<->FP32 Caffe2 ops (#31203)
409151e1bb  # Use [[noreturn]] instead of C10_NORETURN or CAFFE_NORETURN (#30917)
52b8a52e4d  # move AliasWithNameOp to caffe2/operators
5554e5b793  # Docs: c++11 -> c++14 (#30530)
d9c3913dfc  # move BatchPermutationOp to caffe2/operators
e3fecabdcb  # Setup operator registration for distributed package (#31214)
c5d2758c35  # Disable flaky TestMomentumSGD.test_fp16momentum_sgd (#31369)
b0bd35ff13  # caffe2/event: allow multiple errors such as when cancelled (#31335)
d08250c223  # fix zero-batch handling in convtranspose (#24341)
c4121ed8db  # Fix is_fundamental template for MSVC (#30959)
4c341582ea  # modify model to enable loading by blob (#31507)
2099cfa13d  # Fix input_channels divisibility check in concat_split_op (#31448)
7a12ccd003  # optimize FloatToFused8BitRowwiseQuantized and Fused8BitRowwiseQuantizedToFloat (#31470)
7d630278da  # Separate torchbind from Python (#30242)
a54dc87e8e  # revert D18805532 and make numerics of masked adagrad consistent with unmasked adagrad (#30784)
39508501a4  # Create byte-aware word lstm benchmark (#31260)
4983ef8de1  # Integrating MaskedAdagrad
b522a8e1ff  # Optimize zero length input (#31602)
35bee0c729  # separate op for rowwise counter (#31612)
90a187618e  # Integrate masked sparse Adagrad (#31641)
f4e955ff62  # Change PackSegments to ensure consistent behavior between CPU and GPU
b102550d2c  # Allow to pass in masks through db (#31676)
daf00beaba  # Remove duplicated Numa detection code. (#30628)
86a4e2135d  # Do not register `const float *` type on utiliy_ops.cu (#31583)
9407137102  # Update the descriptive error message for enforce fail (#31575)
809ee9d04c  # Enable personalized FC weight_init and sparse_emb weight_init (#31707)
84dfa96f62  # Fix -Wundef warning in conversions.h
8b4feff01d  # Use simd version for fp16 conversions (#31897)
d2fdf140af  # Combine all the user inputs together and convert them to fp16 (#31898)
9e9ca6ec37  # add conversion functions to embedding tables (#31083)
ddff4efa26  # Don't use RTLD_GLOBAL to load _C. (#31162)
ab5eb65e74  # gate torch_global_deps with BUILD_SHARED_LIBS flag (#32011)
28c1258f18  # Scale init for batch-norm and layer-norm (#31983)
