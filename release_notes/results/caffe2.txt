0c18de2623  # Add inferBoundShapeOp
8c6f0c0587  # Detect TorchScript archives in torch.load (#29339)
6c9b188262  # Support in-place update in IndexHashOp (#30275)
ee20e66c48  # replace the SLSRQ for their right emulations in the replayer test (#30367)
45880f4246  # Change logging to remove the word "error" from info log
53785771a7  # Don't build test_cpp_rpc if torch is built without distributed support (#30587)
d32f261f16  # make the order btw div and mul in adagrad update consistent (#30449)
a5b1f6e7d7  # Add missing _API definitions. (#30310)
ca072951d5  # move MaskedAdagrad to caffe2/operators/experimental/optimizers (#30714)
e09c415387  # Back out "make the order btw div and mul in adagrad update consistent" (#30737)
7a2889b014  # Stop producing op_version_set version numbers.
e123d90a93  # Back out "Back out "Back out "Revert D18542342: Boxed variable dispatch""" (#30650)
6848f9abb8  # call fp16<->fp32 routines in fbgemm from Half2Float and Float2Half operators (#30715)
42324cb6e8  # Change interface from map of TensorShape to shapeInfoMap (#30802)
c34ef1aa2e  # Automatic update of fbcode/onnx to c08a7b76cf7c1555ae37186f12be4d62b2c39b3b (#30619)
a42d093db2  # FCTransposed to FbFCPacked (#29766)
bb7befb12c  # Support loading by blob in predictor
313c211f3f  # Calling JITed 8 Bit Fused SLS in FBGEMM from C2 (#30926)
7f5f2e8871  # add ZERO_COLLISION_HASH to caffe2 data type (#30912)
4a751dfc20  # optimize MulGradient for common shapes (#19705)
9047d4df45  # Remove all remaining usages of BUILD_NAMEDTENSOR (#31116)
199e1fb348  # Use AVX2 to increase frequency for FP16<->FP32 Caffe2 ops (#31203)
409151e1bb  # Use [[noreturn]] instead of C10_NORETURN or CAFFE_NORETURN (#30917)
52b8a52e4d  # move AliasWithNameOp to caffe2/operators
5554e5b793  # Docs: c++11 -> c++14 (#30530)
d9c3913dfc  # move BatchPermutationOp to caffe2/operators
e3fecabdcb  # Setup operator registration for distributed package (#31214)
c5d2758c35  # Disable flaky TestMomentumSGD.test_fp16momentum_sgd (#31369)
b0bd35ff13  # caffe2/event: allow multiple errors such as when cancelled (#31335)
d08250c223  # fix zero-batch handling in convtranspose (#24341)
c4121ed8db  # Fix is_fundamental template for MSVC (#30959)
4c341582ea  # modify model to enable loading by blob (#31507)
2099cfa13d  # Fix input_channels divisibility check in concat_split_op (#31448)
7a12ccd003  # optimize FloatToFused8BitRowwiseQuantized and Fused8BitRowwiseQuantizedToFloat (#31470)
7d630278da  # Separate torchbind from Python (#30242)
a54dc87e8e  # revert D18805532 and make numerics of masked adagrad consistent with unmasked adagrad (#30784)
39508501a4  # Create byte-aware word lstm benchmark (#31260)
4983ef8de1  # Integrating MaskedAdagrad
b522a8e1ff  # Optimize zero length input (#31602)
35bee0c729  # separate op for rowwise counter (#31612)
90a187618e  # Integrate masked sparse Adagrad (#31641)
f4e955ff62  # Change PackSegments to ensure consistent behavior between CPU and GPU
b102550d2c  # Allow to pass in masks through db (#31676)
daf00beaba  # Remove duplicated Numa detection code. (#30628)
86a4e2135d  # Do not register `const float *` type on utiliy_ops.cu (#31583)
9407137102  # Update the descriptive error message for enforce fail (#31575)
809ee9d04c  # Enable personalized FC weight_init and sparse_emb weight_init (#31707)
84dfa96f62  # Fix -Wundef warning in conversions.h
8b4feff01d  # Use simd version for fp16 conversions (#31897)
d2fdf140af  # Combine all the user inputs together and convert them to fp16 (#31898)
9e9ca6ec37  # add conversion functions to embedding tables (#31083)
ddff4efa26  # Don't use RTLD_GLOBAL to load _C. (#31162)
ab5eb65e74  # gate torch_global_deps with BUILD_SHARED_LIBS flag (#32011)
28c1258f18  # Scale init for batch-norm and layer-norm (#31983)
f6f1e0aef5  # Automatic update of fbcode/onnx to 65020daafa9183c769938b4512ce543fd5740f8f (#32125)
879620e85e  # [caffe2] fix how np.clip is used in lengths_reducer_fused_{4,8}_rowwise_ops_test (#32086)
8c3ee9f2ba  # [Python] Deprecate use of scipy.misc.logsumexp and scipy.misc.comb (#32209)
0392e8384b  # Fix simple typo: whos -> whose (#31288)
851a7e861b  # Add CAFFE2_API to video decoding functions (#31187)
d75b6b3f9d  # Support shape inference and lowering of SparseLengthsWeightedSumFused4BitRowwise (#32257)
f94aab45fd  # Logical condition reduction (#32201)
ef5ae4823a  # Register RoIAlignRotated with C10
91bdb872ce  # fix spelling mistake: excpected -> expected
4e69352713  # Add 64bit atomic fetch add (#32354)
df514fd8c0  # C++ C2/Glow operator unittest
9b6ec61bfd  # exposing CPU/GPU Copy ops (#32248)
8c40a78277  # Back out "Calling JITed 8 Bit Fused SLS in FBGEMM from C2" (#32381)
ec4be4e58c  # Redundant condition (#32396)
b77c25dec0  # Fix dll load logic for Python 3.8 on Windows (#32215)
14e0bec9f2  # [caffe2] remove unnecessary np.set_printoptions and fix test errors (#32475)
685f090ac8  # [Rowwise Pruning][c2 op] Add Quantile Op (#32448)
e735395fc6  # [caffe2] use 2-stage EmbeddingSpMDM interface (#32271)
bd20274e8f  # [caffe2] use JIT'ed fp32 SLS (#32413)
957a07ffbd  # [ROCm] Enable Caffe2 video operators for ROCm
19bb496a0d  # Enable mkldnn on windows (#31355)
64323ae177  # Back out "Use simd version for fp16 conversions" (#32640)
465ebd58ba  # [JIT] pickle serialization for custom bound classes
ffdcbadeaa  # Minor refactoring to improve code reuse (#32675)
0327e75e14  # Back out "[caffe2] use JIT'ed fp32 SLS" (#32711)
1f78bd0774  # [caffe2] Early error throwing for currupted embeddings
25d33a2ee8  # [JIT] Use Type Level Granularity in Alias Analysis Wildcards (#32251)
e84f9d9d0c  # Fix TensorProtosDBInput AttributeError (#32274)
a840afbeb4  # [pytorch][embeddingbag_8bit] Add include_last_offset option to Fused 8bit EmbeddingBag and parallelize the op (#32683)
fb159b5236  # Some work on eager op binding codegen (gen_python_functions.py) (#29986)
14c15eb3b0  # Py2 -> py3 for caffe2/caffe2/contrib/tensorboard (#32882)
b4b1b100bd  # Add a loop test for onnxified net (#32935)
18d1896ba0  # Fix confusing "does not have GPU support" warning message (#30721)
908b451efb  # Enabling the nccl/rccl test for ROCM environment (#32340)
e76fa9822d  # [C2] Introduce extra_info force CPU tags for auto-generated iteration counter blobs (#32607)
674dca0831  # Automatic update of fbcode/onnx to 8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e (#33075)
6f46962f21  # [1/3] Bind IndexHash to PyTorch (#33015)
6c0dc66cb4  # [caffe2] use JIT'ed fp32 SLS (#33123)
9d9fa2eace  # [2/3] Bind Bucketize to PyTorch (#33014)
1767ae8daf  # [caffe2] remove dnnlowp log code (#33184)
0e753b2818  # Fix SIGABORT caused by double exception in PyTorchStreamReader when file not found. (#33243)
806e7daa1f  # Rename TorchScript compiler to IR emitter to better reflect its function. (#33127)
16685d93e9  # [TVM] Add ReplaceNaN op (#33256)
b98c7d34ed  # [TVM] Add clip op to c2_frontend (#33257)
acea368095  # Fix compilation error when buildng with FFMPEG (#27589)
2635055229  # [ROCm] Enable 3D batch norms through MIOpen (#33262)
e5c7b7b8b5  # Automatic update of fbcode/onnx to 04a29addfd5b912812addb8dea5f8763fbfaad01 (#33328)
92fbf7cf97  # [caffe2] use JIT'ed fp16 SLS (#32432)
ecd3c252b4  # Suport all length one SLS op lowering: C2 part (#33332)
6ade7e3a15  # [ROCm] Enable 3D convolutions through ROCm (#33067)
c57f8984e6  # [caffe2] make order btw div and mul in adgrad consistent (#32974)
96e5dea9f4  # Remove unused variable (#33484)
e5a02aa2fe  # [caffe2] simplify relative error expr (#32999)
81394581a3  # [Caffe2][ThreadPool] Make sure numThreads does not exceed the number of big cores (#33523)
e95282ab28  # [caffe2] make fused rowwise quant/dequant op work for N-dim tensors (#33426)
108fc78395  # [caffe2] fix invalid % escape in inline assembly strings (#33554)
23846d5a38  # [caffe2] use Clang identification macro in various places (#33574)
3498c000e2  # [TVM] Remove dynamic batch size dispatching (#33584)
089d658153  # [TensorExpr] Add classes for memory management in tensor expressions. (#33216)
1a4f997178  # [TensorExpr] Add a class for representing data type. (#33217)
49af9425a7  # [TensorExpr] Add core classes for representing expressions and statements. (#33218)
fc70fc3610  # [TensorExpr] Add IR visitor, IR mutator, and IR evaluator. (#33219)
bb5181b716  # [TensorExpr] Add IR Printer. (#33220)
6474ea404d  # [C2] Native GPU implementation for bucketize (#33529)
9b2b15f4fc  # misc windows warning fixes (#33632)
a2f3c6c26f  # Call RandomNumberSeed() on-demand (#33539)
696527e659  # [caffe2] Add embedding empty ratio checker (disabled by default) (#33145)
bf00b4d305  # [TensorExpr] Add a boilerplate pass for future TensorExpr fusion pass. (#33464)
65864d3634  # [C2] Small improvement for elementwise_mul operator. (#33537)
4460c8b034  # [C2] Tiny changes to adagrad to make it slightly better. (#33727)
ee23944f46  # [Caffe2] Fix shape inference for element-wise operators (#33431)
04f88a3a7b  # Add partition info message to NetDef (#33616)
00f685d2d8  # Add Scalar::type() (#33603)
a7fe200f5f  # [caffe2] simplify caffe2 code with fbgemm handling block size 1 emb (#33774)
1507573a52  # [caffe2] fix no return statement in constexpr function Clang error in TypeIndex.h (#33576)
5dde8cd483  # [caffe2] fix no matching function min/max Clang errors (#33563)
56d9906083  # update mapping of fake operators (#33946)
b678256bfb  # Move glu to Aten(CPU) (#33179)
4fb8679218  # [caffe2] fix field initialization after base Clang errors (#33556)
4377061baf  # [caffe2] fix atomicAdd redeclaration Clang error (#33559)
0e52627358  # Fixing pthreadpool symbol conflict issue. (#33869)
ad17dafc50  # [caffe2] Remove python2 from operator_test (#33977)
cab8772c6c  # Freezing Torchscript modules (#32178)
9239608037  # fix windows clang attributes (#33959)
51d969e86a  # preprocessor cleanup (#33957)
b874c039f6  # Allow checking for cached module before asserting (#33954)
0759191f12  # blacklist spatialBN until bitwise matching (#34092)
8a14b41617  # fix warnings reported by PVS (#33868)
e0b90b87a4  # [C2] Fix slowness of the ReshapeOp. (#33729)
1702152ef9  # fixup unit tests (#34105)
7289e8e865  # [caffe2] std::numeric_limits<double>::quiet_NaN() use instead of ::nan("") (#33566)
9650253d70  # [caffe2] fix ambiguous call to 'fmaxType' THCHalfAutoNumerics.cuh (#33569)
7c20578794  # NNPI op mapping correct SpatialBN NNPI op name (#34176)
9b39ad7f2c  # [jit] Fix iOS build (#34180)
2ba74b741e  # Add backward Int8Quantize shape inference (#34152)
790274bff2  # [caffe2] Fix signed unsigned comparison warning (#34161)
8269c4f3d3  # Added nullptr check for pthradpool_get_threads_count (#34087)
2b79bab029  # [CUDA_FUSER] Fork CUDA fuser (#33527)
75d29f8d3e  # Allow converting IValue to vector<string> (#34269)
4c99351de6  # [AMD] Remove num_gpu check for remote execution (#34318)
879a90b322  # [ModelLoading] Use byte encoding for uint8, fp16 etc. instead of int32 (#34343)
45a504dd2d  # [JIT] Introduce BuiltinOpFunction and integrate into torchbind (#34098)
c218963270  # fix more errors (#34480)
79e1305519  # [net_runner] Get shape info from qtensors (#34321)
965146b818  # [jit] delete netdef converter (#33807)
259d7299db  # [caffe2] do not declare __assert_fail in clang builds (#33893)
0dc0fffca1  # [net_transform] only skip ConstantFill for autogen_grad (#34628)
e95657b87e  # [C++ API] AdaptiveLogSoftmaxWithLoss (#29076)
a54416d208  # [C++ API] Remove deprecated torch::nn::BatchNorm / FeatureDropout / modules_ordered_dict and torch::nn::init::Nonlinearity / FanMode (#34508)
944ea4c334  # ONNX Export Support for CrossEntropyLoss (#33767)
9e6cd98c3f  # Ensure torch_cuda is linked against on Windows (#34288)
fe9b4e3cba  # [DPER3] Blob Reorder (#33579)
721bd11cc3  # [caffe2] Refactor out common util functions from tvm_transformer (#34652)
4ae74b3b25  # [DPER3][Shape Inference] Initial Shape Inference in DPER3 frontend (#33607)
808f84ee35  # [Shape Inference] Update shape inference in dper3 backend - C2 part (#34474)
8f854fb9e2  # [1/n][multi-tower] add partition info in predictor construction (#34175)
4da5569300  # Pass to remove prepacking ops. (#34319)
1d81bd02cc  # Export roi_align_gradient_op to c10 (#34776)
24c9e61e79  # Enable JIT tests on Windows (#27029)
99b91ee2ad  # [fix][tiny][caffe2] Avoid triggering errors when allow ratio is 100% (#34757)
e31d462e92  # [TensorExpr] Pull changes to core classes for representing expressions and statements from the side branch. (#34224)
42b2c8c65d  # [TensorExpr] Add a fuser pass based on tensor expressions. (#34226)
35e7efeb9a  # [TensorExpr] Add CUDA codegen. (#34227)
ea5c86c276  # [TensorExpr] Add LLVM codegen. (#34228)
ef78fa8668  # caffe2::OperatorBase do not need to be aware of at::Tensor functions (#34810)
6d8649dc53  # [caffe2] fix Transpose2D calls in NHWC<->NCHW (#34625)
e70c28856f  # [Caffe2] Move more method implementations from tensor.h to tensor.cc (#34811)
d9b97a4ffd  # [caffe2] open source 2/4-bit SLS operators (#34783)
cfab65d90d  # Fix CMake Dev warning in caffe2/CMakeLists.txt (#34886)
3ad7dfa2cf  # move emulation libraries to contrib (#34861)
959a7138fd  # Support RowWiseSparseAdam on GPU (#34341)
bcbdba450c  # [caffe2] open source 2/4-bit SLS operators (#34903)
a3de359464  # Do not throw from CUDAContext destructor (#34756)
