7ac8efa689  # Skip undefined tensors when moving torch::nn module to a different device (#30523)
18ec4632b3  # Exclude undefined tensors in the result of Module::parameters() / named_paramters() / buffers() / named_buffers() (#30626)
03a73cb9ac  # Remove namespace F = torch::nn::functional from torch/nn/modules/batchhnorm.h (#30684)
244b0bd1a5  # Add docs for how we expose declarations in at:: to torch:: (#30760)
5687ee1d85  # added a serialize function in SGD class to utilize the existing macro for serialization/deserialization calls
536481d9de  # Fix missing virtual destructor (#30927)
672f4cfad9  # Added C++ API test (#30980)
49a5841a9f  # Make Conv{1,2,3}dOptions and ConvTranspose{1,2,3}dOptions different classes (#31005)
47766e648f  # C++ API parity: MultiheadAttention
1ba1799a66  # C++ added 3rd arg of false to BatchNorm/InstanceNorm register_parameter â€¦ (#31873)
114562cf93  # For torch::from_blob() add clue when memory is non-owned. (#31222)
1296e2d55e  # C++ API parity: isinf (#31099)
b6cee03e29  # C++ tensor indexing: add Slice / TensorIndex (#30424)
01010f5705  # Add comments to torch::nn::ConvTranspose{1,2,3}d modules explaining how to use them in a Sequential module (#32223)
be6ffac1b6  # Adagrad optimizer - updated step function, added param_groups, state to optimizers
b564eaf7a8  # Bug fixes: torch::tensor(floating-point values) -> default dtype, and torch::tensor(integer values) ->at::kLong (#32367)
d141465713  # Fix torch::allclose to handle std::numeric_limits<T>::lowest() for integral types (#32978)
91744907d4  # SGD: updated step and class design (#32592)
e9e9331927  # Fractional Max Pooling: output ratios defined as double (#33304)
5d7f42847c  # Add at::Tensor::retain_grad API (#33349)
4724964810  # [C++ API] Expose AnyValue and AnyModuleHolder classes (#33026)
a203dc2e6d  # [C++ API] Allow skipping default arguments in module's forward method when module is used in Sequential (#33027)
e77abb9a5b  # Normalize reward-to-go in C++ actor-critic (#33550)
47e90d774e  # C++/Python API Parity: add pad_sequence (#32387)
533b973fd0  # Fix visibility of torch::nn::RNNImpl::options (#33718)
36919278cc  # C++ tensor multi-dim indexing: add index() and index_put_() overloads, simple indexing tests, merge with Python indexing path (#32841)
0dded4026e  # [C++ API] Add PackedSequence / pack_padded_sequence / pad_packed_sequence / pack_sequence (#33652)
5c33d98b0d  # Add assert_tensor_equal and assert_tensor_not_equal to test/cpp/api/support.h (#30426)
1494005cfd  # C++ tensor indexing: more indexing tests (#30427)
45b8c8dbcb  # [torch] Fix sign-compare warning in `torch::utils::rnn:pack_sequence` (#34185)
76035f050b  # [C++ API Parity] Adam: updated step and class design (#33730)
b8fd88319a  # C++ make torch::nn::Sequential push_back(AnyModule) methods public (#34208)
415595ace4  # [C++ API] Remove init-list form of at::indexing::Slice (#34255)
739d4609c3  # [C++ API] Fix ModuleList compile error: error: 'begin' was not declared in this scope (#34463)
baeb359e7a  # Remove `using namespace torch::autograd` from header files (#34423)
2d24005d18  # [C++ API Parity] rmsprop optimizer update (#33450)
9064fafb6e  # [C++ API] Update torch::nn layer docs (#34522)
3924c55f4c  # [C++ API] Update torch::nn functional docs (#34688)
da11646db1  # [C++ API] Link to module options doc for functional that has same options as module (#34752)
d041d0784e  # [C++ API] RNNCell / LSTMCell / GRUCell layers (#34400)
e23a9dc140  # [C++ API] RNN / GRU / LSTM layer refactoring (#34322)
bdd7dbfd4b  # [C++ API] RNN / GRU / LSTM layer refactoring (#34322)
762be86e63  # [C++ API Parity] [Optimizers] added closure to optimizers (#34790)
d7e4a379a0  # [C++ API Parity] LBFGS optimizer step() update and added closure to the Optimizer step() function (#34564)
38986e1dea  # Split libtorch.so back into libtorch_{cpu,cuda,hip} (#30315)
a5b1f6e7d7  # Add missing _API definitions. (#30310)
5554e5b793  # Docs: c++11 -> c++14 (#30530)
e95657b87e  # [C++ API] AdaptiveLogSoftmaxWithLoss (#29076)
a54416d208  # [C++ API] Remove deprecated torch::nn::BatchNorm / FeatureDropout / modules_ordered_dict and torch::nn::init::Nonlinearity / FanMode (#34508)
9e6cd98c3f  # Ensure torch_cuda is linked against on Windows (#34288)
