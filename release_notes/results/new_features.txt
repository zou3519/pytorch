d12786b24f  # add __torch_function__ API override mechanism (#27064)
9d3402e4cb  # Add the __torch_function__ API override mechanism (#30730)
b47e9b97a2  # Add op bitwise_and (#31104)
ca72df06ae  # disable __torch_function__ overides for operators in torch.functional (#30839)
8dc67a014f  # Add cummax
bab87e4b60  # reimplement __torch_function__ overrides for torch.functional using inline logic (#32194)
5b815d980e  # Added cummin
b1c85dd916  # Custom RNG DispatchKey (#32325)
da015c77a1  # Cummax and Cummin doc update and performance benchmark (#32537)
a8bd1d24c9  # [Documentation] cummin doc fix (#33492)
fa80299bdf  # __torch_function__ overrides for torch.functional and torch.nn.functional (#32799)
fc6a153688  # [WIP] Reanimate gradient scaling API with original scale update heuristic (#33366)
a726827ec8  # Formatting changes for gradient scaling (#33832)
ad2825a2c9  # Add API for listing functions overridable by __torch_function__ (#33791)
2ec779d46c  # PCA and SVD for low-rank matrices, LOBPCG for positive-defined generalized eigenvalue problem (#29488)
3f1ba3c465  # Redo of "Add API for listing functions overridable by __torch_function__" (#34240)
8bae1ed144  # PCA and SVD for low-rank matrices, LOBPCG for positive-defined generalized eigenvalue problem - copy (#34721)
1bac5fd0d3  # add hardsigmoid FP operator to PyTorch (#34545)
6b701de130  # Add types argument to __torch_function__ (#34303)
1f4a4aaf64  # functional autograd api (#34066)
d0577e19f0  # Revert D20346700: [pytorch][PR] Eager autocasting, out-of-place ops only
aaa8f02156  # Eager autocasting, out-of-place ops only (#32140)
8bbafa0b32  # Add logical_and and logical_or (#28162)
bb5dcaf24f  # Add logical_and and logical_or (#30521)
40246fa63c  # Gradient scaling API (#26512)
