1d3f3a1a0c  # Add pybind11 trampoline class for c10d.Store (#30415)
0282c5ae69  # Add helper to aggregate multiple process groups (#25768)
f531815526  # Deprecate tensor.type() (#30281)
619e2ffe23  # Replace deprecated AT_* with TORCH_* to reduce warnings in c10d
5c56986738  # Attach autograd edges only for tensors requiring grad. (#30904)
776fdda753  # Add debug info API for distributed autograd. (#30642)
4f342a61c1  # add the worker IDs outside of addSendRpcBackward to ensure they are (#30914)
06c7420fa2  # Raise error if a block can not be found from a CUDA tensor (#30870)
b01b05790e  # Fix memory leak due to circular dependency. (#31030)
a9ad98fb25  # Remove unused argument "destId" in addSendRpcBackward (#31207)
70013415c7  # DDP should not set grad for globally unused params (#28883)
c5d3be1102  # Remove the second copy on calling dist_autograd_context._known_worker_ids() (#31206)
fde94e7556  # Provide async mode for local autograd engine. (#31230)
20e5c90d82  # accept url query when rank or wolrd_size is specified (#32016)
43eb931c0f  # Remove mis-exposed abort API on ProcessGroup
b894dc06de  # [Pytorch] Propagate errors in clearAndWaitForOutstandingRpcsAsync. (#32952)
ab75d64e6e  # Add ability to abort NCCL communicators from the store. (#32895)
3c4cec56aa  # Enable test_distributed for ROCm but only with nccl backend [REDUX] (#32551)
44af8ee6cd  # Add pybind11 exception translator (#30588)
45c45195cd  # Remove warning about building from source to use the NCCL backend (#34051)
b50825e011  # Make RecordFunction more robust for async use cases (#34122)
e87db8a77b  # Fix example format in Distributed Autograd doc (#34914)
