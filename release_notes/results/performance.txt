25f4ba7c1b  # Improve compare kernel (#29743)
d5c136097a  # improve .view() performance (#30554)
1b5ce05924  # don't use size()/stride() functions in TensorImpl, use size_[d]/stride_[d] instead (#30452)
289e9a07fd  # Move Tanh backward to Aten(CPU+CUDA) (#30224)
f874230d33  # Vectorize smooth L1 loss backward function on CPU. (#30046)
82c3f4861f  # Move hardtanh activation to Aten(CPU, CUDA) (#30152)
44ff7b08d8  # Reduce intrusive_ptr incref/decref costs (#30709)
8d35b6cec7  # embedding_bag make_bag_size optimization (#30701)
daef363b15  # Move Softshrink activation to Aten(CPU+CUDA) (#30229)
d6d6075573  # Optimize LayerNorm with explicit vectorization using Vec256 (#29104)
066e3ed953  # Re-apply "[bert/RoBERTa] Optimize LayerNorm with explicit vectorization using Vec256" (#31127)
60ec53c7fd  # Fix copy kernel speed regression introduced in #29631 (#31279)
7cf8b9bada  # Move leaky_relu to Aten(CPU, CUDA) (#29899)
5e8bac24b4  # Migrate soft_margin_loss from the TH to Aten (CUDA+CPU) (#28135)
489dd6cb90  # Add TORCH_DCHECK macro that checks only in debug builds (#31240)
8f3c0d541e  # Speed up `Tensor::has_names` for unnamed tensors (#31436)
79e30ff3f8  # optimize index_select performance on CPU with TensorIterator (#30598)
34561dadcd  # Don't handle bias inside cudnn_convolution* (#31524)
eb23171bce  # TensorIterator norm update (#31903)
e74a215ade  # Changed clip_grad_norm_ total_norm calculation (#32020)
7b7390778c  # Make an assert on a hotpath trigger only in DEBUG mode. (#32117)
5bc44fb6ea  # TensorIterator unrolling and vectorized load - step 0, 1 (#31974)
cc2d5b15ad  # F.normalize uses clamp_min_ inplace (#32360)
6412ca3ce9  # duplicate symbols with AT_PARALLEL_OPENMP=0 (#32568)
2471ddc96c  # Improved speed of frobenous norm for non-complex dtype (#30871)
3ee6673e99  # Refreshing numel on a stride update is pointless. (#32116)
a9583c1f75  # Vectorize softplus and its backward function on CPU (#32944)
9857d9b4cd  # fix gather regression by not materializing loop vars in the error mes… (#33108)
31370949be  # Add zero_mask function for vectorized functions. (#32985)
7b50e76255  # optimize cat performance on CPU with TensorIterator (#30806)
000a5e2b7f  # bad tbb lambda capture, bad chunk size (#30352)
bc0ab07064  # Opitmize Unfold3d to improve performance of Conv3d (#33191)
0808485c6a  # Workaround performance bug / memory leak in GOMP (#32875)
7dde91b0ae  # Vectorize elu and its backward function on CPU (#32986)
1e76649d30  # fast setup for output tensor in tensor iterator (#33165)
ebb008eb68  # Optimize Unfold3dAcc to improve performance of conv3d backward (#33317)
55fa133cdc  # Remove gpu_kernel_with_index (#33370)
1fe635be3c  # Allow vectorized gpu loop to have different argument types (#33222)
f62f1b2ef0  # Revert "Revert D19964089: [pytorch][PR] Allow vectorized gpu loop to … (#33553)
16d6c17845  # improve roll performance (#33623)
e1bddbbaf6  # Bounds checking for functor execution in vectorized/unrolled kernels (#33642)
a9cef05f5d  # improve EmbeddingBag performance on cuda (#33589)
c6d301220a  # Fix torch.cat() performance regression on single core CPU (#33534)
2fa51dde28  # Remove unnecessary tensor copies (#33732)
ec0f2184ba  # clang intrinsics targeting (#33958)
9b527b35bb  # CUDA Vectorized Dropout (#33879)
f6c883ccea  # TH: Defer to ATen's AVX detection code (#34088)
39f78db7ec  # optimize UpSampleNearest 1d 2d and 3d performance on CPU (#31452)
86fb522acd  # Remove cudaMemcpy on full memory overlap (#34548)
f404537c26  # CUDA Loops: move address computation into policy, make policy.load load all arguments (#33720)
acbca57d18  # improve batch_norm contiguous case's performance (#34530)
b185359fb4  # Avoid clone for sparse tensors during accumulation of grads. (#33427)
3ada2e0d64  # [pytorch][embeddingbag] Parallelize the EmbeddingBag operator (#4049)
