3301794855  # Port ELU activation to Aten (#29275)
a02a5129a8  # Move rrelu to Aten(CPU) (#31094)
4f9d2f74e2  # Port softplus activation to Aten(CPU+CUDA) (#30504)
9ba6a768de  # Add op bitwise_or (#31559)
9a3cb1e859  # Move cauchy to Aten(CPU) (#31824)
5a76335aaa  # Move lshift to Aten (#31566)
99b3f9cac4  # Move log_sigmoid to Aten(CPU) (#30958)
e59e5ba5a3  # Move geometric to Aten(CPU) (#31878)
8098ae455c  # Move rshift to Aten (#31594)
61ee8c972f  # porting scatter_add to ATen (CPU) (#31662)
64de93d8e7  # Move log_normal to Aten(CPU) (#31854)
1c017f0c14  # Migrate max and min (binary) from TH to ATen. (#30851)
d2f66083c5  # porting gather to ATen using TensorIterator with multithreading support. (#32425)
18aab32959  # Move exponential_ from TH to Aten (CPU) (#32501)
8cb05e72c6  # Port BCELoss to ATen to increase accuracy (#31365)
9c2ed2574a  # Vectorized memory access in TensorIterator GPU loop for 1d contiguous case (#32383)
b00345a6f2  # Move normal distribution to Aten(CPU) (#32031)
330d051bd5  # [pytorch] Migrating index_add cuda to ATen (#30573)
367488b001  # Move where cuda implementation to TensorIterator (#32984)
