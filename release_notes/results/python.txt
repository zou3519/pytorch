30874b31a9  # Enable JNI build on Mac host (#30207)
73c9e6e6b6  # Rename function parameters to avoid [-Werror,-Wshadow] (#30276)
5e19460ced  # cache tensor scalar_type in OperandInfo (#30065)
29887f813a  # Remove unused forward declaration (#30154)
faacbfa8bf  # Migrate index_add cpu from TH to ATen (#28421)
3455231e9c  # Expose configuration of Numa directories to setup.py (#30104)
5d7b2089e8  # Draft version: Make AliasAnalysisKind optional in Op Registration API (#30187)
7903fb118f  # Move qkv_same, kv_same into branch (#30142)
a5272cb643  # Error instead of assertion failure for div by sparse (#30260)
35e6c1763e  # Switch Docker image onda-cuda-cxx11-ubuntu1604 to new uniform name (#29943)
2e709763a3  # add wrapper to exclude XLA when running device tests
99a2a0b1ca  # Implement torch.diagonal for named tensors (#30193)
c7f988b8c6  # transport open registration (#30167)
7570b2798a  # updating citation (#30267)
4aa692fc91  # Convert KernelTable to a flat-indexed array rather than a hashtable. (#30332)
7b5045be9d  # Remove LeftRight from OperatorEntry and DispatchTable. (#30333)
24aabe439a  # Make Dispatcher::backendFallbackKernels_ an array (#30340)
fb8c17dde1  # Test cases for backend fallback kernels (#29214)
afdc0bd4ec  # OperatorHandle::callBoxed/callUnboxed (#29330)
959a849a23  # better boxing (#29681)
0c7e4c1d62  # backend fallback test (#29682)
3990e9d1ca  # Improve performance of LeftRight::read() (#30282)
99a46b44ea  # Use correct API macro in VariableHooksInterface. (#30320)
f994377d28  # Turn off scalar_check for lshift, rshift.
94ad7544ae  # Turn off scalar_check for __or__
0c9c62ba6e  # Turn off scalar_checks for __and__ and clone.
ce5f1a1b25  # Turn off scalar_check for masked_select. (#29923)
6e88ddf352  # Turn off scalar_check for _th_addmv and _th_eig as they can never pass. (#29945)
7c6cc1d6d4  # Turn off scalar_checks for _th_multinomial_alias_draw. (#29946)
b8eba7aca9  # Turn off scalar_check for ormqr. (#29947)
16606e1725  # Turn off scalar_check for mode; the underlying code is correct.
7160300638  # Turn off scalar_check for reductions _th_max, _th_min. (#29949)
0c67311878  # Turn off scalar_check for set_(Storage, ...) (#29950)
d7ac90e2ef  # Stop binding std_single and var_single from TH; they aren't used anymore.
0517323dad  # Update osx CI to XCode 9.4 / CUDA 10.0, cudnn 7.6.5 (#30359)
d64e2581cc  # Add list of supported XCode/CUDA versions to README
25f4ba7c1b  # Improve compare kernel (#29743)
0b71e7e1fd  # Refactor QAT Conv module for better extensibility (#30362)
ab2ec4d835  # Fix inexistent parameter in document (#24335)
46e7f31fa3  # Document unsupported types (#30344)
8199596d7e  # Add missing std::move (#30411)
79a830af56  # Turn off scalar_check for Tensor.set_(Tensor) (#29952)
72ac45662b  # Turn off scalar_checks for torch.take. (#29953)
dbce53fe32  # Turn off scalar_check for _th_gather. (#29954)
dcd9f49809  # Specify ordering on singular values and eigenvalues output from torch… (#30389)
b0871f211b  # Make all optimizers consistent so that they don't change gradients inplace
fec903ce00  # Fix test case after get_qparams refactor (#30470)
c5a6c4d6c9  # Adding elementwise kernel also operating on index (#28175)
634f370c63  # Add comment to ops bound at python layer
976d91d30a  # Comment on a set of ops bound at the python layer
4eff2f2007  # Fix missing closing quotes in docs
21d7532dfe  # Add more comment on NumPy detection in Python scripts.
6bd8937aee  # FunctionParameter::set_default_str replace || with &&
829499e626  # avoid Formatting::print() when STRIP_ERROR_MESSAGES is set (#30451)
d2336edcfb  # Boxed variable dispatch (#29934)
fcb7371e65  # Update docs for cpp_extension on Windows (#30392)
106ab487eb  # fix typo in doc
a69be8123a  # Use `gettimeofday` on iOS (#30361)
c1c5622a6a  # Add katex to pytorch-linux-xenial-py3.6-gcc5.4 docker image (#30522)
7d2b0aa693  # add retries to network operations (curl, conda install, git clone) (#30479)
0b25371f5d  # Turn off scalar_check for _th_normal.
87f29557bd  # Ignore logical_and and logical_or in op BC check for now (#30537)
c780610f2d  # Disable test_backward_per_tensor in test_fake_quant (#30594)
e6000a7c04  # Temporarily disable test_numerical_consistency_per_tensor (#30600)
8ee61e0be4  # Fix CPU_INTEL flag error on windows (#30564)
b68d1fc316  # add small input shapes to some ops (#30617)
6deb41c88d  # Update magma to 2.5.1 for Windows and switch CUDA in CI to 9.2
569729527b  # Turn off scalar_checks for exp, cos, cosh, tan, atan, tanh, erf, erfc. (#30434)
19b7d49fac  # Add TOC to CONTRIBUTING.md (#29671)
98ab55fc51  # PRAGMA missing for clang (#30351)
2d0a4e42e9  # Add barriers to fix flaky test_graph_for_py_nested_call and (#30624)
9e3d19412b  # Disable implicit conversion warning (#30529)
db81e13d6b  # Fix TCPStoreTest and improve tcputils::connect() (#30354)
e5b947a3a8  # Raise an error for is_signed on quantized types (#30527)
61798865e3  # Turn off scalar_checks for torch.clamp. (#30435)
8b29701ae5  # Turn off scalar_checks for _th_reciprocal. (#30436)
b446572997  # TestCppExtension now removes /tmp/torch_extensions folder so that it can be used by other users in a multi-user environment. (#30095)
d5c136097a  # improve .view() performance (#30554)
a997f224ac  # Add torch.multiprocessing.create_processes
9740011f10  # Use normal dispatch to get to CUDA threshold kernels, instead of DispatchStub. (#30307)
d43e205026  # Properly include declaration of dispatch in file that registers it. (#30311)
8269f7b652  # Delete redundant THC_API on THCStorage_new (#30312)
a009fc14be  # Workaround hcc bug regarding extern "C" definitions (#30313)
1b12fd33ed  # Add missing trigramma_stub definition. (#30314)
1b5ce05924  # don't use size()/stride() functions in TensorImpl, use size_[d]/stride_[d] instead (#30452)
40146eb48e  # Skip ProcessGroupGlooAyncTest if there is no CUDA available (#30345)
4e6379379c  # fetch before checking out PR tip
4d4d8e0dce  # Update persons_of_interest.rst (#30647)
604a27361f  # remove tuple_parser (#30659)
2ba03e0287  # Enable test_trainer_ps in dist_autograd_test.py
56dd2836ec  # Make zeros argument of torch.where same dtype as other argument (#30661)
a376dd344c  # Added check for torch.where on CPU that both arguments have same dtype (#30662)
dcd1216efe  # Force early initialization of OpenMP in forked children (#29006)
59151d3e43  # autograd/profiler: support merging FunctionEventAvg (#30677)
3cf8382984  # detect_anomaly() for SparseTensors (#29803)
4ac614191a  # Remove exp10 in TH (unused)
76acf5b553  # Remove many unused bfloat16 functions in TH
ab834d5093  # Remove exp10 in TH (unused)
7e472679ff  # pin actions/checkout version
f5c9452beb  # Fix toObject() r-value version (#30713)
d12786b24f  # add __torch_function__ API override mechanism (#27064)
d6ca93b353  # add doc for F.softplus
ec7bb9de1c  # format tri[lu]_indices doc better
a68b790293  # fix ref to nonexistent torch.repeat
1189595875  # Fix Tensor.argsort -> torch.argsort documentation link
d0af07ca4c  # Fix capitalization inconsistency in optim.rst
6918f0ce86  # Move scalar_check for total_weight in NLLLoss functions to code from codegen. (#30665)
fa2aa245cf  # Simplify scalar_check of nll_loss. (#30669)
786de33832  # Move scalar_check logic from codegen to code in NLLLoss. (#30670)
289e9a07fd  # Move Tanh backward to Aten(CPU+CUDA) (#30224)
9d3402e4cb  # Add the __torch_function__ API override mechanism (#30730)
42e79d7e8a  # Kill THNN version of MultiMarginCriterion; it's not used anymore.
1f1ce53e8e  # Don't install pybind11 header directory for system pybind11 installs (#30758)
a939b52ddb  # fix AvgPool2d for 2^31-1 sized inputs, and get test_cuda_kernel_loop_… (#30771)
139aa51962  # Clean up non-C++14 code (#28443)
f2a2fec47c  # CUDA-strided-complex Binary and Unary Op support (#30295)
35a6997863  # Support 0-d tensors in CUDA MultiLabelMarginCriterion. (#30765)
ba1a9871cb  # Turn off scalar_check for is_target for MultiLabelMarginCriterion, which is handled correctly in code. (#30766)
473a044835  # Fix a CUDA memory leak in MultiLabelMarginCriterion error checking. (#30767)
50625798df  # Fix scalar check of MultiLabelMarginLoss. (#30768)
f12332eb51  # Move scalar_check from codegen to code in MultiLabelMarginCriterion. (#30770)
2607772959  # Turn off scalar_checks for SpatialDepthwiseConvolution and SpatialConvolutionMM. (#30789)
fa251cfd97  # Fully deprecate variadic inputs of checkpoint_sequential (#25985)
1578a28692  # Migrate max and min (binary) from TH to ATen. (#27185)
2171f91053  # reenable cuda_kernel_loop_overflow_large test (#30797)
9617d07bd5  # Wrap warning handler in a function to avoid siof (#30800)
be55874f2c  # style fixes to code analyzer (#30808)
c564d794ed  # Add ATen/native/ headers to torch target (#30835)
6486bdfb90  # Fix `os.register_at_fork` not defined on Windows (#30809)
f874230d33  # Vectorize smooth L1 loss backward function on CPU. (#30046)
2ced81f289  # Revert "Default to not build Caffe2 operators on Windows. (#29061)" (#30740)
0974dcc244  # Fix error checking of CUDA multi_margin_loss. (#30825)
e5bd7a7942  # we should have a config-based way to skip flaky tests (#29944)
82c3f4861f  # Move hardtanh activation to Aten(CPU, CUDA) (#30152)
4034aa7621  # make sure windows tests get triggered (#30836)
2011cc1e91  # Fix half->float case of softmax backward when inner_size is not 1 (#30838)
11b3065323  # Run method_tests on CUDA. (#30821)
1d7b40f1c4  # Fix reading `__cuda_array_interface__` without strides (#24947)
60714dfb64  # change index_select scalar_check to retain dimensionality of input. (#30790)
e5d571ae25  # Remove scalar_check from topk, move it to the THC implementation.
377131b0eb  # MultiMarginCriterion: fix scalar_check in the case where reduction == None. (#30826)
0051467118  # Update CITATION from Workshop paper to Conference paper (#30872)
44ff7b08d8  # Reduce intrusive_ptr incref/decref costs (#30709)
d6ddfab11f  # save linux build binary size to Scuba (#30832)
81e4739141  # Move QScheme ops to c10 (#30134)
223f46f5fa  # Fix flake8 warning (#30905)
baccd26df7  # update code analyzer script to handle splitted torch libraries (#30864)
c37de32b23  # Enable len(dataloader) for iterable dataset (#23587)
cd6167ff63  # Upgrade bazel to 1.2.0. (#30885)
8d35b6cec7  # embedding_bag make_bag_size optimization (#30701)
a26238da57  # Enable using `torch.autograd.profiler.record_function` as decorator (#30861)
f1bd8cc286  # Fix lint issues in dist_autograd_test.py (#30928)
4bb497b38e  # MultiheadAttention fixes
190dac13e3  # Use universal references and perfect forwarding in Loops.h. (#30466)
c75bc9067c  # MultiMarginCriterion: move scalar_check from codegen to code.
daef363b15  # Move Softshrink activation to Aten(CPU+CUDA) (#30229)
528fa737ba  # Custom op autograd tests (#30519)
45f0556ba0  # Proper print for one element tuple (#30853)
8a57362000  # Fix index out of bound error in Engine::ready_queue_size when called before start_threads
394d2f7037  # Fix the rendering of the doc of max. (#30779)
ed20937231  # Remove TensorImpl::maybe_zero_dim.
e3d40f857b  # Make nn.Module `forward()` type annotation more permissive (#31057)
5edfe9cb80  # add torch.square (#30719)
d113b22571  # kill PyTorch py2 circle jobs (#29353)
b7652a2f81  # remove py2 flake8 lint (#29357)
d02280b432  # move migration guide to appendix (#31068)
c72dd526a7  # kill py2 onnx builds
9a5fd2eb07  # Fix conflicts in CMAKE_GENERATOR and generator (#30971)
28ee309c9a  # disable onnx py3 gcc5 build (#31100)
d6d6075573  # Optimize LayerNorm with explicit vectorization using Vec256 (#29104)
9305f44854  # Remove BUILD_NAMEDTENSOR from codegen and .cu files (#31047)
3301794855  # Port ELU activation to Aten (#29275)
717274c001  # Add useful warnings for t.grad when it won't be populated for known reasons (#30531)
a929d312ac  # Add dill>=0.3.1 as testing dependency (#31121)
5b03ff0a09  # Update embedding renorm comment to reference fixed issue (#29140)
293a139d79  # add a warning for script classes (#31069)
945ce71b18  # Correctly handle scalar types, fix parse of numpy ints (#30486)
e5a550cd1d  # Fix Test CI by pinning hypothesis and correcting the import (#31137)
0414463007  # doc fix for max method: a warning about different behaviour on CPU and GPU (#31115)
efe683fb2a  # dynamicly quantized linear benchmarking
4f5a4be45f  # Add native/quantized to the list of header rewrites (#31151)
0db6c01301  # Re-enable python 2 builds (#31164)
159835e666  # Add types for the remaining optimizers. (#31130)
4ead2e8996  # Fix CircleCI behavior for non-leaf stack PRs (#31088)
66f2bba852  # Adding function to convert Module to channels last
066e3ed953  # Re-apply "[bert/RoBERTa] Optimize LayerNorm with explicit vectorization using Vec256" (#31127)
bee6344d4e  # remove / rewrite weak module tests (#31193)
f6c31f61c5  # Enabled roll for bool tensor (#31194)
84d6796658  # move AWS ECR gc jobs to circleci (#30996)
b7c148013f  # fix torch square_ benchmark runtime error (#31221)
1ef99cf0ab  # Intrusive_ptr implementation slower than shared_ptr (#30810)
36d17f4105  # abort nccl communicators before throwing operation timed out (#31128)
8fea7a49d6  # pinning hypothesis for windows
3587f769dc  # use propagate_names instead of propagate_names_for_reduction for cumsum and cumprod
9954739956  # Refactor test for unique and unique_consecutive and fix some bugs (#31211)
d7d07e7caf  # thrust is included in SortingKthValue.cu but never used
1ec989404c  # Kill some unnecessary function declarations.
927588df8e  # Switch default memory format of _like operators to Preserve
fde3d707ad  # Switch default memory format of to (and similar) operators to Preserve
c35cddb306  # Switch default memory format of clone operator to Preserve
6e1e09fd10  # Compile time type names (#26618)
2950530031  # caffe2::TypeMeta uses compile time type names (#26619)
7c1b5084a7  # Enable equality operator for bfloat16 CPU scalar types. (#30817)
7cb83bea3b  # Fix static cuda builds on older cmake versions (#30935)
ffe0c1ae4d  # Make test_torch.py pass cuda-memcheck (#29243)
9dc3d8738c  # fix view call on discontiguous tensor in to_sparse_backward (#31223)
60ec53c7fd  # Fix copy kernel speed regression introduced in #29631 (#31279)
0d7391f8b2  # Test cases for custom ops with autograd (#31003)
c95d46abbd  # Remove C++11 compatibility from c10::util::crc64_t (#30920)
9ca61aec0f  # Kill THLogAdd (#31217)
455e85a2f1  # Fix unflatten when dim is a negative integer (#31208)
d401ba1417  # benchmark binary ops in binary_test (#31326)
c6a8f884d8  # add copy_ operator the op bench (#31327)
229ce89b92  # Fix coverage and hypothesis conflict (#31320)
f9010d7648  # remove wipe cache from op bench (#31334)
49eff2f43c  # Kill THSize. (#31218)
d2067569e7  # Kill THTensor_(bhistc). (#31254)
dab5f72543  # we should have a config-based way to skip flaky tests (#30978)
0b8332efb4  # Remove c++11 examples from doc comments (#30925)
e33dea6e4e  # dynamicly quantized lstm benchmarking
e169e02836  # Refactor custom op tests (#31282)
74e59c6fed  # caffe2::TypeInfo fix when using clang-cl on Windows (#31364)
3694749cd1  # Detect dill version in torch.save/load (#30985)
58d2dd5b73  # Enabled flip for bool tensors (#31267)
913323750d  # CODEOWNERS for distributed optimizer. (#31403)
285cc13435  # check devices for all input tensors in index_put (#31280)
c63f8e5ebe  # Fix typo in data.rst docs
4d22c3ba01  # fix docker login, add docker image tag list after purge as html (#31328)
7cf8b9bada  # Move leaky_relu to Aten(CPU, CUDA) (#29899)
5e8bac24b4  # Migrate soft_margin_loss from the TH to Aten (CUDA+CPU) (#28135)
1e80ff7a67  # autograd/profiler: make record_function more threadsafe (#31346)
d2e66b44cc  # Temporary fix to support building pytorch from fbsource (for xplat dependencies) (#31393)
fe707c7849  # Use `default_observer` and `default_weight_observer` in tests (#31424)
489dd6cb90  # Add TORCH_DCHECK macro that checks only in debug builds (#31240)
540b9da41e  # Bump numba version in circleCI config to 0.46.0. (#31435)
49fe7a7401  # Updated documentation for NLLLoss to explain what x, y and w refer to (#31488)
779b128872  # add back in reference to jit_unsupported section (#31486)
9d9bc93bfb  # Added error message to indicate that reduction operations are not supported for dim>=64 (#31476)
8f3c0d541e  # Speed up `Tensor::has_names` for unnamed tensors (#31436)
e67064a96f  # Exclude generated source docs from Google (#31484)
dbe2f265d0  # Better error msg for autograd profiler + multi-worker dataloader crash (#31473)
624088e444  # Don't dispatch to cudnn if it is not possible to make it 32bit by splitting batch dim (#31383)
d0d6e0b5e3  # add type promotion support for sparse tensors (#30429)
b38901aa15  # Test reading `__cuda_array_interface__` inferred strides. (#31451)
6cd987e7c0  # Make fully_qualified_type_name_impl() compatible with VS2017 15.9 (#31455)
df9d5b8a77  # Use macros instead of directly accessing Python object fields (#31388)
3820d6f6b9  # make gc script python2 compatible (#31536)
0b0f90f53c  # Split on batch dimension when 32bit indexing not enough for convolution forward (#31379)
8d8e82883e  # set stream everytime when we get a cuBlas handle (#31537)
b5bbec7bad  # set stream everytime when we get a cuSparse handle (#31538)
700109eb63  # set stream everytime when we get a cuDNN handle (#31541)
cc2d5ca37f  # add enabled API to autograd profiler (#31380)
9459db86bf  # Raise warning for schedulers following chainable shedulers (#31125)
218cfd568d  # Conv transpose/backward split 32bit (#31510)
866c1b1fcc  # Ensure legacy sparse constructor/new doesn't interpret python data as tensor data. (#31490)
5d95a9ca79  # Print all broken ops instead of the first one (#31628)
ec4e347744  # Add Python language reference docs (#30686)
91eb7c26cd  # Fix Typos
909b8eba0d  # cudnn grouped convolution nhwc patch (#31444)
ffcac9ad37  # Clean White List for BC Checks (#31629)
204939b401  # Automatic update of fbcode/onnx to 57ebc587fcf3913b4be93653b0dd58c686447298 (#31642)
e84e7ec556  # Kill aten_custom_call.
ae214f67a5  # updated code to ensure error check for negative dims
3b7916fccd  # Modify the order of arguments position of torch.std and torch.std_mean in doc (#31677)
22d84204f7  # Expose torch.poisson in documentation (#31667)
ee87b01f40  # add additional types to indexing operations dispatch (#31692)
6064223808  # `@slowTest` some slow tests (#31706)
7a3ed36309  # Fix nvcc math functions for MSVC 2019 (#31704)
236b0a318c  # Delete ATen/stub (#31763)
9e9bfbfd8d  # Update old scheduler example usage (#31358)
7078f4b27d  # skip _test_optional_float in BC check (#31786)
d770fbc1d2  # Some modifications to improve readability (#31352)
ed5cd0d742  # Use numeric limits to define TensorTypeSet(FULL) representation (#31668)
5f8308e32d  # Pin Pillow to v6 as PILLOW_VERSION is removed in v7
155376721c  # Pin hypothesis package to 4.57.1 to avoid test failures
dc43f9dc54  # fix test_backward_node_failure flakiness (#31588)
fa0424f224  # add LLVM-dev package to android docker image (#31215)
95cb66570a  # Erase array sizes from types in c10::str(). (#31683)
f56c59ead6  # clarify when to use `as_tuple` in `torch.nonzero`
8c425dd201  # Fix race condition when creating build dir (#30956)
0b9cd410a9  # Fix cumsum error for tensors with zero elements (#31694)
68f3782106  # remove std_single and var_single code in TH (#31608)
b47e9b97a2  # Add op bitwise_and (#31104)
a02a5129a8  # Move rrelu to Aten(CPU) (#31094)
79e30ff3f8  # optimize index_select performance on CPU with TensorIterator (#30598)
b44c0f328e  # Skip same tests in ONNX Python3 CI as in Python2 (#31827)
6b1db202bc  # Add tanh to c10::cuda::compat (#31844)
28c9dd4436  # fix ProcessGroupGlooTest (#31255)
e5b7231edc  # Adding version check for hypothesis deadline
b0a2765103  # move docker image html to correct bucket (#31832)
3f0b330736  # corrected keyword argument name in docs for Tensor.scatter (#31617)
1f2b6d632a  # Refactor tests in pytorch's test/dist_autograd_test.py file (#31803)
c65305e991  # Add a check method for custom type tensor (#31290)
a9dae70bae  # Remove LibIRC logic from cmake. (#31152)
4ef9daf7b2  # Remove dead CAFFE2_LIBS variable (#31155)
58cffbff91  # Add missing TORCH_CUDA_API annotation to throw_nccl_error (#31157)
5d80f63478  # no_grad, enable_grad: support for decorating generator functions (#31792)
34561dadcd  # Don't handle bias inside cudnn_convolution* (#31524)
a561a8448b  # minor doc tweak to use mp.spawn in example (#30381)
20c5dd59bd  # Add stub for transformer.py and MultiheadAttention Class. (#28396)
22044c6f7c  # Use TORCH_CHECK instead of AT_ASSERT in torch::cuda::gather() (#27456)
2f5eefe525  # Raise ValueError if CUDA device is specified without specifying the : (#29087)
5cc62f2913  # Ensure autograd callbacks are called only once for reentrant backward. (#31909)
0e5a6700cc  # Emit warning from deprecated torch function signatures (#31514)
c888473b57  # Restructure docs organization and naming (#31849)
74d69e296e  # Raise an error if torch.cat is given `out` as one of the input tensors (#30577)
bb279c5c63  # named tensor max pooling support
ca72df06ae  # disable __torch_function__ overides for operators in torch.functional (#30839)
8a0503b355  # Run a non-quiet submodule update to prevent timeouts on Circle CI (#31900)
c21f89970f  # Remove c++14-conditional constexpr (#30916)
0dca9c30ca  # constexpr typeid improvements (#31312)
ab60cca488  # Make c10::util::get_fully_qualified_type_name() backwards compatible with clang 4 (#31351)
9116f02beb  # Rename TORCH_DCHECK to TORCH_INTERNAL_ASSERT_DEBUG_ONLY (#31917)
ee817012b2  # Add more tests to the autograd wrt view and inplace (#31147)
2a294aace6  # Remove memory ordering from LeftRight (#31026)
f67851d69a  # Fix c10::util::get_fully_qualified_type_name for MSVC (#31313)
f0072b3af5  # Remove C++11 compatibility from c10::optional (#30919)
c66ca74f03  # Add device debug info to CUDA build (#31929)
54777b1e73  # Avoid reference invalidation in cuda SpectralOps' plan_caches (#31861)
462bfc7fe7  # docker hub image info (#31923)
c299cb05ef  # temporary fix for jit test backward compatibility issues
1314f7f4f4  # Ensure the original grad_mode is restored during backward (#31884)
4f9d2f74e2  # Port softplus activation to Aten(CPU+CUDA) (#30504)
9ba6a768de  # Add op bitwise_or (#31559)
9a3cb1e859  # Move cauchy to Aten(CPU) (#31824)
dedd16b418  # remove THConv code which never be used (#31879)
8c59d48281  # Add doc previewing instructions (#31905)
09a22f3301  # Remove C++ docs contributing page (#31908)
5cc49ed45f  # Document `IValue` (#31904)
eb23171bce  # TensorIterator norm update (#31903)
0dbd5c0bfe  # Added torchvision tests as part of ORT tests (#31835)
8614860210  # Uniformly apply Windows logic in cpp_extensions everywhere (#31161)
5c423cae72  # Add precision tests for CUDA half linspace+logspace (#31962)
5a76335aaa  # Move lshift to Aten (#31566)
99b3f9cac4  # Move log_sigmoid to Aten(CPU) (#30958)
e59e5ba5a3  # Move geometric to Aten(CPU) (#31878)
26f552a3d1  # Javadoc changes (#31956)
bc68a8745f  # Spelling fix in transformer docs
cfdfdf70d7  # remove JSON dumping dependency (#30724)
67c1d930eb  # Lock graph_task before writing leaf_streams. (#31995)
2968faf154  # Update doc about output_differentiability keyword in derivatives.yaml
67ff051ddd  # Remove temporary fix for torchbind in BC check (#31982)
700d1c5cbc  # update CI script to take string docker image version (#31857)
4e84661139  # update llvmlite to 0.30.0 (#31858)
b6f43afaca  # Fix tensordot allowing negative dims (#31954)
c6f41ae01b  # Fix and add more padding mode support for Conv (#31784)
8098ae455c  # Move rshift to Aten (#31594)
03ff3eb94d  # skip TEST_DILL on Python2 (#32027)
16b8ca56b6  # update docker image version (#31848)
638e4ad8b9  # Updated function definition for torch.mode and torch.median in torch docs (#32003)
77c2c78e01  # Fix typographical error in torch.triu docstring (#32067)
e74a215ade  # Changed clip_grad_norm_ total_norm calculation (#32020)
4002fec509  # Display NVCC version in CI for convenience to look at
8e93159fb6  # CUDA 8 cleanup (#32013)
695c4f1bab  # Fix a typo in function name: liner -> linear
5988d36f58  # Fix cumprod error for tensors with zero elements (#32070)
d97413eb7a  # Change python/cpp docs CI to use a CPU-only image (#32102)
701ca68882  # Docs entry for the `is_quantized`
f003008d6e  # Allow TCPStore to pick a port to bind to. (#31674)
26621d101f  # remove simple .data from torch/nn
77c78b7d28  # remove .data from torch/nn doc
8d472bab6b  # Make torch.backends.mkldnn usable without import
5f1a881cb8  # Add private user tensor type IDs for experimentation. (#31830)
1487582ba7  # Switch important CI from CUDA 9 to 10.1 (#31951)
fa60e1150d  # Fix tensor^tensor derivative for 0 base entries
b783a75aa3  # Fix scalar^tensor derivative for scalars that are zero
0664c6bbfd  # Add ccls cache to gitignore (#31437)
b0ac425dc4  # Emit warning from deprecated torch function signatures (#32009)
2bb9dbeffa  # omit constexpr with nvcc on clang (#32149)
4a26bb9b18  # Suppress pip logs (#31912)
