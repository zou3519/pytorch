65f465050b  # Dont use SubgraphRewriter in FoldQuantizeCallIntoBuffer
1cc321deed  # Memoize parseIR calls in graph mode quantization
97fae401f0  # Use LinearPackedParams everywhere
328ec5460f  # refactor the observer removal and quantize tensor
59ca9b7430  # Graph-mode quantization for convolution from traced model (#30245)
7c4b9042ab  # Updates to quantization documentation (#30288)
c12f9a12a8  # Fix quantized ConvReLU3d test (#30266)
b8f50d9cc8  # Support to add dequant for each use of Value (#30145)
661a6c8ef2  # Add `get_qparams` and revert the changes to `calculate_qparams` (#30262)
eccf42fd15  # Bug fix: Handle missing keys in observer state dict during load (#30357)
05a1644ce3  # Fix BC for quantized linear
8bbafa0b32  # Add logical_and and logical_or (#28162)
2d6b2f39e9  # Fix docs so that the example works (#30120)
d0acc9c085  # Switch PyTorch/Caffe2 to C++14 (#30406)
3636cb0364  # windows build (#30556)
0bebfe2143  # Add the explicit per-tensor/per-channel quant info when we print the module (#30591)
89be1a22d4  # split getInvokedMethods (#30546)
08394cede3  # DEFINE_DISPATCH in the correct namespace. (#30308)
7023e13fbb  # Fix mapping white list (#30636)
19cd90d303  # Globally record observer nodes (#30547)
bb5dcaf24f  # Add logical_and and logical_or (#30521)
6e145b4614  # add irregular c10 op registration/invocation cases to test project (#30558)
f73cd28082  # InsertObservers for shared class types (#30548)
756f279d95  # Rename QuantizeHelper to InsertQuantDeQuantHelper (#30549)
3c1bb21cf5  # Invoke more passes in `insertObservers` (#30473)
c4c2e23385  # Supporting making submodules unique (#30037)
1d20c32bf1  # Make `InsertQuantDeQuantHelper` global (#30550)
bf1b4b6fef  # add torch_cpu to the static library list in TorchConfig.cmake.in (#30769)
a7406516d1  # Refactor bias and weight check and add aten::linear pattern (#30474)
1fa4908ac0  # Refactor test_quantization.py and enable `test_nested` (#30475)
f1755d9aea  # Insert GetAttr for quantization parameters instead of Constant (#30551)
58cdf1429c  # Add tests for quantizing traced models (#30476)
4ed2eae2d0  # Add registerQParams function (#30552)
37435d36ed  # Refactor VariableTypeManual (#30649)
6d06b925ba  # Remove `values_to_quantize_` (#30858)
4fd20c0816  # Kill hypothesis deadline testing (#30890)
5bf58274cc  # getQParams return a dictionary of qparams (#30859)
04b9324476  # Factor out getInvokedMethod in `InsertQuantDeQuantHelper` (#30860)
e05ee4c421  # Remove BUILD_NAMEDTENSOR macros (#30894)
27d7dba9ab  # Remove scalar_check specification and codegen. (#30874)
57f29a44c7  # Bug fix of the histogram observers (#30970)
cc319659e3  # qnnpack TanH
3a02ed822b  # Remove `insert_prepack_unpack` and `fold_prepack` for now (#30909)
a2463cbc38  # Adding quantized clamp kernel (#30541)
386cd59d44  # Remove redundant queries of qconfig in `insertObservers` (#31292)
a3cdb7eca3  # Fix default instantation of dynamic quantized LSTM
d6acc87c93  # Guard against copying from quantized Tensor to non-quantized Tensor (#29660)
226c2d79ce  # Get QScheme from observer module (#31293)
b4c48b7e29  # Call `getQSchemeAndQParamMap` later in `quantizeTensors` (#31406)
08de70cad1  # Remove observers in the end (#31407)
29f345831e  # Error out if legacy Tensor.new is called on alternate layouts / dtypes (#31485)
0ae063d5d9  # Fixed concatenation benchmark + added it to the microbenchmarking runs
40e720282c  # Using _floats_wrapper in per_channel_tensor generation (#31780)
5579611544  # Enable foldbn tests (#29220)
620060cb0c  # Quantized H Tangent function (#31031)
6d9a9e379d  # Fix segfault in caffe2 slice test (#31801)
6abfa9ad8a  # Quantized H Tangent function (#31031)
f995ec2076  # Remove qconfig_dict in top level eager mode quantization API (#31972)
