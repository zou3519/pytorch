328ec5460f  # refactor the observer removal and quantize tensor
59ca9b7430  # Graph-mode quantization for convolution from traced model (#30245)
7c4b9042ab  # Updates to quantization documentation (#30288)
c12f9a12a8  # Fix quantized ConvReLU3d test (#30266)
b8f50d9cc8  # Support to add dequant for each use of Value (#30145)
661a6c8ef2  # Add `get_qparams` and revert the changes to `calculate_qparams` (#30262)
eccf42fd15  # Bug fix: Handle missing keys in observer state dict during load (#30357)
05a1644ce3  # Fix BC for quantized linear
8bbafa0b32  # Add logical_and and logical_or (#28162)
2d6b2f39e9  # Fix docs so that the example works (#30120)
d0acc9c085  # Switch PyTorch/Caffe2 to C++14 (#30406)
3636cb0364  # windows build (#30556)
0bebfe2143  # Add the explicit per-tensor/per-channel quant info when we print the module (#30591)
89be1a22d4  # split getInvokedMethods (#30546)
08394cede3  # DEFINE_DISPATCH in the correct namespace. (#30308)
7023e13fbb  # Fix mapping white list (#30636)
19cd90d303  # Globally record observer nodes (#30547)
bb5dcaf24f  # Add logical_and and logical_or (#30521)
6e145b4614  # add irregular c10 op registration/invocation cases to test project (#30558)
f73cd28082  # InsertObservers for shared class types (#30548)
756f279d95  # Rename QuantizeHelper to InsertQuantDeQuantHelper (#30549)
3c1bb21cf5  # Invoke more passes in `insertObservers` (#30473)
c4c2e23385  # Supporting making submodules unique (#30037)
1d20c32bf1  # Make `InsertQuantDeQuantHelper` global (#30550)
bf1b4b6fef  # add torch_cpu to the static library list in TorchConfig.cmake.in (#30769)
a7406516d1  # Refactor bias and weight check and add aten::linear pattern (#30474)
1fa4908ac0  # Refactor test_quantization.py and enable `test_nested` (#30475)
f1755d9aea  # Insert GetAttr for quantization parameters instead of Constant (#30551)
58cdf1429c  # Add tests for quantizing traced models (#30476)
4ed2eae2d0  # Add registerQParams function (#30552)
37435d36ed  # Refactor VariableTypeManual (#30649)
6d06b925ba  # Remove `values_to_quantize_` (#30858)
4fd20c0816  # Kill hypothesis deadline testing (#30890)
5bf58274cc  # getQParams return a dictionary of qparams (#30859)
04b9324476  # Factor out getInvokedMethod in `InsertQuantDeQuantHelper` (#30860)
e05ee4c421  # Remove BUILD_NAMEDTENSOR macros (#30894)
27d7dba9ab  # Remove scalar_check specification and codegen. (#30874)
57f29a44c7  # Bug fix of the histogram observers (#30970)
cc319659e3  # qnnpack TanH
3a02ed822b  # Remove `insert_prepack_unpack` and `fold_prepack` for now (#30909)
a2463cbc38  # Adding quantized clamp kernel (#30541)
386cd59d44  # Remove redundant queries of qconfig in `insertObservers` (#31292)
a3cdb7eca3  # Fix default instantation of dynamic quantized LSTM
d6acc87c93  # Guard against copying from quantized Tensor to non-quantized Tensor (#29660)
226c2d79ce  # Get QScheme from observer module (#31293)
b4c48b7e29  # Call `getQSchemeAndQParamMap` later in `quantizeTensors` (#31406)
08de70cad1  # Remove observers in the end (#31407)
29f345831e  # Error out if legacy Tensor.new is called on alternate layouts / dtypes (#31485)
0ae063d5d9  # Fixed concatenation benchmark + added it to the microbenchmarking runs
40e720282c  # Using _floats_wrapper in per_channel_tensor generation (#31780)
5579611544  # Enable foldbn tests (#29220)
620060cb0c  # Quantized H Tangent function (#31031)
6d9a9e379d  # Fix segfault in caffe2 slice test (#31801)
6abfa9ad8a  # Quantized H Tangent function (#31031)
f995ec2076  # Remove qconfig_dict in top level eager mode quantization API (#31972)
7ad03855dc  # Fix 'template' keyword warning with clang-cl and clang.exe (#32104)
62b06b9fae  # Rename TensorTypeId to DispatchKey (#32154)
4314620ba0  # [jit] Module clone work with shared ClassType (#31970)
8c1268aad3  # Use default scale/zero_point in fake_quantize module instead of None (#32318)
e133d8be3b  # Fix ASAN / potential segfault in quantized Tensor memory allocations.
53429680d5  # Remove stray `@script` (#32235)
ecbf6f99e6  # Removed unused weight update in prepack. Moved zero point update to (#32254)
e1d97025ee  # QNNPACK: Add support for dynamic quantization.
44b270d892  # `insert_quant_dequant` pass support shared class types (#31408)
8e689378c7  # Move some of the helper functions for public use (#32202)
b7c6277c53  # Adding QConfigTypePtrMap (#32203)
583bb97618  # [quant][graphmode] Default to non-inplace in graph mode quantization API (#32204)
d234626267  # [quant][graphmode] Support quantizing shared ClassType with different qconfigs (#32205)
4cd6b5cda6  # [quant] Re-enable test_nested that has different qconfig for shared ClassType (#32206)
fe3eb09da5  # [quant] Re-enable fold_convbn in quantize_script (#32302)
d2bda53f6d  # [quant][graphmode] Call _jit_pass_dedup_module_ueses in quantize_script (#32303)
2bfd33b4ab  # [refactor] Adding FoldConvBatchNorm2dHelper (#32374)
91f10a1de1  # [quant][graphmode][refactor] Better API for fold_convbn (#32380)
169541871a  # Add operator support for dynamic quant on mobile (#32479)
12d5933969  # Bug fix of norm minimization for dev mode (#31462)
f6c46df856  # Adding native qconcat
812b1ad869  # [quantization] FP16 dynamic quantized Linear
9a2691f2fc  # Fix spelling errors
6f7d5bb3e1  # Temporarily disable the test_quantized_rnn test (#32742)
fc2ff7912f  # [quantization] Remove incorrect fp16 dynamic linear/relu op
c2d736cefb  # Add support for Dynamic LSTM quantization on Mobile (#32757)
94ddc2c462  # Resubmit more code fakefp16 mapping unification (#32798)
8ddd5bb0e9  # Don't serialize None values in observer (#32733)
fbe121e395  # Quantized sigmoid function
e2f1288514  # Add utils to inspect fp16/int8 packed weights (#32979)
acd51e13f7  # TorchScript add check if quantized
a23009f98f  # Quantized leaky relu
914610d079  # [pytorch][quant] Add assert for min, max, qmin, qmax for ChooseQuantizationParams (#32739)
d554b112e3  # Add histogram collection and weight prepacking utils (#33125)
d0435604a5  # [quant] Add a quantized batch_norm operator (#33080)
2e88d3d703  # [quant] Add Quantized BatchNorm2d module (#33109)
243cc20451  # Enable inplace relu fusion for training (#33105)
43e015f4b1  # Bug fix in dynamic quantization kernels + better test coverage. (#33320)
5e80ca12bb  # [pt][fbgemm] Turn on USE_FBGEMM on Windows env (#297)
bd3c6e8e91  # avoid large vector copy when query per_channel q_params (#31040)
ecb05f12c3  # Support broadcast for quantized mul kernel (#30442)
22963f42ec  # Delete unnecessary aliasAnalysis specification from operator registrations. (#33093)
941b42428a  # Mobile Backend: NHWC memory layout + XNNPACK integration. (#32509)
996c0adb53  # [quant] Regsiter fake_quant and observer attributes as buffers (#33626)
5b031d961d  # [pt][quant] RNN debug test (#33621)
479e474a37  # [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes (#32379)
98af01ee7c  # [quant] Make FakeQuant use REGISTER_DISPATCH (#33682)
bc5e9e0d55  # [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call (#32809)
cba8af9b24  # [pytorch] Set alias analysis kind to FROM_SCHEMA for qadd, qmul, qclamp, qconcat (#33359)
7caf3c396b  # [quant][graphmode][refactor] Change signature of getModuleAccessPath (#32812)
6aecfd1e80  # Mobile Backend: NHWC memory layout + XNNPACK integration. (#33722)
5ef1c2c5d2  # Back out "[pt][quant] RNN debug test" (#33750)
98526c7444  # Migrate fake_quant_slice to TensorIterator (#33744)
7eba36b1f6  # [quant][graphmode][refactor] Separate preprocess step for insertObserver (#32813)
a13ee18982  # [quant][graphmode] refactor nodeQuantizable (#33171)
8667379133  # [quant][graphmode][refactor] Factor out insertDequantCall (#33172)
038ee01393  # Disable printing of the histogram when dump (#33749)
a8e7ed48f4  # [pt][quant] Parallelize quantize and dequantize (#33765)
45e4b614d1  # Per channel quantization performance improvement (#33772)
c32fa465a5  # Preserve Backward compatibility of models serialized before #31040 (#33796)
fddf73250d  # [JIT] fix resolving of functions in torch/functional. fix compilation of torch.stft (#33504)
4c33222c51  # [quant][graphmode] Replicate dequantize nodes (#33531)
b10761d890  # fix type stub errors (#33762)
afbd04449e  # [quant][graphmode] Swap dequantize after inline for ops that doesn't require observation (#33173)
997b5b5797  # [quant][graphmode][refactor] Simplify signature for insertObserverFor (#33274)
97541a5106  # [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized (#33275)
7c13f576ea  # [quant][graphmode][refactor] Checks for bias and weight (#33273)
7f1112820a  # [quant][graphmode][refactor] Move check for weight outside of insertObserverFor (#33276)
f5f1e5e7f6  # [quant][graphmode][refactor] Factor out getInvokedMethod (#33649)
b98bce8cd4  # Add MemoryFormat to TensorOptions, but not codegen. (#33704)
5b9f1ada30  # [quant][graphmode] Observing input/output values in call site (#33277)
a8fc3d8c2a  # Fix HistogramObserver to not do detach on input (#34114)
e5bbd23ca7  # [quant][graphmode] Skip quantizing input and output in matched module (#32814)
6f52562e75  # [quant][graphmode] Add add_relu pattern in skip values (#32816)
e236e15934  # [quant] Run weight_post_process for QAT (#33852)
9651088228  # Tuck the packing logic into Int8FCPackWeight op (#34289)
ccf4d69b75  # [Lite Interpreter] Enable __setstate__ (#33294)
1cf12b7e53  # [quant] Fix histogram observer to work with QAT on GPU (#34232)
434af5d94a  # [quant] Speed up per-channel min-max observer (#34118)
5608ffc46c  # [PyTorch] Remove const modifiers from passed by value integers in qbatch_norm_fn (#34378)
b0479506a8  # Add the 3d avg pool for video related model (#33339)
65bad41cbe  # Fixed typos in quantization docs / docstrings (#34182)
8a17dc65af  # [quantization] Make FP16 RNN use new prepack op (#34339)
776d2a1e8f  # [quant][graphmode] Handling ops doesn't require observation in insertObservers (#33481)
2e7eef41ac  # [quant][graphmode] Swap quantized functional linear with aten::linear (#33853)
a09c4d3997  # [pt][quant] Vectorized qmul and more methods on qint data types (#34376)
2e88a78d2e  # add quantized_hardtanh (#34097)
f70945b1c3  # fix the quantized batchnorm2d (#34579)
514cba0661  # [JIT] remove builtin interpolate functions (#34514)
43c9cc7a9c  # add quantized ELU activation (#34267)
90ca7a1feb  # [quant][graphmode] Add Finalize function that inlines graph and produce quantized ops (#33927)
0ff4d37933  # [quant][graphmode] Add quantized conv2d-relu fusion pattern (#33279)
5d65b5cd01  # Add the 3d upsample quantized op for video model (#34594)
c9ed111894  # [caffe2][quantization] Add initializer and precision as read-only property to QueryTensorQparam (#34706)
af3a7e2b50  # [jit] small cleanups after script:: removal (#34677)
68758b2fa0  # Add the quantized batch_norm3d and also batch_norm3d fused with relu operators (#34702)
5710374e4e  # [reland][quant][graphmode] Add quantized conv2d-relu fusion pattern (#33279) (#34744)
7dd5da2026  # JIT pass to insert XNNPACK ops (#34048)
373c80ee90  # Fix missing header (#34762)
cec9758afa  # [quant][graphmode] Add quantization pattern for quantized::add_relu (#33532)
b336deb6ee  # [quant][mobile] Not use qnnpack max_pool2d if ceil_mode is true (#34844)
58c5b6d306  # adds quantized implementation of hard sigmoid (#34607)
841f7600bb  # [quant][graphmode] Quantization pattern for aten::linear (#33854)
1c8e086537  # [quant][graphmode][refactor] Change QParamMap to QParamVector (#34314)
