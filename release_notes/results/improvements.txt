99a2a0b1ca  # Implement torch.diagonal for named tensors (#30193)
d64e2581cc  # Add list of supported XCode/CUDA versions to README
3cf8382984  # detect_anomaly() for SparseTensors (#29803)
35a6997863  # Support 0-d tensors in CUDA MultiLabelMarginCriterion. (#30765)
1578a28692  # Migrate max and min (binary) from TH to ATen. (#27185)
c37de32b23  # Enable len(dataloader) for iterable dataset (#23587)
5edfe9cb80  # add torch.square (#30719)
717274c001  # Add useful warnings for t.grad when it won't be populated for known reasons (#30531)
159835e666  # Add types for the remaining optimizers. (#31130)
f6c31f61c5  # Enabled roll for bool tensor (#31194)
7c1b5084a7  # Enable equality operator for bfloat16 CPU scalar types. (#30817)
3694749cd1  # Detect dill version in torch.save/load (#30985)
58d2dd5b73  # Enabled flip for bool tensors (#31267)
9d9bc93bfb  # Added error message to indicate that reduction operations are not supported for dim>=64 (#31476)
dbe2f265d0  # Better error msg for autograd profiler + multi-worker dataloader crash (#31473)
d0d6e0b5e3  # add type promotion support for sparse tensors (#30429)
9459db86bf  # Raise warning for schedulers following chainable shedulers (#31125)
218cfd568d  # Conv transpose/backward split 32bit (#31510)
ee87b01f40  # add additional types to indexing operations dispatch (#31692)
5d80f63478  # no_grad, enable_grad: support for decorating generator functions (#31792)
20c5dd59bd  # Add stub for transformer.py and MultiheadAttention Class. (#28396)
22044c6f7c  # Use TORCH_CHECK instead of AT_ASSERT in torch::cuda::gather() (#27456)
bb279c5c63  # named tensor max pooling support
c6f41ae01b  # Fix and add more padding mode support for Conv (#31784)
8d472bab6b  # Make torch.backends.mkldnn usable without import
14548c2d5b  # out variant for native_batch_norm forward (#29192)
b543e3cd6f  # support empty batch in group normalization (#32401)
510a122d27  # add missing align_corners annotation (#32492)
9af5a97b1d  # Fix nll_loss to support empty tensors on GPU (#31491)
db02a4e4ce  # Support 3D attention mask in MultiheadAttention. (#31996)
3bbb36e02d  # Update linspace types (#32218)
ca9dc67094  # 0-dim batch size input for interpolate. (#32400)
c7bf4d22fe  # added exception args to the returned error message (#32693)
2e359ef86d  # enable empty batch for all flavor of convolutions (#32709)
29fabb1fbc  # make tests for empty inputs check zero parameter grads (#32820)
e87887ccb4  # Update type hints for torch.optim.optimizer.Optimizer (#32900)
c841ab403c  # add missing method annotations to torch.Tensor (#30576)
6996f8d880  # Add missing `default_collate` in dataloader.pyi
5ca7bf453d  # Tests for verifying behaviour of BatchNorm using 0-dim batch sizes. (#32384)
9d94f56ce0  # Backward operation of torch.eig for real eigenvalues (#33090)
a64d0ffe81  # Use int64 in pdist kernel to handle batches >= 46342 #30583 (#31593)
1487137c5b  # add missing default value for LRScheduler.step() (#32411)
0c93c2b142  # Add a warning sign for anomaly detection (#33176) (#33239)
bbdc5b7bd0  # Optimize error checking in mvlgamma (#32665)
879cf0b15a  # fix typing bug of LambdaLR.__init__ (#33271)
3ad59734d7  # Add type annotation for bias in _ConvNd (#32885)
c882425c24  # Add 64-bit indexing support to THC index reductions (#33405)
6a275b696e  # adding IterableDataset to utils.data.__init__ (#33543)
32c93099c4  # Add typing info for data members of utils.data.sampler classes (#33679)
3cf97bc23c  # Fix typing error of torch/nn/modules/container.pyi.in (#33686)
fd175fa8a2  # fix bugs in gen_pyi.py (#33748)
819ca2c285  # add bfloat16 conversion method in type stub (__init__.pyi) (#33747)
877ab3afe3  # Better handing of Autograd+Fork errors. (#33885)
d66c320b10  # disable leaky_relu_ backward calculation with negative slope (#33639)
c18cb1eb52  # Improve dll loading logic on Windows (#33856)
917e56e950  # Throw an error if nbytes is called on a sparse tensor. (#33897)
e568c039bd  # Enable Tensor.random_(from, to) for half on CPU (#34030)
31cc311143  # Expose `CUDACachingAllocator` `raw_alloc` and `raw_delete` to python (#33860)
e4a883e601  # cuDNN convolution try multiple algo (#33073)
ccf6fab65e  # Fix doc and type hints for "torch.add"; fix deprecated python calls in tests (#33935)
7e55494502  # Warns on read-only Numpy array->tensor conversion (#33615)
b1bd950a4d  # Fixed stub for AdamW (#34299)
3e6e2e9b7b  # Print the current Node name in anomaly mode (#33875)
ab2297dfe6  # Add Tensor overload for start in narrow. (#34317)
aedffdf7d8  # Support for Tensor Shape Type Hint (#34595)
b7129050e7  # Makes floor_divide a method, adds sparse floor division (#34552)
9e94e46453  # Check if rnn weights need to be flattened (#34265)
9c4683e8e3  # Revert D20312366: [pytorch][PR] Added type promotion logic for complex numbers
0d8447a9b8  # Warns when performing integer division with div and addcdiv (#34570)
c8f665dcb6  # Added type promotion logic for complex numbers (#34093)
3b7e1cd2cc  # Makes floor_divide a method, adds sparse floor division (#34552)
a1eaaea288  # Revert D20497453: [pytorch][PR] Makes floor_divide a method, adds sparse floor division
b10761d890  # fix type stub errors (#33762)
3671036ef3  # Adds true_divide function, analogous to Python 's, JAX's, NumPy's (true) division (#34236)
f48a8901c5  # Add floor_divide function (#30493)
