99a2a0b1ca  # Implement torch.diagonal for named tensors (#30193)
d64e2581cc  # Add list of supported XCode/CUDA versions to README
3cf8382984  # detect_anomaly() for SparseTensors (#29803)
35a6997863  # Support 0-d tensors in CUDA MultiLabelMarginCriterion. (#30765)
1578a28692  # Migrate max and min (binary) from TH to ATen. (#27185)
c37de32b23  # Enable len(dataloader) for iterable dataset (#23587)
5edfe9cb80  # add torch.square (#30719)
717274c001  # Add useful warnings for t.grad when it won't be populated for known reasons (#30531)
159835e666  # Add types for the remaining optimizers. (#31130)
f6c31f61c5  # Enabled roll for bool tensor (#31194)
7c1b5084a7  # Enable equality operator for bfloat16 CPU scalar types. (#30817)
3694749cd1  # Detect dill version in torch.save/load (#30985)
58d2dd5b73  # Enabled flip for bool tensors (#31267)
9d9bc93bfb  # Added error message to indicate that reduction operations are not supported for dim>=64 (#31476)
dbe2f265d0  # Better error msg for autograd profiler + multi-worker dataloader crash (#31473)
d0d6e0b5e3  # add type promotion support for sparse tensors (#30429)
9459db86bf  # Raise warning for schedulers following chainable shedulers (#31125)
218cfd568d  # Conv transpose/backward split 32bit (#31510)
ee87b01f40  # add additional types to indexing operations dispatch (#31692)
5d80f63478  # no_grad, enable_grad: support for decorating generator functions (#31792)
20c5dd59bd  # Add stub for transformer.py and MultiheadAttention Class. (#28396)
22044c6f7c  # Use TORCH_CHECK instead of AT_ASSERT in torch::cuda::gather() (#27456)
bb279c5c63  # named tensor max pooling support
c6f41ae01b  # Fix and add more padding mode support for Conv (#31784)
8d472bab6b  # Make torch.backends.mkldnn usable without import
14548c2d5b  # out variant for native_batch_norm forward (#29192)
b543e3cd6f  # support empty batch in group normalization (#32401)
510a122d27  # add missing align_corners annotation (#32492)
9af5a97b1d  # Fix nll_loss to support empty tensors on GPU (#31491)
db02a4e4ce  # Support 3D attention mask in MultiheadAttention. (#31996)
3bbb36e02d  # Update linspace types (#32218)
ca9dc67094  # 0-dim batch size input for interpolate. (#32400)
c7bf4d22fe  # added exception args to the returned error message (#32693)
2e359ef86d  # enable empty batch for all flavor of convolutions (#32709)
29fabb1fbc  # make tests for empty inputs check zero parameter grads (#32820)
e87887ccb4  # Update type hints for torch.optim.optimizer.Optimizer (#32900)
c841ab403c  # add missing method annotations to torch.Tensor (#30576)
6996f8d880  # Add missing `default_collate` in dataloader.pyi
5ca7bf453d  # Tests for verifying behaviour of BatchNorm using 0-dim batch sizes. (#32384)
9d94f56ce0  # Backward operation of torch.eig for real eigenvalues (#33090)
a64d0ffe81  # Use int64 in pdist kernel to handle batches >= 46342 #30583 (#31593)
