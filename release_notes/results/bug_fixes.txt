dcd1216efe  # Force early initialization of OpenMP in forked children (#29006)
a939b52ddb  # fix AvgPool2d for 2^31-1 sized inputs, and get test_cuda_kernel_loop_â€¦ (#30771)
473a044835  # Fix a CUDA memory leak in MultiLabelMarginCriterion error checking. (#30767)
9617d07bd5  # Wrap warning handler in a function to avoid siof (#30800)
6486bdfb90  # Fix `os.register_at_fork` not defined on Windows (#30809)
0974dcc244  # Fix error checking of CUDA multi_margin_loss. (#30825)
2011cc1e91  # Fix half->float case of softmax backward when inner_size is not 1 (#30838)
1d7b40f1c4  # Fix reading `__cuda_array_interface__` without strides (#24947)
e3d40f857b  # Make nn.Module `forward()` type annotation more permissive (#31057)
945ce71b18  # Correctly handle scalar types, fix parse of numpy ints (#30486)
9954739956  # Refactor test for unique and unique_consecutive and fix some bugs (#31211)
9dc3d8738c  # fix view call on discontiguous tensor in to_sparse_backward (#31223)
455e85a2f1  # Fix unflatten when dim is a negative integer (#31208)
74e59c6fed  # caffe2::TypeInfo fix when using clang-cl on Windows (#31364)
285cc13435  # check devices for all input tensors in index_put (#31280)
624088e444  # Don't dispatch to cudnn if it is not possible to make it 32bit by splitting batch dim (#31383)
0b0f90f53c  # Split on batch dimension when 32bit indexing not enough for convolution forward (#31379)
8d8e82883e  # set stream everytime when we get a cuBlas handle (#31537)
b5bbec7bad  # set stream everytime when we get a cuSparse handle (#31538)
700109eb63  # set stream everytime when we get a cuDNN handle (#31541)
866c1b1fcc  # Ensure legacy sparse constructor/new doesn't interpret python data as tensor data. (#31490)
ae214f67a5  # updated code to ensure error check for negative dims
7a3ed36309  # Fix nvcc math functions for MSVC 2019 (#31704)
0b9cd410a9  # Fix cumsum error for tensors with zero elements (#31694)
2f5eefe525  # Raise ValueError if CUDA device is specified without specifying the : (#29087)
5cc62f2913  # Ensure autograd callbacks are called only once for reentrant backward. (#31909)
74d69e296e  # Raise an error if torch.cat is given `out` as one of the input tensors (#30577)
f67851d69a  # Fix c10::util::get_fully_qualified_type_name for MSVC (#31313)
54777b1e73  # Avoid reference invalidation in cuda SpectralOps' plan_caches (#31861)
1314f7f4f4  # Ensure the original grad_mode is restored during backward (#31884)
67c1d930eb  # Lock graph_task before writing leaf_streams. (#31995)
b6f43afaca  # Fix tensordot allowing negative dims (#31954)
5988d36f58  # Fix cumprod error for tensors with zero elements (#32070)
fa60e1150d  # Fix tensor^tensor derivative for 0 base entries
b783a75aa3  # Fix scalar^tensor derivative for scalars that are zero
8746f90cf6  # Fix weight backward for cudnn conv of large tensor (#31889)
a2641e6005  # Make type of `Tensor.type()` more specific (#32353)
e37a24b044  # Always return a new tensor from nn.functional.pad (#32350)
4bdfc71421  # Fix race condition for to() backward that spans devices (#31930)
02aa3ba331  # Raise error for code that risk deadlock (#32295)
9e59244b53  # fix view listing in autograd codegen (#32044)
320d1a1573  # Fix wrong typing (torch/nn/parameter.pyi) (#32617)
602394e996  # verify input sizes for instance norm and group norm (#29082)
e36cbb8f2f  # Fixes moving after weight norm application (#32563)
8e4161517e  # div_kernel: throw when dividing by integer zero (#32629)
594cadeb8f  # Make sure temporary vectors are properly initialized in avx2 code (#32722)
8bc889e502  # Fix crash of SobolEngine if default tensor type is cuda (#32496)
b565d9b356  # Logspace fixes (#32744)
413c0f6c29  # Fixes moving after weight norm application (#32563)
0f0972051a  # Cudnn bn size fix (#32763)
7b65acdf9e  # Solves Issue #32750 - torch.prod now works fine with FP16 Input Tensor and FP32 Output Tensor (#32831)
d03c9aaa05  # Fix upsampling test case on ppc (#32786)
29e6f13cd1  # Enable MKL on MacOS if installed (#32905)
7101f6b5c0  # Properly handle NaN in binary max and min (#32541)
7cddc302e5  # min, max: check that operand and outputs are on the same device type (#32862)
df71b3e23a  # properly update _flat_weights in RNN modules (#32939)
7ea6559658  # Add size checks to `torch.stack` (#32931)
fbde3c05b6  # [aten] fix vector memory leak (#32478)
e8581869f2  # Properly update _flat_weights in RNN models (#32989)
3c17cbb6c8  # fix #30480 torch.normal shape checking is broken (#32243)
3e8d813263  # Add more checks to custom Function (#33069)
e7f0b15473  # Remove return value for __exit__ (#32997)
