dcd1216efe  # Force early initialization of OpenMP in forked children (#29006)
a939b52ddb  # fix AvgPool2d for 2^31-1 sized inputs, and get test_cuda_kernel_loop_â€¦ (#30771)
473a044835  # Fix a CUDA memory leak in MultiLabelMarginCriterion error checking. (#30767)
9617d07bd5  # Wrap warning handler in a function to avoid siof (#30800)
6486bdfb90  # Fix `os.register_at_fork` not defined on Windows (#30809)
0974dcc244  # Fix error checking of CUDA multi_margin_loss. (#30825)
2011cc1e91  # Fix half->float case of softmax backward when inner_size is not 1 (#30838)
1d7b40f1c4  # Fix reading `__cuda_array_interface__` without strides (#24947)
e3d40f857b  # Make nn.Module `forward()` type annotation more permissive (#31057)
945ce71b18  # Correctly handle scalar types, fix parse of numpy ints (#30486)
9954739956  # Refactor test for unique and unique_consecutive and fix some bugs (#31211)
9dc3d8738c  # fix view call on discontiguous tensor in to_sparse_backward (#31223)
455e85a2f1  # Fix unflatten when dim is a negative integer (#31208)
74e59c6fed  # caffe2::TypeInfo fix when using clang-cl on Windows (#31364)
285cc13435  # check devices for all input tensors in index_put (#31280)
624088e444  # Don't dispatch to cudnn if it is not possible to make it 32bit by splitting batch dim (#31383)
0b0f90f53c  # Split on batch dimension when 32bit indexing not enough for convolution forward (#31379)
8d8e82883e  # set stream everytime when we get a cuBlas handle (#31537)
b5bbec7bad  # set stream everytime when we get a cuSparse handle (#31538)
700109eb63  # set stream everytime when we get a cuDNN handle (#31541)
866c1b1fcc  # Ensure legacy sparse constructor/new doesn't interpret python data as tensor data. (#31490)
ae214f67a5  # updated code to ensure error check for negative dims
7a3ed36309  # Fix nvcc math functions for MSVC 2019 (#31704)
0b9cd410a9  # Fix cumsum error for tensors with zero elements (#31694)
2f5eefe525  # Raise ValueError if CUDA device is specified without specifying the : (#29087)
5cc62f2913  # Ensure autograd callbacks are called only once for reentrant backward. (#31909)
74d69e296e  # Raise an error if torch.cat is given `out` as one of the input tensors (#30577)
f67851d69a  # Fix c10::util::get_fully_qualified_type_name for MSVC (#31313)
54777b1e73  # Avoid reference invalidation in cuda SpectralOps' plan_caches (#31861)
1314f7f4f4  # Ensure the original grad_mode is restored during backward (#31884)
67c1d930eb  # Lock graph_task before writing leaf_streams. (#31995)
b6f43afaca  # Fix tensordot allowing negative dims (#31954)
5988d36f58  # Fix cumprod error for tensors with zero elements (#32070)
fa60e1150d  # Fix tensor^tensor derivative for 0 base entries
b783a75aa3  # Fix scalar^tensor derivative for scalars that are zero
8746f90cf6  # Fix weight backward for cudnn conv of large tensor (#31889)
a2641e6005  # Make type of `Tensor.type()` more specific (#32353)
e37a24b044  # Always return a new tensor from nn.functional.pad (#32350)
4bdfc71421  # Fix race condition for to() backward that spans devices (#31930)
02aa3ba331  # Raise error for code that risk deadlock (#32295)
9e59244b53  # fix view listing in autograd codegen (#32044)
320d1a1573  # Fix wrong typing (torch/nn/parameter.pyi) (#32617)
602394e996  # verify input sizes for instance norm and group norm (#29082)
e36cbb8f2f  # Fixes moving after weight norm application (#32563)
8e4161517e  # div_kernel: throw when dividing by integer zero (#32629)
594cadeb8f  # Make sure temporary vectors are properly initialized in avx2 code (#32722)
8bc889e502  # Fix crash of SobolEngine if default tensor type is cuda (#32496)
b565d9b356  # Logspace fixes (#32744)
413c0f6c29  # Fixes moving after weight norm application (#32563)
0f0972051a  # Cudnn bn size fix (#32763)
7b65acdf9e  # Solves Issue #32750 - torch.prod now works fine with FP16 Input Tensor and FP32 Output Tensor (#32831)
d03c9aaa05  # Fix upsampling test case on ppc (#32786)
29e6f13cd1  # Enable MKL on MacOS if installed (#32905)
7101f6b5c0  # Properly handle NaN in binary max and min (#32541)
7cddc302e5  # min, max: check that operand and outputs are on the same device type (#32862)
df71b3e23a  # properly update _flat_weights in RNN modules (#32939)
7ea6559658  # Add size checks to `torch.stack` (#32931)
fbde3c05b6  # [aten] fix vector memory leak (#32478)
e8581869f2  # Properly update _flat_weights in RNN models (#32989)
3c17cbb6c8  # fix #30480 torch.normal shape checking is broken (#32243)
3e8d813263  # Add more checks to custom Function (#33069)
e7f0b15473  # Remove return value for __exit__ (#32997)
05281a5671  # Add nice error message if missing overrides in custom autograd.Function
323b0e0a0f  # fix #30480 torch.normal shape checking is broken (#32243) (#33050)
e45343fa14  # TORCH_INTERNAL_ASSERT_DEBUG_ONLY not eating message string (#33251)
cb4e6d025a  # Updates numpy to tensor negative stride error message (#33254)
495bd5818b  # Fix index truncation in argmin/max for large tensors (#33310)
cfb4862673  # [pytorch] correct input size check for GroupNorm (#33008)
2c99ea8654  # Dirac init compatibility with group convolutions (#32825)
a67691e508  # Fix isnan for integral types in MSVC (#33483)
60339a38ed  # Fixes #33001 (#33456)
cdf381c967  # Fix LambdaLR scheduler side effects (#32848)
d19a50bf27  # Add missing weight_decay parameter validation for Adam and AdamW (#33126)
1a25747342  # Check for consistent devices in at::where (#33432)
e2a9ea0f72  # Ensure that lambda is no less than zero in softshrink (#33201)
8291e06f8f  # Fixes cuda->numpy and non-strided->numpy segfaults (#33612)
641750e33c  # Fix NaN handling in torch.mv. (#31666)
54e41a87eb  # Make ELU great again (#33244)
7a8b6c2c6b  # [pytorch] blas gemm fix for k=0 (#33419)
4ef854b4b4  # Fix potential hang when exiting main process (#33721)
a836c4ca78  # Skip manual backward for `cdist` with case `p=2` (#31167)
f597ac6efc  # Fix grid_sample gradients at image borders (#32829)
84101f353e  # Avoid problematic pickle usages on Python 3.8.0 and 3.8.1 (#33824)
a500491cbc  # Fix index_put when tensor length > int_max (#33753)
15bf4892f2  # prevent crash on exit from static destructor race (#33955)
f29110fdf8  # [pytorch] blas gemm fix for k=0 (#33819)
67608cc018  # Fix MKLDNN conv2d 5d weight handling (#34115)
2af64ba3ed  # Allow output to zero-strided tensors if the size is <= 1 along that dim (#34100)
9c5578fd0a  # Make sure Vec256 int32_t and int16_t loadu temprary arrays are properly initialized (#34281)
c6ea71b6e8  # Fix Conv.cpp, &&= is not a C++ operator (#34381)
79d47c1c5f  # Fix the missing ';' in Conv.cpp (#34448)
2b45368e50  # Fix cudnn 64bit indexing issue (#34407)
f5ee46f1cf  # Remove custom function in no_grad block error message (#33896)
c7dd5f89a2  # Fix #33562 (uncaught domain_error on macOS) (#34301)
be3bc1deb1  # convert counter back to list #33229 (#33356)
a22008f91e  # Prohibit copying autograd engines (#34567)
962e362427  # Fix _cat operator (#34591)
157d2d7825  # Fix version check for grad_fn for views (#34145)
cb06cb7b9f  # Remove hotpatches that circumvent MAGMA bug (#34357)
8e8a37d746  # Fix bug in baddbmm corner case (#33467) (#33538)
c258e4732a  # solve conv3d backward get incorrect result problem (#34358)
3ab30753e9  # Make autogen functions correct for multiple outputs and views (#31990)
5fd037ce44  # Fix MagmaInitializesCorrectly_CUDA by using an invertible matrix (#32547)
db8ce7ea2d  # Back out "Make autogen functions correct for multiple outputs and views" (#32681)
3655975565  # Add allow_rebase_history flag and fix codegen functions for multiple views (#32790)
