* add __torch_function__ API override mechanism (#27064) ([#27064](https://github.com/pytorch/pytorch/pull/27064)).
* Add the __torch_function__ API override mechanism (#30730) ([#30730](https://github.com/pytorch/pytorch/pull/30730)).
* Add op bitwise_and (#31104) ([#31104](https://github.com/pytorch/pytorch/pull/31104)).
* disable __torch_function__ overides for operators in torch.functional (#30839) ([#30839](https://github.com/pytorch/pytorch/pull/30839)).
* Add cummax ([#32169](https://github.com/pytorch/pytorch/pull/32169)).
* reimplement __torch_function__ overrides for torch.functional using inline logic (#32194) ([#32194](https://github.com/pytorch/pytorch/pull/32194)).
* Added cummin ([#32238](https://github.com/pytorch/pytorch/pull/32238)).
* Custom RNG DispatchKey (#32325) ([#32325](https://github.com/pytorch/pytorch/pull/32325)).
* Cummax and Cummin doc update and performance benchmark (#32537) ([#32537](https://github.com/pytorch/pytorch/pull/32537)).
* [Documentation] cummin doc fix (#33492) ([#33492](https://github.com/pytorch/pytorch/pull/33492)).
* __torch_function__ overrides for torch.functional and torch.nn.functional (#32799) ([#32799](https://github.com/pytorch/pytorch/pull/32799)).
* [WIP] Reanimate gradient scaling API with original scale update heuristic (#33366) ([#33366](https://github.com/pytorch/pytorch/pull/33366)).
* Formatting changes for gradient scaling (#33832) ([#33832](https://github.com/pytorch/pytorch/pull/33832)).
* Add API for listing functions overridable by __torch_function__ (#33791) ([#33791](https://github.com/pytorch/pytorch/pull/33791)).
* PCA and SVD for low-rank matrices, LOBPCG for positive-defined generalized eigenvalue problem (#29488) ([#29488](https://github.com/pytorch/pytorch/pull/29488)).
* Redo of "Add API for listing functions overridable by __torch_function__" (#34240) ([#34240](https://github.com/pytorch/pytorch/pull/34240)).
* PCA and SVD for low-rank matrices, LOBPCG for positive-defined generalized eigenvalue problem - copy (#34721) ([#34721](https://github.com/pytorch/pytorch/pull/34721)).
* add hardsigmoid FP operator to PyTorch (#34545) ([#34545](https://github.com/pytorch/pytorch/pull/34545)).
* Add types argument to __torch_function__ (#34303) ([#34303](https://github.com/pytorch/pytorch/pull/34303)).
* functional autograd api (#34066) ([#34066](https://github.com/pytorch/pytorch/pull/34066)).
* Revert D20346700: [pytorch][PR] Eager autocasting, out-of-place ops only ([#None](https://github.com/pytorch/pytorch/pull/None)).
* Eager autocasting, out-of-place ops only (#32140) ([#32140](https://github.com/pytorch/pytorch/pull/32140)).
* Add logical_and and logical_or (#28162) ([#28162](https://github.com/pytorch/pytorch/pull/28162)).
* Add logical_and and logical_or (#30521) ([#30521](https://github.com/pytorch/pytorch/pull/30521)).
* Gradient scaling API (#26512) ([#26512](https://github.com/pytorch/pytorch/pull/26512)).
