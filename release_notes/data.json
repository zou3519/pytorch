{"35e6c1763e": {"title": "Switch Docker image onda-cuda-cxx11-ubuntu1604 to new uniform name (#29943)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29943\n\nThis was apparently the same as \"pytorch/pytorch-binary-docker-image-ubuntu16.04:latest\",\nso standardize on that name.\n\nTest Plan:\nThis PR, which is stacked on top of a commit that puts one of the jobs\nusing that container into the set of PR builds.\n\nImported from OSS\n\nDifferential Revision: D18653554\n\nfbshipit-source-id: 40e6c52db02265d61e8166bb1211376faccfc53a", "pr_number": "29943", "files_changed": [".circleci/cimodel/data/binary_build_definitions.py", ".circleci/config.yml"], "labels": []}, "0c18de2623": {"title": "Add inferBoundShapeOp", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30101\n\nReviewed By: ipiszy\n\nDifferential Revision: D18387803\n\nfbshipit-source-id: 5edb6b949257370b62fa6da477bd6ed2f16a9bd1", "pr_number": "30101", "files_changed": ["caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h", "caffe2/proto/metanet.proto", "caffe2/proto/predictor_consts.proto"], "labels": ["fb-exported"]}, "90cb1e67ff": {"title": "Fix exception message in Java Tensor", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30205\n\nTest Plan: Imported from OSS\n\nReviewed By: linbinyu\n\nDifferential Revision: D18653568\n\nPulled By: dreiss\n\nfbshipit-source-id: a5fcb809eba641a7fbd0e99e835eceeb248e680c", "pr_number": "30205", "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/Tensor.java"], "labels": []}, "8c6f0c0587": {"title": "Detect TorchScript archives in torch.load (#29339)", "body": "Summary:\nThis PR looks for a `constants.pkl` file at the top level in a zip file\nin `torch.load`. If found, it calls `torch.jit.load` instead and issues\na warning to call `torch.jit.load` directly\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29339\n\nDifferential Revision: D18611095\n\nPulled By: driazati\n\nfbshipit-source-id: f070a02f6b5509054fc3876b3e8356bbbcc183e1", "pr_number": "29339", "files_changed": ["caffe2/serialize/inline_container.cc", "caffe2/serialize/inline_container.h", "test/test_jit.py", "torch/csrc/jit/init.cpp", "torch/serialization.py"], "labels": ["jit"]}, "2e709763a3": {"title": "add wrapper to exclude XLA when running device tests", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30316\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18659286\n\nPulled By: nairbv\n\nfbshipit-source-id: 86d035bb0c54c612868590c3188cfcd969c3f686", "pr_number": "30316", "files_changed": ["test/common_device_type.py"], "labels": []}, "99a2a0b1ca": {"title": "Implement torch.diagonal for named tensors (#30193)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30193\n\nFeaturing:\n- Added a NoNamesGuard::reset() function that sets NamesMode back to\nwhat it was before the guard. This makes it so that we don't have to\ncreate a new context to run code in an unnamed way.\n- Added a diagonal(Tensor, *, Dimname outdim, Dimname dim1, Dimname dim2, int64_t offset=0)\noverload. All of the non-tensor arguments are keyword only for\nreadability purposes; something like `tensor.diagonal(\"A\", \"B\", \"C\")`\nwould be really confusing.\n\nTest Plan: - Added new tests\n\nDifferential Revision: D18638363\n\nPulled By: zou3519\n\nfbshipit-source-id: ea37b52a19535f84a69be38e95e569e88f307381", "pr_number": "30193", "files_changed": ["aten/src/ATen/NamedTensorUtils.cpp", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/core/NamedTensor.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_namedtensor.py"], "labels": []}, "6c9b188262": {"title": "Support in-place update in IndexHashOp (#30275)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30275\n\n`IndexHash` did not support in-place update.\n\nReviewed By: kennyhorror\n\nDifferential Revision: D18612231\n\nfbshipit-source-id: adeccdf1ceb6107454555ff9cdf66fd5e5773f2a", "pr_number": "30275", "files_changed": ["caffe2/operators/index_hash_ops.cc", "caffe2/python/operator_test/index_hash_ops_test.py"], "labels": ["fb-exported"]}, "9fb879934e": {"title": "Revert D18641413: add unit tests to iOS CI jobs", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18641413\n\nOriginal commit changeset: 12942206f1de\n\nfbshipit-source-id: 4fa76d50fb897db4342d10a4e46a9887e37ef233", "pr_number": null, "files_changed": [".circleci/config.yml", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-nightly-ios-binary-builds.yml", ".circleci/verbatim-sources/workflows-pytorch-ios-builds.yml", "ios/TestApp/README.md", "ios/TestApp/TestApp.xcodeproj/project.pbxproj", "ios/TestApp/TestAppTests/TestAppTests.mm", "ios/TestApp/benchmark/setup.rb"], "labels": []}, "ac103a5d78": {"title": "Remove variable wrapping from register_c10_ops (#29207)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29207\n\nThe logic calling c10 ops from JIT did some variable wrapping to make sure all results are always variables.\nThanks to ezyang, this is not needed anymore because everything is a variable now.\nghstack-source-id: 93345590\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18327507\n\nfbshipit-source-id: 86512c5e19d6972d70f125feae172461c25e3cb6", "pr_number": "29207", "files_changed": ["torch/csrc/jit/register_c10_ops.cpp"], "labels": ["jit"]}, "c7f988b8c6": {"title": "transport open registration (#30167)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30167\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29164\n\n- Created GlooDeviceFactory to hide device creation details\n- Added transport option while on Python interface\n\nThe reason of making the factory class is to make it easier to extend gloo transport in the future\n\nTest Plan: Imported from OSS\n\nReviewed By: satgera, d4l3k\n\nDifferential Revision: D18596527\n\nfbshipit-source-id: e8114162ee8d841c0e0769315b48356b37d6ca0a", "pr_number": "30167", "files_changed": ["torch/lib/c10d/CMakeLists.txt", "torch/lib/c10d/GlooDeviceFactory.cpp", "torch/lib/c10d/GlooDeviceFactory.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["fb-exported"]}, "6a00191fc2": {"title": "Add RpcAgent::getWorkerInfos() (#30241)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30241\n\nWe need an API to get all worker infos. This will be used by backend-agnostic `rpc.wait_all_workers()` API.\nghstack-source-id: 94454935\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork -- test_get_worker_infos\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_get_worker_infos\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_get_worker_infos\n\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_get_worker_infos\n```\n\nDifferential Revision: D5693412\n\nfbshipit-source-id: 5123c8248b6d44fd36b8a5f381dbabb2660e6f0f", "pr_number": "30241", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h"], "labels": []}, "328ec5460f": {"title": "refactor the observer removal and quantize tensor", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30360\n\nDifferential Revision: D18670373\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: 1481d6e4d5ce40376577b8deb0a0f74d5559076e", "pr_number": "30360", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "ee20e66c48": {"title": "replace the SLSRQ for their right emulations in the replayer test (#30367)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30367\n\nuse the SLS emulations that match the hardware\n\nTest Plan: replayer test\n\nDifferential Revision: D18667605\n\nfbshipit-source-id: 89aee630184737b86ecfb09717437e5c7473e42c", "pr_number": "30367", "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc"], "labels": ["fb-exported"]}, "2a7a39c1af": {"title": "(de)serialization of values between C++ and Python (#30108)", "body": "Summary:\nThis PR updates `torch::pickle_save` to use the new zipfile format introduced in #29232 and adds `torch::pickle_load` which can decode the zipfile format. Now that `torch.save/load` use this format as well (if the `_use_new_zipfile_serialization` flag is `True`), raw values saved in Python can be loaded in C++ and vice versa.\n\nFixes #20356\n](https://our.intern.facebook.com/intern/diff/18607087/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30108\n\nPulled By: driazati\n\nDifferential Revision: D18607087\n\nfbshipit-source-id: 067cdd5b1cf9c30ddc7e2e5021a8cceee62d8a14", "pr_number": "30108", "files_changed": ["test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests_setup.py", "torch/csrc/api/include/torch/serialize.h", "torch/csrc/api/src/serialize.cpp", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export.h", "torch/csrc/jit/import.cpp", "torch/csrc/jit/import.h", "torch/csrc/jit/pickle.cpp", "torch/csrc/jit/pickle.h", "torch/csrc/jit/pickler.cpp", "torch/serialization.py"], "labels": ["jit"]}, "59ca9b7430": {"title": "Graph-mode quantization for convolution from traced model (#30245)", "body": "Summary:\nIn the PR, we enhance the graph-mode quantization for aten::_convolution, which could be generated from tracing path.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30245\n\nDifferential Revision: D18671597\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: 78a2470fbb0fe0def55d63c6bda7cbb5c89f7848", "pr_number": "30245", "files_changed": ["test/test_quantization.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "7570b2798a": {"title": "updating citation (#30267)", "body": "Summary:\nNIPS -> NeurIPS\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30267\n\nDifferential Revision: D18672928\n\nPulled By: soumith\n\nfbshipit-source-id: c20f26a0547f94ff39f8ee40e5f0ccc5fcc814af", "pr_number": "30267", "files_changed": ["CITATION"], "labels": []}, "7c4b9042ab": {"title": "Updates to quantization documentation (#30288)", "body": "Summary:\nThis pull request includes fixes for six quantization doc bugs.\n\nhttps://github.com/pytorch/pytorch/issues/30283 - Rendering issue on QConfig\nhttps://github.com/pytorch/pytorch/issues/26305 - Minor doc issue on fuse_modules()\nhttps://github.com/pytorch/pytorch/issues/27451 - Issues with ConvReLU2d, ConvReLU3d, and LinearReLU doc issues\nhttps://github.com/pytorch/pytorch/issues/26899 - Missing docstrings in torch.nn.intrinsic fused functions\nhttps://github.com/pytorch/pytorch/issues/29735 - add discussion of QNNPack to quantization doc page\nhttps://github.com/pytorch/pytorch/issues/27938 - some of the quantized functions lack documentation\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30288\n\nDifferential Revision: D18653368\n\nPulled By: gottbrath\n\nfbshipit-source-id: 410b3dd81ff10909a7f1a7736ca42d7cabf0beb1", "pr_number": "30288", "files_changed": ["docs/source/quantization.rst", "torch/nn/intrinsic/modules/fused.py", "torch/nn/intrinsic/quantized/modules/conv_relu.py", "torch/quantization/fuse_modules.py", "torch/quantization/qconfig.py"], "labels": ["module: docs", "quantization"]}, "4aa692fc91": {"title": "Convert KernelTable to a flat-indexed array rather than a hashtable. (#30332)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30332\n\n-\nghstack-source-id: 94481315\n\nReviewed By: resistor\n\nDifferential Revision: D18660421\n\nfbshipit-source-id: 9f11434f1c3c234c45f586719182053fa81731f0", "pr_number": "30332", "files_changed": ["aten/src/ATen/core/dispatch/DispatchTable.h"], "labels": []}, "7b5045be9d": {"title": "Remove LeftRight from OperatorEntry and DispatchTable. (#30333)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30333\n\nre-export of https://github.com/pytorch/pytorch/pull/30328\nghstack-source-id: 94481321\n\nDifferential Revision: D18661518\n\nfbshipit-source-id: 5a35a1ed2fae3b21a43614957a91d648c21bcca1", "pr_number": "30333", "files_changed": ["aten/src/ATen/core/dispatch/DispatchTable.h", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h"], "labels": []}, "24aabe439a": {"title": "Make Dispatcher::backendFallbackKernels_ an array (#30340)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30340\n\nWe already made OperatorEntry::dispatchTable_ an array to be able to avoid the concurrency primitives there,\nbut Dispatcher::backendFallbackKernels_ has the same issue. Let's make it a table too.\n\nSince there is some code duplication here, we also factor out the concept of a KernelFunctionTable to be used in both places.\nghstack-source-id: 94481317\n\nTest Plan: unit tests\n\nDifferential Revision: D18663426\n\nfbshipit-source-id: ba82ca5c4cae581eea359d5c0c3a5e23b0f8838c", "pr_number": "30340", "files_changed": ["aten/src/ATen/core/dispatch/DispatchTable.h", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": []}, "583c288232": {"title": "Add a OperatorHandle argument to boxed kernels (#29201)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29201\n\nThis is required for boxed backend fallback kernels (e.g. lazy, AMP) because they need to know which op was actually called.\nghstack-source-id: 94481313\n\nTest Plan: I will add unit tests in a diff stacked on top\n\nDifferential Revision: D18282746\n\nfbshipit-source-id: 339a1bbabd6aff31a587b98f095c75104dfc6f99", "pr_number": "29201", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/kernel_functor.h", "aten/src/ATen/core/boxing/kernel_stackbased_test.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "caffe2/core/export_caffe2_op_to_c10.h", "torch/csrc/jit/mobile/register_mobile_ops.cpp"], "labels": []}, "fb8c17dde1": {"title": "Test cases for backend fallback kernels (#29214)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29214\n\n-\nghstack-source-id: 94481312\n\nTest Plan: unit tests\n\nDifferential Revision: D18329308\n\nfbshipit-source-id: 1dbae401f2255c69ed16d436f891b9b60c333d81", "pr_number": "29214", "files_changed": ["aten/src/ATen/core/op_registration/op_registration_test.cpp"], "labels": []}, "afdc0bd4ec": {"title": "OperatorHandle::callBoxed/callUnboxed (#29330)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29330\n\nThis makes for a nicer API, especially in backend fallback kernels who get an OperatorHandle instance and can directly call these methods on it.\nghstack-source-id: 94481322\n\nTest Plan: unit tests stacked on top\n\nDifferential Revision: D18357424\n\nfbshipit-source-id: fa8c638335f246c906c8e16186507b4c486afb3f", "pr_number": "29330", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/function_wrapper.py"], "labels": []}, "aa2862b843": {"title": "Hide the OperatorKernel* argument from the stack based kernel API (#29337)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29337\n\nThis argument is needed by boxing wrappers so they're able to get a pointer to the corresponding unboxed kernel and call into it.\nBut if a kernel is registered in a boxed way, we don't need it and should hide this from the API.\nThis is especially needed for the backend fallback API where users would only be left wondering why this argument is there and what it does.\nAlso, hiding it allows us to potentially totally remove it in a future refactoring if we find some way to do so.\nghstack-source-id: 94481316\n\nTest Plan: unit tests\n\nDifferential Revision: D18361991\n\nfbshipit-source-id: 5cef26c896fe3f2a5db730d3bc79dcd62e7ef492", "pr_number": "29337", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/kernel_functor.h", "aten/src/ATen/core/boxing/kernel_stackbased_test.cpp", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "caffe2/core/export_caffe2_op_to_c10.h", "torch/csrc/jit/mobile/register_mobile_ops.cpp"], "labels": []}, "959a849a23": {"title": "better boxing (#29681)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29681\n\nRemove callUnboxedOnly() and instead use metaprogramming to figure out if an operator can use a boxed fallback or not.\nThis enables boxed fallback for ops in native_functions.yaml even if they don't have `use_c10_dispatcher: full` set, as long as they're in the range of supported types.\nghstack-source-id: 94481320\n\nTest Plan: unit tests\n\nDifferential Revision: D18462653\n\nfbshipit-source-id: 2955e3c4949267520a1734a6a2b919ef5e9684a2", "pr_number": "29681", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/boxing.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/function_wrapper.py"], "labels": []}, "0c7e4c1d62": {"title": "backend fallback test (#29682)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29682\n\nThis PR re-introduces backend_fallback_test.cpp, which was previously called boxed_fallback_test.cpp and showed how to use the backend fallback API.\nghstack-source-id: 94481314\n\nTest Plan: unit tests\n\nDifferential Revision: D18462654\n\nfbshipit-source-id: 3e9b5c8f35c05f9cd795f44a5fefd1a0aaf03509", "pr_number": "29682", "files_changed": ["aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/backend_fallback_test.cpp"], "labels": []}, "3990e9d1ca": {"title": "Improve performance of LeftRight::read() (#30282)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30282\n\nThe atomic increment/decrements in LeftRight::read() were measurable in perf benchmarks. Let's improve their perf.\nghstack-source-id: 94443230\n\nTest Plan: unit tests, perf benchmarks\n\nDifferential Revision: D18650228\n\nfbshipit-source-id: d184ce8288510ab178e7c7da73562609d1ca3c9f", "pr_number": "30282", "files_changed": ["c10/util/LeftRight.h"], "labels": []}, "20dfae4099": {"title": "Fix the crashes for c++ not able to find java class through Jni (#30390)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30390\n\nFix the crashes for c++ not able to find java class through Jni\nghstack-source-id: 94499644\n\nTest Plan: buck install -r fb4a\n\nReviewed By: ljk53\n\nDifferential Revision: D18667992\n\nfbshipit-source-id: aa1b19c6dae39d46440f4a3e691054f7f8b1d42e", "pr_number": "30390", "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/IValue.java", "android/pytorch_android/src/main/java/org/pytorch/Tensor.java"], "labels": []}, "99a46b44ea": {"title": "Use correct API macro in VariableHooksInterface. (#30320)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30320\n\nFixes #30296\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18665704\n\nPulled By: ezyang\n\nfbshipit-source-id: f09a953137fcc105959382254f9b8886af5aea3b", "pr_number": "30320", "files_changed": ["aten/src/ATen/core/VariableHooksInterface.h"], "labels": ["merge-this-please"]}, "f994377d28": {"title": "Turn off scalar_check for lshift, rshift.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29878\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18521746\n\nPulled By: gchanan\n\nfbshipit-source-id: 11fd7db79ac8ae76b1a5df25fb0ff59d81fcf394", "pr_number": "29878", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "94ad7544ae": {"title": "Turn off scalar_check for __or__", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29879\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18521745\n\nPulled By: gchanan\n\nfbshipit-source-id: 93d17d5e9cad5dd6d2c20221d87408c838d74eca", "pr_number": "29879", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "0c9c62ba6e": {"title": "Turn off scalar_checks for __and__ and clone.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29880\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18521732\n\nPulled By: gchanan\n\nfbshipit-source-id: 7fdf5d8a7b93b43ac32067222cb8df5e790900de", "pr_number": "29880", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/function_wrapper.py", "test/test_torch.py"], "labels": []}, "ce5f1a1b25": {"title": "Turn off scalar_check for masked_select. (#29923)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29923\n\nNote that this changes the behavior of masked_select when both \"self\" and \"mask\" are 0-dimensional.\n\nIn previous versions of PyTorch, this would return a 0-dimensional tensor.  But the documentation reads:\n\"Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.\"\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18539560\n\nPulled By: gchanan\n\nfbshipit-source-id: 1637ed2c434fcf8ceead0073aa610581f4a19d21", "pr_number": "29923", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": ["topic: bc-breaking"]}, "6e88ddf352": {"title": "Turn off scalar_check for _th_addmv and _th_eig as they can never pass. (#29945)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29945\n\nBoth functions require at least 1 2-dimensional tensor, so can never return an inferred scalar.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548056\n\nPulled By: gchanan\n\nfbshipit-source-id: f99a41d490b9a5ab5717534c92e4f2e848c743e8", "pr_number": "29945", "files_changed": ["aten/src/ATen/Declarations.cwrap"], "labels": []}, "7c6cc1d6d4": {"title": "Turn off scalar_checks for _th_multinomial_alias_draw. (#29946)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29946\n\nit requires > 0-dimensional tensors.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548050\n\nPulled By: gchanan\n\nfbshipit-source-id: 4d1e3b53bd701137cc2cb674f95627a5e064a274", "pr_number": "29946", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "b8eba7aca9": {"title": "Turn off scalar_check for ormqr. (#29947)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29947\n\nIt requires > 0-dimensional tensors.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548049\n\nPulled By: gchanan\n\nfbshipit-source-id: ce80a42515b59513a0e5ef2b32e2c2b90b4d64f5", "pr_number": "29947", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "16606e1725": {"title": "Turn off scalar_check for mode; the underlying code is correct.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29948\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548053\n\nPulled By: gchanan\n\nfbshipit-source-id: 15cdfc24d3e5123497c72dc09c5e6b28cb5e1f88", "pr_number": "29948", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "7160300638": {"title": "Turn off scalar_check for reductions _th_max, _th_min. (#29949)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29949\n\nThe underlying functions handle this already.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548047\n\nPulled By: gchanan\n\nfbshipit-source-id: 123c9297db4e4315da9b1d996ac8b41aa1b4c7bc", "pr_number": "29949", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "0c67311878": {"title": "Turn off scalar_check for set_(Storage, ...) (#29950)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29950\n\nThe underlying code handles it correctly.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548052\n\nPulled By: gchanan\n\nfbshipit-source-id: 88b737572c816fb0026ac5e66da7e3f4ab686773", "pr_number": "29950", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "d7ac90e2ef": {"title": "Stop binding std_single and var_single from TH; they aren't used anymore.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29951\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548057\n\nPulled By: gchanan\n\nfbshipit-source-id: 0143f694517fa8229e53bd2bc636501804a3f80b", "pr_number": "29951", "files_changed": ["aten/src/ATen/Declarations.cwrap"], "labels": []}, "c12f9a12a8": {"title": "Fix quantized ConvReLU3d test (#30266)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30266\n\nFix quantized ConvReLU3d test\n\nTest Plan: buck test mode/dev-nosan //caffe2/test:quantized -- \"conv\"\n\nReviewed By: hl475\n\nDifferential Revision: D18645717\n\nfbshipit-source-id: bbe93f9daf5046f2aa05363efc7d0e59eaff37bf", "pr_number": "30266", "files_changed": ["test/test_quantized_nn_mods.py", "torch/nn/intrinsic/quantized/modules/conv_relu.py", "torch/quantization/quantize.py"], "labels": ["fb-exported"]}, "0517323dad": {"title": "Update osx CI to XCode 9.4 / CUDA 10.0, cudnn 7.6.5 (#30359)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30359\n\nWe need this for C++14 support\n\nghstack-source-id: 94519850\n\nTest Plan: unit tests\n\nDifferential Revision: D18668868\n\nfbshipit-source-id: 87e8eadf0e60a1699fba4524aea53b306b9a7f24", "pr_number": "30359", "files_changed": [".circleci/config.yml", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/binary-job-specs.yml", ".circleci/verbatim-sources/caffe2-job-specs.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-pytorch-macos-builds.yml"], "labels": []}, "d64e2581cc": {"title": "Add list of supported XCode/CUDA versions to README", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30407\n\nDifferential Revision: D18689043\n\nPulled By: smessmer\n\nfbshipit-source-id: cd772451ef31356ed3045ebb1a9c4f5e5e91bb45", "pr_number": "30407", "files_changed": ["README.md"], "labels": []}, "5c6705e62c": {"title": "add default arg for init_method (#30208)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30208\n\nAdds default arg for init_method so users don't have to pass this in,\nand moves it to `RpcBackendOptions` struct. Removes `init_method` arg from rpc.init_rpc. Also fixes some docs.\nghstack-source-id: 94500475\n\nTest Plan: Unit tests pass.\n\nReviewed By: mrshenli\n\nDifferential Revision: D18630074\n\nfbshipit-source-id: 04b7dd7ec96f4c4da311b71d250233f1f262135a", "pr_number": "30208", "files_changed": ["docs/source/notes/distributed_autograd.rst", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py", "torch/distributed/rpc/backend_registry.py", "torch/distributed/rpc/constants.py"], "labels": []}, "25f4ba7c1b": {"title": "Improve compare kernel (#29743)", "body": "Summary:\nCurrently, the way the compare kernels handle dtypes is very funny (this behavior is introduced in https://github.com/pytorch/pytorch/pull/28427 and I just realize it today):\n\nLet's say `a, b` are two float tensors on CUDA.\n\nIf you do `a < b`, this is what would happen inside the loop:\n- Step 1: Fetch `a` and `b`, dynamically cast them from `float` to `float`. (i.e. check the scalar type to figure out if it needs cast. it doesn't. so do nothing then.)\n- Step 2: compute `a < b`, get a `bool` result\n- Step 3: statically cast the result into `float`\n- Step 3: do a dynamic cast of the result from `float` to `bool` and store the value\n\nAnd if you do `a.lt_(b)`, this is what would happen:\n- Step 1: Fetch `a` and `b`, no casting\n- Step 2: compute `a < b`, get a `bool` result\n- Step 3: statically cast the result into `float`\n- Step 4: store the result to memory, no casting\n\nAlthough dynamic casting happens on registers, it still hurt the performance a bit (~8%).\n\nThis PR fixes this issue. Now for compare kernels, if the output is bool and inputs have the same dtype, then there is no dynamic casting. Otherwise, there will be dynamic casting for each input and output. That is, the dynamic casting behavior of the two cases described above are swapped.\n\nBenchmark on `a < b` for tensor of 1000000000 fp32 elements:\nBefore https://github.com/pytorch/pytorch/issues/28427 6.35 ms\nCurrent master: 6.88 ms\nWith this PR: 6.36 ms\nBenchmark on `a.lt_(b)` does not show any difference across versions.\n\nBesides this, what worries me most is, with type promotion, the logic for tensor iterator is becoming super complicated, and it is hard to see if one change causes the performance regression of others. I suggest we create scripts that could benchmark tensor iterator entirely, review that code and put it somewhere inside the repository (maybe under `/tools` or `/test/scripts`?), and whenever we are not certain about the performance we could run it to check. (I guess not on this PR but on PRs after the script is done. If there are worries about performance, the author of PRs should run the script manually, and the reviewer should remind PR author to do so if necessary) If this is a good idea, I will send a PR for the script.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29743\n\nDifferential Revision: D18671269\n\nPulled By: ngimel\n\nfbshipit-source-id: 89a9c1c8b5fd45d5ae8fe907d65c2fe1a7dfd2dc", "pr_number": "29743", "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/cuda/BinaryCompareKernel.cu", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/cuda/Copy.cu"], "labels": []}, "b8f50d9cc8": {"title": "Support to add dequant for each use of Value (#30145)", "body": "Summary:\nIn this PR, we mainly handle the case there are multiple usage of a Value when inserting the quant-dequant pair. This change will add one dequant for each usage of the Value.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30145\n\nDifferential Revision: D18671600\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: 61324a98861da85b80dcf7e930381311118ae53b", "pr_number": "30145", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "0b71e7e1fd": {"title": "Refactor QAT Conv module for better extensibility (#30362)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30362\n\nRight now the qat modules(qat.ConvBn2d, qat.ConvBnReLU2d, qat.Conv2d)\nare not convinent to support other dimensions of Conv, this PR refactors\nthese modules so that we can support Conv1d/Conv3d better\n\nTest Plan:\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18691152\n\nfbshipit-source-id: 5b561e6b054eadd31b98cabdf1ac67a61ee9b805", "pr_number": "30362", "files_changed": ["test/test_jit.py", "torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/modules/conv.py", "torch/nn/qat/modules/conv.py"], "labels": []}, "ab2ec4d835": {"title": "Fix inexistent parameter in document (#24335)", "body": "Summary:\nThere is no `out` argument to `argsort` according to the source code.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/24335\n\nDifferential Revision: D16829134\n\nPulled By: vincentqb\n\nfbshipit-source-id: 8f91154984cd4a753ba1d6105fb8a9bfa0da22b3", "pr_number": "24335", "files_changed": ["torch/_torch_docs.py"], "labels": ["module: docs", "open source", "triaged"]}, "46e7f31fa3": {"title": "Document unsupported types (#30344)", "body": "Summary:\nThis adds a listing of the parts of the `typing` module that are unsupported\n\nThis is also a first pass decisions on features are 'unlikely to be implemented' vs 'not implemented' so they're open to discussion\n](https://our.intern.facebook.com/intern/diff/18665628/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30344\n\nPulled By: driazati\n\nDifferential Revision: D18665628\n\nfbshipit-source-id: 22b8ebbde23df03839306cdb4344ca18a44f2c29", "pr_number": "30344", "files_changed": ["docs/source/jit.rst"], "labels": []}, "661a6c8ef2": {"title": "Add `get_qparams` and revert the changes to `calculate_qparams` (#30262)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30262\n\n`get_qparams` returns all parameters that's needed to call quantize function\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D18645047\n\nfbshipit-source-id: e57c11a66dac2d589778d412a996796ad5b6f86a", "pr_number": "30262", "files_changed": ["torch/csrc/jit/passes/quantization.cpp", "torch/nn/quantized/modules/utils.py", "torch/quantization/fake_quantize.py", "torch/quantization/observer.py"], "labels": ["jit"]}, "8199596d7e": {"title": "Add missing std::move (#30411)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30411\n\n-\nghstack-source-id: 94526555\n\nTest Plan: unit tests\n\nDifferential Revision: D18690385\n\nfbshipit-source-id: fd348c0887c279694c2f6d287b361c8e07f02ffb", "pr_number": "30411", "files_changed": ["aten/src/ATen/core/dispatch/OperatorEntry.cpp"], "labels": []}, "085dde5965": {"title": "Fix for when PyTorch model trace has RecursiveScriptModules (#30430)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30430\n\nWhen a module isn't a TracedModule, attempt to get name information with `original_name` property on module and default to 'Module' when no such property exists.\n\nTest Plan:\n### Change child module to scripted module:\n```\nmodel = torchvision.models.alexnet()\nmodel.classifier = torch.jit.script(model.classifier)\n```\n### Add graph\n```\nw = SummaryWriter()\nw.add_graph(model, torch.rand((2, 3, 224, 224)))\nw.close()\n```\n### No errors\nHowever, graph is disconnected at parts and hard to understand.\n{F223327878}\n\nReviewed By: sanekmelnikov\n\nDifferential Revision: D18690836\n\nfbshipit-source-id: 42295d06b7c1d48d5401776dca1e0d12cd64b49d", "pr_number": "30430", "files_changed": ["torch/utils/tensorboard/_pytorch_graph.py"], "labels": ["fb-exported"]}, "ab5774547a": {"title": "Add info about transitive dependencies in case of using local aars (#30128)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30128\n\nPreview: https://github.com/pytorch/pytorch/tree/gh/IvanKobzarev/23/head/android\n\nBased on users issue: https://discuss.pytorch.org/t/android-somethings-went-wrong-with-pytorch-android-1-4-0-snapshot/61009/3\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18702658\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 14928baccd58ddbe633fad03038271d8333c4b49", "pr_number": "30128", "files_changed": ["android/README.md"], "labels": []}, "eccf42fd15": {"title": "Bug fix: Handle missing keys in observer state dict during load (#30357)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30357\n\nFix issue https://github.com/pytorch/pytorch/issues/29032 in loading from state dict for observers and fake quant.\nghstack-source-id: 94468814\n\nTest Plan: Ensures that load/save of fake quant and observers with missing keys works correctly.\n\nDifferential Revision: D18668517\n\nfbshipit-source-id: 0eda6f47c39102e55977fc548b9a03664f123ad7", "pr_number": "30357", "files_changed": ["torch/quantization/fake_quantize.py", "torch/quantization/observer.py"], "labels": []}, "584be86c3f": {"title": "Try exporting ONNX with force_outplace=False (#29466)", "body": "Summary:\nThis should resolve https://github.com/pytorch/pytorch/issues/29008. This flag has two effects on the tracer.\n- Remove the underscroll for inplace operators. E.g.: index_put_ ==> index_put. This is handled in utils.py separately as well.\n- Add out as input for backward computation.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29466\n\nReviewed By: hl475\n\nDifferential Revision: D18422815\n\nPulled By: houseroad\n\nfbshipit-source-id: 317b6a3c8a5751fe6fe49d7543e429d281ed0d6d", "pr_number": "29466", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/ir.h", "torch/csrc/jit/passes/remove_inplace_ops.cpp", "torch/onnx/symbolic_opset9.py", "torch/onnx/utils.py"], "labels": []}, "06db5ad707": {"title": "Provide names for operator nodes in ONNX exported graph. (#27342)", "body": "Summary:\nThe PyTorch exporter does not add any name to the ONNX operators in the exported graph. A common request is to add names to op nodes by default. This helps the readability of the graph in visualization tools such a Netron, or when the ONNX graph is printed as a string. Also, it helps with the debuggability of the ONNX graph.\n\nTherefore this PR adds name to operators in the exporters. The names follow a simple format, <op_type>_<index>. Expect files for tests in `test/onnx/test_operators.py` have been updated.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27342\n\nReviewed By: hl475\n\nDifferential Revision: D17790979\n\nPulled By: houseroad\n\nfbshipit-source-id: 1eaae88b5f51f152735a2ff96e22827837e34d9d", "pr_number": "27342", "files_changed": ["test/onnx/expect/TestOperators.test_acos.expect", "test/onnx/expect/TestOperators.test_add_broadcast.expect", "test/onnx/expect/TestOperators.test_add_left_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_right_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_singleton_broadcast.expect", "test/onnx/expect/TestOperators.test_addconstant.expect", "test/onnx/expect/TestOperators.test_addmm.expect", "test/onnx/expect/TestOperators.test_arange_dynamic.expect", "test/onnx/expect/TestOperators.test_argmax.expect", "test/onnx/expect/TestOperators.test_asin.expect", "test/onnx/expect/TestOperators.test_at_op.expect", "test/onnx/expect/TestOperators.test_atan.expect", "test/onnx/expect/TestOperators.test_avg_pool2d.expect", "test/onnx/expect/TestOperators.test_baddbmm.expect", "test/onnx/expect/TestOperators.test_basic.expect", "test/onnx/expect/TestOperators.test_batchnorm.expect", "test/onnx/expect/TestOperators.test_batchnorm_1d.expect", "test/onnx/expect/TestOperators.test_batchnorm_noaffine.expect", "test/onnx/expect/TestOperators.test_batchnorm_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_batchnorm_training.expect", "test/onnx/expect/TestOperators.test_bitshift.expect", "test/onnx/expect/TestOperators.test_c2_op.expect", "test/onnx/expect/TestOperators.test_chunk.expect", "test/onnx/expect/TestOperators.test_clip.expect", "test/onnx/expect/TestOperators.test_clip_max.expect", "test/onnx/expect/TestOperators.test_clip_min.expect", "test/onnx/expect/TestOperators.test_concat2.expect", "test/onnx/expect/TestOperators.test_conv.expect", "test/onnx/expect/TestOperators.test_conv_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_conv_onnx_irv4_opset8.expect", "test/onnx/expect/TestOperators.test_convtranspose.expect", "test/onnx/expect/TestOperators.test_cos.expect", "test/onnx/expect/TestOperators.test_cumsum.expect", "test/onnx/expect/TestOperators.test_det.expect", "test/onnx/expect/TestOperators.test_dict.expect", "test/onnx/expect/TestOperators.test_dict_str.expect", "test/onnx/expect/TestOperators.test_dropout.expect", "test/onnx/expect/TestOperators.test_elu.expect", "test/onnx/expect/TestOperators.test_embedding_bags.expect", "test/onnx/expect/TestOperators.test_empty_like.expect", "test/onnx/expect/TestOperators.test_empty_like_opset7.expect", "test/onnx/expect/TestOperators.test_equal.expect", "test/onnx/expect/TestOperators.test_erf.expect", "test/onnx/expect/TestOperators.test_exp.expect", "test/onnx/expect/TestOperators.test_expand.expect", "test/onnx/expect/TestOperators.test_flatten.expect", "test/onnx/expect/TestOperators.test_flatten2D.expect", "test/onnx/expect/TestOperators.test_fmod.expect", "test/onnx/expect/TestOperators.test_frobenius_norm.expect", "test/onnx/expect/TestOperators.test_full.expect", "test/onnx/expect/TestOperators.test_full_like.expect", "test/onnx/expect/TestOperators.test_gather.expect", "test/onnx/expect/TestOperators.test_gather_opset11.expect", "test/onnx/expect/TestOperators.test_ge.expect", "test/onnx/expect/TestOperators.test_gelu.expect", "test/onnx/expect/TestOperators.test_gt.expect", "test/onnx/expect/TestOperators.test_hardtanh.expect", "test/onnx/expect/TestOperators.test_implicit_expand.expect", "test/onnx/expect/TestOperators.test_index.expect", "test/onnx/expect/TestOperators.test_isnan.expect", "test/onnx/expect/TestOperators.test_le.expect", "test/onnx/expect/TestOperators.test_linear.expect", "test/onnx/expect/TestOperators.test_log_sigmoid.expect", "test/onnx/expect/TestOperators.test_logsoftmax.expect", "test/onnx/expect/TestOperators.test_lt.expect", "test/onnx/expect/TestOperators.test_master_opset.expect", "test/onnx/expect/TestOperators.test_max.expect", "test/onnx/expect/TestOperators.test_maxpool.expect", "test/onnx/expect/TestOperators.test_maxpool_dilations.expect", "test/onnx/expect/TestOperators.test_maxpool_indices.expect", "test/onnx/expect/TestOperators.test_mean.expect", "test/onnx/expect/TestOperators.test_meshgrid.expect", "test/onnx/expect/TestOperators.test_min.expect", "test/onnx/expect/TestOperators.test_mm.expect", "test/onnx/expect/TestOperators.test_narrow.expect", "test/onnx/expect/TestOperators.test_ne.expect", "test/onnx/expect/TestOperators.test_nonzero.expect", "test/onnx/expect/TestOperators.test_norm_p1.expect", "test/onnx/expect/TestOperators.test_norm_p2.expect", "test/onnx/expect/TestOperators.test_ones_like.expect", "test/onnx/expect/TestOperators.test_pad.expect", "test/onnx/expect/TestOperators.test_params.expect", "test/onnx/expect/TestOperators.test_params_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_permute2.expect", "test/onnx/expect/TestOperators.test_pixel_shuffle.expect", "test/onnx/expect/TestOperators.test_pow.expect", "test/onnx/expect/TestOperators.test_prelu.expect", "test/onnx/expect/TestOperators.test_prod.expect", "test/onnx/expect/TestOperators.test_rand.expect", "test/onnx/expect/TestOperators.test_randn.expect", "test/onnx/expect/TestOperators.test_reduce_sum_negative_indices.expect", "test/onnx/expect/TestOperators.test_reduced_mean.expect", "test/onnx/expect/TestOperators.test_reduced_mean_keepdim.expect", "test/onnx/expect/TestOperators.test_reduced_prod.expect", "test/onnx/expect/TestOperators.test_reduced_prod_keepdim.expect", "test/onnx/expect/TestOperators.test_reduced_sum.expect", "test/onnx/expect/TestOperators.test_reduced_sum_keepdim.expect", "test/onnx/expect/TestOperators.test_reducemax.expect", "test/onnx/expect/TestOperators.test_reducemin.expect", "test/onnx/expect/TestOperators.test_remainder.expect", "test/onnx/expect/TestOperators.test_repeat.expect", "test/onnx/expect/TestOperators.test_repeat_dim_overflow.expect", "test/onnx/expect/TestOperators.test_retain_param_name_disabled.expect", "test/onnx/expect/TestOperators.test_round.expect", "test/onnx/expect/TestOperators.test_rrelu.expect", "test/onnx/expect/TestOperators.test_rsqrt.expect", "test/onnx/expect/TestOperators.test_rsub.expect", "test/onnx/expect/TestOperators.test_scatter_add.expect", "test/onnx/expect/TestOperators.test_scatter_add_opset11.expect", "test/onnx/expect/TestOperators.test_selu.expect", "test/onnx/expect/TestOperators.test_sign.expect", "test/onnx/expect/TestOperators.test_sin.expect", "test/onnx/expect/TestOperators.test_slice.expect", "test/onnx/expect/TestOperators.test_slice_dynamic.expect", "test/onnx/expect/TestOperators.test_split.expect", "test/onnx/expect/TestOperators.test_split_with_sizes.expect", "test/onnx/expect/TestOperators.test_sqrt.expect", "test/onnx/expect/TestOperators.test_std.expect", "test/onnx/expect/TestOperators.test_sum.expect", "test/onnx/expect/TestOperators.test_tan.expect", "test/onnx/expect/TestOperators.test_topk.expect", "test/onnx/expect/TestOperators.test_topk_smallest_unsorted.expect", "test/onnx/expect/TestOperators.test_unfold.expect", "test/onnx/expect/TestOperators.test_unique.expect", "test/onnx/expect/TestOperators.test_unsqueeze.expect", "test/onnx/expect/TestOperators.test_upsample_nearest.expect", "test/onnx/expect/TestOperators.test_view.expect", "test/onnx/expect/TestOperators.test_view_flatten.expect", "test/onnx/expect/TestOperators.test_zeros_like.expect", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/verify.py", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export.h", "torch/csrc/jit/python_ir.cpp", "torch/onnx/utils.py"], "labels": ["jit", "module: onnx", "open source", "triaged"]}, "efe1859ad9": {"title": "By default ignore RRef leaks during shutdown (#30217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30217\n\nBefore this commit, RRefContext throws an error if it detects any\nRRef leak during shutdown. However, this requires applications to\nmake sure that is has freed all references to RRefs in application\ncode, which can be a bad debugging experience when for large\napplications. Besides, this also relies on Python GC to free things\nup in time, which might not always be true. After this commit,\nRRefContext would ignore leaking RRefs during shutdown, as shutdown\nis called when the application has finished training and no longer\ncare about local states. Hence, it should be OK to just ignore\nthose leaks and destroy OwnerRRefs. If application would like to\nenforce no leaks, just set torch.distributed.rpc.api._ignore_rref_leak\nto False.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18632546\n\nPulled By: mrshenli\n\nfbshipit-source-id: 2744b2401dafdd16de0e0a76cf8e07777bed0f38", "pr_number": "30217", "files_changed": ["test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/rref.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/distributed/rpc/api.py"], "labels": []}, "2599b9b551": {"title": "Add output_size argument to caffe2 Int8ResizeNearest (#30202)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30202\n\nPytorch Upsample operator has output_size as an argument.\nFor quantized tensor inputs we cannot get the input_size to calculate the width and height scale factor.\nInstead we pass the output_size directly to caffe2 to calculate the scale factors.\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_upsample\n\nImported from OSS\n\nDifferential Revision: D18631478\n\nfbshipit-source-id: 38a39129bc863f4ecf2293acc068e40ab7edc825", "pr_number": "30202", "files_changed": ["caffe2/operators/quantized/int8_resize_nearest_op.cc", "caffe2/operators/quantized/int8_resize_nearest_op.h", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/onnx/symbolic_caffe2.py"], "labels": []}, "0febff36ac": {"title": "Export dynamic unbind/split and __getitem__ (#29136)", "body": "Summary:\nIn ONNX opset 11, a series of sequence ops were added. Operators that are related to Tensor[] in PyTorch can be exported using these sequence ops.\nIn this PR, unbind/split that produces Tensor[], and __getitem__ that takes Tensor[] as input, are exported correctly to ONNX opset 11.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29136\n\nReviewed By: hl475\n\nDifferential Revision: D18309222\n\nPulled By: houseroad\n\nfbshipit-source-id: be12c96bf8d0a56900683ef579f1c808c0a1af21", "pr_number": "29136", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["jit", "module: onnx"]}, "79a830af56": {"title": "Turn off scalar_check for Tensor.set_(Tensor) (#29952)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29952\n\nThe underlying op handles the check correctly.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548048\n\nPulled By: gchanan\n\nfbshipit-source-id: 9ac6fde743408e59ccdfc61bd574ebe6e2862238", "pr_number": "29952", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "72ac45662b": {"title": "Turn off scalar_checks for torch.take. (#29953)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29953\n\nThe underlying function handles it correctly.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548055\n\nPulled By: gchanan\n\nfbshipit-source-id: cc2d0ae37d9689423363d115c6a653cb64840528", "pr_number": "29953", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "dbce53fe32": {"title": "Turn off scalar_check for _th_gather. (#29954)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29954\n\nThe underlying op handles scalar_check correctly.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548054\n\nPulled By: gchanan\n\nfbshipit-source-id: a1b44afa80c2928b78abbfba8b8b5d3608ac0fd3", "pr_number": "29954", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "dcd9f49809": {"title": "Specify ordering on singular values and eigenvalues output from torch\u2026 (#30389)", "body": "Summary:\n\u2026.svd/symeig respectively\n\nChangelog:\n- Adds a note to docstrings of the both functions specifying the ordering\n\nFixes https://github.com/pytorch/pytorch/issues/30301\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30389\n\nDifferential Revision: D18707608\n\nPulled By: zou3519\n\nfbshipit-source-id: b0f73631578f39a24fae9af4997c6491de8be9a8", "pr_number": "30389", "files_changed": ["torch/_torch_docs.py"], "labels": ["open source"]}, "45880f4246": {"title": "Change logging to remove the word \"error\" from info log", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30468\n\nReviewed By: xianjiec\n\nDifferential Revision: D18702959\n\nfbshipit-source-id: a777445bea735dce89182dd95f38907963fab556", "pr_number": "30468", "files_changed": ["caffe2/operators/gather_ranges_to_dense_op.h"], "labels": ["fb-exported"]}, "b0871f211b": {"title": "Make all optimizers consistent so that they don't change gradients inplace", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30257\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18665461\n\nPulled By: albanD\n\nfbshipit-source-id: cfdafef919468a41007881b82fd288b7128baf95", "pr_number": "30257", "files_changed": ["torch/optim/adam.py", "torch/optim/optimizer.py", "torch/optim/sgd.py"], "labels": ["topic: bc-breaking"]}, "fec903ce00": {"title": "Fix test case after get_qparams refactor (#30470)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30470\n\natt\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D18710775\n\nfbshipit-source-id: b1c7c0afbc538ff1d3e19c5d3d6bd425e4f94f06", "pr_number": "30470", "files_changed": ["test/test_jit.py"], "labels": []}, "e9cc4a5942": {"title": "Add @DoNotStrip to nativeNewTensor method. (#30472)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30472\n\nAdd DoNotStrip to nativeNewTensor method.\nghstack-source-id: 94596624\n\nTest Plan:\nTriggered build on diff for automation_fbandroid_fallback_release.\n\nbuck install -r fb4a\n\nTested BI cloaking using pytext lite interpreter.\n\nObverse that logs are sent to scuba table:\n\n{F223408345}\n\nReviewed By: linbinyu\n\nDifferential Revision: D18709087\n\nfbshipit-source-id: 74fa7a0665640c294811a50913a60ef8d6b9b672", "pr_number": "30472", "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/Tensor.java"], "labels": []}, "c5a6c4d6c9": {"title": "Adding elementwise kernel also operating on index (#28175)", "body": "Summary:\nThis PR add `gpu_kernel_with_index` as an addition to element-wise kernel template. It allows kernel to not only operate on input tensor value, but also each values index(view as 1d, so from 0 to numel) within the lambda.\nDirect use case here is to replace thrust::tabulate used in range/arange/linspace. Benifits are:\n- thrust::tabulate causes additional unneccessary synchronization on cpu.\n- Now it works with tensor iterator, output no longer needs to be contiguous and a memcpy is saved\n\nIt can also potentially be reused to add new function to pytorch later, if we see use case both value and index is needed.(for example unify tril/triu into tensor iterator element-wise? add other pattern?)\n\nKnown issues:\nhttps://github.com/pytorch/pytorch/pull/23586 is needed to enable non-contiguous case work properly, since overlapping needs to be checked. Currently non-contiguous tensor falls into TOO_HARD. I could write proper check in this file but I figured using exist method is better. jjsjann123\nIt does not work beyond 32bit indexing. But thrust was erroring on those case too. We could split tensor in caller to enable this. Index changes after split, so it is easier for caller to pass different lambda, and harder for the template to handle it in general.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28175\n\nDifferential Revision: D18708649\n\nPulled By: ngimel\n\nfbshipit-source-id: 382081c96f266ae7b61095fc1f2af41c6b210fa9", "pr_number": "28175", "files_changed": ["aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/RangeFactories.cu"], "labels": []}, "634f370c63": {"title": "Add comment to ops bound at python layer", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30419\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D18714000\n\nPulled By: eellison\n\nfbshipit-source-id: 22ccb941b2db24031921f378c600e68fe70e1346", "pr_number": "30419", "files_changed": ["tools/autograd/templates/python_torch_functions.cpp", "tools/autograd/templates/python_variable_methods.cpp", "torch/csrc/autograd/python_variable.cpp"], "labels": []}, "976d91d30a": {"title": "Comment on a set of ops bound at the python layer", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30420\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D18713999\n\nPulled By: eellison\n\nfbshipit-source-id: 3a8d6e4431cbfe6a78ca047217c1c53c47403841", "pr_number": "30420", "files_changed": ["tools/autograd/templates/python_torch_functions.cpp", "tools/autograd/templates/python_variable_methods.cpp"], "labels": []}, "05a1644ce3": {"title": "Fix BC for quantized linear", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30481\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18714602\n\nPulled By: jamesr66a\n\nfbshipit-source-id: d51206c22cf2446e98053446789c6324c0481321", "pr_number": "30481", "files_changed": ["test/test_quantized_nn_mods.py", "torch/nn/quantized/modules/linear.py"], "labels": []}, "4eff2f2007": {"title": "Fix missing closing quotes in docs", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30448\n\nDifferential Revision: D18711396\n\nPulled By: zou3519\n\nfbshipit-source-id: 6e35e0779716185791273eedca7a93667a6cda90", "pr_number": "30448", "files_changed": ["docs/source/name_inference.rst"], "labels": []}, "92e27c5e89": {"title": "Flag to disable Variable", "body": "Summary:\nusing `buck build mode/opt mode/no-gpu //experimental/ngimel/benchmark_framework_overheads:cpp_benchmark`\n\n```\ndevvm497.prn3.facebook.com:/data/users/bwasti/fbsource/fbcode $ ./cpp_benchmark --niter 10000\ncreating inputs, number of dimensions 1\nstarting op\nbenchmarking 10000 iterations\nusing cpp frontend\nelapsed time per iteration 0.90638 us\n```\n\n```\ndevvm497.prn3.facebook.com:/data/users/bwasti/fbsource/fbcode $ ./cpp_benchmark --niter 10000 --disable_variable_dispatch\ncreating inputs, number of dimensions 1\nstarting op\nbenchmarking 10000 iterations\nusing cpp frontend\nelapsed time per iteration 0.775436 us\n```\n\nTest Plan: let all tests run\n\nReviewed By: smessmer\n\nDifferential Revision: D18654276\n\nfbshipit-source-id: 362812b2c87ec428448b2ac65baac45f492fdce4", "pr_number": null, "files_changed": ["aten/src/ATen/native/TensorProperties.cpp", "c10/core/impl/LocalTensorTypeSet.cpp", "c10/core/impl/LocalTensorTypeSet.h"], "labels": []}, "8bbafa0b32": {"title": "Add logical_and and logical_or (#28162)", "body": "Summary:\nSuperseding https://github.com/pytorch/pytorch/issues/24379 as type promotion has been implemented.\n\nClose https://github.com/pytorch/pytorch/issues/24379\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28162\n\nDifferential Revision: D18580867\n\nPulled By: ailzhang\n\nfbshipit-source-id: 7e4d7c37da4dc8df87314bd4f1f6a7539e46586a", "pr_number": "28162", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/README.md", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_namedtensor.py", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["module: operators"]}, "21d7532dfe": {"title": "Add more comment on NumPy detection in Python scripts.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30417\n\nDifferential Revision: D18716502\n\nPulled By: albanD\n\nfbshipit-source-id: 0b1b86f882e0e24cb6845e4a44708048e7e3b4a8", "pr_number": "30417", "files_changed": ["tools/setup_helpers/cmake.py", "tools/setup_helpers/numpy_.py"], "labels": ["module: build"]}, "6bd8937aee": {"title": "FunctionParameter::set_default_str replace || with &&", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30471\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18710958\n\nPulled By: pbelevich\n\nfbshipit-source-id: 7e5339175c7e16cd975a90bf6b123df728045e4d", "pr_number": "30471", "files_changed": ["torch/csrc/utils/python_arg_parser.cpp"], "labels": []}, "5ada5363fc": {"title": "GenericDict/List type use unshapedType() (#30428)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30428\n\nReported issue https://discuss.pytorch.org/t/incomprehensible-behaviour/61710\n\nSteps to reproduce:\n\n```\nclass WrapRPN(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, features):\n        # type: (Dict[str, Tensor]) -> int\n        return 0\n```\n\n```\n#include <torch/script.h>\n\nint main() {\n  torch::jit::script::Module module = torch::jit::load(\"dict_str_tensor.pt\");\n\n  torch::Tensor tensor = torch::rand({2, 3});\n  at::IValue ivalue{tensor};\n  c10::impl::GenericDict dict{c10::StringType::get(),ivalue.type()};\n  dict.insert(\"key\", ivalue);\n  module.forward({dict});\n}\n```\n\nValueType of `c10::impl::GenericDict` is from the first specified element as `ivalue.type()`\nIt fails on type check in` function_schema_inl.h` !value.type()->isSubtypeOf(argument.type())\nas `DictType::isSubtypeOf` requires equal KeyType and ValueType, while `TensorType`s are different.\n\nFix:\nUse c10::unshapedType for creating Generic List/Dict\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18717189\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 1e352a9c776a7f7e69fd5b9ece558f1d1849ea57", "pr_number": "30428", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp"], "labels": []}, "2d6b2f39e9": {"title": "Fix docs so that the example works (#30120)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30120\n\nThe example given for functional conv2d didn't work. This diff fixes the example in docs so that it works.\n\nFixes https://github.com/pytorch/pytorch/issues/29649\nghstack-source-id: 94601559\n\nTest Plan: Tried the example locally\n\nDifferential Revision: D18604606\n\nfbshipit-source-id: ff1a4f903e2843efe30d962d4ff00e5065cd1d7e", "pr_number": "30120", "files_changed": ["torch/nn/quantized/functional.py"], "labels": []}, "829499e626": {"title": "avoid Formatting::print() when STRIP_ERROR_MESSAGES is set (#30451)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30451\n\nTORCH_CHECK takes __VA_ARGS__ so there is no need to concatenate strings\nbefore calling it. This way it won't call Formatting::print() on the\ntensor when STRIP_ERROR_MESSAGES macro is set. Formatting::print() calls\nseveral specific tensor methods that brings in unnecessary inter-op\ndependencies for static code analysis.\n\nTest Plan: - builds\n\nDifferential Revision: D18703784\n\nPulled By: ljk53\n\nfbshipit-source-id: 1c0628e3ddcb2fd42c475cb161edbef09dfe8eb5", "pr_number": "30451", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp"], "labels": []}, "c1c8105de0": {"title": "Make the warning of using SparseTensor in JIT less noisy", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30499\n\nTest Plan: waitforsandcastle\n\nReviewed By: wanchaol\n\nDifferential Revision: D18705553\n\nfbshipit-source-id: d6e16e3285a74a1c031a5312f7a690f1baf392f8", "pr_number": "30499", "files_changed": ["torch/csrc/jit/pybind_utils.h"], "labels": ["fb-exported", "jit"]}, "512c2a2df5": {"title": "Enable constant folding (#29834)", "body": "Summary:\nSet default do_constant_folding = True\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29834\n\nReviewed By: hl475\n\nDifferential Revision: D18588037\n\nPulled By: houseroad\n\nfbshipit-source-id: b35c06161321629c886e177ea666eff31cebf06a", "pr_number": "29834", "files_changed": ["test/onnx/expect/TestOperators.test_arange_dynamic.expect", "test/onnx/expect/TestOperators.test_baddbmm.expect", "test/onnx/expect/TestOperators.test_batchnorm.expect", "test/onnx/expect/TestOperators.test_batchnorm_1d.expect", "test/onnx/expect/TestOperators.test_batchnorm_noaffine.expect", "test/onnx/expect/TestOperators.test_batchnorm_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_batchnorm_training.expect", "test/onnx/expect/TestOperators.test_bitshift.expect", "test/onnx/expect/TestOperators.test_prelu.expect", "test/onnx/expect/TestOperators.test_retain_param_name_disabled.expect", "test/onnx/expect/TestOperators.test_slice_dynamic.expect", "test/onnx/test_onnx_opset.py", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/onnx/test_verify.py", "test/onnx/verify.py", "torch/onnx/__init__.py", "torch/onnx/utils.py"], "labels": []}, "d2336edcfb": {"title": "Boxed variable dispatch (#29934)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29934\n\nPreviously, when doing boxed dispatch (e.g. custom ops), the dispatcher manually removed the VariableTensorId flag before dispatching\nbecause custom ops don't have variable kernels.\nThis is one of the blockers that prevented us from using the boxed dispatch mechanism for ops from native_functions.yaml because they define variable kernels and need them to be called for autograd.\n\nThis PR changes that. The dispatcher doesn't remove the VariableTensorId flag anymore.\nInstead, to make custom ops work, we implement a variable fallback kernel that is called whenever no other variable kernel was found.\nghstack-source-id: 94618474\n\nTest Plan: unit tests\n\nDifferential Revision: D18542342\n\nfbshipit-source-id: a30ae35d98f89f7ae507151f55c42cfbed54a451", "pr_number": "29934", "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h"], "labels": []}, "1d3f3a1a0c": {"title": "Add pybind11 trampoline class for c10d.Store (#30415)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30415\n\nThis enables subclassing of c10d.Store and implementing its interface in Python.\nghstack-source-id: 94586627\n\nTest Plan: New tests passes.\n\nReviewed By: vladbelous\n\nDifferential Revision: D18693018\n\nfbshipit-source-id: fa1eba4bd11cc09a3d6bf3f35369c885033c63c0", "pr_number": "30415", "files_changed": ["test/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp"], "labels": []}, "0282c5ae69": {"title": "Add helper to aggregate multiple process groups (#25768)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25768\n\nThe round robin process group can be constructed from multiple other\nprocess groups. Every collective call against this new process group\nis delegated to the specified process groups in a round robin fashion.\n\nDoing so may benefit performance when calling into multiple NCCL\nprocess groups. Instead of adding support for round-robin usage of\nNCCL communicators, we achieve the same without changing the NCCL\nprocess group and adding this wrapper class.\n\nThe API to create this round robin process group is a bit harsh. If we\nfind it adds significant benefit we can revisit and make this a first\nclass citizen in the torch.distributed module.\nghstack-source-id: 94578376\n\nTest Plan: The newly added test passes.\n\nReviewed By: chenyangyu1988\n\nDifferential Revision: D17226323\n\nfbshipit-source-id: ec9f754b66f33b983fee30bfb86a1c4c5d74767d", "pr_number": "25768", "files_changed": ["test/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/CMakeLists.txt", "torch/lib/c10d/ProcessGroupRoundRobin.cpp", "torch/lib/c10d/ProcessGroupRoundRobin.hpp"], "labels": ["module: distributed"]}, "1e8ed021c6": {"title": "Support logsoftmax with dim != -1 (#30433)", "body": "Summary:\nPyTorch dim and ONNX axis have different meanings.\nONNX only supports log_softmax with dim = -1. Transpose must be added before and after log_softmax to support other cases.\nThis requires input rank to be known at export time.\nFixes https://github.com/pytorch/pytorch/issues/17918\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30433\n\nReviewed By: hl475\n\nDifferential Revision: D18723520\n\nPulled By: houseroad\n\nfbshipit-source-id: d0ed3b3f051d08d46495a7abfa854edd120dca3a", "pr_number": "30433", "files_changed": ["test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": []}, "ec5c08de74": {"title": "Revert D18580867: Add logical_and and logical_or", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18580867\n\nOriginal commit changeset: 7e4d7c37da4d\n\nfbshipit-source-id: 81fb604c7aef8d847f518f5faa016e7bd0423016", "pr_number": null, "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/README.md", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_namedtensor.py", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": []}, "d0acc9c085": {"title": "Switch PyTorch/Caffe2 to C++14 (#30406)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30406\n\nghstack-source-id: 94642238\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D17908478\n\nfbshipit-source-id: 6e340024591ec2c69521668022999df4a33b4ddb", "pr_number": "30406", "files_changed": ["CMakeLists.txt", "CONTRIBUTING.md", "android/pytorch_android/CMakeLists.txt", "aten/src/ATen/cpu/tbb/CMakeLists.txt", "aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt", "aten/src/ATen/test/test_install/CMakeLists.txt", "c10/CMakeLists.txt", "c10/util/C++17.h", "cmake/Dependencies.cmake", "cmake/MiscCheck.cmake", "cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake", "cmake/TorchConfig.cmake.in", "cmake/public/cuda.cmake", "cmake/public/utils.cmake", "docs/cpp/source/installing.rst", "ios/LibTorch.podspec", "setup.py", "test/custom_operator/CMakeLists.txt", "torch/csrc/jit/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/fuser/cuda/fused_kernel.cpp", "torch/lib/c10d/CMakeLists.txt", "torch/lib/libshm/CMakeLists.txt", "torch/utils/cpp_extension.py"], "labels": ["jit"]}, "fcb7371e65": {"title": "Update docs for cpp_extension on Windows (#30392)", "body": "Summary:\nTargets https://github.com/pytorch/pytorch/issues/30379.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30392\n\nDifferential Revision: D18730438\n\nPulled By: albanD\n\nfbshipit-source-id: f718d006ee8aaaa356c1e15e53a0469f15e8ed41", "pr_number": "30392", "files_changed": ["docs/cpp/source/installing.rst"], "labels": []}, "106ab487eb": {"title": "fix typo in doc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30518\n\nDifferential Revision: D18729361\n\nPulled By: albanD\n\nfbshipit-source-id: 4e386b99e898b9cd8f9a21dff642d0f40355899f", "pr_number": "30518", "files_changed": ["c10/core/TensorTypeId.h"], "labels": []}, "2f42488d36": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/64dc8e79e944585bbc1a26bb564068ebf5e1eccf\nhttps://github.com/facebook/fbzmq/commit/3b2aa3c218c76e4e9dda70e8f98789a5d593eaba\nhttps://github.com/facebook/folly/commit/dc6c17ca9edbe61f5285250d065a4c0f971735f6\nhttps://github.com/facebook/litho/commit/4508ea4e067de44bd1f48ed305a9953ce1be9ca2\nhttps://github.com/facebook/mcrouter/commit/6150034ff31e6228bda2d218ca005bcc6b64d15e\nhttps://github.com/facebook/proxygen/commit/12b7a89a4b6c464cb2ee77507f1e9667d34099f0\nhttps://github.com/facebook/rocksdb/commit/9befbe9b406574e8d8b687799701bd8cebf5163b\nhttps://github.com/facebook/wangle/commit/2fd96cc0706c23248ca88e7bae5ca1cd1185061c\nhttps://github.com/facebookincubator/fizz/commit/68bf04ce4611e0da33b65d9662b8feec4c1c3c88\nhttps://github.com/facebookincubator/katran/commit/19bd96d453343d2264a85a8d5ca11ff4297f8f7b\nhttps://github.com/facebookincubator/mvfst/commit/7229ad4fd7678a222e4556d0bacaca7e819a44f1\nhttps://github.com/facebookincubator/profilo/commit/b2bb2b465bfa5435ea18a76613ad477b8f91e327\nhttps://github.com/pytorch/fbgemm/commit/4c65c9023d4b0d2c431d11018d022adaa8b3091e\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: e7dc6a4ebafdc6a01aff89f4038f5679ed6e7011", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "a69be8123a": {"title": "Use `gettimeofday` on iOS (#30361)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30361\n\n### Summary\n\nBy default, the compiler will choose `clock_gettime` for the iOS build. However, that API is not available until iOS 10. Since the Facebook app still supports iOS 9.0,  we have to use `gettimeofday` instead.\n\n```shell\nxplat/caffe2/torch/csrc/autograd/profiler.h:86:3: error: 'clock_gettime' is only available on iOS 10.0 or newer [-Werror,-Wunguarded-availability]\n\nxplat/caffe2/torch/csrc/autograd/profiler.h:86:17: error: '_CLOCK_MONOTONIC' is only available on iOS 10.0 or newer [-Werror,-Wunguarded-availability]\n```\n\nP.S. the open-sourced version is iOS 12.0 and above, so we don't have this problem.\n\n### Test Plan\n\n- buck build works\n- Don't break CIs\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18730262\n\nPulled By: xta0\n\nfbshipit-source-id: fe6d954b8d3c23cbc9d1e25a2e72e0b0c1d4eaa9", "pr_number": "30361", "files_changed": ["torch/csrc/autograd/profiler.h"], "labels": []}, "c1c5622a6a": {"title": "Add katex to pytorch-linux-xenial-py3.6-gcc5.4 docker image (#30522)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30522\n\nThis is in preparation for moving the docs push CI jobs to depend on\n`pytorch-linux-xenial-py3.6-gcc5.4` rather than\n`pytorch-linux-xenial-cuda9-cudnn7-py3`.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18731108\n\nPulled By: zou3519\n\nfbshipit-source-id: fd753a5ca818fa73a14e4276c33368a247cc40e1", "pr_number": "30522", "files_changed": [".circleci/docker/build.sh"], "labels": []}, "7d2b0aa693": {"title": "add retries to network operations (curl, conda install, git clone) (#30479)", "body": "Summary:\nAddresses some of the top network-related flakiness occurrences.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30479\n\nDifferential Revision: D18736386\n\nPulled By: kostmo\n\nfbshipit-source-id: 9eb5dca0cd0281894a0b304fbaf59a0341d3ff58", "pr_number": "30479", "files_changed": [".circleci/config.yml", ".circleci/docker/common/install_android.sh", ".circleci/docker/common/install_cache.sh", ".circleci/docker/common/install_cmake.sh", ".circleci/scripts/binary_checkout.sh", ".circleci/scripts/binary_install_miniconda.sh", ".circleci/scripts/binary_ios_build.sh", ".circleci/scripts/binary_linux_test.sh", ".circleci/scripts/setup_ci_environment.sh", ".circleci/scripts/setup_linux_system_environment.sh", ".circleci/verbatim-sources/binary-job-specs.yml", ".circleci/verbatim-sources/caffe2-job-specs.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".jenkins/caffe2/build.sh", ".jenkins/pytorch/build.sh", ".jenkins/pytorch/macos-common.sh", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_magma.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_miniconda3.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_mkl.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_sccache.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat"], "labels": []}, "f3631c2464": {"title": "Revert D18542342: Boxed variable dispatch", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18542342\n\nOriginal commit changeset: a30ae35d98f8\n\nfbshipit-source-id: 082992125447c814c90f7934fadf00995e146e0e", "pr_number": null, "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h"], "labels": []}, "0b25371f5d": {"title": "Turn off scalar_check for _th_normal.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29955\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548051\n\nPulled By: gchanan\n\nfbshipit-source-id: c652999ac9e37d2592aa85ef022040fe0700b5cf", "pr_number": "29955", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/cuda/Distributions.cu", "test/test_torch.py"], "labels": []}, "a2ed50c920": {"title": "Revert D17908478: Switch PyTorch/Caffe2 to C++14", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD17908478\n\nOriginal commit changeset: 6e340024591e\n\nfbshipit-source-id: 775d2e29be0bc3a0db64f164c8960c44d4877d5d", "pr_number": null, "files_changed": ["CMakeLists.txt", "CONTRIBUTING.md", "android/pytorch_android/CMakeLists.txt", "aten/src/ATen/cpu/tbb/CMakeLists.txt", "aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt", "aten/src/ATen/test/test_install/CMakeLists.txt", "c10/CMakeLists.txt", "c10/util/C++17.h", "cmake/Dependencies.cmake", "cmake/MiscCheck.cmake", "cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake", "cmake/TorchConfig.cmake.in", "cmake/public/cuda.cmake", "cmake/public/utils.cmake", "docs/cpp/source/installing.rst", "ios/LibTorch.podspec", "setup.py", "test/custom_operator/CMakeLists.txt", "torch/csrc/jit/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/fuser/cuda/fused_kernel.cpp", "torch/lib/c10d/CMakeLists.txt", "torch/lib/libshm/CMakeLists.txt", "torch/utils/cpp_extension.py"], "labels": []}, "87f29557bd": {"title": "Ignore logical_and and logical_or in op BC check for now (#30537)", "body": "Summary:\nGet the CI happy.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30537\n\nReviewed By: hl475\n\nDifferential Revision: D18738567\n\nPulled By: houseroad\n\nfbshipit-source-id: f30a87e22653b83ebdb1b54851460ec245866ecf", "pr_number": "30537", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": []}, "640109ae5d": {"title": "Back out \"Revert D18542342: Boxed variable dispatch\"", "body": "Summary: Original commit changeset: 082992125447\n\nTest Plan: waitforsandcastle\n\nReviewed By: akinh\n\nDifferential Revision: D18737627\n\nfbshipit-source-id: 7f3e32a6ee0c330002ae7fdcc8a35e8b540bb4db", "pr_number": null, "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "caffe2/c2_aten_srcs.bzl"], "labels": []}, "7ac8efa689": {"title": "Skip undefined tensors when moving torch::nn module to a different device (#30523)", "body": "Summary:\nThis fixes high-pri issues such as https://github.com/pytorch/pytorch/issues/30508 and https://github.com/pytorch/pytorch/issues/30462.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30523\n\nDifferential Revision: D18732904\n\nPulled By: yf225\n\nfbshipit-source-id: fe5a7a43838000f5803bd9c01ecfba0c3f02df5d", "pr_number": "30523", "files_changed": ["test/cpp/api/module.cpp", "torch/csrc/api/include/torch/nn/module.h"], "labels": ["module: cpp"]}, "1350b99de4": {"title": "Add local shutdown to process group agent (#30330)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30330\n\nThis is now possible due to previous changes made in `gloo` and `ProcessGroupGloo`. We `abort` the listener thread that is waiting for a message, and join all other threads. The API is changed so that the previous `wait_all_workers` does not destroy the agent, and this is now done in a new `shutdown` method. All callsites are updated appropriately.\n\nghstack-source-id: 94673884\nghstack-source-id: 94673884\n\nTest Plan: Unit tests pass.\n\nReviewed By: mrshenli\n\nDifferential Revision: D18661775\n\nfbshipit-source-id: 5aaa7c14603e18253394224994f6cd43234301c2", "pr_number": "30330", "files_changed": ["docs/source/notes/distributed_autograd.rst", "docs/source/rpc.rst", "test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/api.py"], "labels": []}, "f4e7e9039d": {"title": "Improve process_group_agent() serialization speed (#29785)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29785\n\nTLDR: This change improves process_group's serialization speed:\n  Serialize_Tensor64:     12.38us ->   1.99us  (~-84%)\n  Deserialize_Tensor64:   33.89us ->   5.62us  (~-84%)\n  Serialize_Tensor1M:    525.74us -> 285.43us  (~-45%)\n  Deserialize_Tensor1M:  892.61us -> 273.68us  (~-70%)\n\nAfter speaking with the jit team, we had consensus that torch::save()/load()\nare somewhat high-overhead for RPC serialization, mostly intended for\npersistent disk data.\n\n(Particularly, for large tensors, 35% of the time is spent in CRC checking, even\nwith the fb-side changes to subsitute 40x faster SSE-accelerated crc checking;\nAlso, for small tensors, the zip container overhead is considerable, as is the\noverhead of lexing/parsing an embedded text python program for each RPC).\n\nThe jit team encouraged us to use jit::pickler, with the WriteableTensorData\nway of outputting result tensors (not the default side-tensor table, or\nwith pickling the actual tensors). This ends up just pickling some tensor\nmetadata, and giving us some tensor blobs that we can mindlessly\nblit over the wire (they copy to cpu memory if needed).\n\nThere is yet no standardized container format for the pickled data\n(there is jit::pickle_save() checked in, but but it's experimental,\nno load function is yet provided), but they encouraged us to just use\nsomething sensible for this, and possibly revisit later. For now, I made\nthe directory headers slightly http-inspired.\n\nNote that serialization is just one component of the pipeline, but that\nsaid, we also see reasonable reductions in end-to-end echo times (noisier):\n   ProcessGroupAgent_Echo(Tensor_Small)   855.25us -> 492.65us  (~-42%)\n   ProcessGroupAgent_Echo(Tensor_1M)       10.82ms -> 6.94ms    (~-35%)\n   ProcessGroupAgent_Echo(Small_NoTensor) 688.82us -> 301.72us  (~-56%)\n   ProcessGroupAgent_Echo(1MB_NoTensor)     4.65ms -> 3.71ms    (~-20%)\n\nI moved the \"wire serialization\" logic to a separate file to assist with\nunittesting.\nghstack-source-id: 94694682\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/test/cpp/api:serialize\n  buck test mode/dev-nosan caffe2/test/...\n\nDifferential Revision: D18493938\n\nfbshipit-source-id: 07ddfe87dbe56472bc944f7d070627052c94a8f4", "pr_number": "29785", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/rpc/CMakeLists.txt", "test/cpp/rpc/test_wire_serialization.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h"], "labels": []}, "ec5e471647": {"title": "Reorganize rpc API doc and add introduction (#30491)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30491\n\nOur RPC API docs presents the APIs well but misses a general\nintroduction to the APIs. Readers might be a little lost the first\ntime landing this page. This commits reorganizes the APIs into\nfour components from user's perspective, RPC, RRef, dist autograd,\nand dist optimizer. It also adds an intro to each and briefly\ndiscribes why we provide those.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18723294\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4aced4ab537b070aa780aaaf9724659fd47cb3cb", "pr_number": "30491", "files_changed": ["docs/source/rpc.rst"], "labels": []}, "30d70d5378": {"title": "Make doc source format consistent in rpc/init.cpp", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30515\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18728184\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7b643c7f8225943113fbd7130ff6aadb30c1d4e9", "pr_number": "30515", "files_changed": ["torch/csrc/distributed/rpc/init.cpp"], "labels": []}, "dd52f50fc8": {"title": "Add examples to RRef doc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30516\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18728183\n\nPulled By: mrshenli\n\nfbshipit-source-id: af472ebed0e6dd0a85653b080abd3ac4d482bd26", "pr_number": "30516", "files_changed": ["torch/csrc/distributed/rpc/init.cpp"], "labels": []}, "53785771a7": {"title": "Don't build test_cpp_rpc if torch is built without distributed support (#30587)", "body": "Summary:\nOn the latest master, I get link errors when building one of the tests:\n\n```sh\n/home/pbell/git/pytorch/build/../test/cpp/rpc/test_wire_serialization.cpp:23:\nundefined reference to `torch::distributed::rpc::wireDeserialize(void const*, unsigned long)'\n```\n\nThis seems to be caused by PR https://github.com/pytorch/pytorch/issues/29785 not working with `USE_DISTRIBUTED=0`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30587\n\nDifferential Revision: D18758625\n\nPulled By: jjlilley\n\nfbshipit-source-id: 0ad0703acdbbac22bb4b8317370fbe2606fcb67e", "pr_number": "30587", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["open source"]}, "c780610f2d": {"title": "Disable test_backward_per_tensor in test_fake_quant (#30594)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30594\n\nThis testcase started breaking, clean up for the build.\nghstack-source-id: 94736837\n\nTest Plan: Unittest disabling change\n\nDifferential Revision: D18758635\n\nfbshipit-source-id: 05df1158ff0ccd75e401f352da529fb663b1cae0", "pr_number": "30594", "files_changed": ["test/test_fake_quant.py"], "labels": []}, "e6000a7c04": {"title": "Temporarily disable test_numerical_consistency_per_tensor (#30600)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30600\n\ntest_numerical_consistency_per_tensor in test_fake_quant is failing on Windows.\nghstack-source-id: 94742124\n\nTest Plan: CircleCI tests\n\nDifferential Revision: D18760287\n\nfbshipit-source-id: 7f59355eab74e811bb370ad2836ed2f1def1f621", "pr_number": "30600", "files_changed": ["test/test_fake_quant.py"], "labels": []}, "8ee61e0be4": {"title": "Fix CPU_INTEL flag error on windows (#30564)", "body": "Summary:\n${CMAKE_HOST_SYSTEM_PROCESSOR} get processor name by `uname -p` on linux and `%PROCESSOR_ARCHITECTURE%` on windows\n1. %PROCESSOR_ARCHITECTURE% has value in (AMD64|IA64|ARM64) for 64-bit processor, and (x86) for 32-bit processor\n2. `uname -p` has value like \"(x86_64|i[3-6]+86)\"\nWe cannot tell intel cpu from other cpus by ${CMAKE_HOST_SYSTEM_PROCESSOR}. It is the architecture, not provider.\ni. e. Intel CPU i7-9700K CPU on windows get \"AMD64\"\n\nreference:\n[MSDN](https://docs.microsoft.com/zh-cn/windows/win32/winprog64/wow64-implementation-details?redirectedfrom=MSDN)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30564\n\nDifferential Revision: D18763031\n\nPulled By: ezyang\n\nfbshipit-source-id: 11ae20e66b4b89bde1dcf4df6177606a3374c671", "pr_number": "30564", "files_changed": ["CMakeLists.txt"], "labels": ["merge-this-please"]}, "b68d1fc316": {"title": "add small input shapes to some ops (#30617)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30617\n\nas title\n\nTest Plan: buck run //caffe2/benchmarks/operator_benchmark:benchmark_all_test -- --iterations 1 --operator add,as_strided,cat,chunk,fill,linear,matmul,split\n\nReviewed By: hl475\n\nDifferential Revision: D18764248\n\nfbshipit-source-id: 510cf83542822acfa1b7b5e475b0cc7432f7ac19", "pr_number": "30617", "files_changed": ["benchmarks/operator_benchmark/pt/add_test.py", "benchmarks/operator_benchmark/pt/as_strided_test.py", "benchmarks/operator_benchmark/pt/cat_test.py", "benchmarks/operator_benchmark/pt/chunk_test.py", "benchmarks/operator_benchmark/pt/fill_test.py", "benchmarks/operator_benchmark/pt/linear_test.py", "benchmarks/operator_benchmark/pt/matmul_test.py", "benchmarks/operator_benchmark/pt/split_test.py"], "labels": ["fb-exported"]}, "6deb41c88d": {"title": "Update magma to 2.5.1 for Windows and switch CUDA in CI to 9.2", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30513\n\nDifferential Revision: D18764184\n\nPulled By: ezyang\n\nfbshipit-source-id: 4992869fd6a89471a5d25eb6a9b44ad8eceb480f", "pr_number": "30513", "files_changed": [".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_magma.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", "docs/source/notes/windows.rst"], "labels": []}, "1111a6b810": {"title": "Use pybind11::gil_scoped_* functions instead of AutoGIL/AutoNoGIL (#30274)", "body": "Summary:\nReland of https://github.com/pytorch/pytorch/pull/29095\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30274\n\nDifferential Revision: D18762293\n\nPulled By: ezyang\n\nfbshipit-source-id: d3d50c2dd12bcb678ab25fa708eb6587cc4b66f9", "pr_number": "30274", "files_changed": ["tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_nn_functions.h", "tools/autograd/templates/python_nn_functions_dispatch.h", "tools/autograd/templates/python_torch_functions.cpp", "tools/autograd/templates/python_torch_functions_dispatch.h", "tools/autograd/templates/python_variable_methods.cpp", "tools/autograd/templates/python_variable_methods_dispatch.h", "tools/build_variables.py", "torch/csrc/Exceptions.cpp", "torch/csrc/Exceptions.h", "torch/csrc/README.md", "torch/csrc/autograd/python_anomaly_mode.cpp", "torch/csrc/autograd/python_anomaly_mode.h", "torch/csrc/autograd/python_cpp_function.cpp", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_function.cpp", "torch/csrc/autograd/python_function.h", "torch/csrc/autograd/python_hook.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/cuda/Event.cpp", "torch/csrc/cuda/Module.cpp", "torch/csrc/cuda/Stream.cpp", "torch/csrc/cuda/python_comm.cpp", "torch/csrc/cuda/python_nccl.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/jit/init.cpp", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/python_interpreter.cpp", "torch/csrc/jit/python_ir.cpp", "torch/csrc/jit/python_tracer.cpp", "torch/csrc/jit/script/init.cpp", "torch/csrc/utils/auto_gil.h", "torch/csrc/utils/cuda_lazy_init.cpp", "torch/csrc/utils/init.cpp", "torch/csrc/utils/tensor_list.cpp", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_numpy.cpp", "torch/csrc/utils/throughput_benchmark.cpp"], "labels": ["jit"]}, "d32f261f16": {"title": "make the order btw div and mul in adagrad update consistent (#30449)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30449\n\nThere was an inconsistency in the order of operation between scalar and SIMD code when we compute Adagrad.\nIn this diff we first compute effective_lr = lr / (sqrt(moment) + epsilon) and then multiply with gradient.\n\nTest Plan: CI\n\nReviewed By: protonu\n\nDifferential Revision: D18703416\n\nfbshipit-source-id: 2a8b2a3f5401466549561412bd22f07abac3c598", "pr_number": "30449", "files_changed": ["caffe2/perfkernels/adagrad.h", "caffe2/perfkernels/adagrad_avx.cc", "caffe2/sgd/adagrad_op.h"], "labels": ["fb-exported"]}, "3636cb0364": {"title": "windows build (#30556)", "body": "Summary:\nbased on https://github.com/pytorch/pytorch/pull/28677\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30556\n\nDifferential Revision: D18764040\n\nPulled By: mingbowan\n\nfbshipit-source-id: 53104636800f5887b74a82c154bc5e9603de9322", "pr_number": "30556", "files_changed": [".circleci/config.yml", ".circleci/generate_config_yml.py", ".circleci/verbatim-sources/header-section.yml", ".circleci/verbatim-sources/pytorch-job-specs.yml", ".circleci/verbatim-sources/windows-build-test.yml", ".jenkins/pytorch/win-build.sh", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", ".jenkins/pytorch/win-test.sh", "test/test_cuda.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "9082123038": {"title": "Back out \"Back out \"Revert D18542342: Boxed variable dispatch\"\"", "body": "Summary: Original commit changeset: 7f3e32a6ee0c\n\nTest Plan: waitforsandcastle\n\nReviewed By: ezyang\n\nDifferential Revision: D18766763\n\nfbshipit-source-id: 51bb7aac7cb7ce3df94681e838949e7a156e3ad9", "pr_number": null, "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "caffe2/c2_aten_srcs.bzl"], "labels": []}, "569729527b": {"title": "Turn off scalar_checks for exp, cos, cosh, tan, atan, tanh, erf, erfc. (#30434)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30434\n\nThese are all pointwise ops that are implemented correctly wrt shapes in THC.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18699087\n\nPulled By: gchanan\n\nfbshipit-source-id: 82cb91b00c77bfaca75be497c87fc7ae52daf46c", "pr_number": "30434", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "19b7d49fac": {"title": "Add TOC to CONTRIBUTING.md (#29671)", "body": "Summary:\nThis TOC is manually generated but `CONTRIBUTING.md` seems like its\nstable enough for that to be okay\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29671\n\nPulled By: driazati\n\nDifferential Revision: D18771604\n\nfbshipit-source-id: 0d6c9c6cf1083d3be413219d3cead79c2fe5050b", "pr_number": "29671", "files_changed": ["CONTRIBUTING.md"], "labels": []}, "9c02b88791": {"title": "Add pickler support for Device (#30131)", "body": "Summary:\nThis PR adds (un)pickling support for `c10::Device`. It also adds `torch.device` as a type annotation for device attributes.\n](https://our.intern.facebook.com/intern/diff/18664421/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30131\n\nPulled By: driazati\n\nDifferential Revision: D18664421\n\nfbshipit-source-id: 64378fb42b2d1bbe2bd86259e5ed10f24b5d1e49", "pr_number": "30131", "files_changed": ["aten/src/ATen/core/Dict_inl.h", "test/test_jit.py", "torch/csrc/jit/pickler.cpp", "torch/csrc/jit/pickler.h", "torch/csrc/jit/python_ir.cpp", "torch/csrc/jit/unpickler.cpp", "torch/csrc/jit/unpickler.h", "torch/jit/annotations.py"], "labels": ["jit"]}, "98ab55fc51": {"title": "PRAGMA missing for clang (#30351)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30351\n\nNot sure what proper fix is, clang is having trouble with the loop pragmas. This at least gets things compiling.\nghstack-source-id: 94458450\n\nTest Plan: CI passes\n\nDifferential Revision: D18665812\n\nfbshipit-source-id: b8a899ce4138010cbe308eaa2c0838dd9e15573f", "pr_number": "30351", "files_changed": ["aten/src/TH/generic/THTensorApply.hpp"], "labels": []}, "2d0a4e42e9": {"title": "Add barriers to fix flaky test_graph_for_py_nested_call and (#30624)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30624\n\nThese tests were flaky since we would end up calling the 'verify'\nmethods before some of the RPCs were done. The `check_rpc_done` function might\nnot guarantee this since set_rpc_done sets an appropriate flag in python which\ncauses `check_rpc_done` to pass. Although, there are a few steps after that\nlike attaching the send functions for the response of the RPC that might not\nhave executed by then.\nghstack-source-id: 94781954\n\nTest Plan: Run the tests 100 times.\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D18768786\n\nfbshipit-source-id: a14c3f4b27de14fe5ecc6e90854dc52652f769b8", "pr_number": "30624", "files_changed": ["test/dist_autograd_test.py"], "labels": []}, "968c0d4a46": {"title": "Add support for converting quantized AvgPool2d and Reshape operations (#30490)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30490\n\nAdd symbolic mapping to Int8AvgPool2d and Int8Reshape op in C2\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps\n\nImported from OSS\n\nDifferential Revision: D18740520\n\nfbshipit-source-id: 1606125500c4b549fbc984e7929b7fd5204396a0", "pr_number": "30490", "files_changed": ["caffe2/onnx/backend.cc", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/onnx/symbolic_caffe2.py"], "labels": []}, "9e3d19412b": {"title": "Disable implicit conversion warning (#30529)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30529\n\nWe started to see build failures for multiple services with top-of-trunk LLVM compiler. The failures point to a warning that was treated as error for implicit conversion from long to double. Per discussion on D18642524, I'm disabling this warning from the containing TARGET file. T58053069 opened for code owner to track this - a proper source code fix and more unit test is needed.\n\nTest Plan: local build, sandcastle\n\nReviewed By: smessmer\n\nDifferential Revision: D18668396\n\nfbshipit-source-id: 28c0ff3258c5ba3afd41a0053f9fe1b356a496a8", "pr_number": "30529", "files_changed": ["c10/util/Half.h"], "labels": ["fb-exported"]}, "db81e13d6b": {"title": "Fix TCPStoreTest and improve tcputils::connect() (#30354)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30354\n\nTCPStoreTest would timeout since the TCPStore constructor for the\nserver would block the main thread waiting for workers. The workers themselves\nwere spawned later on once the server store is created. As a result, this test\nwould always timeout.\n\nTo fix the test, I moved the server store to a thread so that the workers can\nregister with the server in parallel.\n\nIn addition to this made a few improvements to tcputils::connect. When\ntcputils::connect() encountered an exception, it always looked at `errno` for\nthe error code. In some cases `errno` could be overwritten and the real error\ncode would be stored in `std::system_error`. As a result, I've modified the\ncode to look at the error code in `std::system_error` if we catch an exception\nof that type.\nghstack-source-id: 94758939\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18668454\n\nfbshipit-source-id: d5a3c57b066b094bfecda9a79d9d31bfa32e17f0", "pr_number": "30354", "files_changed": ["torch/lib/c10d/Utils.cpp", "torch/lib/c10d/test/CMakeLists.txt", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": []}, "4dab29a2bd": {"title": "Fix serialization memory lifetime issue. (#30603)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30603\n\nPickler object needs to be kept in scope until data is written out to the\nfinal serialized string. tensorData in particular is a reference to memory\nowned by the descoped Pickle object.\n\nNoticed this by inspection. In practice, this potential read-after-free here\nis limited to non-cpu tensors, and any such use was very soon after free.\nghstack-source-id: 94756036\n\nTest Plan: existing test suite at buck test mode/dev-nosan caffe2/test:rpc_fork\n\nDifferential Revision: D18760463\n\nfbshipit-source-id: 9de890d66626aa48f13ca376dd9bd50b92e0cb00", "pr_number": "30603", "files_changed": ["torch/csrc/distributed/rpc/utils.cpp"], "labels": []}, "0bebfe2143": {"title": "Add the explicit per-tensor/per-channel quant info when we print the module (#30591)", "body": "Summary:\nAs Title says. We would like to explicitly distinguish per-tensor/per-channel scheme when we print the module.\n\nHere is an example for Lenet after applying the per-channel dynamic quantization:\n\nBefore this PR:\n```\nFloatModel(\n  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): DynamicQuantizedLinear(\n    in_features=800, out_features=500\n    (_packed_params): LinearPackedParams()\n  )\n  (fc2): DynamicQuantizedLinear(\n    in_features=500, out_features=10\n    (_packed_params): LinearPackedParams()\n  )\n)\n```\n\nAfter this PR:\n```\nFloatModel(\n  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): DynamicQuantizedLinear(\n    in_features=800, out_features=500, qscheme=torch.per_channel_affine\n    (_packed_params): LinearPackedParams()\n  )\n  (fc2): DynamicQuantizedLinear(\n    in_features=500, out_features=10, qscheme=torch.per_channel_affine\n    (_packed_params): LinearPackedParams()\n  )\n)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30591\n\nDifferential Revision: D18764366\n\nPulled By: jianyuh\n\nfbshipit-source-id: e897ab42ace6b82b2a90729ba788313c7873de1a", "pr_number": "30591", "files_changed": ["torch/nn/quantized/dynamic/modules/linear.py", "torch/nn/quantized/modules/linear.py"], "labels": []}, "e7fe64f6a6": {"title": "Fix typos (#30606)", "body": "Summary:\nShould be non-semantic.\n\nUses https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines to find likely typos.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30606\n\nDifferential Revision: D18763028\n\nPulled By: mrshenli\n\nfbshipit-source-id: 896515a2156d062653408852e6c04b429fc5955c", "pr_number": "30606", "files_changed": [".jenkins/caffe2/build.sh", "aten/src/ATen/core/MT19937RNGEngine.h", "aten/src/ATen/core/PhiloxRNGEngine.h", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/dlpack.h", "aten/src/ATen/native/Linear.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cpu/avx_mathfun.h", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/LossCTC.cu", "aten/src/ATen/native/cuda/SpectralOps.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/test/test_assert.h", "aten/src/TH/THTensorApply.h", "benchmarks/fastrnns/README.md", "binaries/benchmark_helper.cc", "binaries/convert_and_benchmark.cc", "c10/core/TensorTypeId.h", "c10/core/TensorTypeSet.h", "c10/test/util/bfloat16_test.cpp", "c10/util/Deprecated.h", "c10/util/Half.h", "c10/util/tempfile.h", "caffe2/contrib/tensorrt/tensorrt_op_trt.cc", "caffe2/core/blob_serialization.h", "caffe2/core/context_gpu.cu", "caffe2/core/context_gpu.h", "caffe2/core/net_dag_utils.cc", "caffe2/core/nomnigraph/include/nomnigraph/Transformations/SubgraphMatcher.h", "caffe2/core/operator.h", "caffe2/core/plan_executor.cc", "caffe2/mobile/contrib/ios/ios_caffe_predictor.h", "caffe2/mobile/contrib/ios/mpscnn/mpscnn.mm", "caffe2/operators/concat_split_op.h", "caffe2/operators/gather_ranges_to_dense_op.cc", "caffe2/operators/load_save_op_util.cc", "caffe2/operators/quantized/int8_roi_align_op.h", "caffe2/operators/reducer_functors.h", "caffe2/operators/roi_align_gradient_op.cc", "caffe2/operators/roi_align_gradient_op.cu", "caffe2/operators/roi_align_op.cc", "caffe2/operators/roi_align_op.cu", "caffe2/operators/roi_align_rotated_gradient_op.cu", "caffe2/operators/roi_align_rotated_op.cc", "caffe2/operators/roi_align_rotated_op.cu", "caffe2/operators/segment_reduction_op.h", "caffe2/operators/stats_put_ops.cc", "caffe2/operators/stats_put_ops.h", "caffe2/operators/text_file_reader.cc", "caffe2/operators/utility_ops.h", "caffe2/opt/converter.cc", "caffe2/opt/onnxifi_transformer.cc", "caffe2/predictor/emulator/data_filler.h", "caffe2/proto/caffe2.proto", "caffe2/python/crf.py", "caffe2/python/dlpack.h", "caffe2/python/hypothesis_test.py", "caffe2/python/layer_model_helper.py", "caffe2/python/layers/batch_lr_loss.py", "caffe2/python/layers/feature_sparse_to_dense.py", "caffe2/python/lstm_benchmark.py", "caffe2/python/memonger.py", "caffe2/python/model_helper.py", "caffe2/python/modeling/gradient_clipping.py", "caffe2/python/models/resnet.py", "caffe2/python/modifier_context.py", "caffe2/python/normalizer_context.py", "caffe2/python/onnx/backend.py", "caffe2/python/operator_test/dataset_ops_test.py", "caffe2/python/operator_test/gather_ops_test.py", "caffe2/python/operator_test/pooling_test.py", "caffe2/python/optimizer_context.py", "caffe2/python/optimizer_test_util.py", "caffe2/python/pipeline.py", "caffe2/python/regularizer.py", "caffe2/python/regularizer_context.py", "caffe2/python/schema.py", "caffe2/python/session.py", "caffe2/python/task.py", "caffe2/quantization/server/dnnlowp_test_utils.py", "caffe2/serialize/inline_container.h", "caffe2/sgd/clip_tensor_op.cc", "caffe2/video/video_decoder.cc", "docs/source/community/contribution_guide.rst", "docs/source/distributed.rst", "docs/source/hub.rst", "docs/source/name_inference.rst", "modules/detectron/smooth_l1_loss_op.cc", "scripts/fbcode-dev-setup/onnx_c2_setup.sh", "test/common_utils.py", "test/cpp/api/dataloader.cpp", "test/cpp/api/nn_utils.cpp", "test/cpp/jit/test_utils.h", "test/dist_autograd_test.py", "test/run_test.py", "test/test_cpp_api_parity.py", "test/test_nn.py", "test/test_quantization.py", "test/test_quantized.py", "tools/autograd/utils.py", "tools/clang_tidy.py", "tools/jit/gen_jit_dispatch.py", "torch/_jit_internal.py", "torch/_torch_docs.py", "torch/backends/cudnn/__init__.py", "torch/csrc/DataLoader.cpp", "torch/csrc/api/include/torch/data/dataloader/stateful.h", "torch/csrc/api/include/torch/ordered_dict.h", "torch/csrc/autograd/profiler.cpp", "torch/csrc/distributed/autograd/context/context.h", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/generic/StorageMethods.cpp", "torch/csrc/jit/argument_spec.cpp", "torch/csrc/jit/fuser/cpu/temp_file.h", "torch/csrc/jit/passes/alias_analysis.h", "torch/csrc/jit/passes/create_autodiff_subgraphs.cpp", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/python_ir.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/register_special_ops.cpp", "torch/csrc/jit/script/compiler.cpp", "torch/csrc/jit/script/module.h", "torch/csrc/jit/script/object.h", "torch/csrc/jit/script/parser.cpp", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/csrc/jit/unpickler.cpp", "torch/cuda/__init__.py", "torch/distributed/launch.py", "torch/jit/__init__.py", "torch/lib/libshm/err.h", "torch/multiprocessing/reductions.py", "torch/nn/functional.py", "torch/nn/modules/fold.py", "torch/nn/parallel/scatter_gather.pyi", "torch/nn/utils/prune.py", "torch/onnx/__init__.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset11.py", "torch/utils/checkpoint.py"], "labels": ["jit"]}, "18ec4632b3": {"title": "Exclude undefined tensors in the result of Module::parameters() / named_paramters() / buffers() / named_buffers() (#30626)", "body": "Summary:\nPR https://github.com/pytorch/pytorch/pull/30523 attempted to fix https://github.com/pytorch/pytorch/issues/30508 and https://github.com/pytorch/pytorch/issues/30462, but the fix wasn't complete. This PR makes the following improvements:\n1. Fixes https://github.com/pytorch/pytorch/issues/30508 and https://github.com/pytorch/pytorch/issues/30462 properly by excluding undefined tensors in the result of `Module::parameters()` / `named_parameters()` / `buffers()` / `named_buffers()`, which mirrors the Python API behavior.\n2. Audits all use sites of `Module::parameters_` / `buffers_` and change them to `Module::named_parameters(/*recurse=*/false)` / `named_buffers(/*recurse=*/false)` when appropriate, so that use sites of module parameters / buffers never need to worry about undefined tensors.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30626\n\nDifferential Revision: D18777507\n\nPulled By: yf225\n\nfbshipit-source-id: 55b64b69779e1186342efd3c44857f416334ed6b", "pr_number": "30626", "files_changed": ["test/cpp/api/module.cpp", "test/cpp/api/support.h", "torch/csrc/api/include/torch/nn/cloneable.h", "torch/csrc/api/include/torch/nn/module.h", "torch/csrc/api/include/torch/nn/parallel/data_parallel.h", "torch/csrc/api/src/nn/module.cpp"], "labels": ["module: cpp"]}, "e5b947a3a8": {"title": "Raise an error for is_signed on quantized types (#30527)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30527\n\nWhen we introduced dtype.is_signed we allowed for support of\nquantized types, but we're not sure what the correct result should be.\n\nSee discussion at https://github.com/pytorch/pytorch/pull/29511\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18765410\n\nPulled By: nairbv\n\nfbshipit-source-id: c87cfe999b604cfcbbafa561e04d0d5cdbf41e6d", "pr_number": "30527", "files_changed": ["c10/core/ScalarType.h", "test/test_torch.py", "torch/csrc/Dtype.cpp"], "labels": []}, "61798865e3": {"title": "Turn off scalar_checks for torch.clamp. (#30435)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30435\n\nThe underlying THC implementations are correct.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18699089\n\nPulled By: gchanan\n\nfbshipit-source-id: f5d1319bf48eae36903296dad0b98ed80661f732", "pr_number": "30435", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "8b29701ae5": {"title": "Turn off scalar_checks for _th_reciprocal. (#30436)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30436\n\nThe underlying TH implementation is correct.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18699088\n\nPulled By: gchanan\n\nfbshipit-source-id: e75a588ae4afb0506922ba98208546d5c0de623a", "pr_number": "30436", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "b446572997": {"title": "TestCppExtension now removes /tmp/torch_extensions folder so that it can be used by other users in a multi-user environment. (#30095)", "body": "Summary:\nPrevious behaviour: a user runs tests from `TestCppExtension` class so that `/tmp/torch_extensions` is created under her ownership and not removed afterwards,\nthen the other user's run of the same tests might result in 'Permission denied' exception upon deleting `/tmp/torch_extensions`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30095\n\nDifferential Revision: D18770234\n\nPulled By: ezyang\n\nfbshipit-source-id: 4c6b972e4c4327a94c8b4bf6b0b9998a01c218bb", "pr_number": "30095", "files_changed": ["test/test_cpp_extensions.py"], "labels": ["open source", "triaged"]}, "f9f54201d3": {"title": "Remove deprecated fromIvalue in RRefForkData", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30646\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18777610\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7a749c1035e36bbb464332d3829fd53e2c6cf727", "pr_number": "30646", "files_changed": ["torch/csrc/distributed/rpc/rref.cpp", "torch/csrc/distributed/rpc/rref.h"], "labels": []}, "5a484245d9": {"title": "Change test_invalid_names test to only test constructor of WorkerInfo (#30620)", "body": "Summary:\nThis tests seems to only test that we throw exceptions in the `WorkerInfo` constructor when invalid names are passed in, so I don't think we need to complicate by initializing RPC, and exposing ourselves to potential flakiness.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30620\n\nDifferential Revision: D18766955\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 11643de4d57431e5f46e096c7766de3ab0b9b05a", "pr_number": "30620", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp"], "labels": []}, "d5c136097a": {"title": "improve .view() performance (#30554)", "body": "Summary:\nImprove .view() performance by not calling set_ and instead restriding returned alias. This improves performance of .view() operation from ~500ns to ~360 ns\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30554\n\nTest Plan: covered by existing tests\n\nDifferential Revision: D18759896\n\nPulled By: ngimel\n\nfbshipit-source-id: 9757c93158bc55e9c87dc30ac3415ba8f8b849e5", "pr_number": "30554", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "aten/src/TH/THTensor.cpp", "aten/src/TH/THTensor.hpp"], "labels": []}, "89be1a22d4": {"title": "split getInvokedMethods (#30546)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30546\n\nfactor out this function for later support of quantizing shared types\n\nTest Plan:\ntest_jit.py, test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18776304\n\nfbshipit-source-id: f5a736b0f69019cefe17ec4517da1ae5462f78e1", "pr_number": "30546", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "4d30415f12": {"title": "Add ONNX Scripting Conv Support (#30618)", "body": "Summary:\nConvolution nodes are traced as aten:_convolution and are currently supported in ONNX.\nScripting convolution uses aten:conv<1,2,3>d which are currently not supported in ONNX.\nThis PR adds the symbolics for aten:conv<1,2,3>d and aten:conv_transpose<1,2,3>d\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30618\n\nReviewed By: hl475\n\nDifferential Revision: D18778145\n\nPulled By: houseroad\n\nfbshipit-source-id: 4af0379f29974a1ce8443024d1d87b3eb8d2dd36", "pr_number": "30618", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": []}, "a997f224ac": {"title": "Add torch.multiprocessing.create_processes", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28493\n\nDifferential Revision: D18766066\n\nPulled By: ailzhang\n\nfbshipit-source-id: 7f424c8fae3012be2416cf9bc72ee2dde40c1f89", "pr_number": "28493", "files_changed": ["test/test_multiprocessing_spawn.py", "torch/multiprocessing/__init__.py", "torch/multiprocessing/spawn.py"], "labels": ["module: multiprocessing"]}, "9740011f10": {"title": "Use normal dispatch to get to CUDA threshold kernels, instead of DispatchStub. (#30307)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30307\n\nDispatchStub will stop working when I split CPU/CUDA libraries, because\nthere are some symbols from the templates in DispatchStub stubs which aren't\nproperly exported and I couldn't figure out how to make them dispatch properly.\n\nThis is the only case where DispatchStub is being used to dispatch to CUDA,\nanyway.\n\nThis partially addresses #29844 but I need to also just completely delete\nthe CUDA registration logic from DispatchStub entirely.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762362\n\nPulled By: ezyang\n\nfbshipit-source-id: bdfa8739c0daf23badf3c5af61890a934af00813", "pr_number": "30307", "files_changed": ["aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": []}, "08394cede3": {"title": "DEFINE_DISPATCH in the correct namespace. (#30308)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30308\n\nDispatch is declared in non-anonymous namespace, so it definitely\nshouldn't be defined in an anonymous namespace.  This doesn't seem\nto matter today, but it matters when we split libtorch into two\nlibraries.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762361\n\nPulled By: ezyang\n\nfbshipit-source-id: 484f0fab183c385dd889db9dad3e48e92e0a3900", "pr_number": "30308", "files_changed": ["aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp", "aten/src/ATen/native/quantized/cpu/q_avgpool.cpp", "aten/src/ATen/native/quantized/cpu/qconcat.cpp", "aten/src/ATen/native/quantized/cpu/qpool.cpp"], "labels": []}, "a5b1f6e7d7": {"title": "Add missing _API definitions. (#30310)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30310\n\n- Annotate CUDAGenerator.h with correct TORCH_CUDA_API.\n  This is actually CUDA related functionality with its implementation living\n  in the cuda/ folder.  For some reason it lives at the top level; it\n  should be moved (but that should be handled in another PR.)\n- Add missing TORCH/CAFFE_API annotations to.  All of\n  these functions are used from CUDA code, which means that\n  we need to correctly annotate them if we split CPU/CUDA code\n  into separate libraries.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762357\n\nPulled By: ezyang\n\nfbshipit-source-id: c975a8e4f082fe9f4196c2cca40977623caf4148", "pr_number": "30310", "files_changed": ["aten/src/ATen/CUDAGenerator.h", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/native/sparse/SparseTensorMath.h", "caffe2/operators/load_save_op_util.h"], "labels": []}, "d43e205026": {"title": "Properly include declaration of dispatch in file that registers it. (#30311)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30311\n\nmultinomial_stub must be in scope to register against it.  Somehow,\nthis works today, but when I split torch_cpu and torch_cuda it\ndoesn't.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762358\n\nPulled By: ezyang\n\nfbshipit-source-id: ef9c111292cd02d816af1c94c8bbaadabffaabe5", "pr_number": "30311", "files_changed": ["aten/src/ATen/native/cpu/MultinomialKernel.cpp"], "labels": []}, "8269f7b652": {"title": "Delete redundant THC_API on THCStorage_new (#30312)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30312\n\nIt's not necessary because it's already defined in the header.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762363\n\nPulled By: ezyang\n\nfbshipit-source-id: 418bf355d460dd171ac449559f20bf55415e54ae", "pr_number": "30312", "files_changed": ["aten/src/THC/THCStorage.cpp"], "labels": []}, "a009fc14be": {"title": "Workaround hcc bug regarding extern \"C\" definitions (#30313)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30313\n\nSee comments in code about the bug.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762360\n\nPulled By: ezyang\n\nfbshipit-source-id: 406a01f2f0c3722b381428c89afd67b3c3c19142", "pr_number": "30313", "files_changed": ["aten/src/THC/THCTensorRandom.cu"], "labels": []}, "1b12fd33ed": {"title": "Add missing trigramma_stub definition. (#30314)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30314\n\nSomehow we forgot to define it!\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762356\n\nPulled By: ezyang\n\nfbshipit-source-id: 28afc605ad986266071e3831049ec8a7f71fd695", "pr_number": "30314", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp"], "labels": []}, "f114c33e69": {"title": "Fix iOS CI (#30327)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30327\n\n### Summary\n\nSeems like starting from macOS 10.15, we can no longer get access to the `Downloads` folder in our macOS machines.\n\n```\npermissionError: [Errno 1] Operation not permitted: '/Users/distiller/Downloads'\n```\n\nThe fix is to change the conda download directory to ${HOME}\n\n### Test Plan\n\n- iOS jobs are back to normal\n- Don't break other jobs\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18717380\n\nPulled By: xta0\n\nfbshipit-source-id: cad754076bf4ae5035741aa57a310ad87c76726e", "pr_number": "30327", "files_changed": [".circleci/config.yml", ".circleci/scripts/binary_ios_build.sh", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-nightly-ios-binary-builds.yml", ".circleci/verbatim-sources/workflows-pytorch-ios-builds.yml", "ios/TestApp/README.md", "ios/TestApp/TestApp.xcodeproj/project.pbxproj", "ios/TestApp/TestAppTests/TestAppTests.mm", "ios/TestApp/benchmark/setup.rb"], "labels": []}, "7023e13fbb": {"title": "Fix mapping white list (#30636)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30636\n\nCurrently DeQuantStub is still in whitelist because set union has\nlower precedence than set difference\nfix issue: https://github.com/pytorch/pytorch/issues/29646\n\nTest Plan:\nverified locally that we don't attach qconfig for DeQuantStub\n\nImported from OSS\n\nDifferential Revision: D18775275\n\nfbshipit-source-id: 8da07e40963555671b3d4326c9291706103f858e", "pr_number": "30636", "files_changed": ["torch/quantization/default_mappings.py"], "labels": []}, "1b5ce05924": {"title": "don't use size()/stride() functions in TensorImpl, use size_[d]/stride_[d] instead (#30452)", "body": "Summary:\nThis improved multi-d microbenchmark by ~100 ns, empty_tensor_restride used to be 13% of iteration time, now about 5%\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30452\n\nTest Plan: Covered by existing tests\n\nDifferential Revision: D18704233\n\nPulled By: ngimel\n\nfbshipit-source-id: be527f09183bc31e9d1f63fd49bfbe0998fe167f", "pr_number": "30452", "files_changed": ["c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h"], "labels": ["fb-exported"]}, "19cd90d303": {"title": "Globally record observer nodes (#30547)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30547\n\natt\n\nTest Plan:\ntest_jit.py test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18784752\n\nfbshipit-source-id: 000e140aa86ff12a240d98da71871a5a5053401f", "pr_number": "30547", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "40146eb48e": {"title": "Skip ProcessGroupGlooAyncTest if there is no CUDA available (#30345)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30345\n\nSkip ProcessGroupGlooAyncTest if there is no CUDA available, otherwise in sandcastle non GPU host the test will abort with failing to load CUDA library\nghstack-source-id: 94771241\n\nTest Plan: test skipped on non GPU host\n\nDifferential Revision: D18665322\n\nfbshipit-source-id: 8c7b89aeecc6ec007bee12d864a6058384254e61", "pr_number": "30345", "files_changed": ["torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp"], "labels": []}, "aff693ab1c": {"title": "Ensure MIOpen is called on same stream as operator for RNN (#30672)", "body": "Summary:\nTo ensure synchronization between copying of weights in RNN wei buf, and the operation, both the pyTorch operator as well as underlying MIOpen call must be on the same HIP stream. This is also consistent with MIOpen calls in other pyTorch operators\n\nezyang iotamudelta\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30672\n\nDifferential Revision: D18785683\n\nPulled By: bddppq\n\nfbshipit-source-id: 144611046cb70cfe450680295734203f253ac6e2", "pr_number": "30672", "files_changed": ["aten/src/ATen/native/miopen/RNN_miopen.cpp"], "labels": ["module: rocm", "open source"]}, "bc2e6d10fa": {"title": "Back out \"Revert D17908478: Switch PyTorch/Caffe2 to C++14\"", "body": "Summary: Original commit changeset: 775d2e29be0b\n\nTest Plan: CI\n\nReviewed By: mruberry\n\nDifferential Revision: D18775520\n\nfbshipit-source-id: a350b3f86b66d97241f208786ee67e9a51172eac", "pr_number": null, "files_changed": ["CMakeLists.txt", "CONTRIBUTING.md", "android/pytorch_android/CMakeLists.txt", "aten/src/ATen/cpu/tbb/CMakeLists.txt", "aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt", "aten/src/ATen/test/test_install/CMakeLists.txt", "c10/CMakeLists.txt", "c10/util/C++17.h", "caffe2/operators/deform_conv_op_impl.h", "cmake/Dependencies.cmake", "cmake/MiscCheck.cmake", "cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake", "cmake/TorchConfig.cmake.in", "cmake/public/cuda.cmake", "cmake/public/utils.cmake", "docs/cpp/source/installing.rst", "ios/LibTorch.podspec", "setup.py", "test/custom_operator/CMakeLists.txt", "torch/csrc/jit/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/fuser/cuda/fused_kernel.cpp", "torch/lib/c10d/CMakeLists.txt", "torch/lib/libshm/CMakeLists.txt", "torch/utils/cpp_extension.py"], "labels": []}, "980aead1f8": {"title": "Add support for quantized slice conversion (#30498)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30498\n\nUpdated Int8SliceOp to accept dim, start and end index similar to Pytorch.\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_slice\n\nImported from OSS\n\nDifferential Revision: D18740519\n\nfbshipit-source-id: 2313f37a4936edb150ce04911b241e591e191801", "pr_number": "30498", "files_changed": ["caffe2/operators/quantized/int8_slice_op.cc", "caffe2/operators/quantized/int8_slice_op.h", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/onnx/symbolic_caffe2.py"], "labels": []}, "4e6379379c": {"title": "fetch before checking out PR tip", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30680\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18796189\n\nPulled By: suo\n\nfbshipit-source-id: 99da48e5fd510ffdf4e606c2393eb55d4f6ca8d5", "pr_number": "30680", "files_changed": [".github/workflows/lint.yml"], "labels": []}, "4d4d8e0dce": {"title": "Update persons_of_interest.rst (#30647)", "body": "Summary:\nAdding back the 3 names for the MSFT team - re: ONNX Governance.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30647\n\nDifferential Revision: D18781163\n\nPulled By: jlin27\n\nfbshipit-source-id: 7284ba29841ab41b9807c9d92694630b50de7b6a", "pr_number": "30647", "files_changed": ["docs/source/community/persons_of_interest.rst"], "labels": []}, "604a27361f": {"title": "remove tuple_parser (#30659)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30659\n\nI could only find one usage of TupleParser and it doesn't seem worth maintaining just for that one usage.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18795979\n\nPulled By: nairbv\n\nfbshipit-source-id: 6e50d65fc8fade0944f36ab20d00f1539a3d4cb8", "pr_number": "30659", "files_changed": ["tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/autograd/functions/init.cpp", "torch/csrc/utils/tuple_parser.cpp", "torch/csrc/utils/tuple_parser.h"], "labels": []}, "03a73cb9ac": {"title": "Remove namespace F = torch::nn::functional from torch/nn/modules/batchhnorm.h (#30684)", "body": "Summary:\nThis PR removes `namespace F = torch::nn::functional` from `torch/nn/modules/batchhnorm.h`, so that people don't have to define `torch::nn::functional` as `F` if they don't want to.\n\nFixes https://github.com/pytorch/pytorch/issues/30682.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30684\n\nDifferential Revision: D18795717\n\nPulled By: yf225\n\nfbshipit-source-id: c9feffbeb632cc6b4ce3e6c22c0a78533bab69ad", "pr_number": "30684", "files_changed": ["torch/csrc/api/include/torch/nn/modules/batchnorm.h", "torch/csrc/api/include/torch/nn/modules/instancenorm.h"], "labels": []}, "d4c25add45": {"title": "make sure the counter stays correct in between bailout transitions (#30186)", "body": "Summary:\nThis fixes the second issue reported in https://github.com/pytorch/pytorch/issues/29909 namely, a loop counter is assigned the wrong values after transitioning to a bailout graph.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30186\n\nDifferential Revision: D18646845\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 1f7c601dd9f35892979385ffa132fb0886a4f203", "pr_number": "30186", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/bailout_graph.cpp"], "labels": ["jit"]}, "2ba03e0287": {"title": "Enable test_trainer_ps in dist_autograd_test.py", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30341\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18769574\n\nPulled By: mrshenli\n\nfbshipit-source-id: caf25742fa1fc9dbf6486f5ec981fae3f29784bc", "pr_number": "30341", "files_changed": ["test/dist_autograd_test.py"], "labels": []}, "56dd2836ec": {"title": "Make zeros argument of torch.where same dtype as other argument (#30661)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30661\n\nCherry-picked from https://github.com/pytorch/pytorch/pull/29080\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18781870\n\nPulled By: nairbv\n\nfbshipit-source-id: 9de85aa91bf7e0856f35c7c6238a8923315ed27f\n\nCo-authored-by: ifedan", "pr_number": "30661", "files_changed": ["aten/src/ATen/native/Loss.cpp"], "labels": []}, "a376dd344c": {"title": "Added check for torch.where on CPU that both arguments have same dtype (#30662)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30662\n\nCherry picked from: https://github.com/pytorch/pytorch/pull/29081\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18782295\n\nPulled By: nairbv\n\nfbshipit-source-id: 897ab25ddf8819ca34f5e86c5d3f41debb56cb04\n\nCo-authored-by: ifedan", "pr_number": "30662", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "test/test_nn.py", "test/test_torch.py"], "labels": []}, "dcd1216efe": {"title": "Force early initialization of OpenMP in forked children (#29006)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/28389\n\nIntel's OpenMP implementation sets the thread affinity on the first call to an OpenMP function after a fork. By adding an atfork handler we can force this to happen before a user tries to set the affinity in their own DataLoader `worker_init_fn`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29006\n\nDifferential Revision: D18782456\n\nPulled By: ezyang\n\nfbshipit-source-id: ce0b515256da0cf18ceb125e0cdec99a3311bbd3", "pr_number": "29006", "files_changed": ["test/test_dataloader.py", "torch/__init__.py", "torch/multiprocessing/_atfork.py"], "labels": ["open source", "triaged"]}, "59151d3e43": {"title": "autograd/profiler: support merging FunctionEventAvg (#30677)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30677\n\nCurrently you can only add FunctionEvents to FunctionEventAvg. This makes it so you can add multiple FunctionEventAvg objects together. This is useful for merging multiple profiles together such as when dealing with distributed training.\n\nTest Plan:\nadded unit test\n\n  buck test //caffe2/test:autograd -- test_profiler\n\nReviewed By: bddppq\n\nDifferential Revision: D18785578\n\nfbshipit-source-id: 567a441dec885db7b0bd8f6e0ac9a60b18092278", "pr_number": "30677", "files_changed": ["test/test_autograd.py", "torch/autograd/profiler.py"], "labels": ["fb-exported"]}, "fef4360536": {"title": "remove default constructor in futureInfo (#30197)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30197\n\nThis default constructor was added because std::map's operator[]\nrequires a default constructor. However, instead of using operator[], we can\nuse emplace and remove the constructor, to ensure that the FutureInfo struct\ndoesnt get constructed with garbage values.\nghstack-source-id: 94802453\n\nTest Plan: Unit tests pass.\n\nDifferential Revision: D18627675\n\nfbshipit-source-id: c4cb000e60081478c0fd7308e17103ebbc4dc554", "pr_number": "30197", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": []}, "3cf8382984": {"title": "detect_anomaly() for SparseTensors (#29803)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/28649\n\n1. Modified detect_anomaly() to use isnan()\n2. isnan() for SparseTensors returns a bool Tensor of _values.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29803\n\nDifferential Revision: D18594299\n\nPulled By: ezyang\n\nfbshipit-source-id: 3f4190c569f53219be330584fc604ca43c4a6c7a", "pr_number": "29803", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "test/test_sparse.py", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/engine.cpp"], "labels": []}, "ea3697db69": {"title": "inline to prevent duplicate obj when linking (#30363)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30363\n\ngetting duplicate definition errors when linking test.\nghstack-source-id: 94472892\n\nTest Plan: CI passes\n\nDifferential Revision: D18669686\n\nfbshipit-source-id: 3d3bfc38e4247cf8bea655537824b891b84f67bc", "pr_number": "30363", "files_changed": ["test/cpp/jit/test_misc.cpp"], "labels": ["jit"]}, "4ac614191a": {"title": "Remove exp10 in TH (unused)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30422\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18764186\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 9343a5a7e4edf61ba3b85eaf846b2e149ed6529a", "pr_number": "30422", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cuda/CUDAUnaryOps.cpp", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/TH/generic/THVector.h", "aten/src/TH/generic/THVectorDefault.cpp", "aten/src/THC/THCNumerics.cuh", "aten/src/THC/THCTensorMathReduce.cuh", "aten/src/THC/generic/THCTensorMathPointwise.cu", "aten/src/THC/generic/THCTensorMathPointwise.h", "aten/src/THC/generic/THCTensorMathReduce.cu", "test/test_torch.py"], "labels": ["merge-this-please"]}, "76acf5b553": {"title": "Remove many unused bfloat16 functions in TH", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30329\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18764281\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: bc3f91c6d09d4f73c77fe1492a358128744aee76", "pr_number": "30329", "files_changed": ["aten/src/THC/THCNumerics.cuh"], "labels": []}, "ab834d5093": {"title": "Remove exp10 in TH (unused)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30422\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18764280\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 626b88a115f2efce4a53c6784f0a6660b36c97f9", "pr_number": "30422", "files_changed": ["aten/src/THC/THCNumerics.cuh"], "labels": ["merge-this-please"]}, "bb5dcaf24f": {"title": "Add logical_and and logical_or (#30521)", "body": "Summary:\nWith the CI failure caused in 8bbafa0b32d2899ef6101172d62c6049427c977b fixed (incorrect return type of the lambdas in CUDA kernels)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30521\n\nDifferential Revision: D18770151\n\nPulled By: ailzhang\n\nfbshipit-source-id: 02f0fe1d5718c34d24da6dbb5884ee8b247ce39a", "pr_number": "30521", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/README.md", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_namedtensor.py", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": []}, "6dda241ab8": {"title": "Add RRef.__str__() API", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30609\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18763593\n\nPulled By: mrshenli\n\nfbshipit-source-id: 20f1eea2d6cfe9ab2a27a9677d97dde07c1dca9b", "pr_number": "30609", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/distributed/rpc/rref_context.cpp"], "labels": []}, "63a1542ed2": {"title": "Adding Debug Info for RRef Context", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30610\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18763592\n\nPulled By: mrshenli\n\nfbshipit-source-id: ad8854bdb6250c29eaa0f582d66cfd31394312e5", "pr_number": "30610", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h"], "labels": []}, "b26401f965": {"title": "Dump operator names of a script module (#30467)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30467\n\nIntroduce function jit.export_opnames(module), which returns a list of all operator names used in the module and its submodules. One usage is to have mobile custom build to link only operators in the returned list to save the mobile size.\n\nExample:\nimport torch\nm = torch.jit.load(\"example.pt\")\nprint(torch.jit.export_opnames(m))\n\nThe outputs are in alphabetical order:\n['aten::_convolution', 'aten::add.Tensor', 'aten::add_.Tensor', 'aten::addmm', 'aten::append.Tensor', 'aten::cat', 'aten::dropout', 'aten::embedding', 'aten::matmul', 'aten::max.dim', 'aten::mul.Tensor', 'aten::permute', 'aten::relu', 'aten::t', 'aten::tanh', 'prim::ListConstruct', 'prim::TupleConstruct', 'prim::TupleUnpack']\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18801619\n\nPulled By: iseeyuan\n\nfbshipit-source-id: f9b198d3e82b095daf704ee595d8026ad889bb13", "pr_number": "30467", "files_changed": ["docs/source/torch.rst", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export.h", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py"], "labels": ["jit"]}, "7e472679ff": {"title": "pin actions/checkout version", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30703\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18805447\n\nPulled By: suo\n\nfbshipit-source-id: d58ebe0e90b81c9282d3977f36c53c54cac750d9", "pr_number": "30703", "files_changed": [".github/workflows/lint.yml"], "labels": []}, "d456a538f9": {"title": "op dependency analysis bash driver", "body": "Summary:\nMove the shell script into this separate PR to make the original PR\nsmaller and less scary.\n\nTest Plan:\n- With stacked PRs:\n1. analyze test project and compare with expected results:\n```\nANALYZE_TEST=1 CHECK_RESULT=1 tools/code_analyzer/build.sh\n```\n\n2. analyze LibTorch:\n```\nANALYZE_TORCH=1 tools/code_analyzer/build.sh\n```\n\nDifferential Revision: D18474749\n\nPulled By: ljk53\n\nfbshipit-source-id: 55c5cae3636cf2b1c4928fd2dc615d01f287076a", "pr_number": null, "files_changed": ["scripts/build_mobile.sh", "tools/code_analyzer/build.sh"], "labels": []}, "f5c9452beb": {"title": "Fix toObject() r-value version (#30713)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30713\n\nIt should use moveToIntrusivePtr.\nThis function is a very hot one and used a lot in interpreter loop. e.g.\nGET_ATTR, SET_ATTR. Making a copy and doing incref/decref caused big overhead.\n\nReviewed By: yinghai\n\nDifferential Revision: D18805212\n\nfbshipit-source-id: 3a9368604f71638a21300ad086739c4b50f0644e", "pr_number": "30713", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["fb-exported"]}, "c0299d2707": {"title": "add LLVM code analyzer in order to replace static dispatch", "body": "Summary:\n[Why static dispatch]\nStatic dispatch was introduced to allow stripping out unused ops at link\ntime (with \u201cgc-sections\u201d linker flag) for mobile build.\n\nThe alternative approaches to do \"non-static\" dispatch are:\n* virtual methods - old ATen dispatcher, which has already been deprecated;\n* registry pattern - used by caffe2, c10 and JIT;\n\nHowever, none of them are \u201cgc-sections\u201d friendly. Global registers are\nroot symbols - linker cannot strip out any op if we use registry pattern\nfor mobile.\n\n[Why static dispatch isn\u2019t great]\n* One more code path to maintain;\n* Need recompile framework to add new backends/ops;\n* Doesn\u2019t support AutoGrad yet thus blocks on-device training;\n\n[Static Code Analysis]\nThis PR introduces a LLVM analysis pass. It takes LLVM bitcode /\nassembly as input and generates dependecy graph among aten ops. From a\nset of root ops used by a model, we can calculate transitive closure of\nall dependent ops, then we can ask codegen to only register these ops.\n\n[Approach]\nTo generate the dependency graph it searches for 3 types of connections in\nLLVM bitcode / assembly:\n 1) op registration: op name (schema string literal) -> registered function;\n 2) regular function call: function -> function;\n 3) op invocation: function -> op name (schema string literal)\n\nFor 2) it uses similar algorithm as llvm::LazyCallGraph - not only looks into\ncall/invoke instructions but also recursively searches for function pointers\nin each instruction's operands.\n\nFor 1) and 3) it searches for connections between operator name string\nliterals / function pointers and c10 op registration/invocation API calls in\nLLVM IR graph via \"use\" edges (bi-directional):\n 1. llvm::Value has \"users()\" method to get other llvm::Value nodes that use\n    the value;\n 2. most of types derive from llvm::User which has \"operands()\" method to get\n    other llvm::Value nodes being used by the value;\n\n[Limitation]\nFor now the search doesn't go beyond the function boundary because the\nreference to op name string literals and c10 op registration/invocation\nAPIs are almost always in the same function.\n\nThe script uses regular expression to identify c10 API calls:\n* op_schema_pattern=\"^(aten|quantized|profiler|_test)::[^ ]+\"\n* op_register_pattern=\"c10::RegisterOperators::(op|checkSchemaAndRegisterOp_)\"\n* op_invoke_pattern=\"c10::Dispatcher::findSchema|callOp\"\n\nIf we create helper function around c10 API (e.g. the \"callOp\" method\ndefined in aten/native), we could simply add them to the regular expression\nused to identify c10 API.\n\n[Example]\nIn the following example, it finds out:\n 1) the registered function for \"quantized:add\" operator;\n 2) one possible call path to at::empty() function;\n 3) the called operator name \"aten::empty\":\n\n- \"quantized::add\"\n- c10::detail::wrap_kernel_functor_unboxed_<at::native::(anonymous namespace)::QAdd<false>, at::Tensor (at::Tensor, at::Tensor, double, long)>::call(c10::OperatorKernel*, at::Tensor, at::Tensor, double, long)\n- at::native::(anonymous namespace)::QAdd<false>::operator()(at::Tensor, at::Tensor, double, long)\n- void at::native::DispatchStub<void (*)(at::Tensor&, at::Tensor const&, at::Tensor const&), at::native::qadd_stub>::operator()<at::Tensor&, at::Tensor const&, at::Tensor const&>(c10::DeviceType, at::Tensor&, at::Tensor const&, at::Tensor const&)\n- at::native::DispatchStub<void (*)(at::Tensor&, at::Tensor const&, at::Tensor const&), at::native::qadd_stub>::choose_cpu_impl()\n- void at::native::(anonymous namespace)::qadd_kernel<false>(at::Tensor&, at::Tensor const&, at::Tensor const&)\n- at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool)\n- at::TensorIterator::build()\n- at::TensorIterator::fast_set_up()\n- at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>)\n- \"aten::empty\"\n\n[How do we know it\u2019s correct?]\n* Built a test project that contains different op registration/invocation\n  patterns found in pytorch codebase, including both codegen and non-codegen\n  cases.\n* Tried different optimization flags \u201c-O0\u201d, \u201c-O3\u201d - the result seems to\n  be stable.\n* Filtered by common patterns: \u201caten::\u201d, \u201cat::\u201d, \u201cat::native\u201d,\n  \u201cat::CPUType\u201d, \u201cat::TypeDefault\u201d - manually checked the relationship\n  between function schema strings and corresponding implementations were\n  captured.\n* It can print instruction level data flow and show warning message if it\n  encounters unexpected cases (e.g.: found 0 or multiple op names per\n  registration/invocation API call, found 0 registered functions, etc).\n* Verified consistent results on different linux / macOs hosts. It can\n  handle different STL library ABI reliably, including rare corner cases\n  for short string literals\n\n[Known issues]\n* Doesn\u2019t handle C code yet;\n* Doesn\u2019t handle overload name yet (all variants are collapsed into the\n  main op name);\n\nTest Plan:\n```\nLLVM_DIR=... ANALYZE_TEST=1 CHECK_RESULT=1 scripts/build_code_analyzer.sh\n```\n\nDifferential Revision: D18428118\n\nPulled By: ljk53\n\nfbshipit-source-id: d505363fa0cbbcdae87492c1f2c29464f6df2fed", "pr_number": null, "files_changed": ["tools/code_analyzer/CMakeLists.txt", "tools/code_analyzer/op_dependency.cpp"], "labels": []}, "d12786b24f": {"title": "add __torch_function__ API override mechanism (#27064)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/24015 (see description of that issue for more details).\n\nFor a toy example, see the `DiagonalTensor` and `SubDiagonalTensor` class in test/test_overrides.py.\n\nThis PR currently contains:\n\n* tests for `__torch_function__` behavior\n* modification to `gen_python_functions` and `parse` function signatures and dispatched to correct overloaded argument.\n\nThis feature is inspired by and analogous to NumPy's `__array_function__` protocol ([see NumPy Enhancement Proposal 18](https://numpy.org/neps/nep-0018-array-function-protocol.html#trying-array-function-methods-until-the-right-one-works)).\n\n### Benchmarks:\nSee Nathan's comment below: https://github.com/pytorch/pytorch/pull/27064#issuecomment-554601189\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27064\n\nDifferential Revision: D18645954\n\nPulled By: ezyang\n\nfbshipit-source-id: 54b5e4344d7afdbcf996bb57191b0bdadc7b1767", "pr_number": "27064", "files_changed": ["docs/source/notes/extending.rst", "test/onnx/expect/TestOperators.test_frobenius_norm.expect", "test/onnx/expect/TestOperators.test_meshgrid.expect", "test/onnx/expect/TestOperators.test_unique.expect", "test/run_test.py", "test/test_overrides.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_torch_functions.cpp", "torch/_overrides.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/functional.py"], "labels": ["caffe2", "module: autograd", "module: docs", "module: internals", "module: operators", "module: pybind", "open source", "triaged"]}, "d6ca93b353": {"title": "add doc for F.softplus", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30055\n\nDifferential Revision: D18762624\n\nPulled By: zou3519\n\nfbshipit-source-id: 61da88cbb8cd0f37ac26b0fb8aaacdbe85c724ba", "pr_number": "30055", "files_changed": ["torch/nn/functional.py", "torch/nn/modules/activation.py"], "labels": ["merge-this-please"]}, "ec7bb9de1c": {"title": "format tri[lu]_indices doc better", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30377\n\nDifferential Revision: D18689152\n\nPulled By: zou3519\n\nfbshipit-source-id: 7fab1e39ecd39ef6a3869befcbe217f8d3b6a87e", "pr_number": "30377", "files_changed": ["torch/_torch_docs.py"], "labels": ["merge-this-please"]}, "a68b790293": {"title": "fix ref to nonexistent torch.repeat", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30614\n\nDifferential Revision: D18808517\n\nPulled By: ezyang\n\nfbshipit-source-id: 27f9bda6fbbd1c3c751a0e96fdc336bf724c0b31", "pr_number": "30614", "files_changed": ["torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merge-this-please"]}, "b8792c0438": {"title": "Revert D18645954: add __torch_function__ API override mechanism", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18645954\n\nOriginal commit changeset: 54b5e4344d7a\n\nfbshipit-source-id: 4a7aebb483e6b001130d6f384ccc53c5a808ab13", "pr_number": null, "files_changed": ["docs/source/notes/extending.rst", "test/onnx/expect/TestOperators.test_frobenius_norm.expect", "test/onnx/expect/TestOperators.test_meshgrid.expect", "test/onnx/expect/TestOperators.test_unique.expect", "test/run_test.py", "test/test_overrides.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_torch_functions.cpp", "torch/_overrides.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/functional.py"], "labels": []}, "1189595875": {"title": "Fix Tensor.argsort -> torch.argsort documentation link", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30464\n\nDifferential Revision: D18717657\n\nPulled By: zou3519\n\nfbshipit-source-id: 9894f63c6cb1b5311117441e78805230d1bc09f3", "pr_number": "30464", "files_changed": ["torch/_tensor_docs.py"], "labels": []}, "38986e1dea": {"title": "Split libtorch.so back into libtorch_{cpu,cuda,hip} (#30315)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30315\n\nThe new structure is that libtorch_cpu contains the bulk of our\ncode, and libtorch depends on libtorch_cpu and libtorch_cuda.\nThis is a reland of https://github.com/pytorch/pytorch/pull/29731 but\nI've extracted all of the prep work into separate PRs which can be\nlanded before this one.\n\nSome things of note:\n\n* torch/csrc/cuda/nccl.cpp was added to the wrong list of SRCS, now fixed (this didn't matter before because previously they were all in the same library)\n* The dummy file for libtorch was brought back from the dead; it was previously deleted in #20774\nIn an initial version of the patch, I forgot to make torch_cuda explicitly depend on torch_cpu. This lead to some very odd errors, most notably \"bin/blob_test: hidden symbol `_ZNK6google8protobuf5Arena17OnArenaAllocationEPKSt9type_infom' in lib/libprotobuf.a(arena.cc.o) is referenced by DSO\"\n* A number of places in Android/iOS builds have to add torch_cuda explicitly as a library, as they do not have transitive dependency calculation working correctly\n* I had to torch_cpu/torch_cuda caffe2_interface_library so that they get whole-archived linked into torch when you statically link. And I had to do this in an *exported* fashion because torch needs to depend on torch_cpu_library. In the end I exported everything and removed the redefinition in the Caffe2Config.cmake. However, I am not too sure why the old code did it in this way in the first place; however, it doesn't seem to have broken anything to switch it this way.\n* There's some uses of `__HIP_PLATFORM_HCC__` still in `torch_cpu` code, so I had to apply it to that library too (UGH). This manifests as a failer when trying to run the CUDA fuser. This doesn't really matter substantively right now because we still in-place HIPify, but it would be good to fix eventually. This was a bit difficult to debug because of an unrelated HIP bug, see https://github.com/ROCm-Developer-Tools/HIP/issues/1706\n\nFixes #27215 (as our libraries are smaller), and executes on\npart of the plan in #29235.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18790941\n\nPulled By: ezyang\n\nfbshipit-source-id: 01296f6089d3de5e8365251b490c51e694f2d6c7", "pr_number": "30315", "files_changed": [".circleci/scripts/binary_ios_upload.sh", "android/pytorch_android/CMakeLists.txt", "c10/macros/Export.h", "caffe2/CMakeLists.txt", "caffe2/core/common_gpu.h", "cmake/Caffe2Config.cmake.in", "ios/LibTorch.podspec", "ios/TestApp/benchmark/setup.rb", "scripts/xcode_build.rb", "torch/utils/cpp_extension.py"], "labels": ["module: cpp"]}, "d0af07ca4c": {"title": "Fix capitalization inconsistency in optim.rst", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30608\n\nDifferential Revision: D18808516\n\nPulled By: ezyang\n\nfbshipit-source-id: 4be68be9a8c8c3da7a0b98162bc1050b588fab43", "pr_number": "30608", "files_changed": ["docs/source/optim.rst"], "labels": ["merge-this-please"]}, "ca072951d5": {"title": "move MaskedAdagrad to caffe2/operators/experimental/optimizers (#30714)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30714\n\nMove Masked*Adagrad operators so caffe2/python/optimizer.py can use them.\n\nTest Plan: buck test caffe2/caffe2/operators/experimental/optimizers:masked_adagrad_test\n\nReviewed By: chocjy\n\nDifferential Revision: D18805532\n\nfbshipit-source-id: 49b1f755b31296c62e7a6a8134313b962ad9690c", "pr_number": "30714", "files_changed": ["caffe2/operators/experimental/optimizers/masked_adagrad.cpp", "caffe2/operators/experimental/optimizers/masked_adagrad_test.py"], "labels": ["fb-exported"]}, "a55f125e3b": {"title": "Check the error return of nvrtcGetProgramLogSize and nvrtcGetProgramLog (#30663)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30663\n\nYes they can fail.  See https://github.com/ROCm-Developer-Tools/HIP/issues/1706\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18810088\n\nPulled By: ezyang\n\nfbshipit-source-id: 96186e71c9a195bdbbed811e7ba8dc40bec09eae", "pr_number": "30663", "files_changed": ["torch/csrc/jit/fuser/cuda/fused_kernel.cpp"], "labels": ["jit", "module: rocm"]}, "6e145b4614": {"title": "add irregular c10 op registration/invocation cases to test project (#30558)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30558\n\nMost c10 op registration/invocation cases are generated by aten codegen\nfollowing some fixed pattern, but a handful of them were written\nmanually, mainly for quantized ops. Added these \"irregular\" cases to the\ntest project to verify static code analyzer can handle them as well.\n\nTest:\n- build and run the test project;\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18811098\n\nPulled By: ljk53\n\nfbshipit-source-id: 7bdf17175dfec41c56c0d70f124cc96478135bc4", "pr_number": "30558", "files_changed": ["test/mobile/op_deps/CMakeLists.txt", "test/mobile/op_deps/expected_deps.yaml", "test/mobile/op_deps/main.cc", "test/mobile/op_deps/quantized_ops.cpp", "test/mobile/op_deps/quantized_ops.h", "test/mobile/op_deps/simple_ops.h", "test/mobile/op_deps/utils.cpp"], "labels": []}, "f73cd28082": {"title": "InsertObservers for shared class types (#30548)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30548\n\nClassTypes can be shared among different module instances, but previously we assumed\nthey would be unique, this PR enables the insert_observers pass to work with shared class types\n\nTest Plan:\npython test/test_jit.py\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18802465\n\nfbshipit-source-id: b782e71e44a043af45577ac2b5c83e695155bb8b", "pr_number": "30548", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "756f279d95": {"title": "Rename QuantizeHelper to InsertQuantDeQuantHelper (#30549)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30549\n\nPreparing for later refactoring\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18802464\n\nfbshipit-source-id: 0b5afb143549d93eed4c429125d3d5fd253093a9", "pr_number": "30549", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "6918f0ce86": {"title": "Move scalar_check for total_weight in NLLLoss functions to code from codegen. (#30665)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30665\n\ntotal_weight is a \"hidden\" output just for autograd, so it's not user visible.  The existing test_nn tests cover this (I verified that the new code is executed) and this matches the CPU behavior.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18782709\n\nPulled By: gchanan\n\nfbshipit-source-id: 6d1c20eeaeffa14d06f375b37f11e866587f5fa0", "pr_number": "30665", "files_changed": ["aten/src/ATen/nn.yaml", "aten/src/THCUNN/generic/ClassNLLCriterion.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu"], "labels": []}, "fa2aa245cf": {"title": "Simplify scalar_check of nll_loss. (#30669)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30669\n\nThe inputs can't be 0-d, so we don't need that check in the scalar_check.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18784524\n\nPulled By: gchanan\n\nfbshipit-source-id: d44222dffc91880a6e8c7be69e6e146e60040d43", "pr_number": "30669", "files_changed": ["aten/src/ATen/nn.yaml", "test/test_torch.py"], "labels": []}, "786de33832": {"title": "Move scalar_check logic from codegen to code in NLLLoss. (#30670)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30670\n\nAlso turn off scalar_check for grad_input: it isn't necessary because the input can't be 0-dimensional.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18784523\n\nPulled By: gchanan\n\nfbshipit-source-id: 246d30970457075a0403dd0089317659a2cd2dd4", "pr_number": "30670", "files_changed": ["aten/src/ATen/nn.yaml", "aten/src/THCUNN/generic/ClassNLLCriterion.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu", "test/test_torch.py"], "labels": []}, "9d69c55b0d": {"title": "add MaskedRowWiseSparseAdagrad", "body": "Summary: As title\n\nTest Plan: buck test caffe2/caffe2/fb/optimizers:masked_adagrad_test\n\nReviewed By: chocjy\n\nDifferential Revision: D18736639\n\nfbshipit-source-id: d0d73f75228604d3448651bff2cf34ecc21f9ba6", "pr_number": null, "files_changed": ["caffe2/operators/experimental/optimizers/masked_adagrad.cpp", "caffe2/operators/experimental/optimizers/masked_adagrad_test.py"], "labels": []}, "d38f9117fd": {"title": "Cache compilation of free functions (#30503)", "body": "Summary:\nWe don't have to recompile free functions if we've already compiled them.\n\nImproved compilation of resnet18 by 27%.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30503\n\nDifferential Revision: D18796501\n\nPulled By: eellison\n\nfbshipit-source-id: 2dee0fc5fcf9adc5b92213f8cb813730d71b376f", "pr_number": "30503", "files_changed": ["test/test_jit.py", "torch/jit/__init__.py"], "labels": ["jit"]}, "289e9a07fd": {"title": "Move Tanh backward to Aten(CPU+CUDA) (#30224)", "body": "Summary:\nVitalyFedyunin, This PR is about port Tanh backward to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.Tanh()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    bwd_t = 0\n    for i in range(10000):\n        output = m(input)\n        t1 = _time()\n        output.backward(grad_output)\n        t2 = _time()\n        bwd_t = bwd_t + (t2 - t1)\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d)  backwad avg time is %.2f (ms).\" % (n, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) backwad avg time is 0.12 (ms).\ninput size(128, 10000) backwad avg time is 0.17 (ms).\nCPU\ninput size(128, 100) backwad avg time is 0.05 (ms).\ninput size(128, 10000) backwad avg time is 0.35 (ms).\n```\nAfter:\n```\nGPU:\ninput size(128, 100) backwad avg time is 0.12 (ms).\ninput size(128, 10000) backwad avg time is 0.17 (ms).\nCPU\ninput size(128, 100) backwad avg time is 0.04 (ms).\ninput size(128, 10000) backwad avg time is 0.25 (ms).\n```\n`OMP_NUM_THREADS=1:`\n```\nBefore:\ninput size(128, 100) backwad avg time is 0.03 (ms).\ninput size(128, 10000) backwad avg time is 1.85 (ms).\nAfter:\ninput size(128, 100) backwad avg time is 0.02 (ms).\ninput size(128, 10000) backwad avg time is 1.16 (ms).\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30224\n\nDifferential Revision: D18810045\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: ab37948ab8f76bdaf9f3d1388562eaf29dacc0ea", "pr_number": "30224", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/Tanh.cu", "aten/src/THCUNN/generic/Tanh.cu", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/generic/Tanh.c", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "9d3402e4cb": {"title": "Add the __torch_function__ API override mechanism (#30730)", "body": "Summary:\nThis is a re-do of https://github.com/pytorch/pytorch/issues/27064, which was reverted (https://github.com/pytorch/pytorch/commit/b8792c0438f4292aa813c36207f75eebcbb77a45). This was landed at the same time as other work that added new operators to the `torch` namespace so the check for whether the `torch` namespace is exhaustively checked for overridability was triggering test failures.\n\nI've temporarily disabled that check and added an explanatory comment that the check will be re-enabled in a future PR that will be merged during a time when the commit velocity on PyTorch is lower.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30730\n\nDifferential Revision: D18813270\n\nPulled By: ezyang\n\nfbshipit-source-id: 70477c4656dca8fea6e7bc59259555041fcfbf68", "pr_number": "30730", "files_changed": ["docs/source/notes/extending.rst", "test/onnx/expect/TestOperators.test_frobenius_norm.expect", "test/onnx/expect/TestOperators.test_meshgrid.expect", "test/onnx/expect/TestOperators.test_unique.expect", "test/run_test.py", "test/test_overrides.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_torch_functions.cpp", "torch/_overrides.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/functional.py"], "labels": []}, "42e79d7e8a": {"title": "Kill THNN version of MultiMarginCriterion; it's not used anymore.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30725\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18808767\n\nPulled By: gchanan\n\nfbshipit-source-id: bcc4a6e272036f3d167fc158a53fe7aa1dec51f9", "pr_number": "30725", "files_changed": ["aten/src/THNN/generic/MultiMarginCriterion.c"], "labels": []}, "2308a0ec1b": {"title": "Improve documentation around builtin functions (#30347)", "body": "Summary:\nThis breaks the builtins page into some more sections and adds details about Python built-in functions\n](https://our.intern.facebook.com/intern/diff/18718166/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30347\n\nPulled By: driazati\n\nReviewed By: wanchaol\n\nDifferential Revision: D18718166\n\nfbshipit-source-id: bf43260ab7bcf92cccef684a5ce68cb16020771d", "pr_number": "30347", "files_changed": ["docs/source/_static/css/jit.css", "docs/source/conf.py", "docs/source/jit.rst", "torch/jit/supported_ops.py"], "labels": ["jit"]}, "1707774417": {"title": "AddConstant and findConstant for ClassType (#29217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29217\n\nWe want to preserve constant information in ClassType so that\nusers can access the constants in the module by name.\nThis is also used later for freezing some attribute(converting\nattributes to constant)\n\nTest Plan:\ntbd\n\nImported from OSS\n\nDifferential Revision: D18799955\n\nfbshipit-source-id: fbfbcd5d3f7f560368b96e2a87e270c822a3d03a", "pr_number": "29217", "files_changed": ["aten/src/ATen/core/jit_type.h", "test/cpp/jit/test_class_type.cpp", "torch/csrc/jit/import_source.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/script/class_type.cpp", "torch/csrc/jit/script/script_type_parser.cpp", "torch/csrc/jit/script/script_type_parser.h"], "labels": ["jit"]}, "cd032c7f6a": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/b94ef9fb236f0a50380413ad13195c9d0fe36ac7\nhttps://github.com/facebook/folly/commit/4462a7f00a20bd4589c6436a81ae6deea4beddaa\nhttps://github.com/facebook/litho/commit/16e629c415b9ee6f08974ef755ee8c9441543cbf\nhttps://github.com/facebook/mcrouter/commit/50770702ad11c9879f93550446bfb6cdb8aba450\nhttps://github.com/facebook/proxygen/commit/5b632a5deb92dc76aef464c59b76807dd4e42357\nhttps://github.com/facebookincubator/mvfst/commit/d2fa2cbcd65285bde14b87f3d01745ccd4b1ddd5\nhttps://github.com/facebookincubator/profilo/commit/4e152f651ea939822885171ad1275489be4ad4f2\nhttps://github.com/pytorch/fbgemm/commit/54c89b5f03938c0804517cb6a2d4001e843bed8c\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: 766783d00f8440c1264f13045ae6411233355af6", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "1d8a13147c": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/1e345af4de136b209b32573cc4a4c9cc7be6b5fb\nhttps://github.com/facebookincubator/mvfst/commit/61d54df22ca73688b024f8d6cbc5bb6aa1f23c38\nhttps://github.com/pytorch/fbgemm/commit/dab87e19bf144f93b1eceb4bafdb8d51e3d2a340\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: 88e55e94c7473a7a310338eaaf508e7fc71e0df6", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "569ea63f3b": {"title": "fix anynonzero op", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29423\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18820523\n\nfbshipit-source-id: 55c7a1911121f0aed008bd684b448151bbbf0a8a", "pr_number": "29423", "files_changed": ["torch/csrc/jit/register_prim_ops.cpp"], "labels": ["jit"]}, "1f1ce53e8e": {"title": "Don't install pybind11 header directory for system pybind11 installs (#30758)", "body": "Summary:\nFor system pybind11 installs this is a system header location that should not get installed since it might include other unrelated headers. Since the header is already installed for a system install there's no need to install the headers, so only do the install when we use the bundled pybind11 version.\n\nCloses https://github.com/pytorch/pytorch/issues/29823. Closes https://github.com/pytorch/pytorch/issues/30627.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30758\n\nDifferential Revision: D18820189\n\nPulled By: bddppq\n\nfbshipit-source-id: fcc9fa657897e18c07da090752c912e3be513b17", "pr_number": "30758", "files_changed": ["cmake/Dependencies.cmake"], "labels": []}, "e09c415387": {"title": "Back out \"make the order btw div and mul in adagrad update consistent\" (#30737)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30737\n\nOriginal commit changeset: 2a8b2a3f5401\n\nReverting this to be safe until we address test failures in T58528495\n\nTest Plan: CI\n\nReviewed By: wx1988\n\nDifferential Revision: D18812384\n\nfbshipit-source-id: 2a3ac554024773022ec827f259127e4c8cffe6e2", "pr_number": "30737", "files_changed": ["caffe2/perfkernels/adagrad.h", "caffe2/perfkernels/adagrad_avx.cc", "caffe2/sgd/adagrad_op.h"], "labels": ["fb-exported"]}, "3c1bb21cf5": {"title": "Invoke more passes in `insertObservers` (#30473)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30473\n\nInvoked `ConstantPooling` and `FuseLinear` pass before\n`insertObservers`.\n`ConstantPooling` is for cleanning up traced graph, e.g. when we\nhave to constant node that has the same value, this pass will merge them,\nthis allows us to have less quantization patterns\n`FuseLinear` is to merge the exploded linear function into `aten::linear` so\nthat we can quantize this function properly. We need to fuse it because right now\nthe way we recognize weight and bias is by matching the argument position in certain function\ncalls, e.g. 1st argument of aten::conv2d is weight. Therefore we have to preserve\nthe bounary of the linear function to recognize the weight of linear. Since in the exploded\nlinear code, input of addmm is transposed weight rather than the original weight of linear.\nghstack-source-id: 94887831\n\nTest Plan:\nThis is needed for quantizing traced model tests to pass\n\nImported from OSS\n\nDifferential Revision: D18795722\n\nfbshipit-source-id: 192d9d1e56307e2e1d90e30dce0502e31cb4f829", "pr_number": "30473", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "7a2889b014": {"title": "Stop producing op_version_set version numbers.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28122\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D17959565\n\nPulled By: zdevito\n\nfbshipit-source-id: 701101bd870700eb0c9882c69e2cfdd2524b555e", "pr_number": "28122", "files_changed": ["caffe2/serialize/inline_container.h", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export_module.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/passes/python_print.h"], "labels": ["jit"]}, "c4c2e23385": {"title": "Supporting making submodules unique (#30037)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30037\n\nSupport quantization for modules with reused submodules, e.g. relu (automatically make unique)\nWe first do a pass on the graph to find all duplicate uses of the same module, and record the `Value`s of the\nmodule instance, for each of these values we create a new module and change the access to that module.\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D18821483\n\nfbshipit-source-id: 1698b981e9e9f0c728d9f03fcbcfbd260151f679", "pr_number": "30037", "files_changed": ["test/test_jit.py", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization.h"], "labels": ["jit"]}, "1d20c32bf1": {"title": "Make `InsertQuantDeQuantHelper` global (#30550)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30550\n\nRight now we have a `InsertQuantDeQuantHelper` for each module, but we need\nit to be global because we need to know what graphs have been quantized before\nand based on this information we can decide how to handle the module instance.\n\nTest Plan:\ntest_jit.py, test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18818651\n\nfbshipit-source-id: bfcaf37094ce20a257171a0c99b05b9348ebc13d", "pr_number": "30550", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "a939b52ddb": {"title": "fix AvgPool2d for 2^31-1 sized inputs, and get test_cuda_kernel_loop_\u2026 (#30771)", "body": "Summary:\n\u2026overflow_large to working state\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30771\n\nDifferential Revision: D18821529\n\nPulled By: ngimel\n\nfbshipit-source-id: c5cbf56e686a2a3cfc7274dd96db37289dac7588", "pr_number": "30771", "files_changed": ["aten/src/ATen/native/cuda/AveragePool2d.cu"], "labels": []}, "139aa51962": {"title": "Clean up non-C++14 code (#28443)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28443\n\nWe're now on C++14, so we don't need the else branch of these ifdef's anymore\nghstack-source-id: 94904074\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18069136\n\nfbshipit-source-id: f1613cab9a99ee30f99775e4a60a1b06fd0a03ff", "pr_number": "28443", "files_changed": ["aten/src/ATen/cpu/vec256/vec256.h"], "labels": []}, "f2a2fec47c": {"title": "CUDA-strided-complex Binary and Unary Op support (#30295)", "body": "Summary:\nIn-tree changes to pytorch to support complex numbers are being submitted here.\nOut-of-tree support for CUDA complex numbers is here: [pytorch-cuda-strided-complex extension](https://gitlab.com/pytorch-complex/pytorch-cuda-strided-complex)\n\nChanges so far:\n\n- [x]  Added complex support of torch.empty and torch.fill()\n- [x]  Added complex support of CopyKernels\n    - The 'static_cast_with_inter_type' template function is specialized for the following cases\n        - `dest_t = thrust::complex<dest_value_t>`, `src_t = std::complex<src_value_t>`\n        - `dest_t = std::complex<dest_value_t>`, `src_t = thrust::complex<src_value_t>`\n     - This handles the compile-time case where `dest_value_t=double` and `src_value_t=float`.\n- [x]  Added complex support of BinaryOp kernels\n    - `using thrust_t = typename ztype_cuda<scalar_t>::thrust_t;` converts std::complex<T> ScalarTypes to thrust types and is a no-op of other Scalar Types.\n    - The operator is performed using complex number support defined in `thrust/complex.h`\n    - This could be extended to work with ROCm by using `rocm/complex.h`\n- [x]  Added complex support of UnaryOp kernels\n    - Added CUDA support for `angle()`, `real()`, `imag()`, `conj()`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30295\n\nDifferential Revision: D18781954\n\nPulled By: ezyang\n\nfbshipit-source-id: 25d204c0b8143ee27fda345a5d6a82f095da92a7", "pr_number": "30295", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/detail/ScalarTypeConversions.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cpu/CopyKernel.cpp", "aten/src/ATen/native/cuda/BinaryArithmeticKernel.cu", "aten/src/ATen/native/cuda/BinaryCompareKernel.cu", "aten/src/ATen/native/cuda/FillKernel.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/cuda/zmath.cuh", "aten/src/ATen/native/native_functions.yaml", "c10/util/TypeCast.h"], "labels": ["merged"]}, "c4e9748bc6": {"title": "Provide full path for buck hipification (#30746)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30746\n\nThis diff should be safe as long as open source build succeeds and should have no impact to cuda.\n\nDifferential Revision: D18811302\n\nfbshipit-source-id: a7adab993816cba51842701898fac5019438b664", "pr_number": "30746", "files_changed": ["aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h", "aten/src/ATen/miopen/Utils.h", "aten/src/ATen/native/SharedReduceOps.h", "aten/src/ATen/native/miopen/Conv_miopen.cpp", "aten/src/ATen/native/miopen/RNN_miopen.cpp"], "labels": ["fb-exported", "merged", "module: rocm"]}, "35a6997863": {"title": "Support 0-d tensors in CUDA MultiLabelMarginCriterion. (#30765)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30765\n\nIt is already supported in CPU and is pretty easy to add for consistency.\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30727\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18821557\n\nPulled By: gchanan\n\nfbshipit-source-id: e6aa3e91000ff3fd63941defc7d30aef58ae2f82", "pr_number": "30765", "files_changed": ["aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu", "test/common_nn.py"], "labels": ["merged"]}, "ba1a9871cb": {"title": "Turn off scalar_check for is_target for MultiLabelMarginCriterion, which is handled correctly in code. (#30766)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30766\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30728\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18821555\n\nPulled By: gchanan\n\nfbshipit-source-id: 27acc72f82e94eddeea675ae66e010cfb2fc7421", "pr_number": "30766", "files_changed": ["aten/src/ATen/nn.yaml"], "labels": ["merged"]}, "473a044835": {"title": "Fix a CUDA memory leak in MultiLabelMarginCriterion error checking. (#30767)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30767\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30733\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18821553\n\nPulled By: gchanan\n\nfbshipit-source-id: 8bf0365ce54dd2f07a5d6d0937332d0baf75b350", "pr_number": "30767", "files_changed": ["aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu", "test/test_torch.py"], "labels": ["merged"]}, "50625798df": {"title": "Fix scalar check of MultiLabelMarginLoss. (#30768)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30768\n\nThe behavior didn't match the documentation, because the documentation (for 'none' reduction) reads:\ninput X target -> output\n(N, C) X (N, C) -> (N,)\n(C,) X (C,) -> ()\n\nbut the later case would output (1,).  This also changes the case to:\n() X (C,) -> ()\nfrom:\n() X (C,) -> (C,)\nwhich makes more sense with the above formulas.\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30748\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18821554\n\nPulled By: gchanan\n\nfbshipit-source-id: 3df77c51cf25648cb5fab62a68b09f49c91dab4e", "pr_number": "30768", "files_changed": ["aten/src/ATen/native/LossMultiLabelMargin.cpp", "aten/src/ATen/nn.yaml", "test/common_nn.py", "test/test_torch.py"], "labels": ["merged", "topic: bc-breaking"]}, "f12332eb51": {"title": "Move scalar_check from codegen to code in MultiLabelMarginCriterion. (#30770)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30770\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30753\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18821556\n\nPulled By: gchanan\n\nfbshipit-source-id: 64b7311b1eb3855c4f1981d060accc918b99088d", "pr_number": "30770", "files_changed": ["aten/src/ATen/nn.yaml", "aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu"], "labels": ["merged"]}, "2607772959": {"title": "Turn off scalar_checks for SpatialDepthwiseConvolution and SpatialConvolutionMM. (#30789)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30789\n\nThe input(s) can't be 0-dimensional, so its irrelevant.\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30438\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18825716\n\nPulled By: gchanan\n\nfbshipit-source-id: a4883b795163efcb9d8dba6166d0f2102b6728a2", "pr_number": "30789", "files_changed": ["aten/src/ATen/nn.yaml", "test/test_torch.py"], "labels": ["merged"]}, "fa251cfd97": {"title": "Fully deprecate variadic inputs of checkpoint_sequential (#25985)", "body": "Summary:\nTo support variadic inputs of `checkpoint_sequential` was deprecated at https://github.com/pytorch/pytorch/issues/21006. This case should be warned with `DeprecationWarning` for PyTorch 1.2, but it should be simply failed with `TypeError` since PyTorch 1.3. This patch removes the `DeprecationWarning` for PyTorch 1.2.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25985\n\nDifferential Revision: D18809875\n\nPulled By: albanD\n\nfbshipit-source-id: e84dd8629c04979c4b2dc63e8ada94292e8cedd0", "pr_number": "25985", "files_changed": ["test/test_utils.py", "torch/utils/checkpoint.py"], "labels": ["merged", "module: checkpoint", "module: tests", "open source", "triaged"]}, "1578a28692": {"title": "Migrate max and min (binary) from TH to ATen. (#27185)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27185\n\nTH implementation will be removed after the unary max and min are migrated.\n\nBenchmark: (Debian 10, Release build, gcc 7.4, no turbo)\n\n```python\nimport timeit\nfor device in ('cpu', 'cuda'):\n    print(f'device: {device}')\n    for op in ('max', 'min'):\n        for dtype in ('torch.double', 'torch.float', 'torch.int16', 'torch.int32', 'torch.int64'):\n            for n, t in [(10_000, 200000),\n                        (100_000, 20000)]:\n                print(f'torch.{op}(a, b), numel() == {n} for {t} times, dtype={dtype}')\n                print(timeit.timeit(f'torch.{op}(a)' + (';torch.cuda.synchronize()' if device == 'cuda' else ''),\n                                    setup=f'import torch; a = torch.arange({n}, dtype={dtype}); b = torch.ones({n}, 0, dtype={dtype}) * ({n} / 2)', number=t))\n    print()\n```\n\nBefore:\n\n```\ndevice: cpu\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n2.241763713000182\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.7138833169992722\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.2183356810000987\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.7031846980007685\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.7704679510006827\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.289198366999699\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.7937613740014058\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.2930124340000475\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8032857640009752\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.2908709189996443\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n1.8829010000008566\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.2994690759987861\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n1.8037853410005482\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.2929310759991495\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.8075240359994496\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.2932477679987642\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.7868400779989315\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.2885970789993735\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8389664830010588\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.29402057399966\n\ndevice: cuda\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n4.787109836999662\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.842438002999188\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n3.429616614999759\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.835390076999829\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.940423873000327\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.4108991760003846\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.9318018840003788\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4168134739993548\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n2.9610764919998473\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4189234130008117\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n2.960172712999338\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.4162539499993727\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.8985912560001452\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.4113489299998037\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.9160250799995993\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.4128787690005993\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.8806865219994506\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4086357010000938\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n2.9362181240012433\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4151225870009512\n\n```\n\nAfter:\n\n```\ndevice: cpu\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n2.2685823729998447\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.72004808300062\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.212242640000113\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.7089235590001408\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.7767087259999244\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.2916517639996528\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.8265984959998605\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.3002885240002797\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8084679720004715\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.3012119999993956\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n1.8800218449996464\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.3060645710002063\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.4905043950002437\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.9126290209997023\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.7972335520007618\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.2918074379995232\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.8047651860006226\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.2992197730000044\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8526509560006161\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.3030709570002728\n\ndevice: cuda\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n4.700986622000528\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.8415469050005413\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n3.3051693249999516\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.8321999460004008\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.8086475109994353\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.405110773999695\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.913458047999484\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4236377289998927\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n2.9386842409994642\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4230227469997772\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n3.0341797270002644\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.4289592409995748\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n3.6091147850002017\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n2.036691903999781\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.8256167649997224\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.4078955400000268\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.8631781489993955\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4210130069996012\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n3.0112479260005784\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4297719679998409\n\n```\n\nSolve partly #24594 #24595\n\nClose #25016\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18117070\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: e06d37a8a1405848ba0b9e398870a77eb52bae8b", "pr_number": "27185", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryCompareKernel.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py"], "labels": ["merged", "module: cpu", "module: cuda", "module: operators", "open source", "triaged"]}, "2171f91053": {"title": "reenable cuda_kernel_loop_overflow_large test (#30797)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/30771 has landed, original issue https://github.com/pytorch/pytorch/issues/26838 is now closed\n\ncc peterjc123\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30797\n\nDifferential Revision: D18827307\n\nPulled By: ngimel\n\nfbshipit-source-id: 41b3db5fc9db85daeaa1b53c55b468976c996285", "pr_number": "30797", "files_changed": ["test/test_cuda.py"], "labels": ["merge-this-please", "merged"]}, "f531815526": {"title": "Deprecate tensor.type() (#30281)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/29161.\n\nI looked a bit at the code changes related to this and think I have all of the use cases of `DeprecatedTypeProperties` covered in the message, but suggestions from someone with more context on this would be very much appreciated :)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30281\n\nDifferential Revision: D18830818\n\nPulled By: ezyang\n\nfbshipit-source-id: 1a7fcee15354ae09e6644577e7fa33bd26acfe20", "pr_number": "30281", "files_changed": ["aten/src/ATen/SparseTensorImpl.cpp", "aten/src/ATen/TensorUtils.cpp", "aten/src/ATen/core/Tensor.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/Cross.cpp", "aten/src/ATen/native/Distance.cpp", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/Memory.cpp", "aten/src/ATen/native/PackedSequence.cpp", "aten/src/ATen/native/PointwiseOps.cpp", "aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/SortingUtils.h", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorFactories.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/test/scalar_test.cpp", "c10/core/TensorOptions.h", "test/cpp/api/tensor_options.cpp", "test/cpp/api/tensor_options_cuda.cpp", "test/cpp/jit/test_argument_spec.cpp", "test/cpp_extensions/cuda_extension.cpp", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/Functions.h", "tools/autograd/templates/python_variable_methods.cpp", "torch/csrc/Generator.cpp", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/custom_function.cpp", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/functions/comm.cpp", "torch/csrc/autograd/input_metadata.h", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/autograd/saved_variable.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/node_hashing.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/utils/tensor_apply.cpp", "torch/csrc/utils/tensor_list.cpp", "torch/csrc/utils/tensor_numpy.cpp", "torch/csrc/utils/tensor_types.cpp", "torch/csrc/utils/tensor_types.h", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/Utils.hpp"], "labels": ["merged", "open source", "topic: deprecation"]}, "bf1b4b6fef": {"title": "add torch_cpu to the static library list in TorchConfig.cmake.in (#30769)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30769\n\nThe TorchConfig.cmake is the public cmake we produce in install folder for\n3rd party client code to get all libtorch dependencies easily.\n\nApparently this build flow is not well covered by our CI (which is focused\non 1st party build / shared libraries?) as the little dummy project for\ncode analysis testing purpose was broken by #30315 without fail any CI.\n\nFixed the problem for mobile build and add the dummy project build to mobile\nCI as well.\n\nTest Plan: - make sure new CI pass;\n\nDifferential Revision: D18825054\n\nPulled By: ljk53\n\nfbshipit-source-id: 80506f3875ffbc1a191154bb9e3621c621e08b12", "pr_number": "30769", "files_changed": [".jenkins/pytorch/build-mobile.sh", ".jenkins/pytorch/build.sh", "cmake/TorchConfig.cmake.in", "test/mobile/op_deps/CMakeLists.txt", "test/mobile/op_deps/quantized_ops.cpp"], "labels": ["merged"]}, "9617d07bd5": {"title": "Wrap warning handler in a function to avoid siof (#30800)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30800\n\nSparseNN benchmark crashed due to this.\nWrap warning handler in a function to avoid siof.\n\nTest Plan: Tested locally, SparseNN benchmark no longer crashes.\n\nReviewed By: yinghai\n\nDifferential Revision: D18826731\n\nfbshipit-source-id: 8fcab8a3f38cc20f775409c0686363af3c27d0a6", "pr_number": "30800", "files_changed": ["c10/util/Exception.cpp"], "labels": ["fb-exported", "merged"]}, "be55874f2c": {"title": "style fixes to code analyzer (#30808)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30808\n\nAddressed some comments on #29550 after it's landed.\n\nTest Plan:\n```\nLLVM_DIR=... ANALYZE_TEST=1 CHECK_RESULT=1 tools/code_analyzer/build.sh\nLLVM_DIR=... ANALYZE_TORCH=1 tools/code_analyzer/build.sh -closure=false -debug_path=true\n```\n\nDifferential Revision: D18835100\n\nPulled By: ljk53\n\nfbshipit-source-id: 991d292ddc0211a88b04d0bdc24719f471c7786e", "pr_number": "30808", "files_changed": ["tools/code_analyzer/CMakeLists.txt", "tools/code_analyzer/op_dependency.cpp"], "labels": ["merged"]}, "244b0bd1a5": {"title": "Add docs for how we expose declarations in at:: to torch:: (#30760)", "body": "Summary:\nThis PR adds docs for how we expose declarations in `at::` to `torch::`, to make the semantics more clear.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30760\n\nDifferential Revision: D18833081\n\nPulled By: yf225\n\nfbshipit-source-id: eff4d8815c67f681ce3a930ce99771cf2e55dbd9", "pr_number": "30760", "files_changed": ["torch/csrc/api/include/torch/types.h"], "labels": ["merged", "module: cpp"]}, "c564d794ed": {"title": "Add ATen/native/ headers to torch target (#30835)", "body": "Summary:\nWe dont have ATen/native/*.h in torch target before, and we would like it to be exposed for external use.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30835\n\nDifferential Revision: D18836160\n\nPulled By: zrphercule\n\nfbshipit-source-id: 7330a9c9d8b65f173cc332b1cfeeb18c7dca20a8", "pr_number": "30835", "files_changed": ["aten/src/ATen/CMakeLists.txt", "setup.py"], "labels": ["merged"]}, "6486bdfb90": {"title": "Fix `os.register_at_fork` not defined on Windows (#30809)", "body": "Summary:\nAccording to https://docs.python.org/3.8/library/os.html#os.register_at_fork, this function is only available in Unix platforms.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30809\n\nDifferential Revision: D18828777\n\nPulled By: bddppq\n\nfbshipit-source-id: 3325a984da488bb0a80a5c27131553fbcf78921f", "pr_number": "30809", "files_changed": ["torch/multiprocessing/_atfork.py"], "labels": ["merge-this-please", "merged"]}, "f874230d33": {"title": "Vectorize smooth L1 loss backward function on CPU. (#30046)", "body": "Summary:\nBenchmark (Intel i7-8850H, turbo off, release build, RHEL 7.7):\n\n```\nimport timeit\n\nfor dtype in ('torch.float', 'torch.double'):\n    print(f'dtype={dtype}')\n    for n, t in [(10_000, 100000),\n                (100_000, 20000)]:\n        print(f'numel() == {n} for {t} times')\n        print(timeit.timeit('output.backward(retain_graph=True)', number=t, setup=f\"\"\"\nimport torch\nloss = torch.nn.SmoothL1Loss()\ninput = torch.randn({n}, requires_grad=True)\ntarget = torch.randn({n})\noutput = loss(input, target)\n\"\"\"))\n```\n\nBefore:\n\n```\ndtype=torch.float\nnumel() == 10000 for 100000 times\n6.154701935998673\nnumel() == 100000 for 20000 times\n5.157296671999575\ndtype=torch.double\nnumel() == 10000 for 100000 times\n6.195317157000318\nnumel() == 100000 for 20000 times\n5.099748799999361\n```\n\nAfter:\n\n```\ndtype=torch.float\nnumel() == 10000 for 100000 times\n4.968745516000126\nnumel() == 100000 for 20000 times\n2.4029395039997326\ndtype=torch.double\nnumel() == 10000 for 100000 times\n4.9910852479988534\nnumel() == 100000 for 20000 times\n2.4867371629989066\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30046\n\nDifferential Revision: D18602399\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 4c6c7b7b69ad6bce759786ddd7d6bc1e88ecf6ab", "pr_number": "30046", "files_changed": ["aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp"], "labels": ["merged"]}, "2ced81f289": {"title": "Revert \"Default to not build Caffe2 operators on Windows. (#29061)\" (#30740)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30740\n\nThis reverts commit 7102aceaf88ab71781c6019458bd7a07e86a532f.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18834315\n\nPulled By: ezyang\n\nfbshipit-source-id: 2dbd1cf686864b9840365083182cd6188a285399", "pr_number": "30740", "files_changed": ["CMakeLists.txt", "test/test_jit.py", "test/test_torch.py"], "labels": ["merged"]}, "0974dcc244": {"title": "Fix error checking of CUDA multi_margin_loss. (#30825)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30825\n\nIt didn't verify in the 1-d case that the targets were size 1..\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18833659\n\nPulled By: gchanan\n\nfbshipit-source-id: 9b0276e7b0423fdaf2ba7cfa34bde541558c61f9", "pr_number": "30825", "files_changed": ["aten/src/THCUNN/generic/MultiMarginCriterion.cu", "test/test_nn.py"], "labels": ["merged"]}, "e5bd7a7942": {"title": "we should have a config-based way to skip flaky tests (#29944)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29944\n\nThis particular approach queries our issue tracker for test titles that\nmatch the following format:\n\n```\nDISABLED test_async_grad_guard_with_grad (jit.test_async.TestAsync)\n```\n\nAnd then skips the python test for them. There is 1 second timeout so\nif the internet flakes we still run the test suite, without disabling any\ntests.\n\nThis is intended as a quick fix, similar to ninja unland, to get to a green\nmaster. Long term test disables should go into the code.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18621773\n\nPulled By: zdevito\n\nfbshipit-source-id: 5532f1d5fa3f83f77fc3597126cbb7dba09a3c33", "pr_number": "29944", "files_changed": ["test/common_utils.py", "tools/update_disabled_tests.sh"], "labels": ["merged"]}, "6e38d50352": {"title": "Revert D18117070: Migrate max and min (binary) from TH to ATen.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18117070\n\nOriginal commit changeset: e06d37a8a140\n\nfbshipit-source-id: 49dd33f52e7e3ffcaafc02109a0a0a67545ec7e8", "pr_number": null, "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryCompareKernel.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py"], "labels": []}, "82c3f4861f": {"title": "Move hardtanh activation to Aten(CPU, CUDA) (#30152)", "body": "Summary:\nVitalyFedyunin, This PR is about port Hardtanh activation to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.Hardtanh()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    fwd_t = 0\n    bwd_t = 0\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU\ninput size(128, 100) forward time is 0.02 (ms); backwad avg time is 0.06 (ms).\ninput size(128, 10000) forward time is 0.84 (ms); backwad avg time is 0.44 (ms).\n```\nAfter:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU\ninput size(128, 100) forward time is 0.02 (ms); backwad avg time is 0.05 (ms).\ninput size(128, 10000) forward time is 0.61 (ms); backwad avg time is 0.10 (ms).\n```\n`OMP_NUM_THREADS=1:`\n```\nBefore:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.07 (ms).\ninput size(128, 10000) forward time is 5.21 (ms); backwad avg time is 5.25 (ms).\nAfter:\ninput size(128, 100) forward time is 0.01 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10000) forward time is 1.09 (ms); backwad avg time is 1.09 (ms).\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30152\n\nDifferential Revision: D18815545\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: d23b6b340a7276457f22dce826bcbe3b341d755f", "pr_number": "30152", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/HardTanh.cu", "aten/src/THCUNN/generic/HardTanh.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/HardTanh.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp", "test/test_nn.py"], "labels": ["merged"]}, "4034aa7621": {"title": "make sure windows tests get triggered (#30836)", "body": "Summary:\nwe prefer \"_\" over \"-\" in build names, so change checks in test script\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30836\n\nDifferential Revision: D18840736\n\nPulled By: mingbowan\n\nfbshipit-source-id: 6fdf736496225c5f8ab44906d8f4681b7bf894a7", "pr_number": "30836", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/header-section.yml", ".circleci/verbatim-sources/windows-build-test.yml"], "labels": ["merged"]}, "c1159494a6": {"title": "Revert D18621773: we should have a config-based way to skip flaky tests", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18621773\n\nOriginal commit changeset: 5532f1d5fa3f\n\nfbshipit-source-id: 22239b88a6f9551938e6e2178bf9162e3385b011", "pr_number": null, "files_changed": ["test/common_utils.py", "tools/update_disabled_tests.sh"], "labels": []}, "a51c5f5cbf": {"title": "Add JIT pass to insert permutes for conv ops (#30679)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30679\n\nCaffe2 expects quantized ops to be in NHWC format while pytorch inputs are in NCHW.\nAdd a jit pass to insert permutes to convert from nchw2nhwc before each conv op and add nhwc2nchw permute after the conv op.\nUsing graph rewriter to find consecutive redundant permutes and remove them from the graph\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps\n\nImported from OSS\n\nDifferential Revision: D18790518\n\nfbshipit-source-id: 4dd39cf0b31b21f5586c0edfdce2260d4e245112", "pr_number": "30679", "files_changed": ["caffe2/python/onnx/backend_rep.py", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.h", "torch/onnx/utils.py"], "labels": ["jit", "merged"]}, "a7406516d1": {"title": "Refactor bias and weight check and add aten::linear pattern (#30474)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30474\n\nThere are some common parts in `isBiasOfConvOrLinear` and `isWeightOfConvOrLinear`, we can factor\nthem out, the refactor will allow for easier extension of new patterns\n\nTest Plan:\npython test/test_jit.py\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18795725\n\nfbshipit-source-id: 446463da5e3fa8464db441ed0d9651930487b3b7", "pr_number": "30474", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "7af9d77290": {"title": "Update persons_of_interest.rst", "body": "Updating to add POI for mobile, quantization and an addition to optimizers.", "pr_number": null, "files_changed": ["docs/source/community/persons_of_interest.rst"], "labels": []}, "ef95a72690": {"title": "modify test_local_shutdown_with_rpc to not be flaky (#30837)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30837\n\nThis test would get very occasional flakes, with an error saying the\nRPC timed out. This happened because one worker would still be waiting for the\nreturn value of an RPC, but another worker had already performed its local\nshutdown, so it would not have sent the response. This didn't show up in\ninitial testing since the flakiness is very rare (< 1/100 test runs). This diff\nfixes the issue by not erroring if these RPCs timeout. The reason this is okay\nis because with a local shutdown, we should not expect for all outstanding RPCs\nto be completed, since workers are free to shut down without completing/waiting\non outstanding work.\nghstack-source-id: 95021672\nghstack-source-id: 95021672\n\nTest Plan: Ran the test 1000 times to ensure that it is not flaky.\n\nDifferential Revision: D18775731\n\nfbshipit-source-id: 21074e8b4b4bbab2be7b0a59e80cb31bb471ea46", "pr_number": "30837", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "1fa4908ac0": {"title": "Refactor test_quantization.py and enable `test_nested` (#30475)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30475\n\natt\n\nTest Plan:\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18795727\n\nfbshipit-source-id: c9942c5361e0a34e91a08b8fc27405799db7ff4f", "pr_number": "30475", "files_changed": ["test/test_quantization.py"], "labels": ["merged"]}, "f1755d9aea": {"title": "Insert GetAttr for quantization parameters instead of Constant (#30551)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30551\n\nTo enable quantizing with shared types, we need to insert GetAttr nodes for\nquantization parameters since the code might be shared by multiple module instances\nand we'd like to make quantized module instance also share the same code but with\ndifferent values of attributes.\n\nTest Plan:\ntest_jit.py, test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18818652\n\nfbshipit-source-id: fc95623cac59dcedd9e3f95397524eae515e7a11", "pr_number": "30551", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "58cdf1429c": {"title": "Add tests for quantizing traced models (#30476)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30476\n\natt\n\nTest Plan:\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18795724\n\nfbshipit-source-id: 9253e102bf458d9185f68848071a4e4eff9f9b08", "pr_number": "30476", "files_changed": ["test/test_quantization.py"], "labels": ["merged"]}, "d32aec5ad6": {"title": "Add get_metrics and get_debug_info to rpc agent (#30833)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30833\n\n[rpc] Add get_metrics and get_debug_info to rpc agent\n\nTest Plan: UT and builds\n\nReviewed By: mrshenli\n\nDifferential Revision: D18835068\n\nfbshipit-source-id: f552cf196bb6d54ccd38a44ba981e7d5b15513f0", "pr_number": "30833", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h"], "labels": ["merged"]}, "2011cc1e91": {"title": "Fix half->float case of softmax backward when inner_size is not 1 (#30838)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30572\n\nThat unit test is tested to fail with master and success with this PR.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30838\n\nDifferential Revision: D18841066\n\nPulled By: ngimel\n\nfbshipit-source-id: 86a7ccdb3016c98d62dd0946daff101704cd1f68", "pr_number": "30838", "files_changed": ["aten/src/ATen/native/cuda/SoftMax.cu", "test/test_nn.py"], "labels": ["merged"]}, "b0cba8ceae": {"title": "Replace deprecated AT_ERROR with TORCH_CHECK to reduce warnings in rpc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30794\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18826311\n\nPulled By: mrshenli\n\nfbshipit-source-id: bfd58d30f386bbe9535264b2afce4acbe7ac5b0e", "pr_number": "30794", "files_changed": ["torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/script_call.cpp"], "labels": ["merged"]}, "619e2ffe23": {"title": "Replace deprecated AT_* with TORCH_* to reduce warnings in c10d", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30795\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18826310\n\nPulled By: mrshenli\n\nfbshipit-source-id: 0041ac2e5788e874e0a566abd57a8a90e658da9b", "pr_number": "30795", "files_changed": ["torch/csrc/distributed/c10d/comm.cpp", "torch/csrc/distributed/c10d/ddp.cpp", "torch/csrc/distributed/c10d/reducer.cpp"], "labels": ["merged"]}, "9a858aba5f": {"title": "Moving checks related to options.aliasAnalysis and schema.hasAliasInfo to read callsite (#30671)", "body": "Summary:\n**Context:**\nIn D18530964, we allow not set aliasAnalysis at previous registration call, and then update it to the correct one in following registration call.\n\nBut its not working E2E due to those existing checks.\n\nSo we want to remove or delay those TORCH_CHECKs.\n\nHere is the existing three callsites for operator.aliasAnalysisKind():\nhttps://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/caffe2/torch/csrc/jit/ir.cpp?lines=994%2C995%2C996%2C1001%2C1004\n\nhttps://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/caffe2/torch/csrc/jit/operator.cpp?lines=147%2C155\n\nhttps://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/caffe2/torch/csrc/jit/passes/alias_analysis.cpp?lines=260%2C277%2C380\n\n**Things to check**\n1. Those two checks are different. But since in original op_registration code, if options.schemaOrName_->is_right() is FALSE, we kind of convert it to FunctionSchema type, so in the read callsites, we only need to check the following: options.aliasAnalysisKind_ == AliasAnalysisKind::FROM_SCHEMA ||  !schema.hasAnyAliasInfo()\n\n2. If the three callsites above are indeed needed for those checks.\n\n3. Here we made assumptions that for reads from jit or other places, its always being called after all registrations calls are done. Trying to make sure its a valid assumption\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30671\n\nTest Plan: Will update and refactor the tests soon.\n\nDifferential Revision: D18784623\n\nPulled By: charliechen0401\n\nfbshipit-source-id: 75edea140d0ae3e54820e1aeef010c81fe26416a", "pr_number": "30671", "files_changed": ["aten/src/ATen/core/op_registration/op_registration.cpp", "test/cpp/jit/test_alias_analysis.cpp", "torch/csrc/jit/operator.h"], "labels": ["fb-exported", "jit", "merged"]}, "11b3065323": {"title": "Run method_tests on CUDA. (#30821)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30821\n\nWhile investigating while our tests didn't catch #30704 I noticed that none\nof our tests in method_tests() were being run on CUDA.  This diff moves\nthose tests into the new device-generic test framework so that we also get\nCUDA coverage.  For expediency, I blacklisted all tests which didn't work\non CUDA (rather than fix them); that's something we can leave for future PRs.\nThis is done by way of a new expectedFailure gadget.\n\nNote that all occurences of skipIfNoLapack needed to be replaced with\nskipCPUIfNoLapack.\n\nI punted for test_jit; it's possible those tests should also run CUDA but a JIT\nexpert should take a look here.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18840089\n\nPulled By: ezyang\n\nfbshipit-source-id: 66b613b5024c91d3e391c456bb642be7e73d4785", "pr_number": "30821", "files_changed": ["test/common_device_type.py", "test/common_methods_invocations.py", "test/test_autograd.py", "test/test_jit.py"], "labels": ["merged"]}, "1d7b40f1c4": {"title": "Fix reading `__cuda_array_interface__` without strides (#24947)", "body": "Summary:\nWhen converting a contiguous CuPy ndarray to Tensor via `__cuda_array_interface__`, an error occurs due to incorrect handling of default strides. This PR fixes this problem. It makes `torch.tensor(cupy_ndarray)` works for contiguous inputs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/24947\n\nDifferential Revision: D18838986\n\nPulled By: ezyang\n\nfbshipit-source-id: 2d827578f54ea22836037fe9ea8735b99f2efb42", "pr_number": "24947", "files_changed": ["test/test_numba_integration.py", "torch/csrc/utils/tensor_numpy.cpp", "torch/tensor.py"], "labels": ["merged", "module: internals", "module: numba", "module: numpy", "module: operators", "open source", "triaged"]}, "60714dfb64": {"title": "change index_select scalar_check to retain dimensionality of input. (#30790)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30790\n\nThe index_select documentaiton reads:\n\"The returned tensor has the same number of dimensions as the original tensor (input).\"\n\nBut the implementation would return a 0-dimensional tensor iff both the input and index were 0-dimensional.\nThis change makes it so we retuan a 0-dimensional tensor iff the input is 0-dimensional.\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30502\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18825717\n\nPulled By: gchanan\n\nfbshipit-source-id: aeb10c5107e748af3e264fbdc81fff5dd4833cc4", "pr_number": "30790", "files_changed": ["aten/src/ATen/Declarations.cwrap", "tools/autograd/derivatives.yaml"], "labels": ["merged", "topic: bc-breaking"]}, "e5d571ae25": {"title": "Remove scalar_check from topk, move it to the THC implementation.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30852\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18842662\n\nPulled By: gchanan\n\nfbshipit-source-id: b5e8a4367fce9441be2ddbd026495f1911038221", "pr_number": "30852", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/THC/generic/THCTensorTopK.cu", "test/test_torch.py"], "labels": ["merged"]}, "5687ee1d85": {"title": "added a serialize function in SGD class to utilize the existing macro for serialization/deserialization calls", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30739\n\nDifferential Revision: D18842908\n\nPulled By: anjali411\n\nfbshipit-source-id: 7dc13ff9c4fc126790b88b1b4b5d03425c349d38", "pr_number": "30739", "files_changed": ["torch/csrc/api/include/torch/optim/sgd.h", "torch/csrc/api/src/optim/sgd.cpp"], "labels": ["merged"]}, "377131b0eb": {"title": "MultiMarginCriterion: fix scalar_check in the case where reduction == None. (#30826)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30826\n\nPreviously the scalar_check for the reduction None case was:\ninput.dim() <= 1, but it should be target based, i.e.:\ntarget.dim() == 0.  This follows from the \"correct cases\", i.e.\n(N, C) X (N,) -> (N,)\n(C,) X () -> ()\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18833660\n\nPulled By: gchanan\n\nfbshipit-source-id: 26338b842a8311718c4b89da3e2f1b726d5409b8", "pr_number": "30826", "files_changed": ["aten/src/ATen/native/LossMultiMargin.cpp", "aten/src/ATen/nn.yaml", "test/common_nn.py", "test/test_torch.py"], "labels": ["merged", "topic: bc-breaking"]}, "0051467118": {"title": "Update CITATION from Workshop paper to Conference paper (#30872)", "body": "Summary:\nThe conference paper is finally published at NeurIPS 2019: http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30872\n\nDifferential Revision: D18854253\n\nPulled By: soumith\n\nfbshipit-source-id: 4f91838b1953e976542997959d5571884f739872", "pr_number": "30872", "files_changed": ["CITATION"], "labels": ["merged"]}, "4ed2eae2d0": {"title": "Add registerQParams function (#30552)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30552\n\nFor upcoming changes to support quantizing shared class type\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18818653\n\nfbshipit-source-id: 393a55db69b20a1c00ffa0157ab568cb097915b2", "pr_number": "30552", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "b0e7db5b31": {"title": "Revert D18840736: make sure windows tests get triggered", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18840736\n\nOriginal commit changeset: 6fdf73649622\n\nfbshipit-source-id: 719576e9c717847bfb4b057875a273123e941db3", "pr_number": null, "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/header-section.yml", ".circleci/verbatim-sources/windows-build-test.yml"], "labels": []}, "37435d36ed": {"title": "Refactor VariableTypeManual (#30649)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30649\n\nOperators in VariableTypeManual are now no longer registered against the VariableTypeId key, but they are registered as compound ops. See https://github.com/pytorch/pytorch/issues/30102 for background.\n\nThis also requires the non-variable codegen to ignore them and requires removal of VariableMethodStubs.cpp.\n\nSo, because function_wrapper.py now also needs to know which ops are manual, instead of having a hard-coded list in gen_variable_type.cpp for ops with manual implementation, we now have a `manual_kernel_registration` flag in native_functions.yaml that disables the registration of operator kernels for this operator (the schema is still registered). Then, we manually register the right kernels for the operator.\nghstack-source-id: 95082204\n\nTest Plan: unit tests\n\nDifferential Revision: D18778191\n\nfbshipit-source-id: 0af6f9e43ff4fb9800ce19b286dfccd0fd22cc41", "pr_number": "30649", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/README.md", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/TensorProperties.cpp", "aten/src/ATen/native/VariableMethodStubs.cpp", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp", "aten/src/ATen/native_parse.py", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/VariableType.h", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["merged"]}, "e123d90a93": {"title": "Back out \"Back out \"Back out \"Revert D18542342: Boxed variable dispatch\"\"\" (#30650)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30650\n\nOriginal commit changeset: 51bb7aac7cb7\nghstack-source-id: 95082205\n\nTest Plan: CI\n\nDifferential Revision: D18778190\n\nfbshipit-source-id: 7e9577e88fd0492006b6ea836ec081aea9da6b0c", "pr_number": "30650", "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "caffe2/c2_aten_srcs.bzl", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["merged"]}, "44ff7b08d8": {"title": "Reduce intrusive_ptr incref/decref costs (#30709)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30709\n\nIntrusive_ptr doesn't provide a explicit incref method. When a users want to\nincref the target, they creates a intrusive_ptr to wrap the target, then makes\na copy which does the actual incref, then release both the first intrusive_ptr\nand the copy to prevent decref at deconstruction time. This is very\ninefficient. Instead, do the incref/decref directly.\n\nDifferential Revision: D18798505\n\nfbshipit-source-id: 524d4f30d07d733df09d54423b044d80e4651454", "pr_number": "30709", "files_changed": ["c10/core/TensorImpl.h", "c10/test/util/intrusive_ptr_test.cpp", "c10/util/intrusive_ptr.h"], "labels": ["fb-exported", "merged"]}, "78254eab45": {"title": "Add mobile operator observer for qpl logging.", "body": "Summary: Add mobile operator observer to measure performance of each operator run, the result will also log into QPL event: [MOBILE_OPERATOR_STATS ](https://fburl.com/quicklog/8773a00a).\n\nTest Plan:\nRun pytext model through BI cloaking flow on lite-interpreter and verify logs are sent:\n1. buck install -r fb4a\n2. Go to internal setting and find MobileConfig, search for android_bi_infra_cloaking_iab_models and set the following params:\na. sample_rate: 1.0\nb. enabled: true\nc. use_bytedoc_pytorch_model: true\nd. use_bytedoc_caffe2_model: false\ne. use_full_jit: false\n3. Go back to new feed and scroll down until find an ads which will direct you to offsite webpage;\n4. Click on the ads, wait for the offsite ads loads;\n5. Click back to news feed;\n6. Go to scuba table: https://fburl.com/scuba/er7t4g9u and see all the operator runs have been logged:\n\n{F223250762}\n\nReviewed By: ljk53\n\nDifferential Revision: D18131224\n\nfbshipit-source-id: 23e2f6e2a9851c04b29511b45dc53f3cce03e8a0", "pr_number": null, "files_changed": ["torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h", "torch/csrc/jit/mobile/observer.h"], "labels": []}, "d6ddfab11f": {"title": "save linux build binary size to Scuba (#30832)", "body": "Summary:\nexample: https://fburl.com/scuba/mjheume7\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30832\n\nDifferential Revision: D18857146\n\nPulled By: mingbowan\n\nfbshipit-source-id: 66bcd352922944c227f337a66e8a75e2d7393fd3", "pr_number": "30832", "files_changed": [".circleci/config.yml", ".circleci/scripts/upload_binary_size_to_scuba.py", ".circleci/verbatim-sources/binary-job-specs.yml"], "labels": ["merged"]}, "81e4739141": {"title": "Move QScheme ops to c10 (#30134)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30134\n\nghstack-source-id: 95055387\n\nTest Plan: buck build mode/dev caffe2:generate-code\n\nDifferential Revision: D18609716\n\nfbshipit-source-id: fec39359e0b97387a9b13f8179d72a731cc61808", "pr_number": "30134", "files_changed": ["aten/src/ATen/core/boxing/kernel_functor.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "aten/src/ATen/native/native_functions.yaml", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/script/schema_type_parser.cpp", "torch/csrc/jit/unpickler.cpp"], "labels": ["merged"]}, "6d06b925ba": {"title": "Remove `values_to_quantize_` (#30858)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30858\n\nThis is not needed since we have `values_to_qparams_`\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18848992\n\nfbshipit-source-id: dc81f59967a93abdd5562f1010f02de4f4e60db0", "pr_number": "30858", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "5e6c3fb23b": {"title": "Add more details to explain rpc_backend_options arg in init_rpc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30855\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18847529\n\nPulled By: mrshenli\n\nfbshipit-source-id: b4f0d5797f3b41cce155b7821d6bd34b268bd24e", "pr_number": "30855", "files_changed": ["torch/distributed/rpc/__init__.py"], "labels": ["merged"]}, "642469b706": {"title": "Fix examples in API doc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30856\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18847528\n\nPulled By: mrshenli\n\nfbshipit-source-id: 57f666d9d4b634fb77b1b65debd2b07e2bebd57a", "pr_number": "30856", "files_changed": ["torch/distributed/rpc/api.py"], "labels": ["merged"]}, "26c51468c5": {"title": "Fix examples in RRef API doc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30857\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18847527\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7dc9d28277597f8fc3ef97fa9ac98a312e76e6fb", "pr_number": "30857", "files_changed": ["torch/csrc/distributed/rpc/init.cpp"], "labels": ["merged"]}, "4fd20c0816": {"title": "Kill hypothesis deadline testing (#30890)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30890\n\nWe've received way too many complaints about this functionality making tests flaky, and it's not providing value to us anyway. Let's cut the shit and kill deadline testing\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18857597\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 67e3412795ef2fb7b7ee896169651084e434d2f6", "pr_number": "30890", "files_changed": ["test/hypothesis_utils.py", "test/test_fake_quant.py", "test/test_qat.py", "test/test_quantization.py", "test/test_quantized.py", "test/test_quantized_nn_mods.py"], "labels": ["merged"]}, "223f46f5fa": {"title": "Fix flake8 warning (#30905)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30905\n\n-\nghstack-source-id: 95117983\n\nTest Plan: -\n\nDifferential Revision: D18861981\n\nfbshipit-source-id: b794a7fbe05af29471286c7f665cf3f86541eb5a", "pr_number": "30905", "files_changed": ["aten/src/ATen/function_wrapper.py"], "labels": ["merged"]}, "baccd26df7": {"title": "update code analyzer script to handle splitted torch libraries (#30864)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30864\n\nChange it to handle all archive files under install folder.\n\nTest Plan:\n```\nANALYZE_TEST=1 CHECK_RESULT=1 tools/code_analyzer/build.sh\nANALYZE_TORCH=1 tools/code_analyzer/build.sh\n```\n\nDifferential Revision: D18850317\n\nPulled By: ljk53\n\nfbshipit-source-id: 7c57ae16c82b6ded53aa7df385f3b6074190fc04", "pr_number": "30864", "files_changed": ["tools/code_analyzer/build.sh"], "labels": ["merged"]}, "a77eafa1d8": {"title": "Fix 'initialized after field' error (#30908)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30908\n\nSame as title.\n\nTest Plan: Wait for CI to clear.\n\nReviewed By: bddppq, xw285cornell\n\nDifferential Revision: D18862837\n\nfbshipit-source-id: bc34356b85774fc20ba46d321c8a2bb5d5c727f6", "pr_number": "30908", "files_changed": ["aten/src/ATen/native/cuda/Reduce.cuh"], "labels": ["fb-exported", "merged", "module: rocm"]}, "c37de32b23": {"title": "Enable len(dataloader) for iterable dataset (#23587)", "body": "Summary:\nCopy-paste comment from code for reasoning:\n\n```\n            # NOTE [ IterableDataset and __len__ ]\n            #\n            # For `IterableDataset`, `__len__` could be inaccurate when one naively\n            # does multi-processing data loading, since the samples will be duplicated.\n            # However, no real use case should be actually using that behavior, so\n            # it should count as a user error. We should generally trust user\n            # code to do the proper thing (e.g., configure each replica differently\n            # in `__iter__`), and give us the correct `__len__` if they choose to\n            # implement it (this will still throw if the dataset does not implement\n            # a `__len__`).\n            #\n            # To provide a further warning, we track if `__len__` was called on the\n            # `DataLoader`, save the returned value in `self._len_called`, and warn\n            # if the iterator ends up yielding more than this number of samples.\n```\n\nFixes https://github.com/pytorch/pytorch/issues/30184\nPull Request resolved: https://github.com/pytorch/pytorch/pull/23587\n\nDifferential Revision: D18852625\n\nPulled By: ailzhang\n\nfbshipit-source-id: aea8d4d70c7f21aaa69b35908a6f43026493d826", "pr_number": "23587", "files_changed": ["test/common_utils.py", "test/test_dataloader.py", "torch/utils/data/dataloader.py"], "labels": ["merged", "module: dataloader", "open source", "triaged"]}, "118f1c633b": {"title": "refactor the way we are handling bailout counts", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30410\n\nDifferential Revision: D18733370\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 0ea9dc0f3dd1a47bcc09f1d54745460f9bd71886", "pr_number": "30410", "files_changed": ["torch/csrc/jit/passes/bailout_graph.cpp", "torch/csrc/jit/passes/liveness.cpp"], "labels": ["jit", "merged"]}, "7b97eaeba5": {"title": "Add module level qpl logging. (#30906)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30906\n\nAdd mobile module observer to measure performance of each method run.\nghstack-source-id: 95120194\n\nTest Plan:\nRun pytext model through BI cloaking flow on lite-interpreter and verify logs are sent:\n1. buck install -r fb4a\n2. Go to internal setting and find MobileConfig, search for android_bi_infra_cloaking_iab_models and set the following params:\na. sample_rate: 1.0\nb. enabled: true\nc. use_bytedoc_pytorch_model: true\nd. use_bytedoc_caffe2_model: false\ne. use_full_jit: false\n3. Go back to new feed and scroll down until find an ads which will direct you to offsite webpage;\n4. Click on the ads, wait for the offsite ads loads;\n5. Click back to news feed;\n6. Go to scuba table: https://fburl.com/scuba/4fghwp0b and see all the operator runs have been logged:\n\n{F223456981}\n\nReviewed By: ljk53\n\nDifferential Revision: D18702116\n\nfbshipit-source-id: a9f07eee684e3022cef5ba3c5934f30f20192a85", "pr_number": "30906", "files_changed": ["torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/observer.cpp", "torch/csrc/jit/mobile/observer.h"], "labels": ["jit", "merged"]}, "cd6167ff63": {"title": "Upgrade bazel to 1.2.0. (#30885)", "body": "Summary:\nCompanion diff for https://github.com/pytorch/xla/pull/1464. Should land only after the pytorch/xla PR is in.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30885\n\nDifferential Revision: D18866835\n\nPulled By: ailzhang\n\nfbshipit-source-id: 51f4d2770f8ef873a659579ddd81a42957ffb885", "pr_number": "30885", "files_changed": [".jenkins/pytorch/build.sh"], "labels": ["merged"]}, "8d35b6cec7": {"title": "embedding_bag make_bag_size optimization (#30701)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30701\n\nFrom James' PR https://github.com/pytorch/pytorch/pull/19715\n\nembedding_bag microbenchmarks:\nBaseline: P123020983\nRefactor make_bag_size, no changing at::zeros to at::empty (this diff): P123021393\nInference benchmark on T6_SKL - _embedding_bag self time only:\nbs=40, baseline: .302 ms/iter\nbs=40, with diff: .244 ms/iter\nbs=1 baseline: .148 ms/iter\nbs=1 with diff: .124 ms/iter\nThe bigger gap comes from fb::embedding_bag_byte_rowwise_offsets, I'm looking into that one too.\n\nTest Plan:\nMKL_NUM_THREADS=1 OMP_NUM_THREADS=1 numactl -m 0 -C 3 ./inference_benchmark_nolr_emb.par --pt-scripted-model=traced_model.pt --pt-inputs=\"batch_size_40/pt_inputs.pth\" --iters=3000 --warmup-iters=100\nbuck run mode/opt //caffe2/benchmarks/operator_benchmark:benchmark_all_other_test -- --tag_filter all --iterations 3000 --operators embeddingbag\n\nReviewed By: yinghai, qizzzh\n\nDifferential Revision: D18800166\n\nfbshipit-source-id: 820e6ece0b6ade72ee42409661f92c548f43a4cb", "pr_number": "30701", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp"], "labels": ["fb-exported", "merged"]}, "62b10721fb": {"title": "Actually make flake8 do something (#30892)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30892\n\nFixes all outstanding lints and actually installs a properly configured\nflake8\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18862825\n\nPulled By: suo\n\nfbshipit-source-id: 08e9083338a7309272e17bb803feaa42e348aa85", "pr_number": "30892", "files_changed": [".flake8", ".github/workflows/lint.yml", "aten/src/ATen/native/quantized/cpu/qnnpack/generate-wrapper.py", "test/common_methods_invocations.py", "test/dist_autograd_test.py", "test/jit/_imported_class_test/bar.py", "test/jit/_imported_class_test/very/very/nested.py", "test/jit/test_class_type.py", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "test/rpc_agent_test_fixture.py", "test/test_cuda.py", "test/test_jit.py", "test/test_quantization.py", "test/test_sparse.py", "test/test_torch.py", "test/test_type_promotion.py", "torch/_torch_docs.py", "torch/autograd/gradcheck.py", "torch/functional.py", "torch/jit/__init__.py", "torch/onnx/symbolic_caffe2.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py", "torch/onnx/utils.py", "torch/quantization/_quantize_script.py", "torch/tensor.py"], "labels": ["jit", "merged"]}, "5c56986738": {"title": "Attach autograd edges only for tensors requiring grad. (#30904)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30904\n\nWhen we sent tensors over RPC, on the server side we would call\naddRecvRpcBackward which would call `set_history` on all tensors. This was\nincorrect and set the `requires_grad` flag on tensors that didn't actually need\ngrad.\n\nTo fix this, we only attach autograd edges to tensors that need grads.\nghstack-source-id: 95113672\nghstack-source-id: 95113999\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18828561\n\nfbshipit-source-id: d8942b76e9e4c567f8f1821f125c00d275ea0f90", "pr_number": "30904", "files_changed": ["test/dist_autograd_test.py", "torch/csrc/distributed/autograd/utils.cpp"], "labels": ["merged"]}, "a26238da57": {"title": "Enable using `torch.autograd.profiler.record_function` as decorator (#30861)", "body": "Summary:\n```python\nrecord_function('my_func')\ndef f(x, y):\n    return x + y\n\nwith profile() as p:\n    f(1, 2)\nprint(prof.key_averages().table())\n```\n\n```\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                                  Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\nmy_func                               85.42%           86.796us         87.27%           88.670us         88.670us         1\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\nSelf CPU time total: 101.606us\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30861\n\nDifferential Revision: D18857993\n\nPulled By: bddppq\n\nfbshipit-source-id: eb6b8e2a8d4f3a7f8e5b4cb3da1ee3320acb1ae7", "pr_number": "30861", "files_changed": ["test/test_autograd.py", "torch/autograd/profiler.py"], "labels": ["merged"]}, "63f1b780ba": {"title": "Support exporting aten::copy_ and aten::index_put to ONNX opset 11 (#26941)", "body": "Summary:\n- [x] Add more comments and refactor the logic of `ReshapeToAdvancedIndexingFormat`\n- [x] Add more description here. Cases that are/aren't supported, and how they are supported.\n- [x] Need to merge this PR https://github.com/pytorch/pytorch/issues/27186 to enable testing inplace operators.\n\nWe are now supporting exporting aten::copy_ and aten::index_put to ONNX.\nHere's a breakdown of the different cases in PyTorch code.\n\n```\n# Case 1: Scalar Indices\nx[0, 1, 2] = data\n\n# Case 2: Slice Indices\nx[1:3, :, ::2] = data\n\n# Case 3: Ellipsis Indices\nx[..., 0] = data\n\n# Case 4: Tensor Indices\nind1 = torch.tensor([0, 2])\nind2 = torch.tensor([1, 1])\nx[ind1, ind2] = data\n\n# Case 5: Mixing all the above cases\nind1 = torch.tensor([0, 2])\nind2 = torch.tensor([1, 1])\nx[1:3, ind1, ind2, ..., 3] = data\n```\n\nLimitations:\n\nTensor indices must be consecutive, and 1-d tensors.\n\n```\n# Supported\nind1 = torch.tensor([0, 2])\nind2 = torch.tensor([1, 1])\nx[ind1, ind2] = data\n\n# Not supported\nind1 = torch.tensor([0, 2])\nind2 = torch.tensor([1, 1])\nind3 = torch.tensor([[0], [1]])\nx[ind1, :, ind2] = data\nx[ind3] = data\n```\n\nNegative indices are not supported.\n```\n# Not supported\nx[-1] = data\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26941\n\nDifferential Revision: D17951030\n\nPulled By: houseroad\n\nfbshipit-source-id: 4357777072f53aa0bc4b297aa1ee53457a7f8dec", "pr_number": "26941", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.cpp", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.h", "torch/onnx/symbolic_opset11.py", "torch/onnx/utils.py"], "labels": ["jit", "merged", "module: autograd", "module: build", "module: ci", "module: onnx", "open source", "triaged"]}, "f1bd8cc286": {"title": "Fix lint issues in dist_autograd_test.py (#30928)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30928\n\nghstack-source-id: 95152373\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18872870\n\nfbshipit-source-id: 2cd1ef228da4bd90c13e2f067a0c89b975fa3179", "pr_number": "30928", "files_changed": ["test/dist_autograd_test.py"], "labels": ["merged"]}, "8b6d7698d6": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/40ac0e57c14095399823290fa2bafd6df81c3276\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: ac74c10651a5a4ef67c93a38dc6673f0687e38ae", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "4bb497b38e": {"title": "MultiheadAttention fixes", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30666\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18864094\n\nPulled By: pbelevich\n\nfbshipit-source-id: f7a634b2c7f526282bf918d47b9cc82aa0c0af1d", "pr_number": "30666", "files_changed": ["test/test_nn.py"], "labels": ["merged"]}, "0b33080992": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbzmq/commit/452ebf30a85e385d07a612d696bdd491cb8ffbe8\nhttps://github.com/facebook/wangle/commit/8e85afc8a1b507126ecb996abf58f0e6683c5e2f\nhttps://github.com/facebookincubator/katran/commit/39d204760c2e1f6b8c30d1746e60882532ee135b\nhttps://github.com/pytorch/fbgemm/commit/576037639285dcc268d02b0547aa54e9140b5d33\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: aa1ff805dbe1a1cbe5eb256ed2ba30af587a8707", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "776fdda753": {"title": "Add debug info API for distributed autograd. (#30642)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30642\n\nAdding a couple of basic metrics for distributed autograd which would\nhelp in determining stuckness.\nghstack-source-id: 95156189\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18776478\n\nfbshipit-source-id: a0556ad6fe2b7c3cd0082ee2350c1c78cafaaec5", "pr_number": "30642", "files_changed": ["test/dist_autograd_test.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/distributed/autograd/context/container.cpp", "torch/csrc/distributed/autograd/context/container.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h", "torch/csrc/distributed/autograd/init.cpp"], "labels": ["merged"]}, "6848f9abb8": {"title": "call fp16<->fp32 routines in fbgemm from Half2Float and Float2Half operators (#30715)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30715\n\nChanged caffe2/caffe2/TARGETS file to define USE_FBGEMM for x86 and USE_SSE_ONLY is not defined.\n\nTest Plan: buck test caffe2/caffe2:caffe2_test_cpu -- Float16\n\nReviewed By: jianyuh\n\nDifferential Revision: D18806067\n\nfbshipit-source-id: 1b44b90a9f6dc3c27f81a46038c0f7542ed2bab3", "pr_number": "30715", "files_changed": ["caffe2/operators/half_float_ops.cc", "caffe2/operators/half_float_ops.h"], "labels": ["fb-exported", "merged"]}, "190dac13e3": {"title": "Use universal references and perfect forwarding in Loops.h. (#30466)", "body": "Summary:\nThis simplifies the generated code a bit, saving about 40K off of libtorch.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30466\n\nDifferential Revision: D18836215\n\nPulled By: resistor\n\nfbshipit-source-id: ad75c9e04783bb29cc06afd2022f73f9625dd52b", "pr_number": "30466", "files_changed": ["aten/src/ATen/native/cpu/Loops.h"], "labels": ["merged"]}, "c75bc9067c": {"title": "MultiMarginCriterion: move scalar_check from codegen to code.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30827\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18833658\n\nPulled By: gchanan\n\nfbshipit-source-id: decd42789d92d4fbfeea9b470b3d7333e3862263", "pr_number": "30827", "files_changed": ["aten/src/ATen/nn.yaml", "aten/src/THCUNN/generic/MultiMarginCriterion.cu"], "labels": ["merged"]}, "4f342a61c1": {"title": "add the worker IDs outside of addSendRpcBackward to ensure they are (#30914)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30914\n\nWhen tensors don't require grad, we don't call `addSendRpcBackward`, where we record known workerIDs to clean up the dist autograd context later. But since  https://github.com/pytorch/pytorch/pull/29781, we always include the autograd context ID in RPCs, even if tensors do not require grad. So, it could be possible that we don't release the contexts on some nodes.\n\nThis can contribute to OOMs since the contexts will not be cleaned up in this case, which can be checking by running the unit test without this patch. We can fix this issue by moving the `addKnownWorkerIds`  call to the `getMessageWithAutograd` function.\nghstack-source-id: 95178561\n\nTest Plan: Added a unit test: `test_context_cleanup_tensor_no_grad`\n\nDifferential Revision: D18869191\n\nfbshipit-source-id: b80f66bfd0dd7d01960abe1691d3f44095bb1b2b", "pr_number": "30914", "files_changed": ["test/cpp/dist_autograd/test_dist_autograd.cpp", "test/dist_autograd_test.py", "torch/csrc/distributed/autograd/utils.cpp"], "labels": ["merged"]}, "daef363b15": {"title": "Move Softshrink activation to Aten(CPU+CUDA) (#30229)", "body": "Summary:\nVitalyFedyunin, This PR is about port Softshrink activation to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.Softshrink()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    fwd_t = 0\n    bwd_t = 0\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) forward time is 0.06 (ms); backwad avg time is 0.12 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.18 (ms).\nCPU:\ninput size(128, 100) forward time is 0.19 (ms); backwad avg time is 0.23 (ms).\ninput size(128, 10000) forward time is 17.23 (ms); backwad avg time is 16.83 (ms).\n```\nAfter:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU:\ninput size(128, 100) forward time is 0.08 (ms); backwad avg time is 0.05 (ms).\ninput size(128, 10000) forward time is 0.32 (ms); backwad avg time is 0.08 (ms).\n```\n`OMP_NUM_THREADS=1:`\n```\nBefore:\ninput size(128, 100) forward time is 0.08 (ms); backwad avg time is 0.10 (ms).\ninput size(128, 10000) forward time is 7.58 (ms); backwad avg time is 7.91 (ms).\nAfter:\ninput size(128, 100) forward time is 0.08 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10000) forward time is 7.30 (ms); backwad avg time is 1.02 (ms).\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30229\n\nDifferential Revision: D18810054\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: e19074824396570db45ba488ae4f9fe1b07a5839", "pr_number": "30229", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/SoftShrink.cu", "aten/src/THCUNN/generic/SoftShrink.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/SoftShrink.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "528fa737ba": {"title": "Custom op autograd tests (#30519)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30519\n\nRe-enable them and write a few additional ones\nghstack-source-id: 95143051\n\nTest Plan: unit tests\n\nDifferential Revision: D18729561\n\nfbshipit-source-id: 8cefd8320913d72a450a3324bfd7c88faed072d7", "pr_number": "30519", "files_changed": ["aten/src/ATen/core/op_registration/op_registration_test.cpp"], "labels": ["merged"]}, "536481d9de": {"title": "Fix missing virtual destructor (#30927)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30927\n\nClasses that are used virtually (e.g. have virtual methods) must have a virtual destructor or bad things happen\nghstack-source-id: 95144736\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18870351\n\nfbshipit-source-id: 333af4e95469fdd9103aa9ef17b40cbc4a343f82", "pr_number": "30927", "files_changed": ["torch/csrc/api/include/torch/data/datasets/chunk.h"], "labels": ["merged"]}, "fb36f1c334": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/0f96b98cecb37a531cef04488097c7299fc70d83\nhttps://github.com/facebook/proxygen/commit/8090b337a41555b29ab2a6f53998f02874394dc7\nhttps://github.com/facebook/rocksdb/commit/e43d2c44244e685f7a1b1a163506b911eb1c49f4\nhttps://github.com/facebookincubator/mvfst/commit/70d1c268bf0a4a53730672a363f98825f39a3d52\nhttps://github.com/facebookresearch/pytorch-biggraph/commit/fc6140865be79771b874e84ecf5736c962ee480c\nhttps://github.com/pytorch/fbgemm/commit/4caba2ed655d8054129a22a7adecaad07dea4d42\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 5b4edf4267942ab0cbd2980dc500227e3ce353e3", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "5bf58274cc": {"title": "getQParams return a dictionary of qparams (#30859)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30859\n\nWe can dictionary of quantization parameters to simplify the code\nhandling these things a bit\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18849023\n\nfbshipit-source-id: 09e9860b2656a1affa8776016e16794529bcee3b", "pr_number": "30859", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "45f0556ba0": {"title": "Proper print for one element tuple (#30853)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30853\n\nRight now we print one element tuple as `(val)`, and it will\nbe interpreted as `val` in parsing, this PR changes it\nto `(val,)` so we can recognize the one element tuple in parsing\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18846849\n\nfbshipit-source-id: 42959b9190c2567ef021a861497077c550324b7c", "pr_number": "30853", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "test/cpp/jit/test_ivalue.cpp"], "labels": ["merged"]}, "648bb501a1": {"title": "rename shouldAnnotate api (#30543)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30543\n\n`shouldAnnotate` doesn't make make a ton of sense as a public api\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18833608\n\nPulled By: eellison\n\nfbshipit-source-id: 460ee05d0fa91b1edc640c037be2a6ee8eaf50a6", "pr_number": "30543", "files_changed": ["torch/csrc/jit/passes/alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.h"], "labels": ["jit", "merged"]}, "3eefc06feb": {"title": "add constant prop for immutable types (#30544)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30544\n\nRun Constant Propagation upon compilation only on ops with non-aliasing inputs and outputs. This speeds up the first run of `torchvision.models.resnet18` by over 50% and speeds up compilation by about 25% (although the effects didn't seem additive with with https://github.com/pytorch/pytorch/pull/30503, so I'm going to land this PR first and then see if caching still has a sizable impact).\n\nRunning constant prop only with non-aliasing types does a lot of graph cleanup by removing constant ifs and a bunch of other smaller ops. It also avoids all the jitter problems we had when we tried running full constant prop previously. Bc it is idempotent it doesn't jitter, and it doesn't jitter graphs constructed from tracing because tracing doesn't emit any ops that only involve non-aliasing inputs.\n\nFull constant prop isn't idempotent because what ops are run depends on the state of mutation in alias db, which will often change upon successive iterations of constant propagation, and bc it affects graphs constructed from tracing.\n\nEdit: if we were okay with running constant propagation on graphs constructed from tracing (potentially making them hard to debug), an alternative would be to run constant propagation until the graph reaches a fixed point.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18833607\n\nPulled By: eellison\n\nfbshipit-source-id: 92a0adb4882d67ed5a0db5c279f5e122aeeba54a", "pr_number": "30544", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/passes/constant_propagation.h", "torch/csrc/jit/script/compiler.cpp"], "labels": ["jit", "merged"]}, "82268bf300": {"title": "handle reassignment to inf and nan (#30877)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30877\n\nPreviously, when the environment tried to reassign variables which had been assigned to \"inf\" or \"nan\" it would fail because they are not simple values. Constant prop exposed this, a test was failing internally because of it.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D18861016\n\nPulled By: eellison\n\nfbshipit-source-id: b9b72978a26a0b00b13bf8ea7685825551f5a541", "pr_number": "30877", "files_changed": ["test/test_jit.py", "torch/csrc/jit/import_source.cpp"], "labels": ["jit", "merged"]}, "a38c9b1ade": {"title": "Adding debugging metrics to process group agent", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30884\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18857140\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4ec61d13778dd49467159d0db4b6dd51feaf282b", "pr_number": "30884", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/process_group_agent.cpp"], "labels": ["merged"]}, "8a57362000": {"title": "Fix index out of bound error in Engine::ready_queue_size when called before start_threads", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30967\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18887178\n\nPulled By: mrshenli\n\nfbshipit-source-id: 67baeac9214a4749ce7e9b4d89862c93620b2d5e", "pr_number": "30967", "files_changed": ["torch/csrc/autograd/engine.cpp"], "labels": ["merged"]}, "e9ca13d7f5": {"title": "Add glue code to collect debug info from all components", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30888\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18857139\n\nPulled By: mrshenli\n\nfbshipit-source-id: 5c1bfb83a21a4a57c4297bb94f14baa09520b791", "pr_number": "30888", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/distributed/rpc/__init__.py"], "labels": ["merged"]}, "a03581b927": {"title": "add tests that schemas are valid (#30749)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30749\n\nAdd check to schemas that the schema is sane.\n\nI removed the defaults from symbolic_script because they were in some cases wrong and don't actually do anything. At the point they're invoked the forward should already have matched all arguments.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18864775\n\nPulled By: eellison\n\nfbshipit-source-id: 273d7e96d65b8a3d3de72e2d7bfcdf2417046c6b", "pr_number": "30749", "files_changed": ["aten/src/ATen/core/function_schema.cpp", "aten/src/ATen/core/function_schema.h", "test/test_function_schema.py", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/symbolic_script.cpp"], "labels": ["jit", "merged"]}, "446488960a": {"title": "polish up overloads on free functions (#30356)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30356\n\nThis finishes up the `torch.jit.overload` api for free-functions.\n- defaults now required on the implementation function itself\n- fully follows [overload spec](https://mypy.readthedocs.io/en/latest/more_types.html#function-overloading) such that the following is supported\n\n```\noverload\ndef mouse_event(x1: int, y1: int) -> ClickEvent: ...\ndef mouse_event(x1: int,\n                y1: int,\n                x2: Optional[int] = None,\n                y2: Optional[int] = None): ...\n```\n\nNote: `jit.overload` isn't supported yet for UDT, but is support for modules. This PR doesn't make the same changes for modules, if reviewers think I should include them then I could do so in a follow up PR or wait to land this. Since that's still an internal api I think it's fine, and the changes here would allow us to expose `torch.jit.overload` on free functions.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18864774\n\nPulled By: eellison\n\nfbshipit-source-id: 6c566738bd6f0551a000a9ea8d56e403636b7856", "pr_number": "30356", "files_changed": ["aten/src/ATen/core/function_schema.h", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "73dd8c005a": {"title": "Revert D18864774: polish up overloads on free functions", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18864774\n\nOriginal commit changeset: 6c566738bd6f\n\nfbshipit-source-id: 669192605a3bc1a6ba06bbb5cae54f61637a45ae", "pr_number": null, "files_changed": ["aten/src/ATen/core/function_schema.h", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py"], "labels": []}, "fa6661422f": {"title": "Disable flaky test_rref_context_debug_info", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30990\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18893023\n\nPulled By: mrshenli\n\nfbshipit-source-id: 80b36927f243fa53c4d64f7e7c51097290ffdeee", "pr_number": "30990", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "04b9324476": {"title": "Factor out getInvokedMethod in `InsertQuantDeQuantHelper` (#30860)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30860\n\natt\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18849021\n\nfbshipit-source-id: e5ff260f2f4e88075b0c6b32ccfd8272053ccc41", "pr_number": "30860", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "5205556782": {"title": "Export custom ops (#29752)", "body": "Summary:\nUpdated to export API:\nWhen calling this API, a dict containing the custom opsets (domain and version) used to export the model could be provided.\nWe allow registering one custom opset (domain, version) per ONNX opset. So, when exporting an operator from a custom domain, users need to pass this pair. Default custom opset version is 1.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29752\n\nReviewed By: hl475\n\nDifferential Revision: D18703662\n\nPulled By: houseroad\n\nfbshipit-source-id: 84d22557d132b526169051193d730761798fce60", "pr_number": "29752", "files_changed": ["test/onnx/test_operators.py", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export.h", "torch/csrc/jit/python_ir.cpp", "torch/onnx/__init__.py", "torch/onnx/symbolic_registry.py", "torch/onnx/utils.py"], "labels": ["jit", "merged"]}, "42324cb6e8": {"title": "Change interface from map of TensorShape to shapeInfoMap (#30802)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30802\n\nChange shape_hints from map<string, TensorShape> to ShapeInfoMap to catch dimType info from model file.\n\nReviewed By: ipiszy\n\nDifferential Revision: D18821486\n\nfbshipit-source-id: c5d9ed72e158d3698aba38900aeda00f776745b4", "pr_number": "30802", "files_changed": ["caffe2/opt/backend_transformer_base.cc", "caffe2/opt/backend_transformer_base.h", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h", "caffe2/opt/custom/glow_net_transform.cc", "caffe2/opt/custom/glow_net_transform.h", "caffe2/opt/onnxifi_transformer.cc", "caffe2/opt/onnxifi_transformer.h", "caffe2/opt/shape_info.cc", "caffe2/opt/shape_info.h", "caffe2/opt/tvm_transformer.cc", "caffe2/opt/tvm_transformer.h", "caffe2/python/pybind_state.cc"], "labels": ["fb-exported", "merged"]}, "44428d0ee2": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/6c87dc4d3c867f0c121c2f60544808861ca035f8\nhttps://github.com/facebook/fbzmq/commit/5ec43afc1d3c8566f6aaebe338e76753bde21d3f\nhttps://github.com/facebook/folly/commit/1e3cb8283f9ae0e9082646c58ab4338136b1a58e\nhttps://github.com/facebook/proxygen/commit/3af1c72471600f4086429c6f0db4ffae1c9b901b\nhttps://github.com/facebook/wangle/commit/dc8e6e6e688669981bc13bb9231927b804a78e41\nhttps://github.com/facebookincubator/fizz/commit/405e596d50f2e7b2986357757b6cb7ac7930f906\nhttps://github.com/facebookincubator/katran/commit/f40ae54a5277aedaca3827579161b9c938c593ea\nhttps://github.com/facebookincubator/mvfst/commit/479a14391275340fcf793b2e29797937a45a427b\nhttps://github.com/facebook/fbthrift/commit/e63b40cb4b5cf2421467c499e4c043b0a9d6ec71\nhttps://github.com/facebook/fbzmq/commit/cb5f0670a69313fc14b40c053e8a685c19d8ff4c\nhttps://github.com/facebook/litho/commit/470a664def6a7b2ce327dfa49b340551f27761b3\nhttps://github.com/facebook/proxygen/commit/6e8f70b2d9ef41e035ee8f3f30e80395f59bd498\nhttps://github.com/facebook/wangle/commit/0fb026ca5849d996ea74537bf55c98df8cac7123\nhttps://github.com/facebookincubator/fizz/commit/3595e0cf381afff8923c6ff4cc1d8594bd3c4315\nhttps://github.com/facebookincubator/katran/commit/79b171ffa3f2fa3bb26934cd70c125c8197ffa5b\nhttps://github.com/facebookincubator/mvfst/commit/fb5322d98d76d03f07b90f52549ff3394826a25a\nhttps://github.com/pytorch/fbgemm/commit/cd48fc606be7c86de848206f9c23ee7d1e150479\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 99bee659ea0fca0247d67d2dac12a821e1bd402d", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "f48a8901c5": {"title": "Add floor_divide function (#30493)", "body": "Summary:\nAdds `torch.floor_divide` following the numpy's `floor_divide` api. I only implemented the out-of-place version, I can add the inplace version if requested.\n\nAlso fixes  https://github.com/pytorch/pytorch/issues/27512\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30493\n\nDifferential Revision: D18896211\n\nPulled By: eellison\n\nfbshipit-source-id: ee401c96ab23a62fc114ed3bb9791b8ec150ecbd", "pr_number": "30493", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/torch.rst", "test/test_jit.py", "test/test_namedtensor.py", "torch/_torch_docs.py", "torch/csrc/jit/script/builtin_functions.cpp", "torch/tensor.py"], "labels": ["jit", "merged"]}, "e05ee4c421": {"title": "Remove BUILD_NAMEDTENSOR macros (#30894)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30894\n\nThis PR begins the process of removing BUILD_NAMEDTENSOR macros. There\nwill be followups.\n\nReasons for removing the macros:\n- BUILD_NAMEDTENSOR is always on and has been on since pytorch 1.3.0.\n- Since we don't test building without it, it is useless to keep around.\n- Code becomes nicer to read without the macros\n\nReasons for not removing the macros:\n- potential for feature flagging\n\nNow, I argue against needing to feature flag. The main reason why we\nmight want to feature flag is if we need to disable the feature.\nWe'd need a fast switch to disable the feature if someone discovers\nin the future that named tensors caused some regression in some existing workflows.\n\nIn https://github.com/pytorch/pytorch/pull/25798, I did a variety of\nmacro- and micro- benchmarks to determine the performance impact of named\ntensors on regular tensors.\n\n[The\nmicrobenchmarks](https://github.com/pytorch/pytorch/pull/25798#issuecomment-529014810)\nwere not very stable, and running the\nmicrobenchmarks for more iterations doesn't actually help because the\nnoise is not distributed in a nice way. Instead of microbenchmarks I ran\na [profiler\n(perf)](https://github.com/pytorch/pytorch/pull/25798#issuecomment-555707645)\nto estimate how much overhead named tensors add to unnamed code. I\nestimated the overhead to be less than 100ns for `add` and even smaller\nfor `mm`; there are ways to optimize even futher if we find this to be a\nproblem.\n\n[Initial\nmacrobenchmarks](https://github.com/pytorch/pytorch/pull/25798#issuecomment-530539104)\nwere also not very stable. I ran imagenet for some number of epochs. To\nmake them more stable, I got rid of the data loading (which seemed to\nvary between runs). [In some benchmarkers without data\nloading](https://github.com/pytorch/pytorch/pull/25798#issuecomment-562214053),\nwe can see that the results are less noisy now. These results support\nno noticeable regressions in speed.\n\nTest Plan: - wait for CI\n\nDifferential Revision: D18858543\n\nPulled By: zou3519\n\nfbshipit-source-id: 08bf3853a9f506c6b084808dc9ddd1e835f48c13", "pr_number": "30894", "files_changed": ["aten/src/ATen/NamedTensorUtils.cpp", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/TensorNames.cpp", "aten/src/ATen/TensorNames.h", "aten/src/ATen/core/Dimname.cpp", "aten/src/ATen/core/Dimname.h", "aten/src/ATen/core/NamedTensor.cpp", "aten/src/ATen/core/NamedTensor.h", "aten/src/ATen/core/boxing/boxing.h", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/Distance.cpp", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/Dropout.cpp", "aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/LegacyDefinitions.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/NamedTensor.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/ResizeCommon.h", "aten/src/ATen/native/SoftMax.cpp", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/TensorProperties.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/cuda/CUDAUnaryOps.cpp", "aten/src/ATen/native/cuda/LegacyDefinitions.cpp", "aten/src/ATen/native/quantized/cpu/qreduction.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/templates/TensorMethods.h", "aten/src/ATen/test/Dimname_test.cpp", "aten/src/ATen/test/NamedTensor_test.cpp", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMoreMath.cpp", "tools/autograd/templates/VariableType.h", "tools/autograd/templates/python_variable_methods.cpp", "tools/autograd/templates/variable_factories.h", "torch/csrc/Module.cpp", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h", "torch/csrc/python_dimname.cpp", "torch/csrc/python_dimname.h", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/csrc/utils/tensor_new.cpp"], "labels": ["jit", "merged"]}, "af4040d808": {"title": "resubmit polish up overloads on free functions (#31014)", "body": "Summary:\nResubmitting https://github.com/pytorch/pytorch/pull/30356\n\nSecond commit has reintroduces deleted function which caused revert previously.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31014\n\nDifferential Revision: D18899127\n\nPulled By: eellison\n\nfbshipit-source-id: 9049b8718926c329d9cb46bb96eac6c278e9b866", "pr_number": "31014", "files_changed": ["aten/src/ATen/core/function_schema.h", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "06c7420fa2": {"title": "Raise error if a block can not be found from a CUDA tensor (#30870)", "body": "Summary:\nAfter several discussions, we agreed not to put any extra safety check for recordStream as either the check will cause failures in certain scenarios or there is no need to throw for user errors.\n\nAs a summary, it simply does what is described in https://github.com/pytorch/pytorch/issues/27405, check if a tensor is indeed allocated by a CUDACachingAllocator instance, if it is, then throw internal error if a block can not be retrieved.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30870\n\nDifferential Revision: D18851669\n\nPulled By: yxia11\n\nfbshipit-source-id: c2f01798cd24f1fd0f35db8764057d5d333dab95", "pr_number": "30870", "files_changed": ["aten/src/ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.cpp", "aten/src/ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.h", "c10/cuda/CUDACachingAllocator.cpp", "c10/cuda/CUDACachingAllocator.h", "test/test_cuda.py", "tools/autograd/templates/python_variable_methods.cpp", "torch/csrc/distributed/c10d/ddp.cpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["merged"]}, "c34ef1aa2e": {"title": "Automatic update of fbcode/onnx to c08a7b76cf7c1555ae37186f12be4d62b2c39b3b (#30619)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30619\n\nPrevious import was fea8568cac61a482ed208748fdc0e1a8e47f62f5\n\nIncluded changes:\n- **[c08a7b76](https://github.com/onnx/onnx/commit/c08a7b76)**: doc: fix some typos at ONNXIFI (#2473) <Yorkie Liu>\n- **[4be12d46](https://github.com/onnx/onnx/commit/4be12d46)**: remove workshop update since it is done (#2460) <Prasanth Pulavarthi>\n- **[86107d1b](https://github.com/onnx/onnx/commit/86107d1b)**: Updated with correct URL to LICENSE (#2468) <Ryan Loney>\n- **[9bf6fbb6](https://github.com/onnx/onnx/commit/9bf6fbb6)**: Update Argmin/Argmax (#2461) <Lara Haidar>\n- **[748d81b8](https://github.com/onnx/onnx/commit/748d81b8)**: Fix windows conda build (#2452) <Ashwini Khade>\n- **[a32db1c5](https://github.com/onnx/onnx/commit/a32db1c5)**: Delete duplicate word in comment (#2439) <Haibo Hao>\n- **[e108da9a](https://github.com/onnx/onnx/commit/e108da9a)**: Fix bug in function body verifier (#2390) <G. Ramalingam>\n- **[c3d3ef82](https://github.com/onnx/onnx/commit/c3d3ef82)**: docs: fix typo in IR.md (#2441) <Elliot Waite>\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D18766132\n\nfbshipit-source-id: 13c04f21399579acb87a8f9fac2e4c329b0720b8", "pr_number": "30619", "files_changed": ["caffe2/python/onnx/tests/onnx_backend_test.py", "third_party/onnx"], "labels": ["fb-exported", "merged"]}, "a42d093db2": {"title": "FCTransposed to FbFCPacked (#29766)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29766\n\nAdd FbgemmPackTranspose op to support the packing on FCTransposed weights\n\nAdd FCTransposed to FbFCPacked transformation to Dper fp16 exporter\n\nTest Plan:\n```\nbuck test mode/opt caffe2/caffe2/fb/fbgemm:fb_fc_packed_op_test\n```\n\n```\nbuck test mode/opt caffe2/caffe2/python:layers_test\n```\n\nDifferential Revision: D18482306\n\nfbshipit-source-id: e8f1947b3d0d04892293509ebf88742f5f0f5997", "pr_number": "29766", "files_changed": ["caffe2/python/layers/fc.py", "caffe2/python/layers_test.py"], "labels": ["fb-exported", "merged"]}, "bb7befb12c": {"title": "Support loading by blob in predictor", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30805\n\nReviewed By: ipiszy\n\nDifferential Revision: D18827383\n\nfbshipit-source-id: b97f958768618ca29a02b057667a9b4ee313ad3c", "pr_number": "30805", "files_changed": ["caffe2/opt/custom/glow_net_transform.cc", "caffe2/opt/custom/glow_net_transform.h", "caffe2/opt/onnxifi_transformer.h", "caffe2/proto/predictor_consts.proto"], "labels": ["fb-exported", "merged"]}, "313c211f3f": {"title": "Calling JITed 8 Bit Fused SLS in FBGEMM from C2 (#30926)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30926\n\nCalling the JITed FBGEMM kernel for Fused 8 Bit Sparse Length Sum (Fused8BitRowwiseEmbeddingLookup)\n\nTest Plan:\nbuck test  mode/dbg //caffe2/caffe2/python:lengths_reducer_fused_8bit_rowwise_ops_test\n\nAll tests pass.\n\nReviewed By: jspark1105\n\nDifferential Revision: D18058128\n\nfbshipit-source-id: 0dfa936eb503712c39e53748e015fc156afde86f", "pr_number": "30926", "files_changed": ["caffe2/operators/lengths_reducer_fused_8bit_rowwise_ops.h"], "labels": ["fb-exported", "merged"]}, "394d2f7037": {"title": "Fix the rendering of the doc of max. (#30779)", "body": "Summary:\nClose https://github.com/pytorch/pytorch/issues/30731\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30779\n\nDifferential Revision: D18837317\n\nPulled By: zou3519\n\nfbshipit-source-id: b9b5ba414756a68d4b39a7a7c2d89fee1e3c040f", "pr_number": "30779", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged"]}, "78a00d72b4": {"title": "Revert D18899127: resubmit polish up overloads on free functions", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18899127\n\nOriginal commit changeset: 9049b8718926\n\nfbshipit-source-id: c70a8aa4120aa757dce0926a8ab3cc5c92cd6041", "pr_number": null, "files_changed": ["aten/src/ATen/core/function_schema.h", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py", "torch/jit/_recursive.py"], "labels": []}, "2da3b9a0f6": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/fd8771904e8b12047142e0012bf843fa7540704b\nhttps://github.com/facebook/litho/commit/6bf51e234f6fe4dca00a1769dfeca6b5f9463f27\nhttps://github.com/facebook/rocksdb/commit/6380df5e10a1fe91ad0e7cac233b4de5ffc4a383\nhttps://github.com/pytorch/fbgemm/commit/696c2a235959d0c3df86d0aa2f385b65d7b9cd4c\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 188670fcdc50ccf060eea137698ecfb45484e059", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "47033b49f3": {"title": "Suppress XCode build warnings (#31000)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31000\n\n## Summary\n\nAdd Fastlane configurations to suppress the build warnings from XCode.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18912489\n\nPulled By: xta0\n\nfbshipit-source-id: f2c54d54a12ad2415695d1fcb1800684c7a9e560", "pr_number": "31000", "files_changed": ["ios/TestApp/.gitignore", "ios/TestApp/fastlane/Scanfile"], "labels": ["merged"]}, "27d7dba9ab": {"title": "Remove scalar_check specification and codegen. (#30874)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30874\n\nThese have all been disabled at this point, so there is no difference in the generated code.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18855990\n\nPulled By: gchanan\n\nfbshipit-source-id: 03796b2978e23ef9060063f33241a1cbb39f1cf3", "pr_number": "30874", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/OpaqueTensorImpl.h", "aten/src/ATen/SparseTensorImpl.cpp", "aten/src/ATen/SparseTensorImpl.h", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp", "aten/src/ATen/nn.yaml", "aten/src/ATen/nn_parse.py", "aten/src/ATen/test/broadcast_test.cpp", "aten/src/ATen/test/wrapdim_test.cpp", "aten/src/TH/THTensor.hpp", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/THC/generic/THCTensorIndex.cu", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h"], "labels": ["merged"]}, "57f29a44c7": {"title": "Bug fix of the histogram observers (#30970)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30970\n\nCheck null tensors in the histogram observers\n\nTest Plan: f154576636 vs f154820243\n\nReviewed By: hx89\n\nDifferential Revision: D18865771\n\nfbshipit-source-id: 669c014d914525deee36142e12f013afaf3caf1d", "pr_number": "30970", "files_changed": ["caffe2/quantization/server/activation_distribution_observer.cc"], "labels": ["fb-exported", "merged"]}, "b01b05790e": {"title": "Fix memory leak due to circular dependency. (#31030)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31030\n\nDistAutogradContext held a shared_ptr reference to RecvRpcBackward and\nRecvRpcBackward held a shared_ptr reference to the context. This circular\ndependency caused significant memory leaks. As a result, I'm changing the\nreference in RecvRpcBackward to be a weak_ptr.\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18896389\n\nfbshipit-source-id: e5bc588b6f998885854e3a67de1e82452e8475ce", "pr_number": "31030", "files_changed": ["torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp", "torch/csrc/distributed/autograd/functions/recvrpc_backward.h"], "labels": ["merged"]}, "cc319659e3": {"title": "qnnpack TanH", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31013\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18898903\n\nPulled By: z-a-f\n\nfbshipit-source-id: aa126a98627b808678f629f39853c3b9c70eb2bf", "pr_number": "31013", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt", "aten/src/ATen/native/quantized/cpu/qnnpack/README.md", "aten/src/ATen/native/quantized/cpu/qnnpack/bench/tanh.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/configure.py", "aten/src/ATen/native/quantized/cpu/qnnpack/include/pytorch_qnnpack.h", "aten/src/ATen/native/quantized/cpu/qnnpack/src/tanh.c", "aten/src/ATen/native/quantized/cpu/qnnpack/test/tanh-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/tanh.cc"], "labels": ["merged"]}, "0cbbe050bb": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/b459fcc89f190cb904f12937e34b1025be6e8d1a\nhttps://github.com/facebook/rocksdb/commit/2b060c14982d22e0eca5d10557ebc1e3280e2fcc\nhttps://github.com/pytorch/fbgemm/commit/13a2c072c4f090e4323e48c31e1dd38eded2209c\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 59fb11a977dcb7b2c09acb7fe997b0d5e52f27c4", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "ed20937231": {"title": "Remove TensorImpl::maybe_zero_dim.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30878\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18855989\n\nPulled By: gchanan\n\nfbshipit-source-id: 44087b6136ec40d0a3de5b5a9f03c60d002a1107", "pr_number": "30878", "files_changed": ["c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h"], "labels": ["merged"]}, "8fd85d70be": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebookincubator/mvfst/commit/163b6e2428fc8147d1e39288ac7dcd8d8de53705\nhttps://github.com/facebook/fbthrift/commit/1d7a0e1a4bcdfe6a2fad642401236b052ed83fcc\nhttps://github.com/facebook/proxygen/commit/b8031f09d74be5df747300e3fa2727255b2b148c\nhttps://github.com/pytorch/fbgemm/commit/7fd86a8f6418b88daa76b4306897565cab14548d\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 98b2487b39fb56641641c0947ed09f883755126a", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "e3d40f857b": {"title": "Make nn.Module `forward()` type annotation more permissive (#31057)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31057\n\nThe current signature basically will always fail to type check, because\nmypy enforces that the subclass method's input types must be \"wider\"\nthan their superclass method's input types (i.e. they can vary\ncontravariantly). And nothing is wider than `Any`.\n\nThis change makes it so that any input params are allowed in\n`forward()`. Fixes #29099\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18918034\n\nPulled By: suo\n\nfbshipit-source-id: 9940e9f769b55d580d9d7f23abf6f88edb92627f", "pr_number": "31057", "files_changed": ["torch/nn/modules/module.pyi.in"], "labels": ["merged"]}, "5edfe9cb80": {"title": "add torch.square (#30719)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/30524\nThis adds an new operator `torch.square` to PyTorch\n\nI think it is ready for the first-time review now albanD\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30719\n\nDifferential Revision: D18909268\n\nPulled By: albanD\n\nfbshipit-source-id: 5626c445d8db20471a56fc1d7a3490e77812662b", "pr_number": "30719", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "benchmarks/operator_benchmark/pt/unary_test.py", "docs/source/tensors.rst", "docs/source/torch.rst", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged"]}, "d113b22571": {"title": "kill PyTorch py2 circle jobs (#29353)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29353\n\nFirst step to killing Python 2 everywhere. I don't really know that much\nabout the caffe2 circle jobs so I left them alone for now.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18920563\n\nPulled By: suo\n\nfbshipit-source-id: b37d8427a6ecd4b8a7e16c1ff948e0ce13b5798f", "pr_number": "29353", "files_changed": [".circleci/cimodel/data/binary_build_data.py", ".circleci/cimodel/data/dimensions.py", ".circleci/cimodel/data/pytorch_build_data.py", ".circleci/config.yml", ".circleci/verbatim-sources/workflows-binary-builds-smoke-subset.yml"], "labels": ["merged"]}, "b7652a2f81": {"title": "remove py2 flake8 lint (#29357)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29357\n\nAs title\n\nTest Plan: Imported from OSS\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D18920562\n\nPulled By: suo\n\nfbshipit-source-id: b5dd559cfb0ba6c64b9ccf3655417afb56a7b472", "pr_number": "29357", "files_changed": [".github/workflows/lint.yml"], "labels": ["merged"]}, "3de8584de8": {"title": "Correct definition of nodes that work with Autograd (#30683)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30683\n\nAssume that a node can work with autograd only if it is not a fusion\ngroup and in prim or aten namespaces.\n\nTest Plan: CI\n\nReviewed By: lly-zero-one\n\nDifferential Revision: D18795171\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 301090557e330b58be70e956784f7f0dc343c684", "pr_number": "30683", "files_changed": ["test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp", "torch/csrc/jit/passes/inline_autodiff_subgraphs.h"], "labels": ["jit", "merged"]}, "e42af97349": {"title": "Add quantized concat conversion (#30887)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30887\n\nSupport to convert quantized concat from pytorch to caffe2\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_cat\n\nImported from OSS\n\nDifferential Revision: D18855676\n\nfbshipit-source-id: 5d0cf3f03c61819e168b080afa368b1255d0419c", "pr_number": "30887", "files_changed": ["caffe2/onnx/backend.cc", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/onnx/symbolic_caffe2.py"], "labels": ["merged"]}, "e7e6d56b77": {"title": "Allow async work in rpc RequestCallback processing. (#30637)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30637\n\nRequestCallback api currently forces work to be always synchronous, which,\nas we scale, means we're going to need to throw large number of (mostly\nblocked) threads at the rpc problem. For some activities like dependent\nautograd rpcs, there's not a necessary reason to block in these threads.\n\nIn this change, the RequestCallback api is updated to return a\nshared_ptr<FutureMessage> rather than a Message:\n\n   std::shared_ptr<FutureMessage> operator()(Message& request) const;\n\nWith a futures-style api, RPC ops that wish to be async can then be async,\nwhile short-lived blocking functions (or Python UDFs) can just block.\n\nIn this change, we keep all of the current ops as synchronous (i.e. we block\nand then return a completed FutureMessage). We also update the rpc_agents in\na manner compatible with this sort of parallelism.\n\nHere, we only want to incur overhead when we use the async behavior.\nSome modest extra cost seems unavoidable here (e.g. the allocation for the\nstd::make_shared<>), but we can trivially detect the synchronous/completed\ncase in the rpc_agent and avoid the extra thread-switches/etc. in that case.\nghstack-source-id: 95287026\n\nTest Plan:\n- Basic: buck test mode/dev-nosan caffe2/test/...\n  - Additional testcase in ThriftRpcAgentTest for deferred work.\n\nDifferential Revision: D18774322\n\nfbshipit-source-id: cf49922a71707cfb1726de16f93af23b160385d8", "pr_number": "30637", "files_changed": ["test/cpp/dist_autograd/test_dist_autograd.cpp", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/context/context.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h", "torch/csrc/distributed/rpc/future_message.cpp", "torch/csrc/distributed/rpc/future_message.h", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/request_callback.cpp", "torch/csrc/distributed/rpc/request_callback.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/request_callback_impl.h"], "labels": ["merged", "module: rpc"]}, "d088bd0bad": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/c6506e269830541c6d55f6a1d085ea8c97ebd089\nhttps://github.com/facebook/folly/commit/4427c1a8321bb138f667758172d4e2f47bb0675b\nhttps://github.com/facebook/rocksdb/commit/a6538571788f52b87a3ca83667b8634ee4950f54\nhttps://github.com/facebookincubator/profilo/commit/558f42bd6c266404e58291a5991c66436229866a\nhttps://github.com/pytorch/fbgemm/commit/3839cbaf526adaf5b2f8faa3b963e4396c893920\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 4a253bba6de9a2c2a11a82e33809a370e1b4fd04", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "d02280b432": {"title": "move migration guide to appendix (#31068)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31068\n\nLet's get it out of the early parts now that the recursive API has been\naround for a while\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18920498\n\nPulled By: suo\n\nfbshipit-source-id: 6f4389739dd9e7e5f3014811b452249cc21d88e7", "pr_number": "31068", "files_changed": ["docs/source/jit.rst"], "labels": ["merged"]}, "9f3fe78239": {"title": "peephole optimize type refinements (#31024)", "body": "Summary:\nPeephole optimize out type refinements when they are no longer refining the type.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31024\n\nDifferential Revision: D18920958\n\nPulled By: eellison\n\nfbshipit-source-id: 6d05d9812b9f9dcf001de760a78a2042fb832773", "pr_number": "31024", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/peephole.cpp"], "labels": ["jit", "merged"]}, "c72dd526a7": {"title": "kill py2 onnx builds", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31082\n\nDifferential Revision: D18922689\n\nPulled By: suo\n\nfbshipit-source-id: 98c91b90ee3b1dd13c6020597a0ace741a1597da", "pr_number": "31082", "files_changed": [".circleci/cimodel/data/caffe2_build_data.py", ".circleci/config.yml"], "labels": ["merged"]}, "7f5f2e8871": {"title": "add ZERO_COLLISION_HASH to caffe2 data type (#30912)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30912\n\nAdd a new data type ZERO_COLLISION_HASH .\n\nTest Plan: ci\n\nReviewed By: boryiingsu\n\nDifferential Revision: D18843626\n\nfbshipit-source-id: b2d8280f13c78b4a656cf95822198df59de7b64c", "pr_number": "30912", "files_changed": ["caffe2/core/blob_serialization.cc", "caffe2/proto/caffe2.proto"], "labels": ["fb-exported", "merged"]}, "9a5fd2eb07": {"title": "Fix conflicts in CMAKE_GENERATOR and generator (#30971)", "body": "Summary:\n...specified in -G\n\nhttps://cmake.org/cmake/help/latest/variable/CMAKE_GENERATOR.html\nAccording to the document, the generator could be determined through two methods:\n1. Specify in `-G`\n2. Read from `CMAKE_GENERATOR`\n\nWe should avoid conflicts in these two methods. This fixes https://github.com/pytorch/pytorch/issues/30910.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30971\n\nDifferential Revision: D18927529\n\nPulled By: mingbowan\n\nfbshipit-source-id: e9a179ceb32d6fbabfaeac6cfe9e6170ca170b20", "pr_number": "30971", "files_changed": ["tools/setup_helpers/cmake.py"], "labels": ["merged"]}, "8013ffd400": {"title": "Fix weight_norm export for dim=0 (#31015)", "body": "Summary:\nExported weight_norm is incorrectly reducing over axis 0 as well when dim is set to 0.\nPrevious test case only covers weight with size(0) == 1, which yields the same result whether reduced over or not.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31015\n\nReviewed By: hl475\n\nDifferential Revision: D18900894\n\nPulled By: houseroad\n\nfbshipit-source-id: 19004f51933b37f848dbe4138e617a7a8e35a9ec", "pr_number": "31015", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged"]}, "28ee309c9a": {"title": "disable onnx py3 gcc5 build (#31100)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31100\n\nThis appears to not work right now. Disabling pending an investigation.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18928777\n\nPulled By: suo\n\nfbshipit-source-id: 63089131bad98902979e5cf4373732c85badef9d", "pr_number": "31100", "files_changed": [".circleci/cimodel/data/caffe2_build_data.py", ".circleci/config.yml", ".github/workflows/lint.yml"], "labels": ["merged"]}, "d6d6075573": {"title": "Optimize LayerNorm with explicit vectorization using Vec256 (#29104)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29104\n\nWe would like to provide the vectorized implementation for layer norm. This PR reuses https://github.com/pytorch/pytorch/pull/23349.\n\nTest Plan:\nbuck test mode/dev-nosan //caffe2/test:nn -- \"LayerNorm\"\n\nbuck test mode/dev-nosan //caffe2/test:nn -- \"test_LayerNorm_1d_no_elementwise_affine_eval\"\n\n python run_test.py -i nn -- TestNN.test_LayerNorm_1d_no_elementwise_affine_eval\n\nDifferential Revision: D18293522\n\nfbshipit-source-id: f4cfed6e62bac1b43ee00c32b495ecc836bd9ec5", "pr_number": "29104", "files_changed": ["aten/src/ATen/native/cpu/layer_norm_kernel.cpp"], "labels": ["merged"]}, "65f6e449c7": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbzmq/commit/0f94976f31882b9510f4073003f5f35d6f4b8e80\nhttps://github.com/facebook/litho/commit/be15abd839fbe357f4cdabbc6ecc111e983bd1eb\nhttps://github.com/facebook/wangle/commit/034086d70f923f600b27431044560625b177b36c\nhttps://github.com/facebookincubator/fizz/commit/aa131abdf572284e465e602cca888a1251e9610e\nhttps://github.com/facebookincubator/katran/commit/a3f268f1b525d2ebe0b1196d1a6f55a25eb884a2\nhttps://github.com/pytorch/fbgemm/commit/6394aabc991e19dfcd9e68d8d771d326cf83094d\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: fa99a0a096de1f088e5fa8cd92fdf5fd6c330740", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "9305f44854": {"title": "Remove BUILD_NAMEDTENSOR from codegen and .cu files (#31047)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31047\n\nChangelist:\n- remove BUILD_NAMEDTENSOR from .cu files\n- remove BUILD_NAMEDTENSOR special handling in function_wrapper.py\n- remove BUILD_NAMEDTENSOR from cpp_extension.py. This code actually\ndid nothing because we always compile with BUILD_NAMEDTENSOR.\n\nTest Plan: - run tests\n\nDifferential Revision: D18908442\n\nPulled By: zou3519\n\nfbshipit-source-id: b239e24de58580adaf3cef573350773a38b1e4f0", "pr_number": "31047", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen.py", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/cuda/SortingKthValue.cu", "aten/src/THC/generic/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorMasked.cu", "aten/src/THC/generic/THCTensorMathBlas.cu", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPointwise.cu", "torch/utils/cpp_extension.py"], "labels": ["merged"]}, "4aa30d3c0c": {"title": "Revert D18293522: Optimize LayerNorm with explicit vectorization using Vec256", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18293522\n\nOriginal commit changeset: f4cfed6e62ba\n\nfbshipit-source-id: cdd6d9d36c00b516aecdab549abeeffc4a473829", "pr_number": null, "files_changed": ["aten/src/ATen/native/cpu/layer_norm_kernel.cpp"], "labels": []}, "3301794855": {"title": "Port ELU activation to Aten (#29275)", "body": "Summary:\nVitalyFedyunin, This PR is about port  ELU activation to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.ELU()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    fwd_t = 0\n    bwd_t = 0\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) forward time is 0.04 (ms); backwad avg time is 0.09 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU:\nOMP_NUM_THREADS=56\ninput size(128, 100) forward time is 0.28 (ms); backwad avg time is 0.18 (ms).\ninput size(128, 10000) forward time is 23.53 (ms); backwad avg time is 14.46 (ms).\nOMP_NUM_THREADS=1\ninput size(128, 100) forward time is 0.16 (ms); backwad avg time is 0.08 (ms).\ninput size(128, 10000) forward time is 15.53 (ms); backwad avg time is 6.60 (ms).\n```\nAfter:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU:\nOMP_NUM_THREADS=56\ninput size(128, 100) forward time is 0.24 (ms); backwad avg time is 0.17 (ms).\ninput size(128, 10000) forward time is 0.73 (ms); backwad avg time is 1.11 (ms).\nOMP_NUM_THREADS=1\ninput size(128, 100) forward time is 0.15 (ms); backwad avg time is 0.07 (ms).\ninput size(128, 10000) forward time is 14.40 (ms); backwad avg time is 6.00 (ms).\n```\nHow to set the numbers of thread? using following script:\n```\nnum_threads=$1\nscript=$2\nlast_core=`expr $num_threads - 1`\necho \"using $num_threads OMP threads\"\necho \"bind cores to 0~$last_core\"\nexport OMP_NUM_THREADS=$num_threads\nexport KMP_AFFINITY=granularity=fine,compact,1,0\nnumactl --physcpubind=0-$last_core --membind=0 python $script\n```\nand run .**/run.sh num_threads test.py**.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29275\n\nDifferential Revision: D18587389\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: bea8f3f006c6893090f863d047c01886d195437a", "pr_number": "29275", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/ELU.cu", "aten/src/THCUNN/generic/ELU.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/ELU.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "717274c001": {"title": "Add useful warnings for t.grad when it won't be populated for known reasons (#30531)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/2362 and https://github.com/pytorch/pytorch/issues/19778\n\nTo avoid issues with frozen model, we only consider warning for Tensors that require gradients and are neither leafs nor retain gradients.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30531\n\nDifferential Revision: D18832767\n\nPulled By: albanD\n\nfbshipit-source-id: 743e863dc14ab57713e66da78b2e4d759dfba0ff", "pr_number": "30531", "files_changed": ["test/test_jit.py", "test/test_torch.py", "torch/_tensor_docs.py", "torch/autograd/gradcheck.py", "torch/csrc/autograd/python_variable.cpp", "torch/tensor.py"], "labels": ["merged"]}, "3593981976": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/9b38c6430e13d4ae868263f5d634b5747b635918\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 8801c415c9b00bec46efc102c0daceba59397449", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "a929d312ac": {"title": "Add dill>=0.3.1 as testing dependency (#31121)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31121\n\nFor https://github.com/pytorch/pytorch/pull/30985 .\n\nTest Plan:\n- run `pip install \"dill>=0.3.1\"` locally, check that it actually\ninstalls dill>=0.3.1.\n\nDifferential Revision: D18934871\n\nPulled By: zou3519\n\nfbshipit-source-id: 688a489b9e81134ccb5ab4b099116e3fe6b6b7ae", "pr_number": "31121", "files_changed": [".circleci/docker/common/install_travis_python.sh"], "labels": ["merged"]}, "1f87e823b8": {"title": "Make `nn.Transformer` TorchScript compatible (#28561)", "body": "Summary:\nThis makes `nn.Transformer` usable from TorchScript. It preserves backwards compatibility via `__setstate__` on the encoder/decoder.\n\nFixes https://github.com/pytorch/pytorch/issues/24173\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28561\n\nDifferential Revision: D18124753\n\nPulled By: driazati\n\nfbshipit-source-id: 7314843e5aa9c9bf974c4672e4edb24ed8ef4a6f", "pr_number": "28561", "files_changed": ["test/test_jit.py", "torch/jit/_recursive.py", "torch/nn/modules/activation.py", "torch/nn/modules/transformer.py"], "labels": ["merged"]}, "672f4cfad9": {"title": "Added C++ API test (#30980)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30980\n\nThis stack is a first step toward an effort to fix, clean up and simplify code generation logic. \ufffdPlease see the master [task](https://github.com/pytorch/pytorch/issues/30405) to see related discussions and all the known issues.\n\nMain focus of these changes is TensorOptions in code generation.\nGoals:\n- Remove TensorOptions from generated code wherever it's possible. Leave it only in python/C++ API layers.\n- Refactor TensorOptions logic to a single place.\n- Log all discovered issues.\n\nNon goals:\n- Fix Everything!\n- Remove all the hacks in code generation scripts.\n- Clean up and defector all code generation scripts.\n\n--------------\nIn this PR:\nAdd a test to check that C++ API behavior stays the same after all the changes.\nWhile working on it a bug related to `requires_grad` was found and logged in the master task.\n\n--------------\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18912681\n\nPulled By: izdeby\n\nfbshipit-source-id: 19772a37c92dde820839b79055f348689b99fa77", "pr_number": "30980", "files_changed": ["aten/src/ATen/test/basic.cpp"], "labels": ["merged"]}, "44ecc3a70b": {"title": "Add tracing support for optional Device and Layout (#30979)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30979\n\nThis stack is a first step toward an effort to fix, clean up and simplify code generation logic. \ufffdPlease see the master [task](https://github.com/pytorch/pytorch/issues/30405) to see related discussions and all the known issues.\n\nMain focus of these changes is TensorOptions in code generation.\nGoals:\n- Remove TensorOptions from generated code wherever it's possible. Leave it only in python/C++ API layers.\n- Refactor TensorOptions logic to a single place.\n- Log all discovered issues.\n\nNon goals:\n- Fix Everything!\n- Remove all the hacks in code generation scripts.\n- Clean up and defector all code generation scripts.\n\n--------------\nIn this PR:\nAdd tracing support for optional Device and Layout types.\n\n--------------\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18912685\n\nPulled By: izdeby\n\nfbshipit-source-id: 4a9514ce2eee0041f9bc96636d3ddb4f077675e1", "pr_number": "30979", "files_changed": ["torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h"], "labels": ["jit", "merged"]}, "a53b39f09d": {"title": "Disable flaky test_process_group_debug_info", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31113\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18932365\n\nPulled By: mrshenli\n\nfbshipit-source-id: a2996b6a8d3881be4ffc174b85509aeee8c51c96", "pr_number": "31113", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "4a751dfc20": {"title": "optimize MulGradient for common shapes (#19705)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/19705\n\nOptimizing for a case when there's a consecutive dims that are not broadcasted followed by another consecutive dims that are broadcasted.\nFor example, MulGradient([\"dC\", \"A\", \"B\"], [\"dA\", \"dB\"], broadcast=True, axis=0) where A.shape == dC.shape == [9508, 80] and B.shape == [80] .\n\nTest Plan:\nIn SKL T6,\n\nRunning mul_gradient_benchmark without this optimization\nOperator #0 (dA, MulGradient) 11.9119 ms/iter\n\nAfter this optimization,\nOperator #0 (dA, MulGradient) 0.672759 ms/iter\n\nNeed to land D15291800 before to fix the unit test error\n\nReviewed By: dmudiger\n\nDifferential Revision: D15075415\n\nfbshipit-source-id: 0f97be17cf8f1dacbafa34cd637fb8bc1c5e5387", "pr_number": "19705", "files_changed": ["caffe2/operators/elementwise_mul_gradient_op.cc", "caffe2/operators/elementwise_mul_op.h", "caffe2/python/operator_test/mul_gradient_benchmark.py"], "labels": ["caffe2", "merged", "module: internals", "module: pybind"]}, "dbc8b00816": {"title": "Document WorkerInfo and RpcBackendOptions structures in RPC docs. (#31077)", "body": "Summary:\nWe mention `WorkerInfo` and `RpcBackendOptions` in a couple of different locations in our docs, and these are public classes that the user may use, so we should add the class to the documentation.\n<img width=\"978\" alt=\"Screen Shot 2019-12-10 at 1 42 22 PM\" src=\"https://user-images.githubusercontent.com/8039770/70571759-47db2080-1b53-11ea-9d61-c83985a29dd9.png\">\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31077\n\nDifferential Revision: D18928162\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 67f11eedd87523c469377b791a0ba23704ec3723", "pr_number": "31077", "files_changed": ["docs/source/rpc.rst", "torch/csrc/distributed/rpc/init.cpp"], "labels": ["merged"]}, "5b03ff0a09": {"title": "Update embedding renorm comment to reference fixed issue (#29140)", "body": "Summary:\nAddress last comment in https://github.com/pytorch/pytorch/issues/28546\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29140\n\nDifferential Revision: D18915091\n\nPulled By: albanD\n\nfbshipit-source-id: 756ff5bb6a92d47c80aa9f96ff6f0edea5fd24de", "pr_number": "29140", "files_changed": ["aten/src/ATen/native/Embedding.cpp"], "labels": ["merged"]}, "4b2d356ac1": {"title": "Re-enable test_rref_context_debug_info after enforcing proper synchronization (#30994)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30994\n\nThe flakiness we saw was due to missing barriers(), which caused\nstates leaked into previous or subsequent checks. This commit\nattempts fix this problem by adding barriers before and after each\ncheck.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18893457\n\nPulled By: mrshenli\n\nfbshipit-source-id: 42bcc12efa7e6e43e2841ef23e4bc2543b0236c6", "pr_number": "30994", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "7a8261e962": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/06033e7eb28936f6da2eb2f6f2acc27ab72424e9\nhttps://github.com/facebook/fbzmq/commit/c56d2fa73f88716d6c9bcf520c2a5a5974b36b61\nhttps://github.com/facebook/litho/commit/972f299a62066d342783f4bec070ecf57beca09a\nhttps://github.com/facebook/rocksdb/commit/3717a882897dc94459944fd092eeb59b8992c045\nhttps://github.com/facebook/wangle/commit/ea64a080c65ddf04eb0fbaf59bac49835bf8bc9c\nhttps://github.com/pytorch/fbgemm/commit/b4e0237162d046b6947d27a3fc8bc7ef22408e67\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 73d2d91c851f1905d6d4606a9f8002eb47246852", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "06d874f95b": {"title": "Change startTime_ to endTime_ in FutureInfo (#30342)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30342\n\nThis can eliminate the unnecessary calls to getRPCEndTime(). Reduce lines of code for simplicity.\n\nghstack-source-id: 95377162\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rpc_timeouts\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_rpc_timeouts\n```\n\nDifferential Revision: D5705624\n\nfbshipit-source-id: aca4c4917718124022c09ee0d13cf5ca483402af", "pr_number": "30342", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": ["merged", "module: rpc"]}, "6225443009": {"title": "Expose setNumThreads to android api (#31033)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31033\n\nIntention:\nThere are requests from users to control number of threads from android side:\nhttps://discuss.pytorch.org/t/android-pytorch-forward-method-running-in-a-separate-thread-slow-down-ui-thread/63516/2\nhttps://discuss.pytorch.org/t/threading-of-model-pytorch-android/62490/2\n\nAt the moment `setNumThreads` is placed in `org.pytorch.Module`, but this method changes global threadPool size, in future we will move it to some separate class to repeat python binding structure, which has torch.set_num_threads()\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18923167\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 8d98c2edbff42e9b673509672dce3f2dd03a923e", "pr_number": "31033", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/src/main/java/org/pytorch/INativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/LiteNativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/Module.java", "android/pytorch_android/src/main/java/org/pytorch/NativePeer.java", "caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/ThreadPool.h"], "labels": ["merged"]}, "293a139d79": {"title": "add a warning for script classes (#31069)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31069\n\nJust to clarify that they are still experimental.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18920496\n\nPulled By: suo\n\nfbshipit-source-id: d2f3014592a01a21f7fc60a4ce46dd0bfe5e19e9", "pr_number": "31069", "files_changed": ["docs/source/jit.rst"], "labels": ["merged"]}, "945ce71b18": {"title": "Correctly handle scalar types, fix parse of numpy ints (#30486)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30486\n\nFixes: https://github.com/pytorch/pytorch/issues/29252\n\nThere is some incorrect code in the handling of parsing python numbers that led to issue #29252:\n\nWhen we allow interpretation of a zero-dim numpy integer value\nas a scalar in pytorch, we incorrectly parse the int as a float.\n\nThis PR also fixes the issue described in the \"FIXME\" here:\nhttps://github.com/pytorch/pytorch/pull/27628/files#diff-f539198dd366265fb8dc2d661bc5d5bcR1487\n\nTest Plan: Added a unit test based on the example given in the issue.\n\nDifferential Revision: D18932520\n\nPulled By: nairbv\n\nfbshipit-source-id: f6416f28dfd73ac72c1042042851d76beb5fcf65", "pr_number": "30486", "files_changed": ["test/test_dataloader.py", "test/test_torch.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_numbers.h", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_numpy.cpp", "torch/csrc/utils/tensor_numpy.h"], "labels": ["merged"]}, "e5a550cd1d": {"title": "Fix Test CI by pinning hypothesis and correcting the import (#31137)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31137\n\nOur Test CI is broken because:\n- hypothesis recently did a new release that reorganized their internal\nmodules\n- we were importing something from their internal module structure.\n\nThis PR fixes the CI by doing the following:\n- import SearchStrategy from the correct (public) location\n- Pin the hypothesis version to avoid future surprises.\n\nIn the long term, we should stop install hypothesis every time the CI\nruns and instead install it as a part of our docker build process. See\nhttps://github.com/pytorch/pytorch/issues/31136 for details.\n\nTest Plan:\n- I tested this locally; before this PR test/test_nn.py fails to run but\nafter it does run.\n- Wait for CI\n\nDifferential Revision: D18940817\n\nPulled By: zou3519\n\nfbshipit-source-id: c1ef78faa5a33ddf4d923f947c03cf075a590bb8", "pr_number": "31137", "files_changed": [".jenkins/pytorch/test.sh", "test/hypothesis_utils.py"], "labels": ["merged"]}, "0414463007": {"title": "doc fix for max method: a warning about different behaviour on CPU and GPU (#31115)", "body": "Summary:\nFixes [30708](https://github.com/pytorch/pytorch/issues/30708),\nAdds warning regarding different behaviour of the method depending on device type.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31115\n\nDifferential Revision: D18937365\n\nPulled By: zou3519\n\nfbshipit-source-id: 7c731dd80f8b371de08d7fdfcc2196be15a593e1", "pr_number": "31115", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "679b20b1e4": {"title": "Unify list elements for all list types (#30777)", "body": "Summary:\nPreviously list elements were only unified for tensor lists.\nThis improves error messages and expands the unification logic\nto include all types.\n](https://our.intern.facebook.com/intern/diff/18837726/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30777\n\nPulled By: driazati\n\nDifferential Revision: D18837726\n\nfbshipit-source-id: c4d275562a8429700987569426d694faa8f6002e", "pr_number": "30777", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/test_jit.py", "test/test_jit_py3.py", "torch/csrc/jit/script/compiler.cpp"], "labels": ["jit", "merged"]}, "73f9e81660": {"title": "Make rref fetch calls async. (#31086)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31086\n\nThis change leverages the new future response framework so that server\nthreads don't block until setValue is called. Particulurly, we add a\ngetFuture() method to OwnerRRef so that we get a future that is satisfied\nonce setValue is called.\nghstack-source-id: 95402273\n\nTest Plan: buck test mode/dev-nosan caffe2/test/...\n\nDifferential Revision: D18925272\n\nfbshipit-source-id: 2caf51019e5b5fd7ec45539544780067deb28610", "pr_number": "31086", "files_changed": ["torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref.cpp", "torch/csrc/distributed/rpc/rref.h"], "labels": ["merged"]}, "efe683fb2a": {"title": "dynamicly quantized linear benchmarking", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30148\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18613006\n\nPulled By: z-a-f\n\nfbshipit-source-id: 3851189a2822fd09a5dd97c9d54774727822d2bf", "pr_number": "30148", "files_changed": ["benchmarks/operator_benchmark/pt/qlinear_test.py"], "labels": ["merged"]}, "d81c6bde3b": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/36ab9debf5a445c2f4377a8e138a6257e778e06b\nhttps://github.com/facebook/folly/commit/55e5070f0a6e18ef3e1c1026947d21eb840d8917\nhttps://github.com/facebook/litho/commit/5fed1a6da76e97d14974147451602eb3a3668b3e\nhttps://github.com/facebook/proxygen/commit/9f0f470fce08e0cc749c883e6c0160b098d68b64\nhttps://github.com/facebook/rocksdb/commit/e1dfe80fe01810970bc5e5aa089aa24fd2103f89\nhttps://github.com/facebookincubator/katran/commit/786d2c588cd773b48bbd6c7479a4c568e1ed6bf0\nhttps://github.com/pytorch/fbgemm/commit/6c2b9d596d81c2433154e06c48b7b3e2b9130c86\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 1242688c93ba233f19f3afac174c814ae4c455dc", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "79c27ba4ef": {"title": "Add ONNX Export Support to floor_divide (#31081)", "body": "Summary:\nAdding support for the new ATen op floor_divide which was introduced in https://github.com/pytorch/pytorch/pull/30493/files.\n\nThis operation is used in Torchvision/FasterRCNN-MaskRCNN, which are now failing after the new op was introduced.\nThis PR fixes the failure.\n\ncc: neginraoof\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31081\n\nReviewed By: houseroad\n\nDifferential Revision: D18945316\n\nPulled By: eellison\n\nfbshipit-source-id: 09919c237d618ce7db293c7770f48f7304949dcf", "pr_number": "31081", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged"]}, "97c1e90f46": {"title": "ONNX Interpolate Add Scales Params (#28324)", "body": "Summary:\nFix for : https://github.com/pytorch/pytorch/issues/27176\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28324\n\nReviewed By: hl475\n\nDifferential Revision: D18309133\n\nPulled By: houseroad\n\nfbshipit-source-id: 348bb41393442c6b107d88fc2cd3224e0afa3ccf", "pr_number": "28324", "files_changed": [".jenkins/caffe2/test.sh", "aten/src/ATen/native/UpSample.h", "aten/src/ATen/native/UpSampleBicubic2d.cpp", "aten/src/ATen/native/UpSampleBilinear2d.cpp", "aten/src/ATen/native/UpSampleLinear1d.cpp", "aten/src/ATen/native/UpSampleNearest1d.cpp", "aten/src/ATen/native/UpSampleNearest2d.cpp", "aten/src/ATen/native/UpSampleNearest3d.cpp", "aten/src/ATen/native/UpSampleTrilinear3d.cpp", "aten/src/ATen/native/cuda/UpSample.cuh", "aten/src/ATen/native/cuda/UpSampleBicubic2d.cu", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleNearest1d.cu", "aten/src/ATen/native/cuda/UpSampleNearest2d.cu", "aten/src/ATen/native/cuda/UpSampleNearest3d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_nearest2d.cpp", "test/onnx/expect/TestOperators.test_upsample_nearest.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_scale.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_scale_default_scale_factor.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_size.expect", "test/onnx/test_operators.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_jit.py", "test/test_nn.py", "tools/autograd/derivatives.yaml", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/symbolic_script.cpp", "torch/nn/functional.py", "torch/onnx/symbolic_caffe2.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset8.py", "torch/onnx/symbolic_opset9.py"], "labels": ["jit", "topic: bc-breaking"]}, "85107e72b4": {"title": "Fix type unification With Specialized Tensor Shapes (#31076)", "body": "Summary:\nFix for https://github.com/pytorch/pytorch/issues/30015\n\nWe had a model that failed in shape propagation because we could not unify `Tensor` and `Optional[BoolTensor]`. Tensor not subtyping Optional[BoolTensor] was correct, but we should have unified those two types to `Optional[Tensor]`.\n The fix here is that for immutable types containers (Optional, Tuple Type), we should be attempting to unify with complete shape information, and if that fails, then try to unify those types with unshaped types.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31076\n\nDifferential Revision: D18921802\n\nPulled By: eellison\n\nfbshipit-source-id: aa6890277470c60b349ed1da4d81cc5d71d377f6", "pr_number": "31076", "files_changed": ["aten/src/ATen/core/type.cpp", "test/cpp/jit/test_jit_type.cpp", "test/cpp/jit/tests.h"], "labels": ["jit", "merged"]}, "49a5841a9f": {"title": "Make Conv{1,2,3}dOptions and ConvTranspose{1,2,3}dOptions different classes (#31005)", "body": "Summary:\nCurrently, both `Conv{1,2,3}dOptions` and `ConvTranspose{1,2,3}dOptions` are aliases of the `ConvOptions<{1,2,3}>` class, which causes confusion because the `ConvOptions` class has parameters such as `transposed` that shouldn't be exposed to the end user. (This has caused issues such as https://github.com/pytorch/pytorch/issues/30931.) This PR makes the following improvements:\n1. Rename the original `torch::nn::ConvOptions<N>` class to `torch::nn::detail::ConvNdOptions<N>` class, to signify that it's an implementation detail and should not be used publicly.\n2. Create new classes `torch::nn::ConvOptions<N>` and `torch::nn::ConvTransposeOptions<N>`, which have parameters that exactly match the constructor of `torch.nn.Conv{1,2,3}d` and `torch.nn.ConvTranspose{1,2,3}d` in Python API.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31005\n\nDifferential Revision: D18898048\n\nPulled By: yf225\n\nfbshipit-source-id: 7663d646304c8cb004ca7f4aa4e70d3612c7bc75", "pr_number": "31005", "files_changed": ["torch/csrc/api/include/torch/nn/modules/conv.h", "torch/csrc/api/include/torch/nn/options/conv.h", "torch/csrc/api/src/nn/modules/conv.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "a3ed350eb2": {"title": "Change type of timeoutFutures_ key to time_point instead of duration (#31078)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31078\n\nMake `ProcessGroupAgent::pollTimedOutRPCs` code more conventional.\n\n- Use `std::chrono::time_point` to represent `endTime` instead of `std::chrono::duration`.\n- Replace `std::condition_variable::wait_for(lock, endTime)` with `std::condition_variable::wait_until(lock, endTime)`.\n- Remove the unnecessary `::getRPCRemainingTime()`.\nghstack-source-id: 95408482\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rpc_timeouts\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_rpc_timeouts\n```\n\nDifferential Revision: D5705442\n\nfbshipit-source-id: ba54b7bdb84bc02d05c22360b01290d044bbfcf5", "pr_number": "31078", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": ["merged"]}, "6ab2d1b1a4": {"title": "Partially support tensor lists in loop/concat/stack (#30126)", "body": "Summary:\nThis is a follow-up PR after https://github.com/pytorch/pytorch/pull/29136 ~~and https://github.com/pytorch/pytorch/pull/29171~~\n\nONNX::Loop does not support Sequence type as loop-carried dependencies. Only tensors are supported.\nThis PR adds a pass that converts Sequence loop-carried dependencies to scan_outputs.\nIn opset 11, only the below pattern is supported.\n```\nPTIR graph:\n ...\n %res.1 : Tensor[] = prim::ListConstruct()\n %res : Tensor[] = prim::Loop(%11, %22, %res.1)\n   block0(%i.1 : Tensor, %res.6 : Tensor[]):\n     ...\n     %res.3 : Tensor[] = aten::append(%res.6, %17)\n     -> (%22, %res.3)\n return (%res.3)\n\nONNX graph:\n ...\n %res : Tensor = onnx::Loop(%11, %22)\n   block0(%i.1 : Tensor):\n     ...\n     -> (%22, %17)\n %res_seq : Tensor[] = onnx::SplitToSequence[keepdims=0](%res)\n return (%res_seq)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30126\n\nReviewed By: hl475\n\nDifferential Revision: D18946880\n\nPulled By: houseroad\n\nfbshipit-source-id: 67ee65700513e8a942344a3d647e2e73c19ee3d2", "pr_number": "30126", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/dead_code_elimination.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.cpp", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_registry.py"], "labels": ["jit", "merged"]}, "4f5a4be45f": {"title": "Add native/quantized to the list of header rewrites (#31151)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31151\n\nsame as title. I am not sure why this was not added in the first place.\n\nTest Plan: wait for build to succeed.\n\nReviewed By: bddppq, xw285cornell\n\nDifferential Revision: D18880216\n\nfbshipit-source-id: 8b17d4fbd5dd08c28c52df8b1da77b69d56d65dc", "pr_number": "31151", "files_changed": ["torch/utils/hipify/hipify_python.py"], "labels": ["fb-exported", "merged"]}, "0db6c01301": {"title": "Re-enable python 2 builds (#31164)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31164\n\nWe have a small number of internal projects that still are on Python 2.\nUntil we can figure out how to get rid of them, we need to continue\nsupporting Python 2 for PyTorch.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18949698\n\nPulled By: suo\n\nfbshipit-source-id: 4a9d7e4306ed81576e05f243de472937a2bb1176", "pr_number": "31164", "files_changed": [".circleci/cimodel/data/binary_build_data.py", ".circleci/cimodel/data/caffe2_build_data.py", ".circleci/cimodel/data/dimensions.py", ".circleci/cimodel/data/pytorch_build_data.py", ".circleci/config.yml", ".circleci/verbatim-sources/workflows-binary-builds-smoke-subset.yml", ".github/workflows/lint.yml"], "labels": ["merged"]}, "2488231fe3": {"title": "Tweak pollTimedOutRPCs thread synchronization (#30355)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30355\n\n- Make processTimedOutFutures hold lock.\n- Reduce unnecessary scan on future and future timeout maps.\n- Reduce the scope of lock at a spot.\n- Avoid repeatedly wake up if user set timeout = 0.\n\nghstack-source-id: 95409528\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rpc_timeouts\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_rpc_timeouts\n```\n\nDifferential Revision: D5516149\n\nfbshipit-source-id: 4bb0bd59fa31d9bfaef9f07ac0126782da17f762", "pr_number": "30355", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp"], "labels": ["merged", "module: rpc"]}, "159835e666": {"title": "Add types for the remaining optimizers. (#31130)", "body": "Summary:\n**Patch Description**\nRound out the rest of the optimizer types in torch.optim by creating the stubs for the rest of them.\n\n**Testing**:\nI ran mypy looking for just errors in that optim folder. There's no *new* mypy errors created.\n```\n$ mypy torch/optim | grep optim\n$ git checkout master; mypy torch/optim | wc -l\n968\n$ git checkout typeoptims; mypy torch/optim | wc -l\n968\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31130\n\nReviewed By: stephenroller\n\nDifferential Revision: D18947145\n\nPulled By: vincentqb\n\nfbshipit-source-id: 5b8582223833b1d9123d829acc1ed8243df87561", "pr_number": "31130", "files_changed": ["torch/optim/__init__.pyi", "torch/optim/adadelta.pyi", "torch/optim/adagrad.pyi", "torch/optim/adamax.pyi", "torch/optim/adamw.pyi", "torch/optim/asgd.pyi", "torch/optim/lbfgs.pyi", "torch/optim/rmsprop.pyi", "torch/optim/rprop.pyi", "torch/optim/sparse_adam.pyi"], "labels": ["merged", "module: optimizer", "module: typing"]}, "3a02ed822b": {"title": "Remove `insert_prepack_unpack` and `fold_prepack` for now (#30909)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30909\n\n`fold_prepack` doesn't work anymore after we change `scale`, `zero_point`\nto be attributes, but since the freeze API is coming up, I don't want to\nspend time to make this work since this will be thrown away later.\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18864537\n\nfbshipit-source-id: 649e6b91f2b04b8babacc0afb6bc1530ed7259d3", "pr_number": "30909", "files_changed": ["torch/quantization/_quantize_script.py"], "labels": ["merged", "quantization"]}, "56de8853da": {"title": "Resubmit overload v2 (#31123)", "body": "Summary:\nResubmit of https://github.com/pytorch/pytorch/pull/30356 and https://github.com/pytorch/pytorch/pull/31014 :'(\n\nThe last commit contains the fix. There was an internal FBcode error not able to compile the previous `impl_default->second.equal(default_val.second))` line. I tried various fixes in C++ internally but couldn't figure anything out. This is a good example of the programming costs of going from python -> c++ for different types of objects, because the conceptual overhead has expanded in scope from (python) -> (python, c++, pybind).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31123\n\nDifferential Revision: D18936128\n\nPulled By: eellison\n\nfbshipit-source-id: 7d8fd66a6dd4a3e9838f3a0b68c219b6565a9462", "pr_number": "31123", "files_changed": ["aten/src/ATen/core/function_schema.h", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "c0bcfd0445": {"title": "Revert D18923167: Expose setNumThreads to android api", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18923167\n\nOriginal commit changeset: 8d98c2edbff4\n\nfbshipit-source-id: 7db37cff298c511d0dd9eb373811c769e4a73be9", "pr_number": null, "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/src/main/java/org/pytorch/INativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/LiteNativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/Module.java", "android/pytorch_android/src/main/java/org/pytorch/NativePeer.java", "caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/ThreadPool.h"], "labels": []}, "9047d4df45": {"title": "Remove all remaining usages of BUILD_NAMEDTENSOR (#31116)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31116\n\nChangelist:\n- remove BUILD_NAMEDTENSOR macro\n- remove torch._C._BUILD_NAMEDTENSOR\n- remove all python behavior that relies on torch._C._BUILD_NAMEDTENSOR\n\nFuture:\n- In the next diff, I will remove all usages of\nATen/core/EnableNamedTensor.h since that header doesn't do anything\nanymore\n- After that, we'll be done with the BUILD_NAMEDTENSOR removal.\n\nTest Plan: - run CI\n\nDifferential Revision: D18934951\n\nPulled By: zou3519\n\nfbshipit-source-id: 0a0df0f1f0470d0a01c495579333a2835aac9f5d", "pr_number": "31116", "files_changed": ["CMakeLists.txt", "aten/src/ATen/core/EnableNamedTensor.h", "aten/src/ATen/env.py", "aten/src/ATen/native/README.md", "caffe2/core/macros.h.in", "cmake/Summary.cmake", "test/test_namedtensor.py", "tools/pyi/gen_pyi.py", "torch/_namedtensor_internals.py", "torch/_tensor_str.py", "torch/csrc/Module.cpp", "torch/functional.py"], "labels": ["merged"]}, "bcb0bb7e0e": {"title": "Remove unnecessary ATen/core/EnableNamedTensor.h (#31117)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31117\n\nAfter this diff, we will have completely removed the named tensor\nfeature flagging. This means that named tensors are always on and that\nthere is no mechanism to turn them off. There should be no more follow-up\ndiffs.\n\nI performed the deletion of the header with\n```\nfind . -type f -print0 | xargs -0 sed -i '/#include\n<ATen\\/core\\/EnableNamedTensor.h>/d'\n```\n\nTest Plan: - wait for CI\n\nDifferential Revision: D18934952\n\nPulled By: zou3519\n\nfbshipit-source-id: 253d059074b910fef15bdf885ebf71e0edf5bea5", "pr_number": "31117", "files_changed": ["aten/src/ATen/NamedTensorUtils.cpp", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/TensorNames.cpp", "aten/src/ATen/TensorNames.h", "aten/src/ATen/core/Dimname.cpp", "aten/src/ATen/core/Dimname.h", "aten/src/ATen/core/EnableNamedTensor.h", "aten/src/ATen/core/NamedTensor.cpp", "aten/src/ATen/core/NamedTensor.h", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/LegacyDefinitions.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/NamedTensor.cpp", "aten/src/ATen/native/PointwiseOps.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/SoftMax.cpp", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/TensorProperties.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cuda/CUDAUnaryOps.cpp", "aten/src/ATen/native/cuda/LegacyDefinitions.cpp", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/LegacyTHFunctions.cpp", "aten/src/ATen/templates/NativeFunctions.h", "aten/src/ATen/templates/OpsAlreadyMovedToC10.cpp", "aten/src/ATen/templates/SparseTypeDerived.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/templates/TensorMethods.h", "aten/src/ATen/templates/TypeDefault.cpp", "aten/src/ATen/templates/TypeDefault.h", "aten/src/ATen/templates/TypeDerived.cpp", "aten/src/ATen/templates/TypeDerived.h", "aten/src/ATen/test/Dimname_test.cpp", "aten/src/ATen/test/NamedTensor_test.cpp", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/THC/generic/THCTensorMasked.cu", "aten/src/THC/generic/THCTensorMathBlas.cu", "aten/src/THC/generic/THCTensorMathPointwise.cu", "tools/autograd/templates/VariableType.h", "tools/autograd/templates/python_variable_methods.cpp", "tools/autograd/templates/variable_factories.h", "torch/csrc/Module.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h", "torch/csrc/python_dimname.cpp", "torch/csrc/python_dimname.h", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/csrc/utils/tensor_new.cpp"], "labels": ["jit", "merged"]}, "4ead2e8996": {"title": "Fix CircleCI behavior for non-leaf stack PRs (#31088)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31088\n\nOriginal issue:\nhttps://github.com/pytorch/pytorch/issues/31027\n\nThe problem is that for the stacks of PRs for non-leaf PRs circleCI does not set environment variable `CIRCLE_PULL_REQUEST` which is used to filter out some jobs that should run only on `master`.\n\n(Android job for master includes alll 4 abis (x86, x86_64, armeabi-v7a, arm64-v8a)  and gradle build tries to get results from all 4 abis, for PRs we run only x86 build for resources economy. Thats why not filtered master android job fails as abis apart x86 were not scheduled)\n\nenv variable `CIRCLE_BRANCH ` is set fine and can be used as a workaround to distinguish that this is PR (published with ghstack).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18966385\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 644c5ef07fcf2d718b72695da2cc303da8b94ef4", "pr_number": "31088", "files_changed": [".circleci/scripts/should_run_job.sh"], "labels": ["merged"]}, "66f2bba852": {"title": "Adding function to convert Module to channels last", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28991\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18430810\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 0693d4e31fc6f9831722c29fc83517f16ddfc028", "pr_number": "28991", "files_changed": ["test/test_nn.py", "tools/autograd/templates/python_nn_functions.cpp", "tools/autograd/templates/python_variable_methods.cpp", "torch/nn/modules/module.py"], "labels": ["merged"]}, "066e3ed953": {"title": "Re-apply \"[bert/RoBERTa] Optimize LayerNorm with explicit vectorization using Vec256\" (#31127)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31127\n\nOriginal commit changeset: d22448b90843\n\nOn Skylake T6:\n\nSingle Core:\n(Note that our benchmark generates batch_size=47 for first case and batch_size=56 for the second case. In spite of that, the vectorized version is still faster than the original reference C version without vectorization.)\n- Before the PR:\n```\nnative_layer_norm        0.81%            5.884ms          0.81%            5.884ms          122.580us        NaN              0.000us          0.000us          48               [[47, 1, 1024], [1024], [1024]]\n```\n\n- After the PR:\n```\nnative_layer_norm        0.68%            5.053ms          0.68%            5.053ms          105.272us        NaN              0.000us          0.000us          48               [[56, 1, 1024], [1024], [1024]]\n```\n\n20 Cores:\n- Before the PR:\n```\nnative_layer_norm        1.65%            41.682ms         1.65%            41.682ms         868.365us        NaN              0.000us          0.000us          48               [[61, 64, 1024], [1024], [1024]]\n```\n\n- After the PR:\n```\nnative_layer_norm        1.34%            33.829ms         1.34%            33.829ms         704.771us        NaN              0.000us          0.000us          48               [[61, 64, 1024], [1024], [1024]]\n```\nghstack-source-id: 95420889\n\nTest Plan:\nbuck test mode/dev-nosan //caffe2/test:nn -- \"LayerNorm\"\n\nbuck test mode/dev-nosan //caffe2/test:nn -- \"test_LayerNorm_1d_no_elementwise_affine_eval\"\n\n python run_test.py -i nn -- TestNN.test_LayerNorm_1d_no_elementwise_affine_eval\n\nDifferential Revision: D18936428\n\nfbshipit-source-id: 8cae33d35fb338b5ac49b1597c2709152612d6e5", "pr_number": "31127", "files_changed": ["aten/src/ATen/native/cpu/layer_norm_kernel.cpp"], "labels": ["merged"]}, "bee6344d4e": {"title": "remove / rewrite weak module tests (#31193)", "body": "Summary:\nRemove most of the testing for `weak_script`, since we removed it. Refactor a few of the existing tests to use recursive scripting api.\n\nFix for https://github.com/pytorch/pytorch/issues/23965\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31193\n\nDifferential Revision: D18966291\n\nPulled By: eellison\n\nfbshipit-source-id: 6b1e18c293f55017868a14610d87b69be42bde12", "pr_number": "31193", "files_changed": ["test/test_jit.py"], "labels": ["merged"]}, "f6c31f61c5": {"title": "Enabled roll for bool tensor (#31194)", "body": "Summary:\nFixed this [issue](https://github.com/pytorch/pytorch/issues/31079).\nTested via unit test\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31194\n\nDifferential Revision: D18958141\n\nPulled By: izdeby\n\nfbshipit-source-id: 119bf4d31df10ee02c277f5a4663038470cf7780", "pr_number": "31194", "files_changed": ["aten/src/ATen/native/cuda/TensorTransformations.cu", "test/test_torch.py"], "labels": ["merged"]}, "a38184dbab": {"title": "Only create OwnerRRefs when processing remote calls (#31163)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31163\n\nThe purpose is to unblock integration with TorchScript. Currently,\nan OwnerRRef will be created by either a remote call or a to_here\ncall, whichever arrives first. However, when making RRef an IValue,\nwe need to know the type of value held by the RRef, which is\nretrived by checking the return type of the TorchScript function.\nThe TorchScript function is only avaible during the remote call\nbut not in the to_here() call. Hence, an OwnerRRef can only be\ncreated when processing a remote call. This commit implements this\nbehavior by introducing a conditional variable for every OwnerRRef\nin the RRefContext, and let the to_here() call and PyRRef::unpickle\nblock on the CV until the value is ready.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18949591\n\nPulled By: mrshenli\n\nfbshipit-source-id: 17513c6f1fd766885ea8e1cd38f672a403fa4222", "pr_number": "31163", "files_changed": ["torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h"], "labels": ["merged"]}, "5c936845cf": {"title": "fix torch_train build (#30497)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30497\n\nfix torch_train build\n\nTest Plan: buck build //xplat/caffe2:torch_trainAndroid\n\nReviewed By: dreiss\n\nDifferential Revision: D18719662\n\nfbshipit-source-id: a3d06b4068d502dbe29681d9f26906f2b8c7b622", "pr_number": "30497", "files_changed": ["torch/csrc/jit/export_module.cpp"], "labels": ["fb-exported", "jit", "merged"]}, "84d6796658": {"title": "move AWS ECR gc jobs to circleci (#30996)", "body": "Summary:\nall jobs are currently running with \"--dry-run\", so you can verify if the jobs are doing the right thing.  i'll remove the flag and make it runs every hour same as on Jenkins once this PR is approved.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30996\n\nDifferential Revision: D18971001\n\nPulled By: mingbowan\n\nfbshipit-source-id: 2384bdb50ebdf47aad265395f26be3843f0ce05e", "pr_number": "30996", "files_changed": [".circleci/config.yml", ".circleci/ecr_gc_docker/Dockerfile", ".circleci/ecr_gc_docker/gc.py", ".circleci/ecr_gc_docker/requirements.txt", ".circleci/generate_config_yml.py", ".circleci/validate-docker-version.py", ".circleci/verbatim-sources/docker_build_job.yml", ".circleci/verbatim-sources/docker_jobs.yml", ".circleci/verbatim-sources/workflows-docker-builder.yml", ".circleci/verbatim-sources/workflows-ecr-gc.yml", ".github/workflows/lint.yml"], "labels": ["merged"]}, "1d5af9599d": {"title": "Update ONNX Flatten to accept negative indices in opset 11 (#30751)", "body": "Summary:\nUpdate ONNX Flatten to accept negative indices in opset 11.\nWith this change, some cases of flatten do not rely on the input rank being available.\nFixes : https://github.com/pytorch/pytorch/issues/30512 .\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30751\n\nReviewed By: hl475\n\nDifferential Revision: D18946904\n\nPulled By: houseroad\n\nfbshipit-source-id: a6fa30a9182fff92211e505a19325525c6112f19", "pr_number": "30751", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged"]}, "a2463cbc38": {"title": "Adding quantized clamp kernel (#30541)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30541\n\nghstack-source-id: 95450749\n\nAdding quantized clamp kernel\n\nTest Plan:\nAdded test.\n\nbuck test mode/dev //caffe2/test:quantized -- 'test_qclamp \\(test_quantized\\.TestQuantizedOps\\)' --print-passing-details\n\nDifferential Revision: D18739628\n\nfbshipit-source-id: 38a029ab96c5b0689bb15c67dc4f274883e74975", "pr_number": "30541", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_qint.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/test_quantized.py", "torch/nn/quantized/functional.py"], "labels": ["merged"]}, "5ef0d6f854": {"title": "Remove subgraphNode kind assert in unmergeSubgraph (#31212)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31212\n\nTo be able to use this function more broadly.\n\nTest Plan: unit tests\n\nReviewed By: jackm321\n\nDifferential Revision: D18978913\n\nfbshipit-source-id: d998dc7c7f9540f491a8a4bc5d6d25d9c3bf8764", "pr_number": "31212", "files_changed": ["torch/csrc/jit/passes/utils/subgraph_utils.cpp"], "labels": ["fb-exported", "jit", "merged"]}, "c08f2ea254": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/367861fec0281c51648b6f60e85da1c1663c8c7c\nhttps://github.com/facebook/fbzmq/commit/22f5444c09e2a8a777bd21e65a588e893271e07d\nhttps://github.com/facebook/folly/commit/11c103407d6bdf61059c0c1a4599393ad1822a19\nhttps://github.com/facebook/litho/commit/34507cb3834b317aba8718e0f08bed5751ed061f\nhttps://github.com/facebook/proxygen/commit/16d5e3e5ac34a6f01b1bb347457246309664afdb\nhttps://github.com/facebook/rocksdb/commit/c4ce8e637faf5a6b909f2c725bf5ed49984d4bda\nhttps://github.com/facebookincubator/mvfst/commit/0f7ef7962049b5379094d82d4d9ddd4a02770200\nhttps://github.com/pytorch/fbgemm/commit/330fa4393374cc527402d37f26555a388419bcc8\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 2b6847af7ccba6b53a866e3fded2edf9995b0aaf", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "20a2e526ef": {"title": "build a generic future<T> (#29579)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29579\n\nPer #28923, this diff is to move Future<Message> to torch::utils and extend it to be Future<T>, most of implementations are copied from FutureMessage and ivalue::Future. merge ivalue::Future with Future<T> will be done separately.\n\nThe main difference between Future<T>  and FutureMessage is the error handling, instead of checking message type inside Future to handle error, this future<T> owns has_error_ and error_ states.\n\nalso this future passes value_, has_error_ and error_ states to callbacks for easily read future states.\n\nIn next diff, a torch script rpc async API will be created, before the API returns, it will create an ivalue::Future and passes it to Future<T>'s call back where state of ivalue::Future will be set.  In this way, the torch script rpc async API  can still return a ivalue::Future and call wait() to get its state appropriately afterwards.\nghstack-source-id: 95479525\n\nTest Plan: unit tests\n\nDifferential Revision: D18263023\n\nfbshipit-source-id: 48a65712656a72c2feb0bb3ec8b308c0528986a6", "pr_number": "29579", "files_changed": ["caffe2/CMakeLists.txt", "tools/build_variables.py", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/context/context.h", "torch/csrc/distributed/rpc/future_message.cpp", "torch/csrc/distributed/rpc/future_message.h", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/message.h", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_functions.h", "torch/csrc/distributed/rpc/request_callback.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/rref.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/utils/future.h", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "f30b14dead": {"title": "Fix handling of type comments in body (#30590)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30477. Any type comment after `# type: (...) -> ` is ignored.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30590\n\nDifferential Revision: D18887351\n\nPulled By: driazati\n\nfbshipit-source-id: 162c652f6d7610d14609bbcb25aaa27cdd947a76", "pr_number": "30590", "files_changed": ["test/test_jit.py", "torch/jit/annotations.py"], "labels": ["jit", "merged"]}, "b7c148013f": {"title": "fix torch square_ benchmark runtime error (#31221)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31221\n\nThis is fixing the runtime error introduced in https://github.com/pytorch/pytorch/pull/30719 that added torch square_ operator to the benchmark suite.\n\nTest Plan:\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: square_\n# Mode: Eager\n# Name: square__M512_N512_cpu\n# Input: M: 512, N: 512, device: cpu\nForward Execution Time (us) : 66.291\n\nReviewed By: hl475\n\nDifferential Revision: D18987889\n\nfbshipit-source-id: 09c56e3a73aab5ab661aac2b06429063b3a82fac", "pr_number": "31221", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": ["fb-exported", "merged"]}, "ca8cb3241a": {"title": "Expose setNumThreads to android api (#31205)", "body": "Summary:\nPR https://github.com/pytorch/pytorch/pull/31033 was unlanded due to macos build failure:\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/3916388\n\nThis PR has changes that `setNumThreads` is only for android and moved to separate class `org.pytorch.PytorchAndroid` as a static function which is better as it has global effect\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31205\n\nReviewed By: dreiss\n\nDifferential Revision: D18977250\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 4995859808af498c82933c4db52bd7c7dfae90e5", "pr_number": "31205", "files_changed": ["android/pytorch_android/host/build.gradle", "android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_common.h", "android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/src/main/java/org/pytorch/PytorchAndroid.java", "caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/ThreadPool.h"], "labels": ["merged"]}, "199e1fb348": {"title": "Use AVX2 to increase frequency for FP16<->FP32 Caffe2 ops (#31203)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31203\n\nFor multi-instance environment, AVX2 should help increase the clock frequency.\nghstack-source-id: 95502576\n\nTest Plan: buck test //caffe2/caffe2:caffe2_test_cpu -- \"Float16\"\n\nReviewed By: jspark1105\n\nDifferential Revision: D18962649\n\nfbshipit-source-id: 6532d929a99f41f2f6ad1a1a1962e38ae3ddaecb", "pr_number": "31203", "files_changed": ["caffe2/operators/half_float_ops.cc"], "labels": ["merged"]}, "db90a5b992": {"title": "Switch to open sourced fbjni (#30175)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30175\n\nfbjni was opensourced and java part is published as 'com.facebook.fbjni:fbjni-java-only:0.0.3'\nswitching to it.\nWe still need submodule fbjni inside the repo (which is already pointing to  https://github.com/facebookincubator/fbjni) for so linking.\n\n**Packaging changes**:\nbefore that `libfbjni.so` came from pytorch_android_fbjni dependency, as we also linked fbjni in `pytorch_android/CMakeLists.txt` - it was built in pytorch_android, but excluded for publishing. As we had 2 libfbjni.so there was a hack to exclude it for publishing and resolve duplication locally.\n```\n        if (rootProject.isPublishing()) {\n            exclude '**/libfbjni.so'\n        } else {\n            pickFirst '**/libfbjni.so'\n        }\n```\n\nAfter this change fbjni.so will be packaged inside pytorch_android.aar artefact and we do not need this gradle logic.\n\nI will update README in separate PR after landing previous PR to readme(https://github.com/pytorch/pytorch/pull/30128) to avoid conflicts\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18982235\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 5097df2557858e623fa480625819a24a7e8ad840", "pr_number": "30175", "files_changed": ["android/build.gradle", "android/pytorch_android/build.gradle", "android/test_app/app/build.gradle"], "labels": ["merged"]}, "f7c92f60ba": {"title": "Typo in filename align with classname", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31235\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19001793\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: ae7f410be6b3c291f1feb3027b5b4a6b7ce15ab3", "pr_number": "31235", "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java", "android/pytorch_android/src/main/java/org/pytorch/PytorchAndroid.java"], "labels": ["merged"]}, "1ef99cf0ab": {"title": "Intrusive_ptr implementation slower than shared_ptr (#30810)", "body": "Summary:\nIt was a random coding exercise so I wasn't putting much effort into it; but, I was like \"hey is the current intrusive_ptr implementation optimized enough?\" so I compared it with shared_ptr (using std::shared_from_this).\n\nMy benchmark result shows that intrusive_ptr is actually slower. On my macbook the speed is:\n\n```\n---------------------------------------------------------------\nBenchmark                        Time           CPU Iterations\n---------------------------------------------------------------\nBM_IntrusivePtrCtorDtor         14 ns         14 ns   52541902\nBM_SharedPtrCtorDtor            10 ns         10 ns   71898849\nBM_IntrusivePtrArray         14285 ns      14112 ns      49775\nBM_SharedPtrArray            13821 ns      13384 ns      51602\n```\n\nWanted to share the results so someone could probably take a look if interested.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30810\n\nReviewed By: yinghai\n\nDifferential Revision: D18828785\n\nPulled By: bddppq\n\nfbshipit-source-id: 202e9849c9d8a3da17edbe568572a74bb70cb6c5", "pr_number": "30810", "files_changed": [".jenkins/caffe2/test.sh", "c10/CMakeLists.txt", "c10/benchmark/CMakeLists.txt", "c10/benchmark/intrusive_ptr_benchmark.cpp"], "labels": ["merged"]}, "36d17f4105": {"title": "abort nccl communicators before throwing operation timed out (#31128)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31128\n\nWhen operation times out due to some errors that are not detected by nccl communicators, ncclCommWatchdog can not check this time out error and thus can not abort ncclComms accordingly. So explicitly abort ncclComms here before throwing this timed out exception to users, after this, ncclCommWatchdog can detect nccl communicators are aborted and clean up devNCCLCommMap_ accordingly. if throwing timed out excepiton without aborting nccl communicators here, it was observed that CUDA GPU will have 100% utilization and can not run new events successfully.\nghstack-source-id: 95528488\n\nTest Plan: newly revised test _test_nccl_errors_blocking passed with the changes in this diff; the reviesed test failed withtout the changes in this diff\n\nReviewed By: isunjin\n\nDifferential Revision: D18928607\n\nfbshipit-source-id: be65a05ce4ff005f0c7fed36ae8e28903e8ffe2b", "pr_number": "31128", "files_changed": ["test/test_c10d.py", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp"], "labels": ["merged"]}, "b64baa963f": {"title": "Robustify rpc_agent handlers with generic Future<T> (#31224)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31224\n\nIf a future coming back to a rpc_agent server is satisfied with an\nexception, ensure this information is propagated back over the wire.\nghstack-source-id: 95522418\n\nTest Plan: buck test mode/dev-nosan caffe2/torch/fb/distributed/thriftRpcBackend/...\n\nDifferential Revision: D18979185\n\nfbshipit-source-id: 99848ae805cc2d48948809a238f61a2e0ef234c9", "pr_number": "31224", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/utils/future.h"], "labels": ["merged"]}, "8fea7a49d6": {"title": "pinning hypothesis for windows", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31169\n\nDifferential Revision: D19036734\n\nPulled By: mingbowan\n\nfbshipit-source-id: 2205a40720329cb53e741c9827c9049142759588", "pr_number": "31169", "files_changed": [".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat"], "labels": ["merged"]}, "a9ad98fb25": {"title": "Remove unused argument \"destId\" in addSendRpcBackward (#31207)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31207\n\nCleanup after #30914.\n\nIn #30914, `autogradContext->addKnownWorkerId(dst);` was moved out of `addSendRpcBackward()`.\n\nSo `addSendRpcBackward()` does not need `dstId` as it's argument anymore.\nghstack-source-id: 95509218\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:dist_autograd_fork -- test_context_cleanup_tensor_no_grad\n```\n\nDifferential Revision: D5742365\n\nfbshipit-source-id: accd041a594ec18d369231f5590289828d87baa7", "pr_number": "31207", "files_changed": ["test/cpp/dist_autograd/test_dist_autograd.cpp", "torch/csrc/distributed/autograd/utils.cpp", "torch/csrc/distributed/autograd/utils.h"], "labels": ["merged"]}, "3587f769dc": {"title": "use propagate_names instead of propagate_names_for_reduction for cumsum and cumprod", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31134\n\nDifferential Revision: D18964172\n\nPulled By: anjali411\n\nfbshipit-source-id: 3050c6d283a469a858378c44ac2ab9102baefce5", "pr_number": "31134", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp"], "labels": ["merged"]}, "9954739956": {"title": "Refactor test for unique and unique_consecutive and fix some bugs (#31211)", "body": "Summary:\nTests for unique_dim will be refactored in a separate PR.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31211\n\nDifferential Revision: D19034968\n\nPulled By: ngimel\n\nfbshipit-source-id: 855d326b37638b5944f11fbbce03394cf000daf9", "pr_number": "31211", "files_changed": ["aten/src/ATen/native/Unique.cpp", "aten/src/ATen/native/cuda/Unique.cu", "test/test_torch.py"], "labels": ["merged"]}, "cd3f05b44d": {"title": "Small fixes for hipification (#31200)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31200\n\nWe do not hipify these files when doing out of place.\n\nTest Plan: wait for CI to clear.\n\nDifferential Revision: D18963683\n\nfbshipit-source-id: eeba8597143f26417d0a8181a4c746139afefa24", "pr_number": "31200", "files_changed": ["aten/src/ATen/native/Distributions.h", "aten/src/ATen/native/SharedReduceOps.h"], "labels": ["fb-exported", "merged"]}, "d7d07e7caf": {"title": "thrust is included in SortingKthValue.cu but never used", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31263\n\nDifferential Revision: D19042793\n\nPulled By: ngimel\n\nfbshipit-source-id: 28f06c46a53e15f106ebee6c36e2ad25a3676bd2", "pr_number": "31263", "files_changed": ["aten/src/ATen/native/cuda/SortingKthValue.cu"], "labels": ["merged"]}, "1ec989404c": {"title": "Kill some unnecessary function declarations.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31216\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18986640\n\nPulled By: gchanan\n\nfbshipit-source-id: 30630d9ea025bb510f85e9627cbb4ba46de5e93d", "pr_number": "31216", "files_changed": ["aten/src/TH/generic/THTensor.hpp"], "labels": ["merged"]}, "927588df8e": {"title": "Switch default memory format of _like operators to Preserve", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30087\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18624986\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 8e434966f872ffaddf1249248ea445cbbab300ce", "pr_number": "30087", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "test/test_torch.py"], "labels": ["merged"]}, "fde3d707ad": {"title": "Switch default memory format of to (and similar) operators to Preserve", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30088\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18624984\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 54901786d7496c7dce785140b0585ac9093b1d86", "pr_number": "30088", "files_changed": ["aten/src/ATen/native/TensorConversions.cpp", "test/test_cuda.py", "test/test_torch.py"], "labels": ["merged"]}, "c35cddb306": {"title": "Switch default memory format of clone operator to Preserve", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30089\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18624985\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 8d315b08b7b5858fd0a81d3375b44ccb94787ad4", "pr_number": "30089", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "test/test_torch.py"], "labels": ["merged", "topic: bc-breaking"]}, "6e1e09fd10": {"title": "Compile time type names (#26618)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26618\n\nImplement a mechanism to get type names at compile time\nIn a future diff, I'm planning to introduce this to caffe2::TypeMeta and a few other places.\nghstack-source-id: 95337871\n\nTest Plan: unit tests\n\nDifferential Revision: D17519253\n\nfbshipit-source-id: e14017f962fd181d147accb3f53fa8d6ee42a3f8", "pr_number": "26618", "files_changed": ["c10/test/util/ConstexprCrc_test.cpp", "c10/test/util/TypeIndex_test.cpp", "c10/util/ConstexprCrc.h", "c10/util/TypeIndex.h", "c10/util/string_view.h"], "labels": ["merged", "module: internals"]}, "2950530031": {"title": "caffe2::TypeMeta uses compile time type names (#26619)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26619\n\nghstack-source-id: 95348564\n\nTest Plan: unit tests\n\nDifferential Revision: D17519252\n\nfbshipit-source-id: 337ec76d17172dd1af60a1676d69964a41dcb7a1", "pr_number": "26619", "files_changed": ["aten/src/ATen/core/blob.h", "c10/test/util/typeid_test.cpp", "c10/util/TypeIndex.h", "c10/util/typeid.cpp", "c10/util/typeid.h"], "labels": ["merged", "module: internals"]}, "7c1b5084a7": {"title": "Enable equality operator for bfloat16 CPU scalar types. (#30817)", "body": "Summary:\nSee https://github.com/pytorch/xla/issues/1330 for reference.\n\nmruberry ailzhang FYI\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30817\n\nDifferential Revision: D18847375\n\nPulled By: mruberry\n\nfbshipit-source-id: d1efedf8b975b8d9b55cf0ddf141818eaa7c91f0", "pr_number": "30817", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp"], "labels": ["merged"]}, "7cb83bea3b": {"title": "Fix static cuda builds on older cmake versions (#30935)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/pull/28378#issuecomment-562597033\n\nTo reproduce the failure I had to downgrade to `cmake 3.9` (Ubuntu 18 uses 3.10 apparently). These older `cmake` versions unfortunately don't seem to allow `target_link_libraries(INTERFACE)` to be used with imported libraries. Switching back to `set_property(TARGET)` fixes the issue.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30935\n\nDifferential Revision: D18956912\n\nPulled By: albanD\n\nfbshipit-source-id: a2b728ee3268599a428b7878c988e1edef5d9dda", "pr_number": "30935", "files_changed": ["cmake/public/cuda.cmake"], "labels": ["merged", "open source"]}, "70013415c7": {"title": "DDP should not set grad for globally unused params (#28883)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/28294 DDP should not set grad for globally unused parameters\n\nDDP currently computes the param to bucket mapping upfront, and allreduce grads for all params in every iteration. Even if params are unused, it will just set grad to zero. With such behavior, optimizer cannot tell if a param indeed has a zero grad or it is not used in the current iteration. This could trigger convergence problems for optimizers with weight decay and momentum such as SGD. However, DDP cannot simply set grad to None for local unused parameters, as local unused parameters might be used in other processes, and hence we still need to allreduce its grad. Instead DDP should figure out the globally unused parameters and skip touching their grad in the end of backward.\n\nImplementation summary:\n* Add locally used parameter map for each model replica.\n* Mark the locally unused parameters in the end of forward and then reduce to get the globally unused parameters.\n* In the end of backward skip touching grad for those globally unused parameters.\n* Add a unit test test_global_local_unused_params_grad\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28883\n\nDifferential Revision: D18491530\n\nPulled By: mrshenli\n\nfbshipit-source-id: 24e9b5f20df86c34ddbf9c7106250fd6ce186699", "pr_number": "28883", "files_changed": ["test/test_c10d.py", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h"], "labels": []}, "065685180d": {"title": "Loading module from android asset (#30378)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30378\n\nLoading module directly from android assets. Iteration on https://github.com/pytorch/pytorch/pull/30109\nLoading Module:\n```\nmModule = AndroidUtils.loadModuleFromAsset(assetName, getAssets());\n```\n\n`org.pytorch.AndroidUtils` is excluded from pytorch_jni host build\n\nTesting:\ntest_app module load switched to this approach and works fine\n```\ngradle test_app:installMobNet2QuantDebug -PABI_FILTERS=x86 && adb shell am start -n org.pytorch.testapp.mobNet2Quant/org.pytorch.testapp.MainActivity\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18893269\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: a7c73776f40e9c67bef233da05db56cc6efbe76a", "pr_number": "30378", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_common.h", "android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/src/main/java/org/pytorch/NativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java", "android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java"], "labels": ["merged"]}, "58eb15f41c": {"title": "JIT Type parser for mobile (#30391)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30391\n\nA Type parser to parse the python string of a Type. For example,\n\"Tuple[str, Optional[float], Dict[str, List[Tensor]], int]\".\nPlease refer to test_type_parser.cpp for the usage.\n\nOne of the use cases is in lite interpreter, types needs to be serialized (directly calling the python_str() of the Type) and deserialized (calling parseType(str)).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18924268\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 830d411563abfbeec023f01e7f8f4a1796f9a59a", "pr_number": "30391", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_mobile_type_parser.cpp", "test/cpp/jit/tests.h", "tools/build_variables.py", "torch/csrc/jit/mobile/type_parser.cpp", "torch/csrc/jit/script/init.h", "torch/csrc/jit/script/lexer.h", "torch/csrc/jit/script/parser_constants.h", "torch/csrc/jit/script/script_type_parser.cpp", "torch/csrc/jit/script/string_to_type.cpp"], "labels": ["jit", "merged"]}, "57ee7dab87": {"title": "Wraps assert statements in cuda kernels (#31276)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31276\n\nChange assert --> CUDA_ASSERT_KERNEL to avoid hip undefined __assert_fail()\n\nThis is similar to https://github.com/pytorch/pytorch/pull/13902 in caffe2 land.\n\nTest Plan: wait for CI to clear\n\nReviewed By: bddppq\n\nDifferential Revision: D19047582\n\nfbshipit-source-id: 34703b03786c8eee9c78d2459eb54bde8dc21a57", "pr_number": "31276", "files_changed": ["aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/THC/THCTensorScatterGather.cu", "aten/src/THC/THCTensorTopK.cuh", "aten/src/THCUNN/BCECriterion.cu", "aten/src/THCUNN/ClassNLLCriterion.cu", "aten/src/THCUNN/SpatialClassNLLCriterion.cu", "c10/macros/Macros.h", "c10/util/TypeCast.h"], "labels": ["fb-exported", "merged", "module: rocm"]}, "701e05dcbb": {"title": "Buck test targets robolectric,instrumentattion", "body": "Summary:\nBuck targets for robolectric and instrumentation tests for pytorch android:\n```\nbuck test fbsource//fbandroid/mode/server //xplat/caffe2/android:test_host\n```\n```\nbuck test //xplat/caffe2/android:test_instrumentation\n```\nFor both:\n```\nbuck test fbsource//fbandroid/mode/server //xplat/caffe2/android:pytorch\n```\n\nModels in assets:\n`pt_android_test_asset` - creates buck target that can be included in both robolectric and instrumentation tests that contains asset created from provided torchscript sources as separate file, using the latest binaries of libtorch.\n\n`pt_gen_test_asset_bin`  does that tacing, usage format\n```\ngenerate_test_asset input_file.jit output_file.py\n```\n\nExample of test-host setup for users of pytorch android:\nrobolectric tests:\n\n```\nload(\"fbsource//xplat/caffe2:pt_defs.bzl\", \"pt_android_test_asset\", \"pt_predictor_binary\", \"PT_ANDRIOID_TEST_HOST_JNI_DEPS\")\n\npt_android_test_asset(\n    name = \"test_asset\",\n    src = \"test_asset.jit\",\n    asset_name = \"test_asset.pt\",\n)\n\nrobolectric3_test(\n    name = \"example_test_host\",\n    srcs = [...],\n    jni_deps = PT_ANDRIOID_TEST_HOST_JNI_DEPS,\n    deps = [\n        \":pytorch_common\",\n        \":test_asset\",\n        \"//fbandroid/java/com/facebook/soloader/annotation:annotation\",\n        \"//fbandroid/java/com/facebook/testing/robolectric/v3:v3\",\n        \"//fbandroid/libraries/soloader/java/com/facebook/soloader:soloader\",\n        \"//fbandroid/third-party/java/robolectric3/robolectric:robolectric\",\n    ],\n)\n```\n\nCOMMON_LINKER_FLAGS = [\"-Wl,--no-as-needed\"] can not be applied on MacOs\n\nTest Plan:\n```\n[twsvcscm@od0187.atn1 /data/sandcastle/boxes/fbsource (b416b20a)]$ buck test fbsource//fbandroid/mode/server //xplat/caffe2/android:pytorch\nParsing buck files: finished in 7.2 sec\nCreating action graph: finished in 0.7 sec\nBuilding: finished in 11.9 sec (100%) 791/791 jobs, 0 updated\n  Total time: 19.9 sec\nTesting: finished in 11.0 sec (30 PASS/0 FAIL)\nRESULTS FOR //xplat/caffe2/android:test_host //xplat/caffe2/android:test_instrumentation\nPASS     159ms 15 Passed   0 Skipped   0 Failed   org.pytorch.PytorchHostTests\nPASS     152ms 15 Passed   0 Skipped   0 Failed   org.pytorch.PytorchInstrumentedTests (localhost:31930)\nTESTS PASSED\n```\n\nOSS changes test:\n```\ngradle -p android pytorch_android:cAT passes\n```\n\nReviewed By: dreiss\n\nDifferential Revision: D18799005\n\nfbshipit-source-id: 881609826a837efebc8526aee40355c5a62947d0", "pr_number": null, "files_changed": ["android/pytorch_android/generate_test_asset.cpp", "android/pytorch_android/src/androidTest/java/org/pytorch/PytorchInstrumentedTests.java", "android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/test_asset.jit"], "labels": []}, "ffe0c1ae4d": {"title": "Make test_torch.py pass cuda-memcheck (#29243)", "body": "Summary:\nMake the following changes:\n- When there are more than 10k errors, cuda-memcheck only shows 10k errors, in this case we shouldn't raise an Exception\n- Add UNDER_CUDA_MEMCHECK environment to allow disabling `pin_memory` tests when running cuda-memcheck.\n- Add a `--ci` command option, when turned on, then this script would run output to stdout instead of writing a file, and exit with an error if cuda-memcheck fails\n- Add a `--nohang` command option. When turned on, then hang would be treated as pass instead of error\n- Do simple filtering on the test to run: if `'cpu'` in the test name but not `'cuda'` is not in the test name\n- Add `--split` and `--rank` to allowing splitting the work (NVIDIA CI has a limitation of 3 hours, we have to split the work to satisfy this limitation)\n- The error summary could be `ERROR SUMMARY: 1 error`, or `ERROR SUMMARY: 2 errors`, the tail could be `error` or `errors`, it is not of the same length. The script is fixed to handle this case.\n- Ignore errors from `cufft`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29243\n\nDifferential Revision: D18941701\n\nPulled By: mruberry\n\nfbshipit-source-id: 2048428f32b66ef50c67444c03ce4dd9491179d2", "pr_number": "29243", "files_changed": ["test/common_device_type.py", "test/scripts/cuda_memcheck_common.py", "test/scripts/run_cuda_memcheck.py", "test/test_torch.py"], "labels": ["merged"]}, "ec92711aac": {"title": "Fix error message in incorrect rref.localValue() call (#31199)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/31198, see the issue for more details. We throw an error when `local_value()` is called on a non-owned rref, but the incorrect node name is printed in the error message. This PR fixes that and adds a relevant unit test.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31199\n\nDifferential Revision: D19072014\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 760c20bfd2fbf286eaaca19500469509a575cfec", "pr_number": "31199", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/py_rref.cpp"], "labels": ["merged"]}, "0e50c1b0d9": {"title": "Replace assert with cuda assert macro (#31297)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31297\n\nFollow-up to https://github.com/pytorch/pytorch/pull/31276\n\nThis is final replacement needed for aten out of place hipification.\n\nTest Plan: wait for CI to clear.\n\nReviewed By: bddppq\n\nDifferential Revision: D19070209\n\nfbshipit-source-id: 1428cd0ddfb5a8f4e234fabce822285e898047ea", "pr_number": "31297", "files_changed": ["aten/src/THC/THCTensorIndex.cu"], "labels": ["fb-exported", "merged", "module: rocm"]}, "9dc3d8738c": {"title": "fix view call on discontiguous tensor in to_sparse_backward (#31223)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30820\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31223\n\nDifferential Revision: D19044172\n\nPulled By: ngimel\n\nfbshipit-source-id: ac9fa71197d4f6c5b90a26e8d23360250745a2e2", "pr_number": "31223", "files_changed": ["aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp", "test/common_methods_invocations.py"], "labels": ["merged"]}, "60ec53c7fd": {"title": "Fix copy kernel speed regression introduced in #29631 (#31279)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31271\n\nThis fixes copy kernel speed regression introduced in https://github.com/pytorch/pytorch/issues/29631.\n\nThe previous implementation forces the compiler to instantiate `static_cast_with_inter_type` because it is passed as an argument of a function. This behavior makes it impossible for compilers to do optimizations like automatic vectorization, and, function call itself is expensive compared to a single casting instruction.\n\nTo check the change, run\n```\nreadelf -Ws /home/xgao/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so | grep static_cast_with_inter_type\n```\n\nOn nightly build, we have output\n```\n168217: 0000000001852bf0     5 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIsdE5applyEd\n168816: 0000000001852d30    33 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIfEaE5applyEa\n168843: 00000000018531f0     7 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIblE5applyEl\n168930: 0000000001852c20     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIslE5applyEl\n168935: 00000000018528d0   124 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIfNS_4HalfEE5applyES1_\n169023: 0000000001852f30    17 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIdEhE5applyEh\n169713: 00000000018525c0     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIahE5applyEh\n170033: 0000000001852c10     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIsiE5applyEi\n170105: 0000000001852bd0     5 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIshE5applyEh\n170980: 0000000001852fc0    27 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIdES1_IfEE5applyES3_\n171398: 0000000001852810    13 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIdbE5applyEb\n171574: 00000000018532e0    35 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIbNS_8BFloat16EE5applyES1_\n171734: 0000000001852b20     6 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIlSt7complexIdEE5applyES2_\n172422: 0000000001853350    54 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeINS_8BFloat16EaE5applyEa\n172704: 00000000018533c0    38 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeINS_8BFloat16EfE5applyEf\n172976: 0000000001852890    10 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIflE5applyEl\n173038: 0000000001852f80     9 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIdEfE5applyEf\n173329: 00000000018531c0    20 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIbfE5applyEf\n173779: 00000000018524d0     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIhiE5applyEi\n174032: 0000000001852960    14 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIfNS_8BFloat16EE5applyES1_\n174334: 0000000001852d60    29 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIfEdE5applyEd\n174470: 0000000001852c60   124 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIsNS_4HalfEE5applyES1_\n174770: 0000000001852bc0    15 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIlNS_8BFloat16EE5applyES1_\n176408: 0000000001853980   144 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeINS_4HalfEbE5applyEb\n176475: 0000000001852790   128 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIdNS_4HalfEE5applyES1_\n....\n```\n\nAnd after this PR, we get empty output\n```\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31279\n\nDifferential Revision: D19075587\n\nPulled By: ngimel\n\nfbshipit-source-id: c20088241f39fa40c1d055f0a46eb5b9ece52e71", "pr_number": "31279", "files_changed": ["aten/src/ATen/native/cpu/CopyKernel.cpp"], "labels": ["merged"]}, "930d0751e6": {"title": "Java Tensor hybrid, owns at::Tensor, no memcopy for java outputs. (#30501)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30501\n\n**Motivation**:\nIn current state output of libtorch Module forward,runMethod is mem copied to java ByteBuffer, which is allocated, at least in some versions of android, on java heap. That could lead to intensive garbage collection.\n\n**Change**:\nOutput java tensor becomes owner of output at::Tensor and holds it (as `pytorch_jni::TensorHybrid::tensor_` field) alive until java part is not destroyed by GC. For that org.pytorch.Tensor becomes 'Hybrid' class in fbjni naming and starts holding member field `HybridData mHybridData;`\n\nIf construction of it starts from java side - java constructors of subclasses (we need all the fields initialized, due to this `mHybridData` is not declared final, but works as final) call `this.mHybridData = super.initHybrid();` to initialize cpp part (`at::Tensor tensor_`).\n\nIf construction starts from cpp side - cpp side is initialiaed using provided at::Tensor with `makeCxxInstance(std::move(tensor))` and is passed to java method `org.pytorch.Tensor#nativeNewTensor` as parameter `HybridData hybridData`, which holds native pointer to cpp side.\n\nIn that case `initHybrid()` method is not called, but parallel set of ctors of subclasses are used, which stores `hybridData` in `mHybridData`.\n\nRenaming:\n`JTensor` -> `TensorHybrid`\n\nRemoved method:\n`JTensor::newAtTensorFromJTensor(JTensor)` becomes trivial `TensorHybrid->cthis()->tensor()`\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18893320\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: df94775d2a010a1ad945b339101c89e2b79e0f83", "pr_number": "30501", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_lite.cpp", "android/pytorch_android/src/main/java/org/pytorch/Tensor.java"], "labels": ["merged"]}, "0d7391f8b2": {"title": "Test cases for custom ops with autograd (#31003)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31003\n\n-\nghstack-source-id: 95663728\n\nTest Plan: unit tests\n\nDifferential Revision: D18896189\n\nfbshipit-source-id: d71f7678fff644536fe30452ee21a5a7df1f1f0b", "pr_number": "31003", "files_changed": ["test/custom_operator/op.cpp", "test/custom_operator/op.h", "test/custom_operator/test_custom_ops.cpp", "test/custom_operator/test_custom_ops.py", "torch/script.h"], "labels": ["merged"]}, "c95d46abbd": {"title": "Remove C++11 compatibility from c10::util::crc64_t (#30920)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30920\n\ndeletecode\nghstack-source-id: 95255641\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869640\n\nfbshipit-source-id: c3d7f4e1a29caff9fd8a8141c258f6f1c3fd830c", "pr_number": "30920", "files_changed": ["c10/util/ConstexprCrc.h"], "labels": ["merged"]}, "409151e1bb": {"title": "Use [[noreturn]] instead of C10_NORETURN or CAFFE_NORETURN (#30917)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30917\n\nThis is a C++14 feature, we can use this now.\nghstack-source-id: 95255753\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869637\n\nfbshipit-source-id: dd02036b9faeaffa64b2d2d305725443054da31b", "pr_number": "30917", "files_changed": ["c10/macros/Macros.h", "c10/util/Logging.h", "caffe2/core/common.h"], "labels": ["merged"]}, "9ca61aec0f": {"title": "Kill THLogAdd (#31217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31217\n\nIt doesn't seem to be used.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18986642\n\nPulled By: gchanan\n\nfbshipit-source-id: 96d615df82731d2224d403ab6e2cad6d4c6674fd", "pr_number": "31217", "files_changed": ["aten/src/TH/CMakeLists.txt", "aten/src/TH/TH.h", "aten/src/TH/THLogAdd.cpp", "aten/src/TH/THLogAdd.h"], "labels": ["merged"]}, "455e85a2f1": {"title": "Fix unflatten when dim is a negative integer (#31208)", "body": "Summary:\nChangelog:\n- Wrap dim to be a positive integer when dim is negative\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31208\n\nTest Plan:\n- Updated tests in test_namedtensor.py\n\nFixes https://github.com/pytorch/pytorch/issues/31184\n\nDifferential Revision: D19036569\n\nPulled By: zou3519\n\nfbshipit-source-id: 86e01e20988dee7c4b6c73232f66282d687f9a2c", "pr_number": "31208", "files_changed": ["aten/src/ATen/native/NamedTensor.cpp", "test/test_namedtensor.py"], "labels": ["merged", "open source"]}, "d401ba1417": {"title": "benchmark binary ops in binary_test (#31326)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31326\n\nas title\n\nTest Plan:\n```\nbuck run caffe2/benchmarks/operator_benchmark/pt:binary_test -- --iterations 1\n\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: add\n# Mode: Eager\n# Name: add_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.float32\n# Input: in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.float32\nForward Execution Time (us) : 28080.802\n\nReviewed By: hl475\n\nDifferential Revision: D19120113\n\nfbshipit-source-id: 1105de208f7609cc6d74f0b5bc6fe75f19146b28", "pr_number": "31326", "files_changed": ["benchmarks/operator_benchmark/benchmark_all_other_test.py", "benchmarks/operator_benchmark/pt/binary_test.py"], "labels": ["fb-exported", "merged"]}, "c6a8f884d8": {"title": "add copy_ operator the op bench (#31327)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31327\n\nAdds copy_ operator to the benchmark suite\n\nTest Plan:\n```\nbuck run caffe2/benchmarks/operator_benchmark/pt:binary_test -- --iterations 1 --operators copy_\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: copy_\n# Mode: Eager\n# Name: copy__M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32\n# Input: M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32\nForward Execution Time (us) : 60.645\n\nReviewed By: hl475\n\nDifferential Revision: D19122910\n\nfbshipit-source-id: e5f0b0e2612daae0201b1b4a87f52b971e0cc4a8", "pr_number": "31327", "files_changed": ["benchmarks/operator_benchmark/pt/binary_test.py"], "labels": ["fb-exported", "merged"]}, "643ca5def2": {"title": "Replace c10::guts::stuff with std::stuff (#30915)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30915\n\nSince we now have C++14, we don't need these c10::guts helpers anymore\nghstack-source-id: 95777609\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869639\n\nfbshipit-source-id: 97716f932297c64c6e814410ac47b444c33d4e2e", "pr_number": "30915", "files_changed": ["aten/src/ATen/core/DeprecatedTypePropertiesRegistry.cpp", "aten/src/ATen/core/List.h", "aten/src/ATen/core/NamedTensor.cpp", "aten/src/ATen/core/NamedTensor.h", "aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/boxing.h", "aten/src/ATen/core/boxing/kernel_function.h", "aten/src/ATen/core/boxing/kernel_function_legacy_test.cpp", "aten/src/ATen/core/boxing/kernel_function_test.cpp", "aten/src/ATen/core/boxing/kernel_functor.h", "aten/src/ATen/core/boxing/kernel_functor_test.cpp", "aten/src/ATen/core/boxing/kernel_lambda_legacy_test.cpp", "aten/src/ATen/core/boxing/kernel_lambda_test.cpp", "aten/src/ATen/core/boxing/kernel_stackbased_test.cpp", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/op_registration/infer_schema.h", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/cpu/vec256/vec256.h", "aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/native/QuantizedLinear.cpp", "aten/src/ATen/native/cpu/Loops.h", "aten/src/ATen/native/cpu/Reduce.h", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/test/NamedTensor_test.cpp", "aten/src/TH/generic/THTensorRandom.cpp", "binaries/benchmark_helper.cc", "binaries/convert_and_benchmark.cc", "binaries/predictor_verifier.cc", "c10/core/TensorOptions.h", "c10/test/util/Metaprogramming_test.cpp", "c10/test/util/TypeList_test.cpp", "c10/util/Array.h", "c10/util/C++17.h", "c10/util/Half.h", "c10/util/Metaprogramming.h", "c10/util/TypeIndex.h", "c10/util/TypeList.h", "c10/util/TypeTraits.h", "c10/util/either.h", "c10/util/intrusive_ptr.h", "c10/util/order_preserving_flat_hash_map.h", "c10/util/string_view.h", "c10/util/typeid.cpp", "c10/util/typeid.h", "caffe2/core/common.h", "caffe2/core/export_c10_op_to_caffe2.h", "caffe2/core/net_async_base.cc", "caffe2/core/net_async_task_future.cc", "caffe2/core/net_async_task_graph.cc", "caffe2/core/net_parallel.cc", "caffe2/core/net_test.cc", "caffe2/core/observer_test.cc", "caffe2/core/operator.cc", "caffe2/core/operator_schema.cc", "caffe2/core/plan_executor.cc", "caffe2/mobile/contrib/ulp2/ulp.cc", "caffe2/mobile/contrib/ulp2/ulp_neon.cc", "caffe2/mobile/contrib/ulp2/ulp_test.cc", "caffe2/observers/operator_attaching_net_observer.h", "caffe2/observers/time_observer_test.cc", "caffe2/onnx/backend_rep.cc", "caffe2/operators/counter_ops.cc", "caffe2/operators/filler_op.h", "caffe2/operators/quantized/int8_test_utils.h", "caffe2/operators/string_ops_test.cc", "caffe2/opt/backend_cutting.cc", "caffe2/opt/nql/tests/GraphMatcherTest.cc", "caffe2/opt/optimize_ideep.cc", "caffe2/predictor/predictor_test.cc", "caffe2/predictor/predictor_utils.cc", "caffe2/python/mpi_python.cc", "caffe2/quantization/server/dynamic_histogram.cc", "caffe2/queue/blobs_queue_db.cc", "caffe2/serialize/file_adapter.cc", "caffe2/serialize/inline_container.cc", "caffe2/sgd/iter_op.cc", "caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/WorkersPool.h", "caffe2/video/video_decoder.cc", "modules/observers/perf_observer.cc", "torch/csrc/autograd/custom_function.h", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h", "torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_req.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp", "torch/csrc/distributed/autograd/utils.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/python_call.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_remote_call.cpp", "torch/csrc/distributed/rpc/python_resp.cpp", "torch/csrc/distributed/rpc/rref_proto.cpp", "torch/csrc/distributed/rpc/script_call.cpp", "torch/csrc/distributed/rpc/script_remote_call.cpp", "torch/csrc/distributed/rpc/script_resp.cpp", "torch/csrc/jit/import.cpp", "torch/csrc/jit/init.cpp", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/passes/guard_elimination.cpp", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/pickle.cpp", "torch/csrc/jit/register_c10_ops.cpp", "torch/csrc/jit/script/lexer.h", "torch/custom_class.h", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": ["jit", "merged"]}, "c5d3be1102": {"title": "Remove the second copy on calling dist_autograd_context._known_worker_ids() (#31206)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31206\n\nImprovement on #25525.\n\n- DistAutogradContext::getKnownWorkerIds() returns a unordered_map as temp value. No need to copy this temp value A into another temp value B.\nghstack-source-id: 95736296\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:dist_autograd_fork --  test_worker_ids_recorded\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:dist_autograd_fork_thrift -- test_context_cleanup_tensor_with_grad\n```\n\nDifferential Revision: D5707771\n\nfbshipit-source-id: 9fea83dc69b02047aef8b02a73028a260ac0be40", "pr_number": "31206", "files_changed": ["test/dist_autograd_test.py", "torch/csrc/distributed/autograd/init.cpp"], "labels": ["merged"]}, "229ce89b92": {"title": "Fix coverage and hypothesis conflict (#31320)", "body": "Summary:\nTemporarily enforcing versions for all envs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31320\n\nDifferential Revision: D19122781\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: fe6473b177367371387d4b3b873131e7ecfbc0f8", "pr_number": "31320", "files_changed": [".jenkins/caffe2/test.sh"], "labels": ["merged"]}, "f9010d7648": {"title": "remove wipe cache from op bench (#31334)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31334\n\nThe wipe cache logic was introduced hoping to reduce the variations in the benchmark results. Based on our experiments result, it didn't actually help with that. In addition, several engineers had encountered the issue of missing cpuinfo.h which was used in the wipe cache logic. So this diff removes that feature to ensure smooth installation and running of the op bench.\n\nTest Plan:\n```\nbuck run caffe2/benchmarks/operator_benchmark/pt:add_test -- --iterations 1\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: add\n# Mode: Eager\n# Name: add_M1_N1_K1_cpu\n# Input: M: 1, N: 1, K: 1, device: cpu\nForward Execution Time (us) : 111.192\n\nA/B test also pass Benchmark Run #2476535015\n\nReviewed By: hl475\n\nDifferential Revision: D19126970\n\nfbshipit-source-id: 9b1ab48c121838836ba6e0ae664a48fe2d18efdd", "pr_number": "31334", "files_changed": ["benchmarks/operator_benchmark/benchmark_core.py", "benchmarks/operator_benchmark/benchmark_runner.py", "benchmarks/operator_benchmark/pt_extension/extension.cpp"], "labels": ["fb-exported", "merged"]}, "10ce1765be": {"title": "Introducing ScalarTypeType and LayoutType (#31074)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31074\n\nAs the title,\nIt's step 1 in https://github.com/pytorch/pytorch/pull/30694#issuecomment-564205276.\n\nNot using those types in any other place.\n\nTest Plan: Making sure all unit tests and build pass successfully.\n\nDifferential Revision: D18916246\n\nfbshipit-source-id: c8213307ed196e1b51ce1a2a7c10869dcd45b79e", "pr_number": "31074", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/unpickler.cpp"], "labels": ["fb-exported", "jit", "merged"]}, "0e548a76eb": {"title": "Upgrade exported ONNX IR version to 6 (#31025)", "body": "Summary:\nUpgrade IR version from 4 to 6, below is change doc from ONNX. The upgrade should be backward compatible.\n\n```\n  // IR VERSION 5 published on March 18, 2019\n  // - Add message TensorAnnotation.\n  // - Add quantization annotation in GraphProto to map tensor with its scale and zero point quantization parameters.\n  IR_VERSION_2019_3_18 = 0x0000000000000005;\n\n  // IR VERSION 6 published on Sep 19, 2019\n  // - Add support for sparse tensor constants stored in model.\n  //   - Add message SparseTensorProto\n  //   - Add sparse initializers\n  IR_VERSION = 0x0000000000000006;\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31025\n\nReviewed By: hl475\n\nDifferential Revision: D18935444\n\nPulled By: houseroad\n\nfbshipit-source-id: 9ba47f9657fa1a668db291cf04af07d5e8d73c21", "pr_number": "31025", "files_changed": ["test/onnx/expect/TestOperators.test_acos.expect", "test/onnx/expect/TestOperators.test_add_broadcast.expect", "test/onnx/expect/TestOperators.test_add_left_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_right_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_singleton_broadcast.expect", "test/onnx/expect/TestOperators.test_addconstant.expect", "test/onnx/expect/TestOperators.test_addmm.expect", "test/onnx/expect/TestOperators.test_arange_dynamic.expect", "test/onnx/expect/TestOperators.test_argmax.expect", "test/onnx/expect/TestOperators.test_asin.expect", "test/onnx/expect/TestOperators.test_at_op.expect", "test/onnx/expect/TestOperators.test_atan.expect", "test/onnx/expect/TestOperators.test_avg_pool2d.expect", "test/onnx/expect/TestOperators.test_baddbmm.expect", "test/onnx/expect/TestOperators.test_basic.expect", "test/onnx/expect/TestOperators.test_batchnorm.expect", "test/onnx/expect/TestOperators.test_batchnorm_1d.expect", "test/onnx/expect/TestOperators.test_batchnorm_noaffine.expect", "test/onnx/expect/TestOperators.test_batchnorm_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_batchnorm_training.expect", "test/onnx/expect/TestOperators.test_bitshift.expect", "test/onnx/expect/TestOperators.test_c2_op.expect", "test/onnx/expect/TestOperators.test_chunk.expect", "test/onnx/expect/TestOperators.test_clip.expect", "test/onnx/expect/TestOperators.test_clip_max.expect", "test/onnx/expect/TestOperators.test_clip_min.expect", "test/onnx/expect/TestOperators.test_concat2.expect", "test/onnx/expect/TestOperators.test_conv.expect", "test/onnx/expect/TestOperators.test_conv_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_conv_onnx_irv4_opset8.expect", "test/onnx/expect/TestOperators.test_convtranspose.expect", "test/onnx/expect/TestOperators.test_cos.expect", "test/onnx/expect/TestOperators.test_cumsum.expect", "test/onnx/expect/TestOperators.test_det.expect", "test/onnx/expect/TestOperators.test_dict.expect", "test/onnx/expect/TestOperators.test_dict_str.expect", "test/onnx/expect/TestOperators.test_dropout.expect", "test/onnx/expect/TestOperators.test_elu.expect", "test/onnx/expect/TestOperators.test_embedding_bags.expect", "test/onnx/expect/TestOperators.test_empty_like.expect", "test/onnx/expect/TestOperators.test_empty_like_opset7.expect", "test/onnx/expect/TestOperators.test_equal.expect", "test/onnx/expect/TestOperators.test_erf.expect", "test/onnx/expect/TestOperators.test_exp.expect", "test/onnx/expect/TestOperators.test_expand.expect", "test/onnx/expect/TestOperators.test_flatten.expect", "test/onnx/expect/TestOperators.test_flatten2D.expect", "test/onnx/expect/TestOperators.test_fmod.expect", "test/onnx/expect/TestOperators.test_frobenius_norm.expect", "test/onnx/expect/TestOperators.test_full.expect", "test/onnx/expect/TestOperators.test_full_like.expect", "test/onnx/expect/TestOperators.test_gather.expect", "test/onnx/expect/TestOperators.test_gather_opset11.expect", "test/onnx/expect/TestOperators.test_ge.expect", "test/onnx/expect/TestOperators.test_gelu.expect", "test/onnx/expect/TestOperators.test_gt.expect", "test/onnx/expect/TestOperators.test_hardtanh.expect", "test/onnx/expect/TestOperators.test_implicit_expand.expect", "test/onnx/expect/TestOperators.test_index.expect", "test/onnx/expect/TestOperators.test_isnan.expect", "test/onnx/expect/TestOperators.test_layer_norm_aten.expect", "test/onnx/expect/TestOperators.test_le.expect", "test/onnx/expect/TestOperators.test_linear.expect", "test/onnx/expect/TestOperators.test_log_sigmoid.expect", "test/onnx/expect/TestOperators.test_logsoftmax.expect", "test/onnx/expect/TestOperators.test_lt.expect", "test/onnx/expect/TestOperators.test_master_opset.expect", "test/onnx/expect/TestOperators.test_max.expect", "test/onnx/expect/TestOperators.test_maxpool.expect", "test/onnx/expect/TestOperators.test_maxpool_dilations.expect", "test/onnx/expect/TestOperators.test_maxpool_indices.expect", "test/onnx/expect/TestOperators.test_mean.expect", "test/onnx/expect/TestOperators.test_meshgrid.expect", "test/onnx/expect/TestOperators.test_min.expect", "test/onnx/expect/TestOperators.test_mm.expect", "test/onnx/expect/TestOperators.test_narrow.expect", "test/onnx/expect/TestOperators.test_ne.expect", "test/onnx/expect/TestOperators.test_nonzero.expect", "test/onnx/expect/TestOperators.test_norm_p1.expect", "test/onnx/expect/TestOperators.test_norm_p2.expect", "test/onnx/expect/TestOperators.test_ones_like.expect", "test/onnx/expect/TestOperators.test_pad.expect", "test/onnx/expect/TestOperators.test_params.expect", "test/onnx/expect/TestOperators.test_params_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_permute2.expect", "test/onnx/expect/TestOperators.test_pixel_shuffle.expect", "test/onnx/expect/TestOperators.test_pow.expect", "test/onnx/expect/TestOperators.test_prelu.expect", "test/onnx/expect/TestOperators.test_prod.expect", "test/onnx/expect/TestOperators.test_rand.expect", "test/onnx/expect/TestOperators.test_randn.expect", "test/onnx/expect/TestOperators.test_reduce_sum_negative_indices.expect", "test/onnx/expect/TestOperators.test_reduced_mean.expect", "test/onnx/expect/TestOperators.test_reduced_mean_keepdim.expect", "test/onnx/expect/TestOperators.test_reduced_prod.expect", "test/onnx/expect/TestOperators.test_reduced_prod_keepdim.expect", "test/onnx/expect/TestOperators.test_reduced_sum.expect", "test/onnx/expect/TestOperators.test_reduced_sum_keepdim.expect", "test/onnx/expect/TestOperators.test_reducemax.expect", "test/onnx/expect/TestOperators.test_reducemin.expect", "test/onnx/expect/TestOperators.test_remainder.expect", "test/onnx/expect/TestOperators.test_repeat.expect", "test/onnx/expect/TestOperators.test_repeat_dim_overflow.expect", "test/onnx/expect/TestOperators.test_retain_param_name_disabled.expect", "test/onnx/expect/TestOperators.test_round.expect", "test/onnx/expect/TestOperators.test_rrelu.expect", "test/onnx/expect/TestOperators.test_rsqrt.expect", "test/onnx/expect/TestOperators.test_rsub.expect", "test/onnx/expect/TestOperators.test_scatter_add.expect", "test/onnx/expect/TestOperators.test_scatter_add_opset11.expect", "test/onnx/expect/TestOperators.test_selu.expect", "test/onnx/expect/TestOperators.test_sign.expect", "test/onnx/expect/TestOperators.test_sin.expect", "test/onnx/expect/TestOperators.test_slice.expect", "test/onnx/expect/TestOperators.test_slice_dynamic.expect", "test/onnx/expect/TestOperators.test_split.expect", "test/onnx/expect/TestOperators.test_split_with_sizes.expect", "test/onnx/expect/TestOperators.test_sqrt.expect", "test/onnx/expect/TestOperators.test_std.expect", "test/onnx/expect/TestOperators.test_sum.expect", "test/onnx/expect/TestOperators.test_tan.expect", "test/onnx/expect/TestOperators.test_topk.expect", "test/onnx/expect/TestOperators.test_topk_smallest_unsorted.expect", "test/onnx/expect/TestOperators.test_transpose.expect", "test/onnx/expect/TestOperators.test_type_as.expect", "test/onnx/expect/TestOperators.test_unfold.expect", "test/onnx/expect/TestOperators.test_unique.expect", "test/onnx/expect/TestOperators.test_unsqueeze.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_scale.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_scale_default_scale_factor.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_size.expect", "test/onnx/expect/TestOperators.test_view.expect", "test/onnx/expect/TestOperators.test_view_flatten.expect", "test/onnx/expect/TestOperators.test_zeros_like.expect", "torch/csrc/jit/export.cpp", "torch/csrc/onnx/init.cpp", "torch/csrc/onnx/onnx.h", "torch/onnx/__init__.py"], "labels": ["jit", "merged"]}, "52b8a52e4d": {"title": "move AliasWithNameOp to caffe2/operators", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31281\n\nReviewed By: houseroad\n\nDifferential Revision: D19053453\n\nfbshipit-source-id: 350bfd5c001db9c17916dcae7ade8f56db1e9841", "pr_number": "31281", "files_changed": ["caffe2/operators/alias_with_name.cc", "caffe2/operators/alias_with_name.cu", "caffe2/operators/alias_with_name.h", "caffe2/python/operator_test/alias_with_name_test.py", "caffe2/python/operator_test/torch_integration_test.py"], "labels": ["fb-exported", "merged"]}, "49eff2f43c": {"title": "Kill THSize. (#31218)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31218\n\nIt isn't used.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18986641\n\nPulled By: gchanan\n\nfbshipit-source-id: 0a434941d12193941f097232c18ffe4268bf5f82", "pr_number": "31218", "files_changed": ["aten/src/TH/CMakeLists.txt", "aten/src/TH/TH.h", "aten/src/TH/THSize.cpp", "aten/src/TH/THSize.h"], "labels": ["merged"]}, "d2067569e7": {"title": "Kill THTensor_(bhistc). (#31254)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31254\n\nIt's not used.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19022923\n\nPulled By: gchanan\n\nfbshipit-source-id: caa5e6b7a133f24f8f3349fd1e53147f8dd3fd97", "pr_number": "31254", "files_changed": ["aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp"], "labels": ["merged"]}, "dab5f72543": {"title": "we should have a config-based way to skip flaky tests (#30978)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30978\n\nThis particular approach queries our issue tracker for test titles that\nmatch the following format:\n\n```\nDISABLED test_async_grad_guard_with_grad (jit.test_async.TestAsync)\n```\n\nAnd then skips the python test for them. There is 1 second timeout so\nif the internet flakes we still run the test suite, without disabling any\ntests.\n\nThis is intended as a quick fix, similar to ninja unland, to get to a green\nmaster. Long term test disables should go into the code.\n\nTest Plan: Imported from OSS\n\nPulled By: zdevito\n\nDifferential Revision: D18890532\n\nfbshipit-source-id: fe9447e59a6d5c9ad345f7c3ff15d63b6d2a09e2", "pr_number": "30978", "files_changed": ["test/common_utils.py", "tools/update_disabled_tests.sh"], "labels": ["merged"]}, "cc8d6342fc": {"title": "make profiling take no_grad flags into account (#31071)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31071\n\nPreviously the profiler would think Tensors would require grad, even\nwhen the no_grad flag is enabled during execution. This makes the profiling\nand guards respect the no_grad flag, which eliminates extra differentiable\ngraphs that appear in the backward graph (where no_grad is typically enabled).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18915468\n\nPulled By: zdevito\n\nfbshipit-source-id: 1ae816a16ab78ae5352825cc6b4a68ed7681a089", "pr_number": "31071", "files_changed": ["test/test_jit_fuser.py", "torch/csrc/jit/interpreter.cpp", "torch/csrc/jit/interpreter.h", "torch/csrc/jit/profiling_record.cpp", "torch/csrc/jit/profiling_record.h"], "labels": ["jit", "merged"]}, "5554e5b793": {"title": "Docs: c++11 -> c++14 (#30530)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30530\n\nSwitch some mentions of \"C++11\" in the docs to \"C++14\"\nghstack-source-id: 95812049\n\nTest Plan: testinprod\n\nDifferential Revision: D18733733\n\nfbshipit-source-id: b9d0490eb3f72bad974d134bbe9eb563f6bc8775", "pr_number": "30530", "files_changed": ["CONTRIBUTING.md", "aten/conda/meta.yaml", "caffe2/contrib/aten/README.md", "cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake", "docs/cpp/source/frontend.rst", "docs/cpp/source/notes/tensor_basics.rst"], "labels": ["merged"]}, "0b8332efb4": {"title": "Remove c++11 examples from doc comments (#30925)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30925\n\n-\nghstack-source-id: 95810835\n\nTest Plan: it's just comments\n\nDifferential Revision: D18869634\n\nfbshipit-source-id: 346498ae2472dbfe23ef40533bff891fde9922c4", "pr_number": "30925", "files_changed": ["c10/util/Metaprogramming.h", "c10/util/TypeList.h"], "labels": ["merged"]}, "d9c3913dfc": {"title": "move BatchPermutationOp to caffe2/operators", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31350\n\nReviewed By: houseroad\n\nDifferential Revision: D19053527\n\nfbshipit-source-id: 50d11f137d0f5c07e8ad899a3a84d56a042bbc32", "pr_number": "31350", "files_changed": ["caffe2/operators/batch_permutation_op.cc", "caffe2/operators/batch_permutation_op.cu", "caffe2/operators/batch_permutation_op.h", "caffe2/operators/batch_permutation_op_gpu_test.cc", "modules/detectron/batch_permutation_op.cc", "modules/detectron/batch_permutation_op.cu", "modules/detectron/batch_permutation_op.h"], "labels": ["fb-exported", "merged"]}, "f0243ea712": {"title": "Use [[deprecated]] instead of C10_DEPRECATED (#30918)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30918\n\nThis is a C++14 feature we can use now\nghstack-source-id: 95811482\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869636\n\nfbshipit-source-id: b5b3d78b61b6ceb2deda509131f8502e95b1d057", "pr_number": "30918", "files_changed": ["aten/src/ATen/Dispatch.h", "aten/src/ATen/core/TensorAccessor.h", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/TensorBody.h", "c10/core/MemoryFormat.h", "c10/core/Scalar.h", "c10/core/ScalarType.h", "c10/util/ArrayRef.h", "c10/util/Deprecated.h", "c10/util/Exception.h", "torch/csrc/api/include/torch/nn/modules/container/named_any.h", "torch/csrc/utils/auto_gil.h", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["merged"]}, "e33dea6e4e": {"title": "dynamicly quantized lstm benchmarking", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30149\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18613005\n\nPulled By: z-a-f\n\nfbshipit-source-id: 966bfe2c862b1b4006b228bd9115c5c1cd3ad8cf", "pr_number": "30149", "files_changed": ["benchmarks/operator_benchmark/pt/qrnn_test.py"], "labels": ["merged"]}, "e3fecabdcb": {"title": "Setup operator registration for distributed package (#31214)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31214\n\nThis set up the basic infrastructure for distributed autograd and rpc to\nbind their operators to TorchScript, since the whole distributed package\nis builtin behind the `USE_DISTRIBUTED` flag, we separate the\nregistration and build it only when the flag is on.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19137160\n\nfbshipit-source-id: ff47dc4c380ebe273fe0eea9e5e3fccfbd6466d7", "pr_number": "31214", "files_changed": ["caffe2/CMakeLists.txt", "test/dist_autograd_test.py", "test/test_dist_autograd_spawn.py", "tools/build_variables.py", "torch/csrc/jit/register_distributed_ops.cpp", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "c5d2758c35": {"title": "Disable flaky TestMomentumSGD.test_fp16momentum_sgd (#31369)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/31368\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31369\n\nDifferential Revision: D19147072\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 6fad13be7b35f992d84a20f23877cad05ff18616", "pr_number": "31369", "files_changed": ["caffe2/python/operator_test/momentum_sgd_test.py"], "labels": ["merged"]}, "e169e02836": {"title": "Refactor custom op tests (#31282)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31282\n\nIntroduce a helper to easily call stack ops\nghstack-source-id: 95855728\n\nTest Plan: unit tests\n\nDifferential Revision: D19061515\n\nfbshipit-source-id: a7d6329e26cd3d94730d88c8a6393e10bfbd8e9b", "pr_number": "31282", "files_changed": ["test/custom_operator/test_custom_ops.cpp"], "labels": ["merged"]}, "e0ab255a51": {"title": "Updates to serialization.md (#31372)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31372\n\nKeeping it current with the latest changes.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19145986\n\nPulled By: suo\n\nfbshipit-source-id: 88122e66fa87a354ef8e87faffe58551074e3f03", "pr_number": "31372", "files_changed": ["torch/csrc/jit/docs/serialization.md"], "labels": ["jit", "merged"]}, "4ec2448580": {"title": "Update OVERVIEW.md (#31373)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31373\n\nJust some housekeeping\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19145987\n\nPulled By: suo\n\nfbshipit-source-id: ae8142dab2bddcf0b628c27c426ca26334c48238", "pr_number": "31373", "files_changed": ["torch/csrc/jit/docs/OVERVIEW.md"], "labels": ["jit", "merged"]}, "e5631119f6": {"title": "use expect instead of casting in register_c10_ops (#31401)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31401\n\nAs title, just a mechanical change\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19152965\n\nPulled By: suo\n\nfbshipit-source-id: 6bb27df7c8f542c55110286c156358ba0936269f", "pr_number": "31401", "files_changed": ["torch/csrc/jit/register_c10_ops.cpp"], "labels": ["jit", "merged"]}, "7e81d72d12": {"title": "remove unnecessary arg from create_script_module (#31017)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31017\n\nThis arg is now derivable from another one. So we don't need to pass\nboth\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18904111\n\nPulled By: suo\n\nfbshipit-source-id: ea74ea9c2ae83d9e0e6977b0eb6629f53545e2e4", "pr_number": "31017", "files_changed": ["torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "82d52bc718": {"title": "remove remnants of properties hack (#31018)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31018\n\nProperties are now disallowed so this hack is no longer necessary\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18904112\n\nPulled By: suo\n\nfbshipit-source-id: 83448da677082d59355729bb72d9f9f4c31ea756", "pr_number": "31018", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "878b0e35f7": {"title": "Simplify recursive script compilation flow. (#31019)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31019\n\nNo more `recurisve_script`, just direct calls to `create_script_module`.\nThis reduces the number of pathways through the frontend, and the\nuniformity is useful for a future PR.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18904113\n\nPulled By: suo\n\nfbshipit-source-id: 7de061dfef0cbdfc9376408fc6c1167b81803f01", "pr_number": "31019", "files_changed": ["torch/jit/__init__.py", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "3c8892aa0c": {"title": "avoid doing quadratic work in concrete type inference (#31020)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31020\n\nBefore, the recursive scripting process re-did the concrete type\ninference process for every submodule call. This changes things so that\nthe concrete type inference process only occurs once (at the top level),\nand we re-use all the inferred concrete types while recursively\ncompiling submodules.\n\nThis is both more efficient (we don't do n^2 work inferring concrete\ntypes) and less bug-prone (since we infer the concrete type only once,\nthere is no possibility of a mismatch).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18904110\n\nPulled By: suo\n\nfbshipit-source-id: 6560b85ae29fe5e9db1ee982dbf8bc222614b8d8", "pr_number": "31020", "files_changed": ["torch/csrc/jit/script/concrete_module_type.cpp", "torch/csrc/jit/script/concrete_module_type.h", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "c05538b831": {"title": "Move TorchScript language reference to its own page (#31138)", "body": "Summary:\nPreview: https://driazati.github.io/pytorch_doc_previews/jit.html#torchscript-language\n](https://our.intern.facebook.com/intern/diff/18941024/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31138\n\nPulled By: driazati\n\nDifferential Revision: D18941024\n\nfbshipit-source-id: d0ff600870a14c4a7c6ce54867d152072a12c48c", "pr_number": "31138", "files_changed": ["docs/source/jit.rst", "docs/source/jit_language_reference.rst"], "labels": ["merged"]}, "74e59c6fed": {"title": "caffe2::TypeInfo fix when using clang-cl on Windows (#31364)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31364\n\nclang-cl defines both `_MSC_VER` and `__clang__`. Names are mangled clang style though. calling `extract` with the wrong name mangling pattern will throw `std::logic_error`. This crashes on Windows when `get_fully_qualified_type_name` is called because it is marked with `noexcept`.\n\nTest Plan: Windows builds no longer crash on startup.\n\nReviewed By: mattjgalloway\n\nDifferential Revision: D19142064\n\nfbshipit-source-id: 516b9b63daeff30f5c097d192b0971c7a42db57e", "pr_number": "31364", "files_changed": ["c10/util/TypeIndex.h"], "labels": ["fb-exported", "merged"]}, "3694749cd1": {"title": "Detect dill version in torch.save/load (#30985)", "body": "Summary:\nFix for issue https://github.com/pytorch/pytorch/issues/28313\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30985\n\nDifferential Revision: D19142947\n\nPulled By: zou3519\n\nfbshipit-source-id: 10e3a182a99e80ca8c9c8328b6f8764b27d78eb3", "pr_number": "30985", "files_changed": ["test/common_utils.py", "test/test_torch.py", "torch/serialization.py"], "labels": ["merged", "open source", "triaged"]}, "3e59e80429": {"title": "Revert D18941024: Move TorchScript language reference to its own page", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18941024\n\nOriginal commit changeset: d0ff600870a1\n\nfbshipit-source-id: 01c0eac4c9741f27b91d710616e71a0d769f6f6a", "pr_number": null, "files_changed": ["docs/source/jit.rst", "docs/source/jit_language_reference.rst"], "labels": []}, "58d2dd5b73": {"title": "Enabled flip for bool tensors (#31267)", "body": "Summary:\nFix this [issue](https://github.com/pytorch/pytorch/issues/31213)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31267\n\nDifferential Revision: D19047249\n\nPulled By: izdeby\n\nfbshipit-source-id: f58ca3ac88aab28742b8d345400270f7d31c3856", "pr_number": "31267", "files_changed": ["aten/src/ATen/native/TensorTransformations.cpp", "aten/src/ATen/native/cuda/TensorTransformations.cu", "test/test_torch.py"], "labels": ["merged"]}, "386cd59d44": {"title": "Remove redundant queries of qconfig in `insertObservers` (#31292)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31292\n\natt\nAlso we need to do this check after we call `insertObservers` on invoked modules\nas well since qconfig can be None for parent module while being valid for invoked modules\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19146668\n\nfbshipit-source-id: be6811353d359ed3edd5415ced29a4999d86650b", "pr_number": "31292", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "359c39b3c2": {"title": "Use global lock instead of per instance lock. (#31404)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31404\n\nMultiple \"trainers\" could each create different instances of DistributedOptimizer, which means we can still have a race condition unless we do a trully global per worker lock.\nghstack-source-id: 95874624\n\nTest Plan: run unit tests -- unfortunatelly due to the non-deterministic behavior it's not clear how to unit test this properly.\n\nDifferential Revision: D19154248\n\nfbshipit-source-id: fab6286c17212f534f1bd1cbdf9f0de002d48c74", "pr_number": "31404", "files_changed": ["torch/distributed/optim/optimizer.py"], "labels": ["merged"]}, "913323750d": {"title": "CODEOWNERS for distributed optimizer. (#31403)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31403\n\nghstack-source-id: 95874532\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19154217\n\nfbshipit-source-id: a18ebe646b97c83cc0eb0821b10b4c76d5ce2878", "pr_number": "31403", "files_changed": ["CODEOWNERS"], "labels": ["merged"]}, "285cc13435": {"title": "check devices for all input tensors in index_put (#31280)", "body": "Summary:\nFix for https://github.com/pytorch/pytorch/issues/30960\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31280\n\nDifferential Revision: D19149114\n\nPulled By: ngimel\n\nfbshipit-source-id: af185a98ac6ea614f43bbf865de02ea113d4ed56", "pr_number": "31280", "files_changed": ["aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/TensorIterator.cpp", "test/test_indexing.py"], "labels": ["merged"]}, "c63f8e5ebe": {"title": "Fix typo in data.rst docs", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31395\n\nDifferential Revision: D19160010\n\nPulled By: zou3519\n\nfbshipit-source-id: cbc4e719e69117e8747617729d240c72e7a4e3dd", "pr_number": "31395", "files_changed": ["docs/source/data.rst"], "labels": ["merged"]}, "47766e648f": {"title": "C++ API parity: MultiheadAttention", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/27309\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D17766736\n\nPulled By: pbelevich\n\nfbshipit-source-id: 7a5f2399f081945d31d4c13d7a8d248c387fc1a6", "pr_number": "27309", "files_changed": ["test/cpp/api/modules.cpp", "torch/csrc/api/include/torch/nn/functional/activation.h", "torch/csrc/api/include/torch/nn/modules/activation.h", "torch/csrc/api/include/torch/nn/options/activation.h", "torch/csrc/api/src/nn/modules/activation.cpp", "torch/csrc/api/src/nn/options/activation.cpp"], "labels": ["merged", "module: cpp"]}, "4d22c3ba01": {"title": "fix docker login, add docker image tag list after purge as html (#31328)", "body": "Summary:\nexample of the generated html: http://ossci-docker.s3-website.us-east-1.amazonaws.com/pytorch.html\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31328\n\nDifferential Revision: D19147113\n\nPulled By: mingbowan\n\nfbshipit-source-id: 5104e92d4490f047a6474e2b12aed3293b52a9df", "pr_number": "31328", "files_changed": [".circleci/config.yml", ".circleci/ecr_gc_docker/gc.py", ".circleci/verbatim-sources/docker_jobs.yml"], "labels": ["merged"]}, "b0bd35ff13": {"title": "caffe2/event: allow multiple errors such as when cancelled (#31335)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31335\n\nWhen an error occurs in a net we end up cancelling all the async ops. If one error occurs it's highly likely other errors will occur as well.\n\nTypically we see:\n1. SendOp failed due to a network error\n2. async scheduling cancels all other ops via `SetFinished(\"Cancelled\");`\n3. Another SendOp fails due to a network error and crashes the process when the exception is thrown.\n\nThis changes caffe2 ops to allow failing twice.\n\nTest Plan: buck test //caffe2/caffe2:caffe2_test_cpu\n\nReviewed By: andrewwdye\n\nDifferential Revision: D19106548\n\nfbshipit-source-id: 4b7882258a240894cc16d061a563c83a3214d3d9", "pr_number": "31335", "files_changed": ["caffe2/core/event.cc", "caffe2/core/event_test.cc"], "labels": ["fb-exported", "merged"]}, "7cf8b9bada": {"title": "Move leaky_relu to Aten(CPU, CUDA) (#29899)", "body": "Summary:\nVitalyFedyunin, This PR is about port LeakyReLU activation to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.LeakyReLU()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    fwd_t = 0\n    bwd_t = 0\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\n\nCPU:\nOMP_NUM_THREADS=56\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.14 (ms).\ninput size(128, 10000) forward time is 4.21 (ms); backwad avg time is 8.02 (ms).\nOMP_NUM_THREADS=1\ninput size(128, 100) forward time is 0.02 (ms); backwad avg time is 0.07 (ms).\ninput size(128, 10000) forward time is 1.98 (ms); backwad avg time is 6.21 (ms)\n```\nAfter:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\n\nCPU:\nOMP_NUM_THREADS=56\ninput size(128, 100) forward time is 0.02 (ms); backwad avg time is 0.04 (ms).\ninput size(128, 10000) forward time is 0.03 (ms); backwad avg time is 0.09 (ms).\nOMP_NUM_THREADS=1\ninput size(128, 100) forward time is 0.01 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10000) forward time is 0.47 (ms); backwad avg time is 1.02 (ms).\n```\nHow to set the numbers of thread? using following script:\n```\nnum_threads=$1\nscript=$2\nlast_core=`expr $num_threads - 1`\necho \"using $num_threads OMP threads\"\necho \"bind cores to 0~$last_core\"\nexport OMP_NUM_THREADS=$num_threads\nexport KMP_AFFINITY=granularity=fine,compact,1,0\nnumactl --physcpubind=0-$last_core --membind=0 python $script\n```\nand run .**/run.sh num_threads test.py**.\n\nFixes https://github.com/pytorch/pytorch/issues/24583 #24584 https://github.com/pytorch/pytorch/issues/24720 #24721\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29899\n\nDifferential Revision: D18816231\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: afb1e43a99317d17f50cff1b593cd8f7a0a83da2", "pr_number": "29899", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/LeakyReLU.cu", "aten/src/THCUNN/generic/LeakyReLU.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/LeakyReLU.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "5e8bac24b4": {"title": "Migrate soft_margin_loss from the TH to Aten (CUDA+CPU) (#28135)", "body": "Summary:\nFix: https://github.com/pytorch/pytorch/issues/24631, https://github.com/pytorch/pytorch/issues/24632, https://github.com/pytorch/pytorch/issues/24764, https://github.com/pytorch/pytorch/issues/24765\n\nPort of TH SoftMarginCriterion to ATen using un-fused tensor operators but with custom backward code. This is a follow-up/fixc of reverted PR https://github.com/pytorch/pytorch/issues/27673.\n\nBenchmark results:\n\nCPU became faster, GPU slower. To reach previous TH perf probably manual fusion is necessary.\n\n### WITH patch\n```\nCPU warmup 1000 took 7.997200009413064e-05\nCPU warmup 10000 took 0.0008116499957395718\nCPU warmup 100000 took 0.0012691459996858612\nCPU warmup TOTAL time 0.0021982479956932366\nCPU forward 1000 took 7.320100849028677e-05\nCPU forward 10000 took 0.00015837099635973573\nCPU forward 100000 took 0.0010471990099176764\nCPU forward 1000000 took 0.01238470000680536\nCPU forward 10000000 took 0.12747182900784537\nCPU forward 100000000 took 1.2076255190040683\nCPU forward TOTAL time 1.3488940890092636\nCPU for- & backward 1000 took 0.00032587299938313663\nCPU for- & backward 10000 took 0.0006926299975020811\nCPU for- & backward 100000 took 0.002146183993318118\nCPU for- & backward 1000000 took 0.019158899012836628\nCPU for- & backward 10000000 took 0.2957490350090666\nCPU for- & backward 100000000 took 1.7630806300003314\nCPU for- & backward TOTAL time 2.081367089995183\n\nGPU warmup 1000 took 0.0004558280052151531\nGPU warmup 10000 took 0.0002567449992056936\nGPU warmup 100000 took 0.0001593509950907901\nGPU warmup TOTAL time 0.0009442300070077181\nGPU forward 1000 took 0.00015061900194268674\nGPU forward 10000 took 0.00015258099301718175\nGPU forward 100000 took 0.00015409699699375778\nGPU forward 1000000 took 0.0008183339959941804\nGPU forward 10000000 took 0.004424853003001772\nGPU forward 100000000 took 0.04356115800328553\nGPU forward TOTAL time 0.04938192600093316\nGPU for- & backward 1000 took 0.0008062430133577436\nGPU for- & backward 10000 took 0.0006074949924368411\nGPU for- & backward 100000 took 0.0007091690058587119\nGPU for- & backward 1000000 took 0.001022183001623489\nGPU for- & backward 10000000 took 0.009945805999450386\nGPU for- & backward 100000000 took 0.0944173600000795\nGPU for- & backward TOTAL time 0.28060428200114984\n```\n\n### WITHOUT patch\n```\nCPU warmup 1000 took 6.394000956788659e-05\nCPU warmup 10000 took 0.00038220599526539445\nCPU warmup 100000 took 0.0034939230099553242\nCPU warmup TOTAL time 0.003981974994530901\nCPU forward 1000 took 4.7855006414465606e-05\nCPU forward 10000 took 0.000347569992300123\nCPU forward 100000 took 0.003367935001733713\nCPU forward 1000000 took 0.03605044000141788\nCPU forward 10000000 took 0.35935167300340254\nCPU forward 100000000 took 3.630371332008508\nCPU forward TOTAL time 4.029640004009707\nCPU for- & backward 1000 took 0.00028494100843090564\nCPU for- & backward 10000 took 0.0006738200027029961\nCPU for- & backward 100000 took 0.0051178760040784255\nCPU for- & backward 1000000 took 0.04925115800870117\nCPU for- & backward 10000000 took 0.7172313440096332\nCPU for- & backward 100000000 took 5.441953932997421\nCPU for- & backward TOTAL time 6.21466830400459\n\nGPU warmup 1000 took 0.001803738996386528\nGPU warmup 10000 took 0.00041877900366671383\nGPU warmup 100000 took 0.0003870719956466928\nGPU warmup TOTAL time 0.0026561370032140985\nGPU forward 1000 took 0.00037833399255760014\nGPU forward 10000 took 0.00038825398951303214\nGPU forward 100000 took 0.0003841099969577044\nGPU forward 1000000 took 0.0007090550061548129\nGPU forward 10000000 took 0.0016171559982467443\nGPU forward 100000000 took 0.013463679002597928\nGPU forward TOTAL time 0.017010531009873375\nGPU for- & backward 1000 took 0.0007374050037469715\nGPU for- & backward 10000 took 0.0006343529967125505\nGPU for- & backward 100000 took 0.0006375070079229772\nGPU for- & backward 1000000 took 0.0007550300069851801\nGPU for- & backward 10000000 took 0.002672752001672052\nGPU for- & backward 100000000 took 0.023170708998804912\nGPU for- & backward TOTAL time 0.20251446698966902\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28135\n\nDifferential Revision: D18001447\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: ad90dc1cca42dcaf3ea9e17e4f8fd79cee0a293e", "pr_number": "28135", "files_changed": ["aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/SoftMarginCriterion.cu", "aten/src/THCUNN/generic/SoftMarginCriterion.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/SoftMarginCriterion.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "fb30a48b4e": {"title": "add unsupported section (#31329)", "body": "Summary:\nAdd a section for unsupported ops, and modules. Automatically generate the properties and attributes that aren't bound, and for ops that have semantic mismatches set up tests so the docs stay up to date.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31329\n\nDifferential Revision: D19164472\n\nPulled By: eellison\n\nfbshipit-source-id: 46290bb8a64d9de928cfb1eda5ff4558c3799c88", "pr_number": "31329", "files_changed": ["docs/source/jit.rst", "docs/source/jit_unsupported.rst", "test/jit/unsupported_ops.py", "test/test_jit.py", "torch/jit/unsupported_tensor_ops.py"], "labels": ["jit", "merged"]}, "1f50cfc24d": {"title": "Throw a better error for int too big for int64_t", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29931\n\nPulled By: driazati\n\nDifferential Revision: D19124934\n\nfbshipit-source-id: 91841d7ba4f2f6142c51fba07b7faa14bb817e3a", "pr_number": "29931", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/tree_views.h"], "labels": ["jit", "merged"]}, "7692494c67": {"title": "Fix hex literal parsing (#29935)", "body": "Summary:\nStacked PRs\n * #29940 - [jit] Fix parsing of big float literals\n * **#29935 - [jit] Fix hex literal parsing**\n * #29931 - [jit] Throw a better error for int too big for int64_t\n\nPreviously these were all parsed as `0`\n](https://our.intern.facebook.com/intern/diff/19124944/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29935\n\nPulled By: driazati\n\nDifferential Revision: D19124944\n\nfbshipit-source-id: 1ee0c1dee589933363a5efba069a2cfaf94373c5", "pr_number": "29935", "files_changed": ["c10/util/string_utils.h", "test/test_jit.py", "torch/csrc/jit/script/tree_views.h"], "labels": ["jit", "merged"]}, "d08250c223": {"title": "fix zero-batch handling in convtranspose (#24341)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/24341\n\nConvTransposeOp doesn't crash for zero-batch, but it doesn't modify the output blob. This leads to buggy behaviour especially when running the same network twice using different input, or backprop during training.\n\nSeems `ConvTransposeUnpoolBase<Context>::GetOutputSize` works for zero-batch, so I remove the check for `input.numel() > 0`, and reshape the output blob before returning.\n\nFor CudnnConvTransposeGradientOp, it's a bit verbose to set `dfilter` and `dbias`, it's a  seems the Cudnn can handle it, so simply remove the `X.numel() == 0` branch.\n\nTest Plan: buck test mode/dev-nosan caffe2/caffe2/python/operator_test:conv_transpose_test -- --run-disabled\n\nReviewed By: BIT-silence\n\nDifferential Revision: D16807606\n\nfbshipit-source-id: 0d72c5bd8f2e03c34465e7b530cca548d9bdd5e1", "pr_number": "24341", "files_changed": ["caffe2/operators/conv_transpose_op_cudnn.cc", "caffe2/operators/conv_transpose_op_impl.h", "caffe2/operators/conv_transpose_op_mobile_impl.h", "caffe2/operators/conv_transpose_unpool_op_base.h", "caffe2/python/operator_test/conv_transpose_test.py"], "labels": ["caffe2", "merged", "module: pybind"]}, "ae2487bf4d": {"title": "Move TorchScript language reference to its own page (#31138)", "body": "Summary:\nStacked PRs\n * #31146 - [jit] Cleanup after moving language reference\n * **#31138 - [jit] Move TorchScript language reference to its own page**\n\nPreview: https://driazati.github.io/pytorch_doc_previews/jit.html#torchscript-language\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31138\n\nPulled By: driazati\n\nDifferential Revision: D19167375\n\nfbshipit-source-id: d37110d85fc8b8d2c741be49846e873de1357c2a", "pr_number": "31138", "files_changed": ["docs/source/jit.rst", "docs/source/jit_language_reference.rst"], "labels": ["merged"]}, "503a4e9019": {"title": "Cleanup after moving language reference (#31146)", "body": "Summary:\nStacked PRs\n * **#31146 - [jit] Cleanup after moving language reference**\n * #31138 - [jit] Move TorchScript language reference to its own page\n\nPreview: https://driazati.github.io/pytorch_doc_previews/jit.html#torchscript-language\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31146\n\nPulled By: driazati\n\nDifferential Revision: D19167390\n\nfbshipit-source-id: f28daed36754a553264fc8ac142ed22c3e26d63e", "pr_number": "31146", "files_changed": ["docs/source/jit.rst", "docs/source/jit_language_reference.rst", "torch/_jit_internal.py", "torch/jit/__init__.py", "torch/jit/supported_ops.py"], "labels": ["jit", "merged"]}, "148bcd3ee5": {"title": "Add support for builtins as attributes (#31269)", "body": "Summary:\nFixes #27495\n\nThis adds builtins as another piece of a concrete type. They're separate from normal functions since they represent the `BuiltinFunction` sugared value (which is a direct call to a builtin op). It also moves the builtins related logic from `jit/__init__.py` to `jit/_builtins.py` so it can be used from `jit/_recursive.py` to look up functions in the builtins table.\n](https://our.intern.facebook.com/intern/diff/19149779/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31269\n\nPulled By: driazati\n\nDifferential Revision: D19149779\n\nfbshipit-source-id: d4e5e5d7d7d528b75a2f503e6004394251a4e82d", "pr_number": "31269", "files_changed": ["test/jit/test_type_sharing.py", "test/test_jit.py", "torch/csrc/jit/script/concrete_module_type.cpp", "torch/csrc/jit/script/concrete_module_type.h", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/jit/__init__.py", "torch/jit/_builtins.py", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "1e80ff7a67": {"title": "autograd/profiler: make record_function more threadsafe (#31346)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31346\n\nThis makes it so that if profiling is enabled/disabled from a different thread while a RecordFunction span is active via an op it doesn't crash the process.\n\nWe currently see when using torch.distributed.rpc to enable/disable profiling on other nodes while other things are running.\n\nTest Plan: buck test //caffe2/test:autograd -- test_record_function\n\nReviewed By: albanD\n\nDifferential Revision: D19133258\n\nfbshipit-source-id: 30712b06c6aa051789948de2918dcfb9b78967ba", "pr_number": "31346", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/record_function.h", "torch/csrc/autograd/record_function_ops.cpp"], "labels": ["fb-exported", "merged"]}, "a3cdb7eca3": {"title": "Fix default instantation of dynamic quantized LSTM", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31433\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19164539\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 7045817ab3dfb530c4480a10523c4c6bcdbfc7eb", "pr_number": "31433", "files_changed": ["test/test_jit.py", "test/test_quantization.py", "torch/nn/quantized/dynamic/modules/rnn.py"], "labels": ["merged"]}, "d2e66b44cc": {"title": "Temporary fix to support building pytorch from fbsource (for xplat dependencies) (#31393)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31393\n\npytorch build was set up with the include paths (-I) relative to fbcode/. This works well for fbcode builds, but doesn't work for the new fbcode_deps args for xplat build targets that work across xplat and fbcode. When these targets are built, the include paths need to be relative to fbsource, so fbcode/ suffix needs to be added to those paths.\n\nLonger term, to properly fix this, we need to use raw_headers with public_include_directories specified for all of these targets.\n\nTest Plan: buck test mode/dev //papaya/integration/service/local/test:mnist_federated_system_test -- 'MnistFederatedSystemTest\\.test' --run-disabled\n\nReviewed By: mzlee\n\nDifferential Revision: D19148465\n\nfbshipit-source-id: a610e84bf4cad5838e54e94bae71b957c4b6d4b5", "pr_number": "31393", "files_changed": ["tools/build_variables.py"], "labels": ["fb-exported", "merged"]}, "e7d25a3e4d": {"title": "add a suggested alternative to _get_trace_graph", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31441\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19165646\n\nPulled By: suo\n\nfbshipit-source-id: 96a264bc55ceafd798d92b986d319cddbb0d9c69", "pr_number": "31441", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "e1509cb468": {"title": "Add support for `del` (#31273)", "body": "Summary:\nAdds the `del` keyword to the parser and corresponding `aten::Delete` op for lists and dicts\n\nFixes #20615\n](https://our.intern.facebook.com/intern/diff/19054937/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31273\n\nPulled By: driazati\n\nDifferential Revision: D19054937\n\nfbshipit-source-id: c535ea16a9e62d176f8ad45947670fc3535af77c", "pr_number": "31273", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_list_dict.py", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/script/compiler.cpp", "torch/csrc/jit/script/lexer.h", "torch/csrc/jit/script/parser.cpp", "torch/csrc/jit/script/python_tree_views.cpp", "torch/csrc/jit/script/tree_views.h", "torch/jit/frontend.py"], "labels": ["jit", "merged"]}, "fe707c7849": {"title": "Use `default_observer` and `default_weight_observer` in tests (#31424)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31424\n\natt\n\nTest Plan:\ntest_jit.py\n\nImported from OSS\n\nDifferential Revision: D19162368\n\nfbshipit-source-id: 33b95ba643eeeae942283bbc33f7ceda8d14c431", "pr_number": "31424", "files_changed": ["test/test_jit.py"], "labels": ["merged"]}, "1bb800cf5c": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/f5d37bdcfd1292b28eb1834ede9ad3ccfd368efd\nhttps://github.com/facebook/fbzmq/commit/21ba9e3692fbbc1e772646f991a210a7d484b77c\nhttps://github.com/facebook/folly/commit/576eeaee273b5e0d2d1ed2899c0b2f87f1516983\nhttps://github.com/facebook/litho/commit/7ba1f57d53ec0433d19e7e0c3fd4318850b949a1\nhttps://github.com/facebook/proxygen/commit/e520f8f5b37c3b38eab7d1107ec88da015781e1f\nhttps://github.com/facebook/rocksdb/commit/54f9092b0c12d99971f340e180d42fffc9f73bc1\nhttps://github.com/facebook/wangle/commit/88bb770ce1d22518ad2025f1e3a79a0726a7ea7e\nhttps://github.com/facebookincubator/fizz/commit/d91888de6c8a3f4ff94c44087bf9035df7245d9e\nhttps://github.com/facebookincubator/katran/commit/ff06eb088122380f1918a34815aa28f5a5ffdb20\nhttps://github.com/facebookincubator/mvfst/commit/fdaeb6ea30d22fa8158e610fdfc6d0d35aa23231\nhttps://github.com/facebookincubator/profilo/commit/1fd432f00f04a015486b87f7f2104fd78606a30d\nhttps://github.com/pytorch/fbgemm/commit/60b7cb3408fdbdcdd0cba1cf58044586ceec7ba1\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: f63bd0a879f4d08e159f530f595067f5a09ffe70", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "dff7b945bf": {"title": "Avoid sending large unneeded data over wire in process_group_agent. (#31357)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31357\n\nIf a user selects a subset of a Tensor and sends it in an RPC, we were sending\nthe whole original Tensor Storage over the network.\n\nWhile this sounds reasonable, in practice, we observed view-like Tensors being sent\nover rpc, where only 1% of the data in the provided Tensor's Storage was\nactually used/needed.\n\nThe simple solution here is to just force a clone in the serializer code if we see that\nless than (arbitrary) half the bits are used, and the tensor is more than a nominal few KB.\nAdd related tests to ensure this doesn't break.\n\nAn alternate approach would be to modify the Pickler. That said, since Pickler is shared by more\ncomponents, the logic might be harder to tailor appropriately at that layer (particularly\ngiven that the Pickler has explicit logic to share a single Storage* among several Tensors\nthat commonly point to the same Storage*).\n\nIt's possible that we might want to further refine the basic thresholds in this change.\nIn practice, we've seen a mostly bimodal distribution thus far for the percent of Tensor\nStorage referred by a Tensor in observed rpcs (i.e. either 90%+ or sub-10% of the Storage\nreferenced), hence the existing 50% threshold here is probably not an unreasonable\nstarting point.\nghstack-source-id: 95925474\n\nTest Plan: buck test mode/dev caffe2/test/cpp/rpc/...\n\nDifferential Revision: D19137056\n\nfbshipit-source-id: e2b3a4dd0cc6e1de820fd0740aa1d59883dbf8d4", "pr_number": "31357", "files_changed": ["test/cpp/rpc/test_wire_serialization.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h"], "labels": ["merged"]}, "1bb6c51421": {"title": "Fix getAttribute (#31011)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31011\n\n`getAttribute` is supposed to throw when there the attribute is not\nfound rather than return a `nullptr`.\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18898417\n\nfbshipit-source-id: 0fe7d824b978ad19bb5ef094d3aa560e9fc57f87", "pr_number": "31011", "files_changed": ["aten/src/ATen/core/jit_type.h", "test/cpp/api/serialize.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/script/object.h", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/csrc/jit/script/sugared_value.cpp"], "labels": ["merged"]}, "fb24f7c4ad": {"title": "catch all exceptions in converting default values to ivalues (#31398)", "body": "Summary:\nPreviously we would only catch `py::cast_error` which led to incomprehensible error messages like: `TypeError: 'NoneType' object is not iterable`. We are running arbitrary pybind code here, and not doing anything with the error message, so we should be less restrictive with the types of errors we catch.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31398\n\nDifferential Revision: D19166655\n\nPulled By: eellison\n\nfbshipit-source-id: 84db8b3714c718b475913f2f4bb6f19e62f2d9ec", "pr_number": "31398", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/init.cpp"], "labels": ["jit", "merged"]}, "489dd6cb90": {"title": "Add TORCH_DCHECK macro that checks only in debug builds (#31240)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31240\n\nFollow up on discoveries/discussions in https://github.com/pytorch/pytorch/pull/30810\n\nMimic the `DCHECK` macro from https://github.com/pytorch/pytorch/blob/e5eb871/c10/util/logging_is_not_google_glog.h#L117-L125\n\nWith this change the perf gap is eliminated:\n\n```\n================================================================================\nProgram Output:\n================================================================================\nRun on (36 X 1601 MHz CPU s)\n2019-12-12 20:12:13\n-----------------------------------------------------------------\nBenchmark                          Time           CPU Iterations\n-----------------------------------------------------------------\nBM_IntrusivePtrCtorDtor           23 ns         23 ns   30914703\nBM_SharedPtrCtorDtor              27 ns         27 ns   25895944\nBM_IntrusivePtrArray/16          503 ns        503 ns    1392139\nBM_IntrusivePtrArray/32         1006 ns       1006 ns     695749\nBM_IntrusivePtrArray/64         2013 ns       2013 ns     347714\nBM_IntrusivePtrArray/128        4024 ns       4024 ns     173964\nBM_IntrusivePtrArray/256        8047 ns       8047 ns      86994\nBM_IntrusivePtrArray/512       16106 ns      16106 ns      43461\nBM_IntrusivePtrArray/1024      32208 ns      32207 ns      21731\nBM_IntrusivePtrArray/2048      64431 ns      64430 ns      10865\nBM_IntrusivePtrArray/4096     128940 ns     128938 ns       5429\nBM_SharedPtrArray/16             503 ns        503 ns    1392128\nBM_SharedPtrArray/32            1006 ns       1006 ns     695940\nBM_SharedPtrArray/64            2012 ns       2012 ns     347817\nBM_SharedPtrArray/128           4024 ns       4023 ns     173927\nBM_SharedPtrArray/256           8069 ns       8069 ns      86741\nBM_SharedPtrArray/512          16143 ns      16142 ns      43357\nBM_SharedPtrArray/1024         32283 ns      32283 ns      21685\nBM_SharedPtrArray/2048         64718 ns      64717 ns      10817\nBM_SharedPtrArray/4096        129469 ns     129466 ns       5407\n================================================================================\n```\n```\n================================================================================\nProgram Output:\n================================================================================\nRun on (80 X 2001 MHz CPU s)\n2019-12-12 20:12:23\n-----------------------------------------------------------------\nBenchmark                          Time           CPU Iterations\n-----------------------------------------------------------------\nBM_IntrusivePtrCtorDtor           18 ns         18 ns   38630411\nBM_SharedPtrCtorDtor              22 ns         22 ns   32356114\nBM_IntrusivePtrArray/16          402 ns        402 ns    1739637\nBM_IntrusivePtrArray/32          805 ns        805 ns     869818\nBM_IntrusivePtrArray/64         1610 ns       1609 ns     434881\nBM_IntrusivePtrArray/128        3218 ns       3218 ns     217437\nBM_IntrusivePtrArray/256        6436 ns       6436 ns     108739\nBM_IntrusivePtrArray/512       12882 ns      12882 ns      54356\nBM_IntrusivePtrArray/1024      25763 ns      25763 ns      27177\nBM_IntrusivePtrArray/2048      51532 ns      51531 ns      13590\nBM_IntrusivePtrArray/4096     103091 ns     103091 ns       6778\nBM_SharedPtrArray/16             402 ns        402 ns    1740165\nBM_SharedPtrArray/32             804 ns        804 ns     869035\nBM_SharedPtrArray/64            1610 ns       1610 ns     434975\nBM_SharedPtrArray/128           3218 ns       3218 ns     217505\nBM_SharedPtrArray/256           6457 ns       6457 ns     108510\nBM_SharedPtrArray/512          12909 ns      12909 ns      54249\nBM_SharedPtrArray/1024         25810 ns      25810 ns      27127\nBM_SharedPtrArray/2048         51763 ns      51763 ns      13531\nBM_SharedPtrArray/4096        103506 ns     103505 ns       6759\n================================================================================\n```\n\nTest Plan:\nbuck test caffe2/c10/...\nbuck test mode/opt caffe2/c10/...\n\nDifferential Revision: D18998243\n\nfbshipit-source-id: ddf0a118a80efe032b52d403867c1f416c721590", "pr_number": "31240", "files_changed": ["c10/test/util/exception_test.cpp", "c10/test/util/intrusive_ptr_test.cpp", "c10/util/Exception.h", "c10/util/intrusive_ptr.h"], "labels": ["fb-exported", "merged"]}, "1e116a5089": {"title": "Revert D19054937: Add support for `del`", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19054937\n\nOriginal commit changeset: c535ea16a9e6\n\nfbshipit-source-id: e57d31811441947b7ee38c8c2b16eecde5005792", "pr_number": null, "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_list_dict.py", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/script/compiler.cpp", "torch/csrc/jit/script/lexer.h", "torch/csrc/jit/script/parser.cpp", "torch/csrc/jit/script/python_tree_views.cpp", "torch/csrc/jit/script/tree_views.h", "torch/jit/frontend.py"], "labels": []}, "fc3103b116": {"title": "fixing a naming issue in creating a residual loop node in a bailout graph (#31400)", "body": "Summary:\nThis addresses the issue of differentiating between `%4` in\n`%12 : int, %y.1 : Tensor = prim::Loop(%9, %6, %4, %3)` and `%y.5 : Double(3) = aten::cat(%22, %4) # test_jit.py:3772:24` in `%4` loop's body in a residual continuation loop, because these should be different values.\n\n```\n[DUMP profiling_graph_executor_impl.cpp:124] with prim::BailoutTemplate_0 = graph(%z.1 : int,\n[DUMP profiling_graph_executor_impl.cpp:124]       %size.1 : int):\n[DUMP profiling_graph_executor_impl.cpp:124]   %2 : Tensor = prim::Constant[value= 1  1 [ CPUDoubleType{2} ]]()\n[DUMP profiling_graph_executor_impl.cpp:124]   %3 : Double(2) = prim::BailOut[index=0](%2, %z.1, %size.1)\n[DUMP profiling_graph_executor_impl.cpp:124]   %4 : int = prim::Constant[value=0]() # test_jit.py:3772:54\n[DUMP profiling_graph_executor_impl.cpp:124]   %5 : None = prim::Constant()\n[DUMP profiling_graph_executor_impl.cpp:124]   %6 : bool = prim::Constant[value=1]() # test_jit.py:3770:16\n[DUMP profiling_graph_executor_impl.cpp:124]   %counters.1 : int[] = prim::ListConstruct()\n[DUMP profiling_graph_executor_impl.cpp:124]   %8 : int = prim::Constant[value=8]()\n[DUMP profiling_graph_executor_impl.cpp:124]   %9 : int = aten::__round_to_zero_floordiv(%size.1, %8)\n[DUMP profiling_graph_executor_impl.cpp:124]   %10 : int = aten::mul(%9, %8)\n[DUMP profiling_graph_executor_impl.cpp:124]   %11 : int = aten::sub(%size.1, %10)\n[DUMP profiling_graph_executor_impl.cpp:124]   %12 : int, %y.1 : Tensor = prim::Loop(%9, %6, %4, %3) # test_jit.py:3770:16\n[DUMP profiling_graph_executor_impl.cpp:124]     block0(%i.2 : int, %15 : int, %y.7 : Tensor):\n[DUMP profiling_graph_executor_impl.cpp:124]       %17 : Double(2) = prim::BailOut[index=1](%y.7, %z.1, %counters.1, %9, %11, %i.2, %15)\n[DUMP profiling_graph_executor_impl.cpp:124]       %18 : int[] = aten::append(%counters.1, %15) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %19 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %20 : Tensor = aten::ones(%19, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %21 : Double(1) = prim::BailOut[index=2](%20, %z.1, %counters.1, %9, %11, %i.2, %15, %17)\n[DUMP profiling_graph_executor_impl.cpp:124]       %22 : Tensor[] = prim::ListConstruct(%17, %21)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.5 : Double(3) = aten::cat(%22, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %24 : int = prim::Constant[value=1]()\n[DUMP profiling_graph_executor_impl.cpp:124]       %25 : int = aten::add(%15, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %26 : int[] = aten::append(%counters.1, %25) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %27 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %28 : Tensor = aten::ones(%27, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %29 : Double(1) = prim::BailOut[index=3](%28, %z.1, %counters.1, %9, %11, %i.2, %y.5, %25)\n[DUMP profiling_graph_executor_impl.cpp:124]       %30 : Tensor[] = prim::ListConstruct(%y.5, %29)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.9 : Double(4) = aten::cat(%30, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %32 : int = aten::add(%25, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %33 : int[] = aten::append(%counters.1, %32) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %34 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %35 : Tensor = aten::ones(%34, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %36 : Double(1) = prim::BailOut[index=4](%35, %z.1, %counters.1, %9, %11, %i.2, %y.9, %32)\n[DUMP profiling_graph_executor_impl.cpp:124]       %37 : Tensor[] = prim::ListConstruct(%y.9, %36)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.10 : Double(5) = aten::cat(%37, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %39 : int = aten::add(%32, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %40 : int[] = aten::append(%counters.1, %39) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %41 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %42 : Tensor = aten::ones(%41, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %43 : Double(1) = prim::BailOut[index=5](%42, %z.1, %counters.1, %9, %11, %i.2, %y.10, %39)\n[DUMP profiling_graph_executor_impl.cpp:124]       %44 : Tensor[] = prim::ListConstruct(%y.10, %43)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.11 : Double(6) = aten::cat(%44, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %46 : int = aten::add(%39, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %47 : int[] = aten::append(%counters.1, %46) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %48 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %49 : Tensor = aten::ones(%48, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %50 : Double(1) = prim::BailOut[index=6](%49, %z.1, %counters.1, %9, %11, %i.2, %y.11, %46)\n[DUMP profiling_graph_executor_impl.cpp:124]       %51 : Tensor[] = prim::ListConstruct(%y.11, %50)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.12 : Double(7) = aten::cat(%51, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %53 : int = aten::add(%46, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %54 : int[] = aten::append(%counters.1, %53) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %55 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %56 : Tensor = aten::ones(%55, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %57 : Double(1) = prim::BailOut[index=7](%56, %z.1, %counters.1, %9, %11, %i.2, %y.12, %53)\n[DUMP profiling_graph_executor_impl.cpp:124]       %58 : Tensor[] = prim::ListConstruct(%y.12, %57)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.13 : Double(8) = aten::cat(%58, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %60 : int = aten::add(%53, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %61 : int[] = aten::append(%counters.1, %60) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %62 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %63 : Tensor = aten::ones(%62, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %64 : Double(1) = prim::BailOut[index=8](%63, %z.1, %counters.1, %9, %11, %i.2, %y.13, %60)\n[DUMP profiling_graph_executor_impl.cpp:124]       %65 : Tensor[] = prim::ListConstruct(%y.13, %64)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.14 : Double(9) = aten::cat(%65, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %67 : int = aten::add(%60, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %68 : int[] = aten::append(%counters.1, %67) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %69 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %70 : Tensor = aten::ones(%69, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %71 : Double(1) = prim::BailOut[index=9](%70, %z.1, %counters.1, %9, %11, %i.2, %y.14, %67)\n[DUMP profiling_graph_executor_impl.cpp:124]       %72 : Tensor[] = prim::ListConstruct(%y.14, %71)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.15 : Tensor = aten::cat(%72, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %74 : int = aten::add(%67, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       -> (%6, %74, %y.15)\n[DUMP profiling_graph_executor_impl.cpp:124]   %75 : Double(10) = prim::BailOut[index=10](%y.1, %z.1, %counters.1, %11, %12)\n[DUMP profiling_graph_executor_impl.cpp:124]   %76 : int, %y : Tensor = prim::Loop(%11, %6, %12, %75) # test_jit.py:3770:16\n[DUMP profiling_graph_executor_impl.cpp:124]     block0(%i.1 : int, %79 : int, %y.6 : Tensor):\n[DUMP profiling_graph_executor_impl.cpp:124]       %81 : Double(*) = prim::BailOut[index=11](%y.6, %z.1, %counters.1, %11, %i.1, %79)\n[DUMP profiling_graph_executor_impl.cpp:124]       %82 : int[] = aten::append(%counters.1, %79) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %83 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %84 : Tensor = aten::ones(%83, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %85 : Double(1) = prim::BailOut[index=12](%84, %counters.1, %11, %i.1, %79, %81)\n[DUMP profiling_graph_executor_impl.cpp:124]       %86 : Tensor[] = prim::ListConstruct(%81, %85)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.4 : Tensor = aten::cat(%86, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %88 : int = prim::Constant[value=1]()\n[DUMP profiling_graph_executor_impl.cpp:124]       %89 : int = aten::add(%79, %88)\n[DUMP profiling_graph_executor_impl.cpp:124]       -> (%6, %89, %y.4)\n[DUMP profiling_graph_executor_impl.cpp:124]   %90 : Double(12) = prim::BailOut[index=13](%y, %counters.1)\n[DUMP profiling_graph_executor_impl.cpp:124]   %91 : (Tensor, int[]) = prim::TupleConstruct(%90, %counters.1)\n[DUMP profiling_graph_executor_impl.cpp:124]   return (%91)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31400\n\nDifferential Revision: D19172750\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 85d3aac4e80b65b83b6be3c0bca8075a731a2b7e", "pr_number": "31400", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/bailout_graph.cpp"], "labels": ["jit", "merged"]}, "540b9da41e": {"title": "Bump numba version in circleCI config to 0.46.0. (#31435)", "body": "Summary:\nThe current numba version doesn't appear to actually work with our numba-cuda tests (numba.cuda.is_available()) fails.\n\nPrevious attempts to upgrade were blocked by https://github.com/numba/numba/issues/4368.\n\nIt's a bit unclear to me, but I believe 0.46.0 fixes the above version.  I'm verify that we catch that issue in CI via https://github.com/pytorch/pytorch/pull/31434.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31435\n\nDifferential Revision: D19166865\n\nPulled By: gchanan\n\nfbshipit-source-id: e01fa48c577e35de178423db7a7f79ac3dd3894d", "pr_number": "31435", "files_changed": [".circleci/docker/common/install_conda.sh"], "labels": ["merged"]}, "28376e826d": {"title": "Fix lint", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31463\n\nPulled By: driazati\n\nDifferential Revision: D19173580\n\nfbshipit-source-id: 6e5bb24949ec357c4d5b29a16d1733b664f21e05", "pr_number": "31463", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "6d6a91fb0f": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/proxygen/commit/58a1ec274caa1aa12fec9c0c31b6b3d7fdc39ab8\nhttps://github.com/facebook/litho/commit/24da1c8b6697e1de80534c714ed808584e9875b7\nhttps://github.com/facebook/rocksdb/commit/77d5ba78879ab90d93a5ff2373c0be3ff8153d5d\nhttps://github.com/pytorch/fbgemm/commit/c7b80d7ab5cbc60bafb0c75b31f72e55b55402e5\n\nTest Plan: n/a\n\nReviewed By: tgreenidge\n\nfbshipit-source-id: be872df9014b795b279b93bd81efbaa41f2d0fd7", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "c4121ed8db": {"title": "Fix is_fundamental template for MSVC (#30959)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30932\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30959\n\nDifferential Revision: D18891797\n\nPulled By: mingbowan\n\nfbshipit-source-id: e6c36ee80065e66117873e768f86f507c48aaef1", "pr_number": "30959", "files_changed": ["c10/util/TypeTraits.h", "c10/util/typeid.h", "caffe2/core/context.h", "caffe2/core/context_base.h", "caffe2/ideep/utils/ideep_context.h"], "labels": ["merged"]}, "d6acc87c93": {"title": "Guard against copying from quantized Tensor to non-quantized Tensor (#29660)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29660\n\natt\n\nTest Plan:\npython test/test_quantized_tensor.py\n\nImported from OSS\n\nDifferential Revision: D18799897\n\nfbshipit-source-id: 5d1b4ef84f5ae8eba830784b74485d78fa1e6fcf", "pr_number": "29660", "files_changed": ["aten/src/ATen/native/Copy.cpp", "test/test_quantized_tensor.py"], "labels": ["merged"]}, "49fe7a7401": {"title": "Updated documentation for NLLLoss to explain what x, y and w refer to (#31488)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/31385\n\nIn the current documentation for NLLLoss, it's unclear what `y` refers to in the math section of the loss description. There was an issue(https://github.com/pytorch/pytorch/issues/31295) filed earlier where there was a confusion if the loss returned for reduction=mean is right or not, perhaps because of lack in clarity of formula symbol description in the current documentation.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31488\n\nDifferential Revision: D19181391\n\nPulled By: anjali411\n\nfbshipit-source-id: 8b75f97aef93c92c26ecbce55b3faf2cd01d3e74", "pr_number": "31488", "files_changed": ["torch/nn/modules/loss.py"], "labels": ["merged"]}, "779b128872": {"title": "add back in reference to jit_unsupported section (#31486)", "body": "Summary:\nIt was added in https://github.com/pytorch/pytorch/pull/31329 and removed in a bad merge in https://github.com/pytorch/pytorch/pull/31138/\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31486\n\nDifferential Revision: D19181967\n\nPulled By: eellison\n\nfbshipit-source-id: 7e4b4a9b2042c30ec18f7f737bc4a9a56fac7d92", "pr_number": "31486", "files_changed": ["docs/source/jit.rst"], "labels": ["merged"]}, "9d9bc93bfb": {"title": "Added error message to indicate that reduction operations are not supported for dim>=64 (#31476)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/23159\nCurrently we don't support reduction operations for dim>=64 and we should give a descriptive RuntimeError indicating the same\nDiff: D19179039\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31476\n\nDifferential Revision: D19179039\n\nPulled By: anjali411\n\nfbshipit-source-id: 58568f64627bf3df6b3e00a1498544c030e74a0e", "pr_number": "31476", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "test/test_torch.py"], "labels": ["merged"]}, "8f3c0d541e": {"title": "Speed up `Tensor::has_names` for unnamed tensors (#31436)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31436\n\nTensor::has_names is slower than it should be for unnamed tensors\nbecause of the following:\n- it always tries to access the TLS for NamesMode. Unnamed tensors don't\nneed to peek at NamesMode to determine if they have names or not.\n- There is some virtual function being called because TensorImpl is in\nc10 and NamedTensorMeta is in libtorch.\n\nThis PR short-circuits Tensor::has_names for unnamed tensors by\nchecking if the underlying TensorImpl hold a pointer to NamedTensorMeta\nor not. If the NamedTensorMeta is nullptr; then the tensor is definitely\nunnamed.\n\nBenchmarks:\n- I have a dedicated benchmarking machine where I isolate a single CPU\nand make sure it runs at a fixed frequency.\n- I benchmarked torch.add, which calls `tensor::has_names` three times.\n- The TL;DR is that torch.add between size-1 unnamed tensors gets sped up\n~200ns after this change which is a 9% improvement.\n- Before, on my machine:\nhttps://gist.github.com/zou3519/dfd648a1941d584711d850754e0694bc\n- After on my machine:\nhttps://gist.github.com/zou3519/e78f0d8980b43d0d9c3e3e78ecd0d4d5\n\nTest Plan: - run tests\n\nDifferential Revision: D19166510\n\nPulled By: zou3519\n\nfbshipit-source-id: 1888a4e92d29152a5e3b778a95e531087e532f53", "pr_number": "31436", "files_changed": ["aten/src/ATen/templates/TensorMethods.h", "c10/core/TensorImpl.h"], "labels": ["merged"]}, "e67064a96f": {"title": "Exclude generated source docs from Google (#31484)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31484\n\nSee https://github.com/pytorch/pytorch/issues/26123 for context.\n\nPreviously, when someone googles for `pytorch \"adaptive_max_pool2d\"`,\nhttps://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html\nis the first result. This PR changes the docs build script to exclude\nall such generated source docs under `_modules/` from Google.\n\nIt does this by doing a search for `<head>` and then appending\n`<meta name=\"robots\" content=\"noindex\">`.\nThe [google developer\ndocs](https://support.google.com/webmasters/answer/93710?hl=en) suggest\nthat this is the right way to prevent google from indexing the page.\n\nIn the future, when the CI\nbuilds documentation (both master and stable docs), the newly created\ndocs under _modules will have the meta noindex tag.\n\nTest Plan:\n- I ran `find \"$install_path/_modules\" -name \"*.html\" -print0 | xargs -0\nsed -i '/<head>/a \\ \\ <meta name=\"robots\" content=\"noindex\">'` on a docs\nbuild locally and checked that it does indeed append the meta noindex\ntag after `<head>`.\n- In a few days we should rerun the search to see if these pages are\nstill being indexed.\n\nDifferential Revision: D19180300\n\nPulled By: zou3519\n\nfbshipit-source-id: 5f5aa95a85dd9f065607c2a16f4cdd24ed699a83", "pr_number": "31484", "files_changed": [".circleci/scripts/python_doc_push_script.sh"], "labels": ["merged"]}, "dbe2f265d0": {"title": "Better error msg for autograd profiler + multi-worker dataloader crash (#31473)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31473\n\nMitigates #6313\n\nA common use case for the autograd profiler is to use it to run over an\nentire model, including dataloading. The following will crash:\n- run autograd profiler in CUDA mode\n- Use a multi-worker DataLoader (presumably with the 'fork' spawn\nmethod)\n- because the autograd profiler initializes CUDA and forking after CUDA is\ninitialized is bad.\n\nThis PR puts in a nice error message when this happens so that users\naren't too confused. The new error message looks like:\nhttps://gist.github.com/zou3519/903f15c3e86bad4585b7e5ce14cc1b70\n\nTest Plan:\n- Tested locally.\n- I didn't add a test case for this because it's hard to write a test\ncase that doesn't completely stop the rest of our test suite from\nrunning.\n\nDifferential Revision: D19178080\n\nPulled By: zou3519\n\nfbshipit-source-id: c632525ba1f7b168324f1aa55416e5250f56a086", "pr_number": "31473", "files_changed": ["torch/autograd/profiler.py", "torch/csrc/autograd/profiler_cuda.cpp"], "labels": ["merged"]}, "226c2d79ce": {"title": "Get QScheme from observer module (#31293)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31293\n\nPreviously we check the number of elements in scale to determine if we are using per channel quantization,\nbut we should get qscheme information from observer module directly and we'll expose this information\nto caller as well\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19146669\n\nfbshipit-source-id: ea430eeae0ef8f441be39aa6dcc1bb530b065554", "pr_number": "31293", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "57caeb3fc1": {"title": "Fix builtins table (#31492)", "body": "Summary:\nFixes a bad merge that is breaking distributed tests on master\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31492\n\nPulled By: driazati\n\nDifferential Revision: D19180978\n\nfbshipit-source-id: f69f525e2c7f61194686f07cf75db00eb642882f", "pr_number": "31492", "files_changed": ["torch/jit/_builtins.py"], "labels": ["jit", "merged"]}, "348d42114e": {"title": "Kill MessageType::SHUTDOWN related logic in pg agent (#31270)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/pull/30330 got rid of the need to send a `MessageType::SHUTDOWN` message, so we can now remove the logic/utils for this type of message.\n\nI think we can also delete the enum entry in the `enum MessageType`, but we may want to keep it in case the logic in https://github.com/pytorch/pytorch/pull/30710 is ever moved to C++.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31270\n\nTest Plan: All existing unit tests pass\n\nDifferential Revision: D19146983\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 35b185411f9446d7d4dfc37a6cb5477cf041e647", "pr_number": "31270", "files_changed": ["torch/csrc/distributed/rpc/message.cpp", "torch/csrc/distributed/rpc/message.h", "torch/csrc/distributed/rpc/process_group_agent.cpp"], "labels": ["merged"]}, "457286a383": {"title": "fix missing type check in dictionary literal", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31375\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19145440\n\nPulled By: zdevito\n\nfbshipit-source-id: 69909089586149ef766b4858d3420864a81b2493", "pr_number": "31375", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/compiler.cpp"], "labels": ["jit", "merged"]}, "87768e5ade": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/286867987eae3bd3cf081b8ddbb97e9f40eec059\nhttps://github.com/facebook/folly/commit/09cbf47ea54344fdcbf0ff165e5cab6efe531410\nhttps://github.com/facebook/litho/commit/db100834c1ffcdf5270ff2a473ac14369d3cd7f4\nhttps://github.com/facebook/rocksdb/commit/1ba92b858292ff9b110601eb03460d8ca1cb4b86\nhttps://github.com/facebookincubator/mvfst/commit/60240e3f0869fc4346617b37e370042048cf1367\nhttps://github.com/facebook/fbthrift/commit/beb5c4798eae7d7c3bf2b70a4a061c36bc8255a5\nhttps://github.com/facebook/proxygen/commit/c37eb5d377f88802d4a5d5b965f3a053629cefcc\nhttps://github.com/facebookincubator/mvfst/commit/1ada29037c8067a12f4de7b89fcca4f9e52ad633\nhttps://github.com/pytorch/fbgemm/commit/f12539bbc9cc5786b3854a3fa1c8299c68e79be2\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 75b16ea1bc038b599b3540d0615dd9eb9ecfda74", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "624088e444": {"title": "Don't dispatch to cudnn if it is not possible to make it 32bit by splitting batch dim (#31383)", "body": "Summary:\nAlso a step towards supporting 64bit indexing in convolution.\n\nSee also: https://github.com/pytorch/pytorch/pull/31379\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31383\n\nDifferential Revision: D19183443\n\nPulled By: ngimel\n\nfbshipit-source-id: 0c2030fac147e629d7be0c29f0683ec2b3f28c71", "pr_number": "31383", "files_changed": ["aten/src/ATen/native/ConvUtils.h", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/NNPACK.cpp", "aten/src/ATen/native/cudnn/Conv.cpp", "aten/src/ATen/native/miopen/Conv_miopen.cpp", "aten/src/ATen/native/mkldnn/Conv.cpp", "aten/src/ATen/native/mkldnn/Utils.cpp", "aten/src/ATen/native/mkldnn/Utils.h", "test/test_nn.py"], "labels": ["merged"]}, "06dbef663d": {"title": "Add support for `del` (#31273)", "body": "Summary:\nAdds the `del` keyword to the parser and corresponding `aten::Delete` op for lists and dicts\n\nFixes #20615\n](https://our.intern.facebook.com/intern/diff/19181473/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31273\n\nPulled By: driazati\n\nDifferential Revision: D19181473\n\nfbshipit-source-id: c42a2d43ec361a98e0c425232981edc9c39388c4", "pr_number": "31273", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_list_dict.py", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/script/compiler.cpp", "torch/csrc/jit/script/lexer.h", "torch/csrc/jit/script/parser.cpp", "torch/csrc/jit/script/python_tree_views.cpp", "torch/csrc/jit/script/tree_views.h", "torch/jit/frontend.py"], "labels": ["jit", "merged"]}, "4c341582ea": {"title": "modify model to enable loading by blob (#31507)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31507\n\nThis script is used to generate a model with bound shape inference and\nblob reorder, which are requirements for big model loading on T17.\n1. Load existing model.\n2. Do bound shape inference and blob reorder (put embedding blobs at the end).\n3. Save the modified model.\n\nTest Plan:\nGenerated a new moel and tested on NNPI.\nP124181047 (mismatch is AA variance)\n\nReviewed By: ipiszy\n\nDifferential Revision: D19165467\n\nfbshipit-source-id: c3522fc5dc53b7ec652420558e9e8bf65a1ccfae", "pr_number": "31507", "files_changed": ["caffe2/opt/bound_shape_inferencer.cc"], "labels": ["fb-exported", "merged"]}, "e9ef087d2d": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/357842e09120fe896db5f039f56791ebe7594595\nhttps://github.com/facebook/folly/commit/d62f47c763cf12d2f5b228c38a2035d1bc972144\nhttps://github.com/pytorch/fbgemm/commit/dc94cd49725e814f50d942c81383937a883365c4\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: dcb9813e1469cc867d9c826daa873c535ef408ab", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "d0d6e0b5e3": {"title": "add type promotion support for sparse tensors (#30429)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30429\n\nalso fix a bug in uncoalesced division\n\nGeneral approach here is that we:\n* compute the common dtype based on input tensors\n* error if the output tensor is specified and the common type can't be cast back to the output type (e.g. for inplace ops)\n* convert input tensor (values) to the common dtype\n* perform the op as normal (computing at the common dtype instead of the result type).\n* convert/copy the result values back to that of the result tensor (for in-place ops).\n\nFor uncoalesced division we need to coalesce, because an integral tensor with values=[1,1] at the same index divided by 2 would give 1/2 + 1/2 =0 instead of 2/2=1.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19143223\n\nPulled By: nairbv\n\nfbshipit-source-id: 480fa334c0b2b3df046818f2342cfd4e2d9d892a", "pr_number": "30429", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "test/test_type_promotion.py"], "labels": ["merged"]}, "b38901aa15": {"title": "Test reading `__cuda_array_interface__` inferred strides. (#31451)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31451\n\nThe PR that fixed this, https://github.com/pytorch/pytorch/pull/24947, didn't add a test.\n\nFixes: https://github.com/pytorch/pytorch/issues/31443\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19170020\n\nPulled By: gchanan\n\nfbshipit-source-id: bdbf09989ac8a61b1b70bb1ddee103caa8ef435b", "pr_number": "31451", "files_changed": ["test/test_numba_integration.py"], "labels": ["merged"]}, "2099cfa13d": {"title": "Fix input_channels divisibility check in concat_split_op (#31448)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31448\n\nReplace `(!x%y)` with `(x%y != 0)`\n\nTest Plan: CI\n\nReviewed By: orionr\n\nDifferential Revision: D19165492\n\nfbshipit-source-id: 246635fb8ddd5823196bcef9d0e6cdf1c349015e", "pr_number": "31448", "files_changed": ["caffe2/operators/concat_split_op.cc"], "labels": ["fb-exported", "merged"]}, "6cd987e7c0": {"title": "Make fully_qualified_type_name_impl() compatible with VS2017 15.9 (#31455)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31455\n\nIn 15.9, __FUNCSIG__ unwraps using definitions as well as preserves noexcept qualifiers\n\nTest Plan: Build caffe2 on Windows using VS2017\n\nDifferential Revision: D19166204\n\nfbshipit-source-id: b6c5f70e5262d13adf585f77b92223cf5f1e78dd", "pr_number": "31455", "files_changed": ["c10/util/TypeIndex.h"], "labels": ["fb-exported", "merge-this-please", "merged"]}, "0b57b383b1": {"title": "Im2col export (#30972)", "body": "Summary:\nAdded im2col to opset 11.\nThis symbolic is used to export torch.nn.Unfold\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30972\n\nReviewed By: hl475\n\nDifferential Revision: D18946921\n\nPulled By: houseroad\n\nfbshipit-source-id: 13dd0cbae899700df32fd74d6dff1f29033a2b4c", "pr_number": "30972", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py"], "labels": ["merged"]}, "7a12ccd003": {"title": "optimize FloatToFused8BitRowwiseQuantized and Fused8BitRowwiseQuantizedToFloat (#31470)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31470\n\nOptimize performance of these two operators.\nAdditionally use nearbyint instead of round to be consistent with 4-bit embedding table quantization.\n\nReviewed By: hyuen\n\nDifferential Revision: D19072103\n\nfbshipit-source-id: efe96f14aeff7958cceb453ed625d3fd693891ff", "pr_number": "31470", "files_changed": ["caffe2/operators/fused_rowwise_8bit_conversion_ops.cc", "caffe2/operators/fused_rowwise_8bit_conversion_ops.h", "caffe2/perfkernels/fused_8bit_rowwise_conversion.cc", "caffe2/perfkernels/fused_8bit_rowwise_conversion.h", "caffe2/perfkernels/fused_8bit_rowwise_conversion_avx2.cc", "caffe2/python/fused_8bit_rowwise_conversion_ops_test.py"], "labels": ["fb-exported", "merged"]}, "256db1e61b": {"title": "Add fake parsing for torchbind classes in schema type parser", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31506\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19187722\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 4529409454d64393a821b8fa795db39bc82da8fc", "pr_number": "31506", "files_changed": ["torch/csrc/jit/script/schema_type_parser.cpp"], "labels": ["jit", "merged"]}, "5375ceae80": {"title": "run optimizations on pre-profiled graph (#31392)", "body": "Summary:\nThis is the first stab at running profile-insensitive optimizations on pre-profiled graphs. Running those optimizations has a potential to simplify graphs greatly before GuardElimination and GuardElimination should be able to remove more guards.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31392\n\nDifferential Revision: D19173639\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 2485a2a598c10f9b5445efb30b16439ad4551b3f", "pr_number": "31392", "files_changed": ["test/test_jit.py", "torch/csrc/jit/profiling_graph_executor_impl.cpp", "torch/csrc/jit/profiling_graph_executor_impl.h"], "labels": ["jit", "merged"]}, "df9d5b8a77": {"title": "Use macros instead of directly accessing Python object fields (#31388)", "body": "Summary:\nThe Python C API documentation states \"Access to the [PyObject]\nmembers must be done by using the macros Py_REFCNT and Py_TYPE.\"\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31388\n\nDifferential Revision: D19161790\n\nPulled By: colesbury\n\nfbshipit-source-id: ac9a3738c913ad290a6d3460d0d657ec5c13b711", "pr_number": "31388", "files_changed": ["torch/csrc/Exceptions.h", "torch/csrc/tensor/python_tensor.cpp"], "labels": ["merged"]}, "b4c48b7e29": {"title": "Call `getQSchemeAndQParamMap` later in `quantizeTensors` (#31406)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31406\n\nPreviously we record quantization parameters for a given value when we collect the observer nodes,\nbut actually the quantization parameter can vary depending on each module instance, to achieve\nthat, we need to delay the call to later stage and only record the `Value*` that's needed\nin `collectObserverNodesAndValueToQuantize` function\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19162369\n\nfbshipit-source-id: e0f97e322d18a281bf15b6c7bbb04c3dfacb512f", "pr_number": "31406", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "08de70cad1": {"title": "Remove observers in the end (#31407)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31407\n\nRemove observers in the end instead of before quantize tensor\nsince we still need them to find the quantization paramters for each module instance\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19162367\n\nfbshipit-source-id: f817af87183f6c42dc97becea85ddeb7e050e2b1", "pr_number": "31407", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "35b249769d": {"title": "Exclude lite interpreter Java files from OSS host build", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31204\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19200610\n\nPulled By: dreiss\n\nfbshipit-source-id: 0cf41c99b4c2604afc2dccfebbea213c0e1f9638", "pr_number": "31204", "files_changed": ["android/pytorch_android/host/build.gradle"], "labels": ["merged"]}, "81329c907d": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/cbce6d17bbdaea19bf1d8ba0db0ac6793dcfa5de\nhttps://github.com/facebook/folly/commit/4762e080cfdede1006d4a80a689e45ba486bb77f\nhttps://github.com/facebook/litho/commit/174107c0a41ab6cffe73d70e40cd4ff7305b1911\nhttps://github.com/facebook/mcrouter/commit/8dee0e00585c9e3ee7c2a0a9b53d51ba725b9ecf\nhttps://github.com/facebook/proxygen/commit/ce52b27b4d73d7332053b2c64e1dedbaef171ae2\nhttps://github.com/facebook/rocksdb/commit/f89dea4fec99b582626bf210f938e653e20d4dc7\nhttps://github.com/facebookincubator/mvfst/commit/b269fc595c7ef2531d5c1d05768dca77421bf88c\nhttps://github.com/facebookincubator/profilo/commit/5b014c641ef01eca96317ab79f90c73a59340c0b\nhttps://github.com/pytorch/fbgemm/commit/ae2d7e11a270eea03912eeae117f9fa50fa32526\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 252ea5198c3fe4ecfe24e878ea701c48c57618de", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "11854bcd38": {"title": "Add test to torch.jit.export_opnames, make the _C function private", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31446\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19172851\n\nPulled By: iseeyuan\n\nfbshipit-source-id: f06d8766ed73c9abe4ebf41c402ee64880d745be", "pr_number": "31446", "files_changed": ["docs/source/torch.rst", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "3a19980b78": {"title": "Tensor class created from java does not call native methods", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31520\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D19199477\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: ba51454586a9385dba4ab73936f907346e0105d1", "pr_number": "31520", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/src/main/java/org/pytorch/Tensor.java", "android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java"], "labels": ["merged"]}, "c808eed04a": {"title": "Nightly dimension, input shape in gradle (#30195)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30195\n\n1. Added flavorDimensions 'build' local/nightly\nto be able to test the latest nightlies\n\n```\ncls && gradle clean test_app:installMobNet2QuantNightlyDebug -PABI_FILTERS=x86 --refresh-dependencies && adb shell am start -n org.pytorch.testapp.mobNet2Quant/org.pytorch.testapp.MainActivity\n```\n\n 2. To be able to change all new model setup editing only `test_app/build.gradle`\n Inlined model asset file names to `build.gradle`\n\nExtracted input tensor shape to `build.gradle` (BuildConfig)\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18893394\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 1fae9989d6f4b02afb42f8e26d0f3261d7ca929b", "pr_number": "30195", "files_changed": ["android/gradle.properties", "android/test_app/app/build.gradle", "android/test_app/app/src/main/AndroidManifest.xml", "android/test_app/app/src/main/java/org/pytorch/testapp/CameraActivity.java", "android/test_app/app/src/main/java/org/pytorch/testapp/Constants.java", "android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java", "android/test_app/app/src/main/java/org/pytorch/testapp/Result.java", "android/test_app/app/src/main/java/org/pytorch/testapp/Utils.java", "android/test_app/app/src/main/res/layout/activity_camera.xml", "android/test_app/app/src/main/res/layout/texture_view.xml", "android/test_app/gradle.properties"], "labels": ["merged"]}, "3820d6f6b9": {"title": "make gc script python2 compatible (#31536)", "body": "Summary:\nget rid of f-string, somehow we still have python2\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31536\n\nDifferential Revision: D19204187\n\nPulled By: mingbowan\n\nfbshipit-source-id: da8e17e4dccdd6fd1b0e92eb4740f5a09a8a4209", "pr_number": "31536", "files_changed": [".circleci/ecr_gc_docker/gc.py"], "labels": ["merged"]}, "0b0f90f53c": {"title": "Split on batch dimension when 32bit indexing not enough for convolution forward (#31379)", "body": "Summary:\nPartially fixes https://github.com/pytorch/pytorch/issues/22496\n\nThis is just a first step towards the support of 64bit convolution on CUDA. In the forward of convolution, if the total tensor size is larger than 2^31, then we split it on the batch dimension. I want to get some review feedback before moving forward for the same splitting approach for backward.\n\nThere are real-world use cases that even when N=1 the input is still larger than 2^31. For this case, the splitting would be complicated, so I am planning to modify `use_cudnn` to just dispatch to the slow fallback kernel in PyTorch in a later PR.\n\nUpdate: `later PR` is https://github.com/pytorch/pytorch/pull/31383\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31379\n\nDifferential Revision: D19192018\n\nPulled By: ngimel\n\nfbshipit-source-id: c26ecc56319ac67c4d5302ffed246b8d9b5eb972", "pr_number": "31379", "files_changed": ["aten/src/ATen/native/cudnn/Conv.cpp", "test/test_nn.py"], "labels": ["merged"]}, "8d8e82883e": {"title": "set stream everytime when we get a cuBlas handle (#31537)", "body": "Summary:\nI don't see any reason for not doing so, because it is a common error that people forget to set the stream. And I don't think there is a reason for not running on the current stream.\n\nThis is just for cublas, cusparse and cudnn should be modified also.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31537\n\nDifferential Revision: D19206908\n\nPulled By: ngimel\n\nfbshipit-source-id: ba2b2b74e9847f0495c76dbc778751a9f23f8b36", "pr_number": "31537", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "aten/src/ATen/cuda/CublasHandlePool.cpp", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu", "aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu", "aten/src/THC/THCBlas.cu"], "labels": ["merged"]}, "b5bbec7bad": {"title": "set stream everytime when we get a cuSparse handle (#31538)", "body": "Summary:\ncuSparse version of https://github.com/pytorch/pytorch/pull/31537\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31538\n\nDifferential Revision: D19206895\n\nPulled By: ngimel\n\nfbshipit-source-id: a32c0bc310189a89a0098837438d62458b5c0a7c", "pr_number": "31538", "files_changed": ["aten/src/ATen/cuda/CuSparseHandlePool.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cu"], "labels": ["merged"]}, "700109eb63": {"title": "set stream everytime when we get a cuDNN handle (#31541)", "body": "Summary:\ncudnn version of https://github.com/pytorch/pytorch/pull/31537\n\nhttps://github.com/pytorch/pytorch/pull/31532 is a quick fix and this is a bigger change. This would deprecate https://github.com/pytorch/pytorch/pull/31532, but we could also merge https://github.com/pytorch/pytorch/pull/31532 first for a quick fix and then work on this later.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31541\n\nDifferential Revision: D19206753\n\nPulled By: ngimel\n\nfbshipit-source-id: 3352f923d13a9baf0971f64f8b7ce03e9a8b42b1", "pr_number": "31541", "files_changed": ["aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/cudnn/Handle.cpp", "aten/src/ATen/cudnn/Utils.h", "aten/src/ATen/native/cudnn/AffineGridGenerator.cpp", "aten/src/ATen/native/cudnn/BatchNorm.cpp", "aten/src/ATen/native/cudnn/Conv.cpp", "aten/src/ATen/native/cudnn/GridSampler.cpp", "aten/src/ATen/native/cudnn/LossCTC.cpp", "aten/src/ATen/native/cudnn/RNN.cpp"], "labels": ["merged"]}, "7d630278da": {"title": "Separate torchbind from Python (#30242)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30242\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29501\n\nCurrently blocked on schema serialization issue\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18463063\n\nPulled By: jamesr66a\n\nfbshipit-source-id: c12a1b644eb9bf04e68ff93cccf91d6cb3e75359", "pr_number": "30242", "files_changed": [".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/test.sh", "aten/src/ATen/core/custom_class.cpp", "aten/src/ATen/core/function_schema.h", "caffe2/CMakeLists.txt", "test/cpp/jit/test_custom_class.cpp", "test/custom_operator/CMakeLists.txt", "test/custom_operator/classes.cpp", "test/custom_operator/test_custom_classes.py", "test/test_jit.py", "tools/build_variables.py", "torch/CMakeLists.txt", "torch/_classes.py", "torch/csrc/jit/custom_class.cpp", "torch/csrc/jit/custom_class.h", "torch/csrc/jit/init.cpp", "torch/csrc/jit/operator.h", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/python_custom_class.cpp", "torch/csrc/jit/python_custom_class.h", "torch/csrc/jit/register_c10_ops.cpp", "torch/csrc/jit/script/schema_type_parser.cpp", "torch/custom_class.h"], "labels": ["fb-exported", "jit"]}, "cc2d5ca37f": {"title": "add enabled API to autograd profiler (#31380)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31380\n\nFor being able to profile async RPCs, we attach a `RecordFunction` object to the future that is created during the RPC to persist it across the lifetime of the RPC (this is implemented in the next PR: ). Since we'd only like to do this when profiling is enabled, this PR adds an enabled API to the autograd profiler.\nghstack-source-id: 96053933\n\nTest Plan: Modified unit test.\n\nDifferential Revision: D19050391\n\nfbshipit-source-id: aa382110e69d06b4a84c83b31d2bec2d8a81ba10", "pr_number": "31380", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler.cpp", "torch/csrc/autograd/profiler.h"], "labels": ["merged"]}, "fe76af96ed": {"title": "fix test_process_group_debug_info flaky test (#31533)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31533\n\nFixes this test that was flaky and has been disabled (see\nhttps://github.com/pytorch/pytorch/issues/31112)\nghstack-source-id: 96038999\n\nTest Plan: Run the test 1000 times and ensure that it passes.\n\nDifferential Revision: D19203366\n\nfbshipit-source-id: 7978cbb8ca0989a0a370a36349cdd4db3bb8345b", "pr_number": "31533", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "9459db86bf": {"title": "Raise warning for schedulers following chainable shedulers (#31125)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/29697.\n\nRaise warning for schedulers following chainable schedulers in https://github.com/pytorch/pytorch/issues/26423. See explanation for\n* [new warning when load/save](https://github.com/pytorch/pytorch/issues/29697#issuecomment-564655802)\n* [change from deprecation to user warning](https://github.com/pytorch/pytorch/issues/29697#issuecomment-564659775).\n\ngchanan -- This should go in the upcoming release following https://github.com/pytorch/pytorch/issues/26423.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31125\n\nDifferential Revision: D19143740\n\nPulled By: vincentqb\n\nfbshipit-source-id: 35b55fe6c5b39ca5a68b1a6e19f14eb95b9a784e", "pr_number": "31125", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["merged"]}, "68e5172382": {"title": "Support optional float parameters (float?, optional<double>). (#31517)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31517\n\nThis is going to be used by upsample (which currently uses magic values to represent optionals).\n\nFor now, we just introduce a fake function for testing (torch._test_optional_float(x)).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19198721\n\nPulled By: gchanan\n\nfbshipit-source-id: 0a1382fde0927c5d277d02d62bfb31fb574b8c74", "pr_number": "31517", "files_changed": ["aten/src/ATen/native/Pooling.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "test/test_jit.py", "test/test_torch.py", "tools/autograd/gen_python_functions.py", "tools/jit/gen_jit_dispatch.py", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h", "torch/csrc/utils/python_arg_parser.h"], "labels": ["jit", "merged"]}, "fb63c0e2c9": {"title": "Remove -Wno-unused-private-field", "body": "Test Plan: Sanity check\n\nReviewed By: nlutsenko\n\nDifferential Revision: D18833450\n\nfbshipit-source-id: c69b6679b4caa3e868ca41113cd502c8905a776b", "pr_number": null, "files_changed": ["caffe2/mobile/contrib/ios/mpscnn/mpscnn.mm"], "labels": []}, "218cfd568d": {"title": "Conv transpose/backward split 32bit (#31510)", "body": "Summary:\nBasically the same as https://github.com/pytorch/pytorch/pull/31379 except for that I write a separate function `split_batch_dim_to_32bit_out` for the logic. This function could also be used for convolution forward, and I will rebase this PR after https://github.com/pytorch/pytorch/issues/31379 get merged and then change `raw_cudnn_convolution_forward_out` to use `split_batch_dim_to_32bit_out` here.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31510\n\nDifferential Revision: D19210563\n\nPulled By: ngimel\n\nfbshipit-source-id: e20bb82b6360aa2c0e449e127188c93f44e1e9b4", "pr_number": "31510", "files_changed": ["aten/src/ATen/native/cudnn/Conv.cpp", "test/test_nn.py"], "labels": ["merged"]}, "446e9af5b9": {"title": "Fix parsing of big float literals (#29940)", "body": "Summary:\nStacked PRs\n * **#29940 - [jit] Fix parsing of big float literals**\n * #29935 - [jit] Fix hex literal parsing\n * #29931 - [jit] Throw a better error for int too big for int64_t\n](https://our.intern.facebook.com/intern/diff/19186604/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29940\n\nPulled By: driazati\n\nDifferential Revision: D19186604\n\nfbshipit-source-id: 6ef66588a5cf956f281e7bd1e5584ef06f5296e9", "pr_number": "29940", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/tree_views.h"], "labels": ["jit", "merged"]}, "46ad80c839": {"title": "Fix null pointer dereference on Android for strtod_c (#31582)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31582\n\nD19124934 removed a dummy pointer passed to strtod_c() that's used only for Android (https://fburl.com/diffusion/zkv34jf1). Without it, jit parsing on Android start throwing SIGSEGV due to null pointer dereferencing. This diff adds the dummy pointer back.\n\nTest Plan: Tests\n\nReviewed By: driazati, shoumikhin\n\nDifferential Revision: D19221071\n\nfbshipit-source-id: 2e230c3fbfa873c3f7b92f73c87ee766ac182115", "pr_number": "31582", "files_changed": ["torch/csrc/jit/script/tree_views.h"], "labels": ["fb-exported", "jit", "merged"]}, "363d8be787": {"title": "Bypass _TorchScriptTesting_StackString::pop in BC check now (#31586)", "body": "Summary:\nFailed result: https://circleci.com/gh/pytorch/pytorch/4054919?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link/console\n\nOriginal PR: https://github.com/pytorch/pytorch/pull/30242\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31586\n\nReviewed By: hl475\n\nDifferential Revision: D19222086\n\nPulled By: houseroad\n\nfbshipit-source-id: 96db2bf18fa06eaebdd558e86615e26b95f34516", "pr_number": "31586", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "a54dc87e8e": {"title": "revert D18805532 and make numerics of masked adagrad consistent with unmasked adagrad (#30784)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30784\n\nInstead of putting experimental Masked*Adagrad to OSS, we decided to change D18805278 .\n\nTest Plan: CI\n\nReviewed By: chocjy\n\nDifferential Revision: D18824265\n\nfbshipit-source-id: 3d893fe6c441f2ff7af4c497cf81b9c49363e7a8", "pr_number": "30784", "files_changed": ["caffe2/operators/experimental/optimizers/masked_adagrad.cpp", "caffe2/operators/experimental/optimizers/masked_adagrad_test.py"], "labels": ["fb-exported", "merged"]}, "29f345831e": {"title": "Error out if legacy Tensor.new is called on alternate layouts / dtypes (#31485)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31485\n\nFixes: https://github.com/pytorch/pytorch/issues/22158\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19196499\n\nPulled By: gchanan\n\nfbshipit-source-id: a01ea7641b5fcd00a9d267243539ff64a5492e5f", "pr_number": "31485", "files_changed": ["test/test_mkldnn.py", "test/test_quantized_tensor.py", "torch/csrc/utils/tensor_new.cpp"], "labels": ["merged"]}, "e2951d586d": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/11a904583d932e8f69dfe2f7afb913aac2cd2c8b\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: f00bf65aebddb4541faa2626d42ac436e090ee89", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "866c1b1fcc": {"title": "Ensure legacy sparse constructor/new doesn't interpret python data as tensor data. (#31490)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31490\n\nWhen this happens, a dense tensor is constructed from a sparse constructor.\n\nFixes: https://github.com/pytorch/pytorch/issues/16154\n\nTest Plan: Imported from OSS\n\nReviewed By: cpuhrsch, mrshenli\n\nDifferential Revision: D19196498\n\nPulled By: gchanan\n\nfbshipit-source-id: 57a6324833e35f3e62318587ac74267077675b93", "pr_number": "31490", "files_changed": ["test/test_sparse.py", "torch/csrc/utils/tensor_new.cpp"], "labels": ["merged"]}, "cf46bcace8": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/rocksdb/commit/faebc336da211a9815e19c4a18f79f30dece2111\nhttps://github.com/pytorch/fbgemm/commit/23d8703808fe2d61bd082115e230168dc03023a5\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 0368879112c318607821bbf3a081669dade19148", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "5d95a9ca79": {"title": "Print all broken ops instead of the first one (#31628)", "body": "Summary:\nOriginally, we only print one broken schema. With this changeset, all the broken schemas are printed out.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31628\n\nReviewed By: hl475\n\nDifferential Revision: D19231444\n\nPulled By: houseroad\n\nfbshipit-source-id: 3dd5b4609a6a9a9046e95f2f30deb9beeb5dcd56", "pr_number": "31628", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "ec4e347744": {"title": "Add Python language reference docs (#30686)", "body": "Summary:\nThis exposes our audit of https://docs.python.org/3/reference/ with descriptions for each line item.\n\nTo generate the `.rst` from the Quip:\n\n```bash\npip install m2r\nm2r jit_language_reference.md\n```\n\nhttps://driazati.github.io/pytorch_doc_previews/30686/jit.html#python-functions-and-modules\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30686\n\nPulled By: driazati\n\nDifferential Revision: D19219587\n\nfbshipit-source-id: 249db9b5ee20e38804d4302bbfeca7d54f27d0bd", "pr_number": "30686", "files_changed": ["docs/source/jit.rst", "docs/source/jit_python_reference.rst"], "labels": ["merged"]}, "34dce8e348": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/folly/commit/a40d60834183ef94e8a9cde9f13b58696dd6d940\nhttps://github.com/facebookincubator/fizz/commit/50e0ea13e5dbc09140c324aa74a8fdbcf8c42dc6\nhttps://github.com/pytorch/fbgemm/commit/bcbdec74f4c8c3104a35b0bf87deec7c0fa5f14c\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 3de13d5b9b20ec18927ee3f0224df789172a3e9c", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "91eb7c26cd": {"title": "Fix Typos", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31630\n\nDifferential Revision: D19233162\n\nPulled By: zou3519\n\nfbshipit-source-id: c2716a2df2b2ccfeda7718b484e9605515ecdf01", "pr_number": "31630", "files_changed": ["aten/src/ATen/native/Normalization.cpp"], "labels": ["merged"]}, "39508501a4": {"title": "Create byte-aware word lstm benchmark (#31260)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31260\n\n1. Update the LiteLM dataset conversion script (fbcode/pytext/fb/tools/lite_lm_dataset_to_tensorproto.py)\n2. Created a benchmark json file for byte-aware lstm word model (xplat/aibench/specifications/models/caffe2/assistant/lite_lm_len5.json)\n3. In order to run the model -- created an int64 Tensor for the model, added batch gather ops to the BUCK file\n\nTest Plan:\n```\n1. Create tensorproto of the model input\nbuck run mode/opt //pytext/fb/tools:byte_lm_dataset_to_tensorproto -- --in-path /mnt/vol/pytext/smart_keyboard/aibench/test_5.txt --out-path /mnt/vol/pytext/smart_keyboard/aibench/byteAwareWordLM/ --hidden_dim 203 --layers_num 2 --max_seq_len 64 --max_byte_len 15\n\n2. Run the aibench command\nbuck run fbsource//xplat/aibench:run_bench -- -b aibench/specifications/models/caffe2/assistant/lm_byte_lstm_len5.json --remote --devices SM-G960U-8.0.0-26\n```\n\nReviewed By: gardenia22\n\nDifferential Revision: D17785682\n\nfbshipit-source-id: 351c3c8bae16449e72ac641522803b23a83349be", "pr_number": "31260", "files_changed": ["caffe2/python/utils.py"], "labels": ["fb-exported", "merged"]}, "909b8eba0d": {"title": "cudnn grouped convolution nhwc patch (#31444)", "body": "Summary:\nEarlier cudnn version doesn't support grouped convolution in NHWC well. Legit\nconfiguration in later cudnn version might return CUDNN_STATUS_NOT_SUPPORTED.\nWe are falling back to NCHW when runtime check of cudnn version is < 7.6.0 to\nkeep the logic simple.\n\nNote:\nWe might update the heuristics, 7.6.0 is very conservative.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31444\n\nDifferential Revision: D19232414\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 4c2d79ed347c49cd388bbe5b2684dbfa233eb2a3", "pr_number": "31444", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cudnn/Conv.cpp", "test/test_nn.py"], "labels": ["merged"]}, "4983ef8de1": {"title": "Integrating MaskedAdagrad", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31640\n\nTest Plan: unit test\n\nReviewed By: ellie-wen\n\nDifferential Revision: D18805278\n\nfbshipit-source-id: 1def4a89b7e4e04385c762bf127d95c5e513180e", "pr_number": "31640", "files_changed": ["caffe2/python/optimizer.py"], "labels": ["fb-exported", "merged"]}, "ffcac9ad37": {"title": "Clean White List for BC Checks (#31629)", "body": "Summary:\nDelete obsolete items\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31629\n\nReviewed By: hl475\n\nDifferential Revision: D19231522\n\nPulled By: houseroad\n\nfbshipit-source-id: 393ed630f7854b643c8fa8c5f3f576718934de96", "pr_number": "31629", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "204939b401": {"title": "Automatic update of fbcode/onnx to 57ebc587fcf3913b4be93653b0dd58c686447298 (#31642)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31642\n\nPrevious import was c08a7b76cf7c1555ae37186f12be4d62b2c39b3b\n\nIncluded changes:\n- **[57ebc587](https://github.com/onnx/onnx/commit/57ebc587)**: python_out does not recognize dllexport_decl. (#2482) <xkszltl>\n- **[477a9b87](https://github.com/onnx/onnx/commit/477a9b87)**: Edited PythonAPIOverview.md (#2491) <AlexMuresan>\n- **[59b9f908](https://github.com/onnx/onnx/commit/59b9f908)**: Minor correction type (#2411) <Jhuo IH>\n- **[cdc8b861](https://github.com/onnx/onnx/commit/cdc8b861)**: fix the optimize pass of fuse_consecutive_transposes (#2471) <XavierAtShanghai>\n- **[ad1f5567](https://github.com/onnx/onnx/commit/ad1f5567)**: Add clarification for bias quantization in QlinearConv Op spec (#2464) <Ashwini Khade>\n- **[d9a73ccc](https://github.com/onnx/onnx/commit/d9a73ccc)**: Add remove operator and function requirements to the add new op doc. (#2486) <Emad Barsoum>\n\nTest Plan: cont build\n\nReviewed By: hl475\n\nDifferential Revision: D19234753\n\nfbshipit-source-id: 4b7de1407d9b64e584f6e6d68cbe03fa1b4c854d", "pr_number": "31642", "files_changed": ["third_party/onnx"], "labels": ["fb-exported", "merged"]}, "b522a8e1ff": {"title": "Optimize zero length input (#31602)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31602\n\nPull Request resolved: https://github.com/pytorch/glow/pull/3943\n\nZero length input is something we hit fairly frequently in practice. Previous handling of global TensorPool involves two locks per input (acquire and reclaim). Here we use a specialized anchor tensor to host zero length input. Note that it is only padded to max sequence length. If necessary, an easy extension can be added to pad to max `InputPlaceholder.getType().size()`.\n\nReviewed By: jfix71\n\nDifferential Revision: D19192467\n\nfbshipit-source-id: cafdc1eb7bf9b9d6ead04a0243b0be838f6b71cd", "pr_number": "31602", "files_changed": ["caffe2/opt/onnxifi_op.h", "third_party/foxi"], "labels": ["fb-exported", "merged"]}, "e84e7ec556": {"title": "Kill aten_custom_call.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/25613\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D17172503\n\nPulled By: gchanan\n\nfbshipit-source-id: 1456ecca8f459d008e335412cd7084bdfcb93439", "pr_number": "25613", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/function_wrapper.py", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/THC/generic/THCTensorMath.cu"], "labels": ["merged", "module: cpu", "module: cuda", "module: internals", "module: operators"]}, "35bee0c729": {"title": "separate op for rowwise counter (#31612)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31612\n\nCount the number recent update on rows. Exponential decay is applied on the counter with decay rate r, such that\n    r^{counter_halflife} = 0.5;\nIf counter_halflife is nonpositive, this operator is turned off.\n\nTest Plan: added unittest\n\nReviewed By: chocjy\n\nDifferential Revision: D19217921\n\nfbshipit-source-id: 96d850123e339212cc0e0ef352ea8a1b1bf61dfa", "pr_number": "31612", "files_changed": ["caffe2/python/operator_test/rowwise_counter_test.py", "caffe2/sgd/rowwise_counter.cc", "caffe2/sgd/rowwise_counter.h"], "labels": ["fb-exported", "merged"]}, "647569e546": {"title": "get rid of choco install (#30897)", "body": "Summary:\n7zip and cmake are part of base image, no need to re-install. Remove the install step can make build/test more stable.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30897\n\nDifferential Revision: D19232961\n\nPulled By: mingbowan\n\nfbshipit-source-id: fa3bbd1325839a2a977bf13fdbd97fda43793b8d", "pr_number": "30897", "files_changed": [".circleci/config.yml", ".circleci/scripts/vs_install.ps1", ".circleci/verbatim-sources/pytorch-job-specs.yml", ".circleci/verbatim-sources/windows-build-test.yml", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_miniconda3.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", "aten/src/ATen/Dispatch.h", "aten/src/ATen/core/TensorAccessor.h", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/TensorBody.h", "c10/core/MemoryFormat.h", "c10/core/Scalar.h", "c10/core/ScalarType.h", "c10/util/ArrayRef.h", "c10/util/Deprecated.h", "c10/util/Exception.h", "test/test_jit.py", "test/test_nn.py", "torch/csrc/api/include/torch/nn/modules/container/named_any.h", "torch/csrc/utils/auto_gil.h", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["merged"]}, "ae214f67a5": {"title": "updated code to ensure error check for negative dims", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31636\n\nDifferential Revision: D19233031\n\nPulled By: anjali411\n\nfbshipit-source-id: c29265ddd1f887f1a0b98aca56a2691d7584353d", "pr_number": "31636", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "test/test_torch.py"], "labels": ["merged"]}, "90a187618e": {"title": "Integrate masked sparse Adagrad (#31641)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31641\n\nAssuming mask is provided as a tensor\n\nTest Plan: unit test\n\nReviewed By: ellie-wen\n\nDifferential Revision: D18928737\n\nfbshipit-source-id: a4f3dd51769c2b56e5890043e91c18e6128be082", "pr_number": "31641", "files_changed": ["caffe2/python/optimizer.py"], "labels": ["fb-exported", "merged"]}, "e8e47c0a1b": {"title": "Split RRef class into abstract RRef and RRefBase (#28942)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28942\n\nThe new abstract RRef class contains only user-facing RRef APIs.\nIt will be later moved to a common folder so that it can be shared\nby jit and distributed packages to provide TorchScript support.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18240590\n\nPulled By: mrshenli\n\nfbshipit-source-id: ac28cfc2c8039ab7131b537b2971ed4738710acb", "pr_number": "28942", "files_changed": ["tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref.cpp", "torch/csrc/distributed/rpc/rref.h", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/distributed/rpc/rref_interface.h"], "labels": ["merged"]}, "3b7916fccd": {"title": "Modify the order of arguments position of torch.std and torch.std_mean in doc (#31677)", "body": "Summary:\nChange log:\n\n- [x] Change the order of arguments position of torch.std and torch.std_mean in doc.\n- [x] Correct a spelling mistake of torch.std_mean in doc.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31677\n\nDifferential Revision: D19247372\n\nPulled By: ngimel\n\nfbshipit-source-id: 8685f5207c39be524cdc81250430beac9d75f330", "pr_number": "31677", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged"]}, "22d84204f7": {"title": "Expose torch.poisson in documentation (#31667)", "body": "Summary:\nChangelog:\n- Add doc string for torch.poisson briefing current behavior\n- Check for non-positive entries in the tensor passed as input to torch.poisson\n\nCloses https://github.com/pytorch/pytorch/issues/31646\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31667\n\nDifferential Revision: D19247371\n\nPulled By: ngimel\n\nfbshipit-source-id: b53d105e73bf59a45beeb566f47365c3eb74efca", "pr_number": "31667", "files_changed": ["aten/src/ATen/native/Distributions.cpp", "docs/source/torch.rst", "torch/_torch_docs.py"], "labels": ["merged"]}, "ee87b01f40": {"title": "add additional types to indexing operations dispatch (#31692)", "body": "Summary:\n- Fixes https://github.com/pytorch/pytorch/issues/31672\n- Adds Bfloat16 dispatch to the indexing operations that were missing it\n    - index_put on cuda does not have bfloat16 dispatch, because I'm not sure bfloat16 math ops work on cuda\n\nNote: `index_put_` with `accum=True` is enabled for `bool`, which does not make much sense, but I'm not the one who started it, so this behavior is preserved.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31692\n\nDifferential Revision: D19249561\n\nPulled By: ngimel\n\nfbshipit-source-id: 1269196194f7b9f611b32be198c001704731a78f", "pr_number": "31692", "files_changed": ["aten/src/ATen/AccumulateType.h", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/Indexing.cu", "test/test_indexing.py"], "labels": ["merged"]}, "6064223808": {"title": "`@slowTest` some slow tests (#31706)", "body": "Summary:\nThese are all the jit tests that take > 10 seconds according to `pytest test/test_jit.py --durations=15`\n\n```\n32.76s call     test/test_jit.py::TestModels::test_super_resolution\n32.20s call     test/test_jit.py::TestModels::test_neural_style\n30.90s call     test/test_jit.py::TestJit::test_export_batchnorm\n25.95s call     test/test_jit.py::TestJit::test_dropout_module_requires_grad\n22.24s call     test/test_jit.py::TestJitGeneratedModule::test_nn_Transformer\n12.38s call     test/test_jit.py::TestScript::test_fuser_double_float_codegen\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31706\n\nPulled By: driazati\n\nDifferential Revision: D19251567\n\nfbshipit-source-id: 8e76f717506b8bf28d1a63ce302feb0446dc9141", "pr_number": "31706", "files_changed": ["test/jit/test_models.py", "test/test_jit.py"], "labels": ["merged"]}, "dd0f2f0c19": {"title": "add float[] str[] constants (#31503)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31503\n\nAdd support for float lists and string lists constants, which enables better constant propagation + constant pooling + freezing.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19233558\n\nPulled By: eellison\n\nfbshipit-source-id: 4f7c6d9ddbe7623757a9a20606ce5f394e14e93d", "pr_number": "31503", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/cpp/jit/test_constant_propagation.cpp", "test/cpp/jit/test_irparser.cpp", "test/test_jit.py", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/passes/python_print.cpp"], "labels": ["jit", "merged"]}, "f4e955ff62": {"title": "Change PackSegments to ensure consistent behavior between CPU and GPU", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31673\n\nReviewed By: Wakeupbuddy, BIT-silence\n\nDifferential Revision: D18925762\n\nfbshipit-source-id: e0c318e97f69b14a54f43c176af57d98fbc16c9f", "pr_number": "31673", "files_changed": ["caffe2/operators/pack_segments.cc", "caffe2/python/operator_test/pack_ops_test.py"], "labels": ["fb-exported", "merged"]}, "39297bfe08": {"title": "Fix flaky test_debug_info. (#31675)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31675\n\nThis test could be flaky since there could be inflight RPC requests as\npart of startup which might not have finished. As a result, if they finish\nbetween the different calls to retrieve debug_info, there could be a problem\nsince we would report separate information. As a result, we wait to ensure\nthe metrics stabilize to avoid flakiness.\nghstack-source-id: 96188488\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19242588\n\nfbshipit-source-id: 8f3db7e7365acbd3742e6ec0c2ddcca68f27db9e", "pr_number": "31675", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "b102550d2c": {"title": "Allow to pass in masks through db (#31676)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31676\n\nFacebook\uff1a\n\nPreviously we assumed mask is passed in as a tensor which is not feasible for sparse parameter.\nHere we allow to pass in the mask through db path which requires the masks to be stored in some db first.\n\nTest Plan: unit tests\n\nReviewed By: ellie-wen\n\nDifferential Revision: D18928753\n\nfbshipit-source-id: 75ca894de0f0dcd64ce17b13652484b3550cbdac", "pr_number": "31676", "files_changed": ["caffe2/python/optimizer.py"], "labels": ["fb-exported", "merged"]}, "1499b894c4": {"title": "Apply clang-format to csrc/distributed/rpc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31681\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19247085\n\nPulled By: mrshenli\n\nfbshipit-source-id: ce6c1710663eecda3641d8dcf80ef16f9d21b93e", "pr_number": "31681", "files_changed": ["torch/csrc/distributed/rpc/python_call.cpp", "torch/csrc/distributed/rpc/python_resp.cpp", "torch/csrc/distributed/rpc/rref_proto.cpp"], "labels": ["merged"]}, "7a3ed36309": {"title": "Fix nvcc math functions for MSVC 2019 (#31704)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31108.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31704\n\nDifferential Revision: D19256110\n\nPulled By: mingbowan\n\nfbshipit-source-id: a4aba2830aba002497f70a75ef995e5e7de08393", "pr_number": "31704", "files_changed": ["aten/src/ATen/native/cuda/PowKernel.cu"], "labels": ["merged"]}, "cb1af5f61f": {"title": "Revert D19233558: add float[] str[] constants", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19233558\n\nOriginal commit changeset: 4f7c6d9ddbe7\n\nfbshipit-source-id: a5020a9169e349a5970323471d673e8cd7818c66", "pr_number": null, "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/cpp/jit/test_constant_propagation.cpp", "test/cpp/jit/test_irparser.cpp", "test/test_jit.py", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/passes/python_print.cpp"], "labels": []}, "236b0a318c": {"title": "Delete ATen/stub (#31763)", "body": "Summary:\nThis folder contained an empty CombinedStub file which isn't explicitly used anywhere.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31763\n\nDifferential Revision: D19262563\n\nPulled By: ezyang\n\nfbshipit-source-id: 5d095c93d6f7a1cc35f5919aa6006b31c2376b18", "pr_number": "31763", "files_changed": ["aten/src/ATen/stub/CombinedStub.cpp"], "labels": ["merged"]}, "c4f10e0fe7": {"title": "Renaming scales parameter for interpolate (#31526)", "body": "Summary:\nPR separated from https://github.com/pytorch/pytorch/pull/31274.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31526\n\nReviewed By: zou3519\n\nDifferential Revision: D19221931\n\nPulled By: gchanan\n\nfbshipit-source-id: 81958a9910867ac9d62f2b47abc49384526c4e51", "pr_number": "31526", "files_changed": ["aten/src/ATen/native/Pooling.cpp", "aten/src/ATen/native/UpSample.h", "aten/src/ATen/native/UpSampleBicubic2d.cpp", "aten/src/ATen/native/UpSampleBilinear2d.cpp", "aten/src/ATen/native/UpSampleLinear1d.cpp", "aten/src/ATen/native/UpSampleNearest1d.cpp", "aten/src/ATen/native/UpSampleNearest2d.cpp", "aten/src/ATen/native/UpSampleNearest3d.cpp", "aten/src/ATen/native/UpSampleTrilinear3d.cpp", "aten/src/ATen/native/cuda/UpSample.cuh", "aten/src/ATen/native/cuda/UpSampleBicubic2d.cu", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleNearest1d.cu", "aten/src/ATen/native/cuda/UpSampleNearest2d.cu", "aten/src/ATen/native/cuda/UpSampleNearest3d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_nearest2d.cpp", "test/backward_compatibility/check_backward_compatibility.py", "test/onnx/test_operators.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_jit.py", "test/test_nn.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/symbolic_script.cpp", "torch/nn/functional.py", "torch/onnx/symbolic_caffe2.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset8.py", "torch/onnx/symbolic_opset9.py"], "labels": ["jit", "merged"]}, "9e9bfbfd8d": {"title": "Update old scheduler example usage (#31358)", "body": "Summary:\nUpdate the old example usage in CosineAnnealingWarm, `scheduler.step()` should be called after `optimizer.step()`.\n\nhttps://github.com/pytorch/pytorch/issues/20028#issuecomment-566061580\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31358\n\nDifferential Revision: D19199311\n\nPulled By: vincentqb\n\nfbshipit-source-id: cb29b95f8277d2dfa75ec2a83c1af03a5c9c9a69", "pr_number": "31358", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["merged", "open source", "triaged"]}, "37fc59e847": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/17caab3d7bdfa8d828fc5fe2ce99ef90e85fa35d\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: f4828cd5c81615d0df86f915b3abb6a58509aa79", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "7078f4b27d": {"title": "skip _test_optional_float in BC check (#31786)", "body": "Summary:\nSkip _test_optional_float\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31786\n\nReviewed By: hl475\n\nDifferential Revision: D19265059\n\nPulled By: houseroad\n\nfbshipit-source-id: 6b95bd3b8cad83a4c459c0603befaaeeade6cdff", "pr_number": "31786", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "d770fbc1d2": {"title": "Some modifications to improve readability (#31352)", "body": "Summary:\nIn the long string, formalstring thinks it is good to have a name.\n\nWhen using dict, literal is better for readability and faster than dict constructor.\n\nI always appreciate your efforts in creating the world's best frameworks.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31352\n\nDifferential Revision: D19191967\n\nPulled By: ngimel\n\nfbshipit-source-id: 21f063b163b67de8cf9761a4db5991f74318e991", "pr_number": "31352", "files_changed": ["benchmarks/fastrnns/bench.py", "torch/serialization.py", "torch/utils/hooks.py"], "labels": ["merged", "open source", "triaged"]}, "ed5cd0d742": {"title": "Use numeric limits to define TensorTypeSet(FULL) representation (#31668)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31668\n\nThis also removes an annoying warning about change of sign conversion\n\nTest Plan: Run unit tests\n\nReviewed By: ezyang\n\nDifferential Revision: D19238631\n\nfbshipit-source-id: 29b50abac635e530d5b0453c3a0f36a4573fbf5b", "pr_number": "31668", "files_changed": ["c10/core/TensorTypeSet.h"], "labels": ["fb-exported", "merged"]}, "feb0ccdbfd": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/123ae291fc17cb84e6064573c5afba06723839e8\nhttps://github.com/facebook/fbthrift/commit/b9e9d4f7d92b985330e9cbca2c1287bdad96a3b4\nhttps://github.com/facebook/fbzmq/commit/86ea03e727843b68f3791739c8639210c03a75ee\nhttps://github.com/facebook/folly/commit/1cd1bfb6680997ecd705a51e8991045bbcd7776d\nhttps://github.com/facebook/mcrouter/commit/917504ac42750ccde47975c01329c439d6efe927\nhttps://github.com/facebook/proxygen/commit/06cc65203083ddec5da6a03832d63c9a3aa71cbc\nhttps://github.com/facebook/wangle/commit/e63819cbe30c7b676efa258ee5dd45c5a250f0df\nhttps://github.com/facebookincubator/fizz/commit/6d21d8cfd3c17a7ecd6c23b26ac85fde96177997\nhttps://github.com/facebookincubator/katran/commit/b636829d55d8cf51485bbc7830315d35290580c7\nhttps://github.com/facebookincubator/mvfst/commit/19d0faece2b167dae66cf4e7f4dc171bb33ecfde\nhttps://github.com/pytorch/fbgemm/commit/9860344e107864b4abe1f23df5d59ff92c26fcee\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 1de7509af788dc7861cfc779936fbc9e0146a5a5", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "5f8308e32d": {"title": "Pin Pillow to v6 as PILLOW_VERSION is removed in v7", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31777\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19264247\n\nPulled By: mrshenli\n\nfbshipit-source-id: 52b0a3629e3a96ef2f9d3e289b9f7bb6a2745786", "pr_number": "31777", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml"], "labels": ["merged"]}, "155376721c": {"title": "Pin hypothesis package to 4.57.1 to avoid test failures", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31794\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19266039\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4b1839c4de2b4476c8173a79582c861bf4fa998f", "pr_number": "31794", "files_changed": [".jenkins/pytorch/macos-test.sh"], "labels": ["merged"]}, "dc43f9dc54": {"title": "fix test_backward_node_failure flakiness (#31588)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31588\n\nPer title. This test can sometimes fail with a different error regex\nthan the one that is currently tested, so add this error regex to make the test\npass consistently.\n\nDifferential Revision: D19222275\n\nfbshipit-source-id: 89c95276d4d9beccf9e0961f970493750d78a96b", "pr_number": "31588", "files_changed": ["test/dist_autograd_test.py"], "labels": ["merged"]}, "fa0424f224": {"title": "add LLVM-dev package to android docker image (#31215)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31215\n\nInstall LLVM-dev package for code analysis CI job: #30937\n\nLLVM-dev package is not related to android NDK but the whole code\nanalysis thing is for mobile custom build so choose this docker image.\n\nTest Plan: - wait docker image to build?\n\nDifferential Revision: D19193223\n\nPulled By: ljk53\n\nfbshipit-source-id: 54a79daf8d98fa7c8b9eed11f519e1c7b1614be8", "pr_number": "31215", "files_changed": [".circleci/docker/build.sh"], "labels": ["merged"]}, "fc598f9023": {"title": "generate op dependency graph as python code", "body": "Summary:\nAdd support to print op dependence as python code so that both custom\nbuild script and BUCK can import it without yaml parser.\n\nTest Plan:\n- generate the file:\n```\nANALYZE_TORCH=1 FORMAT=py DEPLOY=1 tools/code_analyzer/build.sh -closure=false\n```\n\n- load the file in python:\n```\npython\n>>> from tools.code_analyzer.generated.torch import TORCH_DEPS\n>>> print(TORCH_DEPS)\n```\n\nDifferential Revision: D18894639\n\nPulled By: ljk53\n\nfbshipit-source-id: e304d0525a07a13cf6e8a9317cd22637200d044c", "pr_number": null, "files_changed": ["tools/code_analyzer/build.sh", "tools/code_analyzer/op_dependency.cpp"], "labels": []}, "5be8dac329": {"title": "Remove non-ascii character from torch/onnx/symbolic_opset11.py", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31814\n\nReviewed By: houseroad\n\nDifferential Revision: D19270742\n\nPulled By: bddppq\n\nfbshipit-source-id: 80800d588e63701d6e1b5838d7ada993f0246a81", "pr_number": "31814", "files_changed": ["torch/onnx/symbolic_opset11.py"], "labels": ["merged"]}, "f39105b68f": {"title": "add num_pending_users to debug info (#31539)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31539\n\nAdding this metric primarily because it is needed to unblock unit\ntests for https://github.com/pytorch/pytorch/pull/31381. It also may be useful\nto look at this metric to see the number of pending RRef forks that currently\nexist.\nghstack-source-id: 96230360\n\nTest Plan: Modified the relevant unit test.\n\nDifferential Revision: D19204158\n\nfbshipit-source-id: 016345e52cd02cc5f46837bffd8d589ba8575f29", "pr_number": "31539", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/rref_context.cpp"], "labels": ["merged"]}, "95cb66570a": {"title": "Erase array sizes from types in c10::str(). (#31683)", "body": "Summary:\nThis dramatically reduces the number of instantiations and eliminates\n~900KB of code from my local build of libtorch_cpu.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31683\n\nDifferential Revision: D19258364\n\nPulled By: resistor\n\nfbshipit-source-id: addb921a26289978ffd14c203325ca7e35a4515b", "pr_number": "31683", "files_changed": ["c10/util/StringUtil.h"], "labels": ["merged"]}, "f56c59ead6": {"title": "clarify when to use `as_tuple` in `torch.nonzero`", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31798\n\nDifferential Revision: D19272332\n\nPulled By: zou3519\n\nfbshipit-source-id: 954d086a7b9f1a719e0dac303a4253bf7ec8e9f4", "pr_number": "31798", "files_changed": ["torch/_torch_docs.py"], "labels": ["merge-this-please", "merged"]}, "8c425dd201": {"title": "Fix race condition when creating build dir (#30956)", "body": "Summary:\nThe original `check-and-act` style can raise `FileExistsError` when multiple processes are jit-compiling the extension on the same node.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30956\n\nDifferential Revision: D19262570\n\nPulled By: ezyang\n\nfbshipit-source-id: bb18c72e42648770b47f9378ac7c3929c3c03efc", "pr_number": "30956", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["merged"]}, "daf00beaba": {"title": "Remove duplicated Numa detection code. (#30628)", "body": "Summary:\ncmake/Dependencies.cmake (https://github.com/pytorch/pytorch/blob/1111a6b8100386af82f5612ef551004188cf396c/cmake/Dependencies.cmake#L595-L609) has already detected Numa. Duplicated detection and variables may lead to\nincorrect results.\n\nClose https://github.com/pytorch/pytorch/issues/29968\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30628\n\nDifferential Revision: D18782479\n\nPulled By: ezyang\n\nfbshipit-source-id: f74441f03367f11af8fa59b92d656c6fa070fbd0", "pr_number": "30628", "files_changed": ["c10/CMakeLists.txt", "c10/macros/cmake_macros.h.in", "c10/util/numa.cpp", "caffe2/core/macros.h.in", "cmake/MiscCheck.cmake"], "labels": ["merged", "module: build"]}, "0b9cd410a9": {"title": "Fix cumsum error for tensors with zero elements (#31694)", "body": "Summary:\nCurrently `cumsum` crashes for tensors with non-empty dimensions but with zero elements, which could happen when some dimension is zero. This commit fixes the error by checking both `dim()` and `numel()` in cumsum backward\n\nFixes https://github.com/pytorch/pytorch/issues/31515\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31694\n\nReviewed By: mrshenli\n\nDifferential Revision: D19266613\n\nPulled By: leedtan\n\nfbshipit-source-id: 9407e0aa55440fed911c01a3580bb6c5eab62a16", "pr_number": "31694", "files_changed": ["test/test_torch.py", "tools/autograd/templates/Functions.cpp"], "labels": ["merged"]}, "68f3782106": {"title": "remove std_single and var_single code in TH (#31608)", "body": "Summary:\nstd_single and var_single in TH never be used, remove them.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31608\n\nDifferential Revision: D19270920\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: e106a42383bf224f7e2c1c092b95484d23af4b0a", "pr_number": "31608", "files_changed": ["aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/THC/generic/THCTensorMathReduce.cu", "aten/src/THC/generic/THCTensorMathReduce.h"], "labels": ["merged"]}, "b47e9b97a2": {"title": "Add op bitwise_and (#31104)", "body": "Summary:\nRefer to https://github.com/pytorch/pytorch/pull/25665,  add `bitwise_and` operator.\nBenchmark script :\n```\nimport timeit\n#for __and__\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__and__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a & b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\")', number=t))\n#for __iand__\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__iand__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a & b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n```\nDevice: **Tesla P100, skx-8180**\nCuda verison: **9.0.176**\n\nBefore:\n```\n__and__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.1766007635742426\ndevice: cpu, dtype: torch.uint8, 100000 times           0.17322628945112228\ndevice: cpu, dtype: torch.int16, 100000 times           0.17650844901800156\ndevice: cpu, dtype: torch.int32, 100000 times           0.17711848113685846\ndevice: cpu, dtype: torch.int64, 100000 times           0.18240160401910543\ndevice: cuda, dtype: torch.int8, 100000 times           1.273967768996954\ndevice: cuda, dtype: torch.uint8, 100000 times          1.2778537990525365\ndevice: cuda, dtype: torch.int16, 100000 times          1.2753686187788844\ndevice: cuda, dtype: torch.int32, 100000 times          1.2797665279358625\ndevice: cuda, dtype: torch.int64, 100000 times          1.2933144550770521\n__and__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.031139614060521126\ndevice: cpu, dtype: torch.uint8, 10000 times            0.03091452084481716\ndevice: cpu, dtype: torch.int16, 10000 times            0.022756479680538177\ndevice: cpu, dtype: torch.int32, 10000 times            0.025045674294233322\ndevice: cpu, dtype: torch.int64, 10000 times            0.024164282716810703\ndevice: cuda, dtype: torch.int8, 10000 times            0.12820732593536377\ndevice: cuda, dtype: torch.uint8, 10000 times           0.12775669433176517\ndevice: cuda, dtype: torch.int16, 10000 times           0.12697868794202805\ndevice: cuda, dtype: torch.int32, 10000 times           0.12832533661276102\ndevice: cuda, dtype: torch.int64, 10000 times           0.1280576130375266\n__iand__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.3687064303085208\ndevice: cpu, dtype: torch.uint8, 100000 times           0.36253443732857704\ndevice: cpu, dtype: torch.int16, 100000 times           0.362891579978168\ndevice: cpu, dtype: torch.int32, 100000 times           0.37680106051266193\ndevice: cpu, dtype: torch.int64, 100000 times           0.3689364707097411\ndevice: cuda, dtype: torch.int8, 100000 times           1.419940729625523\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4247053815051913\ndevice: cuda, dtype: torch.int16, 100000 times          1.4191444097086787\ndevice: cuda, dtype: torch.int32, 100000 times          1.4305962566286325\ndevice: cuda, dtype: torch.int64, 100000 times          1.4567416654899716\n__iand__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.06224383972585201\ndevice: cpu, dtype: torch.uint8, 10000 times            0.06205617543309927\ndevice: cpu, dtype: torch.int16, 10000 times            0.05016433447599411\ndevice: cpu, dtype: torch.int32, 10000 times            0.05216377507895231\ndevice: cpu, dtype: torch.int64, 10000 times            0.06139362137764692\ndevice: cuda, dtype: torch.int8, 10000 times            0.14827249851077795\ndevice: cuda, dtype: torch.uint8, 10000 times           0.14801877550780773\ndevice: cuda, dtype: torch.int16, 10000 times           0.14952312968671322\ndevice: cuda, dtype: torch.int32, 10000 times           0.14999118447303772\ndevice: cuda, dtype: torch.int64, 10000 times           0.14951884001493454\n```\nAfter:\n```\n__and__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.23157884553074837\ndevice: cpu, dtype: torch.uint8, 100000 times           0.23063660878688097\ndevice: cpu, dtype: torch.int16, 100000 times           0.23005440644919872\ndevice: cpu, dtype: torch.int32, 100000 times           0.23748818412423134\ndevice: cpu, dtype: torch.int64, 100000 times           0.24106105230748653\ndevice: cuda, dtype: torch.int8, 100000 times           1.4394256137311459\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4436759827658534\ndevice: cuda, dtype: torch.int16, 100000 times          1.4631587155163288\ndevice: cuda, dtype: torch.int32, 100000 times          1.459101552143693\ndevice: cuda, dtype: torch.int64, 100000 times          1.4784048134461045\n__and__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.028442862443625927\ndevice: cpu, dtype: torch.uint8, 10000 times            0.028130197897553444\ndevice: cpu, dtype: torch.int16, 10000 times            0.025318274274468422\ndevice: cpu, dtype: torch.int32, 10000 times            0.02519288007169962\ndevice: cpu, dtype: torch.int64, 10000 times            0.028299466706812382\ndevice: cuda, dtype: torch.int8, 10000 times            0.14342594426125288\ndevice: cuda, dtype: torch.uint8, 10000 times           0.145280827768147\ndevice: cuda, dtype: torch.int16, 10000 times           0.14673697855323553\ndevice: cuda, dtype: torch.int32, 10000 times           0.14499565307050943\ndevice: cuda, dtype: torch.int64, 10000 times           0.14582364354282618\n__iand__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.25548241566866636\ndevice: cpu, dtype: torch.uint8, 100000 times           0.2552562616765499\ndevice: cpu, dtype: torch.int16, 100000 times           0.25905191246420145\ndevice: cpu, dtype: torch.int32, 100000 times           0.26635489892214537\ndevice: cpu, dtype: torch.int64, 100000 times           0.26269810926169157\ndevice: cuda, dtype: torch.int8, 100000 times           1.485458506271243\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4742380809038877\ndevice: cuda, dtype: torch.int16, 100000 times          1.507783885113895\ndevice: cuda, dtype: torch.int32, 100000 times          1.4926990242674947\ndevice: cuda, dtype: torch.int64, 100000 times          1.519851053133607\n__iand__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.03425929415971041\ndevice: cpu, dtype: torch.uint8, 10000 times            0.03293587639927864\ndevice: cpu, dtype: torch.int16, 10000 times            0.029559112153947353\ndevice: cpu, dtype: torch.int32, 10000 times            0.030915481969714165\ndevice: cpu, dtype: torch.int64, 10000 times            0.03292469773441553\ndevice: cuda, dtype: torch.int8, 10000 times            0.15792148280888796\ndevice: cuda, dtype: torch.uint8, 10000 times           0.16000914946198463\ndevice: cuda, dtype: torch.int16, 10000 times           0.1600684942677617\ndevice: cuda, dtype: torch.int32, 10000 times           0.16162546630948782\ndevice: cuda, dtype: torch.int64, 10000 times           0.1629159888252616\n```\nFix  https://github.com/pytorch/pytorch/issues/24508, https://github.com/pytorch/pytorch/issues/24509,  https://github.com/pytorch/pytorch/issues/24655, https://github.com/pytorch/pytorch/issues/24656.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31104\n\nDifferential Revision: D18938930\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: a77e805a0b84e8ace16c6e648c2f67dad44f2e44", "pr_number": "31104", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/THC/THCTensorMathPairwise.cu", "aten/src/THC/THCTensorMathPointwise.cuh", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPairwise.h", "aten/src/THC/generic/THCTensorMathPointwise.cu", "aten/src/THC/generic/THCTensorMathPointwise.h", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged"]}, "a02a5129a8": {"title": "Move rrelu to Aten(CPU) (#31094)", "body": "Summary:\nVitalyFedyunin, this PR is about port rrelu activation to Aten:\nTest script:\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\n\ndef _time():\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.RReLU(0.1, 0.3).train()\n# for inference\n#m = nn.RReLU(0.1, 0.3).eval()\n#warm up\nfor n in [1, 10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.randn(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [1, 10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.randn(128, n, device=device)\n    fwd_t = 0\n    bwd_t = 0\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\n**Before:**\n```\nTraining:\ninput size(128, 1) forward time is 0.01 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 10) forward time is 0.03 (ms); backwad avg time is 0.04 (ms).\ninput size(128, 100) forward time is 0.17 (ms); backwad avg time is 0.06 (ms).\ninput size(128, 1000) forward time is 1.45 (ms); backwad avg time is 0.07 (ms).\ninferecne:\ninput size(128, 1) forward time is 0.01 (ms).\ninput size(128, 10) forward time is 0.01 (ms).\ninput size(128, 100) forward time is 0.02 (ms).\ninput size(128, 1000) forward time is 0.15 (ms).\n```\n**After:**\n```\nTraining:\ninput size(128, 1) forward time is 0.01 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 10) forward time is 0.03 (ms); backwad avg time is 0.04 (ms).\ninput size(128, 100) forward time is 0.17 (ms); backwad avg time is 0.07 (ms).\ninput size(128, 1000) forward time is 1.43 (ms); backwad avg time is 0.08 (ms).\ninferecne:\ninput size(128, 1) forward time is 0.02 (ms).\ninput size(128, 10) forward time is 0.02 (ms).\ninput size(128, 100) forward time is 0.02 (ms).\ninput size(128, 1000) forward time is 0.03 (ms).\n```\n**OMP_NUM_THREADS=1:**\n```\nBefore:\nTraining:\ninput size(128, 1) forward time is 0.01 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 100) forward time is 0.15 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 1000) forward time is 1.45 (ms); backwad avg time is 0.14 (ms).\ninferecne:\ninput size(128, 1) forward time is 0.01 (ms).\ninput size(128, 10) forward time is 0.01 (ms).\ninput size(128, 100) forward time is 0.02 (ms).\ninput size(128, 1000) forward time is 0.20 (ms).\n\nAfter:\nTraining:\ninput size(128, 1) forward time is 0.01 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 100) forward time is 0.15 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 1000) forward time is 1.43 (ms); backwad avg time is 0.15 (ms).\ninferecne:\ninput size(128, 1) forward time is 0.01 (ms).\ninput size(128, 10) forward time is 0.02 (ms).\ninput size(128, 100) forward time is 0.02 (ms).\ninput size(128, 1000) forward time is 0.06 (ms).\n```\nFix https://github.com/pytorch/pytorch/issues/24755, https://github.com/pytorch/pytorch/issues/24756.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31094\n\nDifferential Revision: D19270936\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 11bb3236b1037a558022d3777d1f9a429af2bffe", "pr_number": "31094", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/THNN/generic/RReLU.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "9c9d3cd550": {"title": "Revert D19262570: Fix race condition when creating build dir", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19262570\n\nOriginal commit changeset: bb18c72e4264\n\nfbshipit-source-id: 40675ef6ef4c98629deaaef0b25956f92534ff50", "pr_number": null, "files_changed": ["torch/utils/cpp_extension.py"], "labels": []}, "0ae063d5d9": {"title": "Fixed concatenation benchmark + added it to the microbenchmarking runs", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31587\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19221813\n\nPulled By: z-a-f\n\nfbshipit-source-id: ee0eb60da7899b23fdc63326302d1e2fd4b540ee", "pr_number": "31587", "files_changed": ["benchmarks/operator_benchmark/benchmark_all_quantized_test.py", "benchmarks/operator_benchmark/pt/qcat_test.py"], "labels": ["merged"]}, "79e30ff3f8": {"title": "optimize index_select performance on CPU with TensorIterator (#30598)", "body": "Summary:\nThis PR aims at improving `index_select` performance on CPU with `TensorIterator`.\nThe code has equally effective optimization for both contiguous tensor and non-contiguous tensor.\nThe code will try to parallel inner loop in case the slice of copy is large enough, otherwise it will parallel on outer loop.\nThus both the user scenarios from DLRM (from `Embedding`) and Fairseq transformer is covered.\n\n1. for contiguous input, single socket: **1.25x** performance speedup\n2. for non-contiguous input, single socket: **799x** performance speedup\n3. for contiguous input, single core: same performance\n4. for non-contiguous input, single core: **31x** performance speedup\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30598\n\nDifferential Revision: D19266892\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 7aaf8e2c861b4a96250c968c4dd95c8d2c5b92d7", "pr_number": "30598", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.h"], "labels": ["merged"]}, "b44c0f328e": {"title": "Skip same tests in ONNX Python3 CI as in Python2 (#31827)", "body": "Summary:\nresolve https://github.com/pytorch/pytorch/issues/31103\n\nvgg models were not tested in Python2 but are turned on in Python3\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31827\n\nReviewed By: houseroad\n\nDifferential Revision: D19274123\n\nPulled By: bddppq\n\nfbshipit-source-id: c48beb574e8b03b2adbd6c9d8ca3f600bee93024", "pr_number": "31827", "files_changed": ["scripts/onnx/test.sh"], "labels": ["merged"]}, "457c57d9f7": {"title": "use unordered_set instead of vector for futureTimeouts key in (#31813)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31813\n\nCloses https://github.com/pytorch/pytorch/issues/31804. We were using\nan `std::vector` for the key for a map that keeps track of futures to mark them\nif they timeout, but we can instead use an `unordered_set`. This results in a\nfaster lookup in the code block where we remove futureIDs from this set when\nthey complete successfully. Previously we were finding them via a linear\n`std::find`. Switching it to a constant time find will help performance in the\ncase where a large number of futures are scheduled to time out at the same\ntime, or if there is no timeout enforced.\n\nTo benchmark a rough perf improvement, I created 50k futures with the same\ntimeout. Before this PR, the lookup `std::find(futuresAtTime.begin(),\nfuturesAtTime.end(), id)` took ~200us, now it takes 1us.\nghstack-source-id: 96251355\n\nTest Plan: Unit tests pass.\n\nDifferential Revision: D19269798\n\nfbshipit-source-id: 1a0fa84a478ee27a16ab0b9fa6f5413b065a663e", "pr_number": "31813", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": ["merged"]}, "86a4e2135d": {"title": "Do not register `const float *` type on utiliy_ops.cu (#31583)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31583\n\nBut rather use `float *`, which is alredy registered\n\nTest Plan: CI\n\nReviewed By: xianjiec\n\nDifferential Revision: D19221405\n\nfbshipit-source-id: eb8eabcf828745022bc1e4185a0e65abd19a8f04", "pr_number": "31583", "files_changed": ["caffe2/operators/utility_ops.cu"], "labels": ["fb-exported", "merged"]}, "40e720282c": {"title": "Using _floats_wrapper in per_channel_tensor generation (#31780)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31780\n\nWe need to specify width to ensure the generated float is representable by `float32`\nfixes: https://github.com/pytorch/pytorch/issues/31774\n\nTest Plan:\nci\n\nImported from OSS\n\nDifferential Revision: D19275165\n\nfbshipit-source-id: 50560b4208c562b6bcd2abccadd234f29fbb4b0a", "pr_number": "31780", "files_changed": [".jenkins/pytorch/macos-test.sh", "test/hypothesis_utils.py", "test/test_quantized.py"], "labels": ["merged"]}, "9407137102": {"title": "Update the descriptive error message for enforce fail (#31575)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31575\n\nWe need a new exception class specifically for the enforce_finite operator, because we need to map it to a specific python exception ExitException, not the RuntimeError type that all c10::Errors get mapped to by default. This diff includes:\n- Define c10::EnforceFiniteNotMet\n- API CAFFE_ENFORCE_FINITE to throw c10::EnforceFiniteNotMet\n- Map from c10::EnforceFiniteNotMet to python ExitException\n- Apply CAFFE_ENFORCE_FINITE in caffe2 op\n\nTest Plan:\n- integration test pass: https://fburl.com/fblearner/xwkzbqyo\n- integration test with D19213617: https://fburl.com/fblearner/479y4jrj Generate error message as desired\n\n- Example:\n  - Original error message  f157597803\n{F225477055}\n\n  - Updated error message  (with D19213617 to generate the error): f158571327\n{F225477071}\n\nReviewed By: zheng-xq\n\nDifferential Revision: D19206240\n\nfbshipit-source-id: bd256862801d5957a26b76d738edf4e531f03827", "pr_number": "31575", "files_changed": ["c10/util/Exception.h", "c10/util/Logging.cpp", "c10/util/Logging.h", "caffe2/operators/enforce_finite_op.h"], "labels": ["fb-exported", "merged"]}, "6b1db202bc": {"title": "Add tanh to c10::cuda::compat (#31844)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31844\n\nAdd tanh to c10::cuda::compat\n\nTest Plan: unittest\n\nReviewed By: bddppq\n\nDifferential Revision: D19277230\n\nfbshipit-source-id: d2cceea58722393ecb90aacec05b692dbb92d467", "pr_number": "31844", "files_changed": ["c10/cuda/CUDAMathCompat.h"], "labels": ["fb-exported", "merged"]}, "c829c6f3d2": {"title": "Disable flaky test_debug_info", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31847\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19278009\n\nPulled By: mrshenli\n\nfbshipit-source-id: 652fa6741a48f35d9f8f54534e84d64fdd96b439", "pr_number": "31847", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "27488773b0": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/8c7c0e201ea82c66b452b3e473e5385c2431b4c7\nhttps://github.com/facebook/folly/commit/b84db9a9718c661208e984b4a4c91e01cc69bf4b\nhttps://github.com/facebook/mcrouter/commit/0524fa0b3604d7eb4489713dd60748cea436a8e1\nhttps://github.com/facebookincubator/katran/commit/2df7b2ba54fd534efaba76aecd70dbf499ffaac6\nhttps://github.com/facebookincubator/mvfst/commit/80553514ed815c150cda5627d18a7fd92a7cfb76\nhttps://github.com/pytorch/fbgemm/commit/4eb66bc7aafd4ad188ccdc261f498b68ba3bc6c8\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 97d0605beabcfc15236038215208acf034f8eba4", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "28c9dd4436": {"title": "fix ProcessGroupGlooTest (#31255)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31255\n\nThis test had 2 issues. A timeout would occasionally happen due to a timeout of 50ms, and CUDA could would get compiled and run on CPU, leading to errors. This PR fixes those issues.\n\nDifferential Revision: D19028231\n\nfbshipit-source-id: e50752228affe0021e7c0caa83bce78d76473759", "pr_number": "31255", "files_changed": ["torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["merged"]}, "e5b7231edc": {"title": "Adding version check for hypothesis deadline", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31262\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19036700\n\nPulled By: z-a-f\n\nfbshipit-source-id: 8e898a6f064dfb4876aa0d3cc299288b5af7b37d", "pr_number": "31262", "files_changed": ["test/hypothesis_utils.py"], "labels": ["merged"]}, "5fe3604987": {"title": "Preserve constant from ConcreteModuleType to ClassType (#29218)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29218\n\nWe need to be able to access constant in module.\n\nTest Plan:\ntbd\n\nImported from OSS\n\nDifferential Revision: D18846847\n\nfbshipit-source-id: 22d2c485c3c449bc14ad798f6e1a0c64fc8fb346", "pr_number": "29218", "files_changed": ["torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/script/concrete_module_type.cpp", "torch/csrc/jit/script/concrete_module_type.h", "torch/csrc/jit/script/python_sugared_value.cpp"], "labels": ["jit", "merged"]}, "b0a2765103": {"title": "move docker image html to correct bucket (#31832)", "body": "Summary:\nsave docker image version to docker.pytorch.org bucket to be served with http://docker.pytorch.org\n\ntest result: https://s3.amazonaws.com/docker.pytorch.org/pytorch.html\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31832\n\nDifferential Revision: D19281263\n\nPulled By: mingbowan\n\nfbshipit-source-id: d906a72d419876c81a570a2086b2d8d2c47d5d17", "pr_number": "31832", "files_changed": [".circleci/ecr_gc_docker/gc.py"], "labels": ["merged"]}, "8420f205ee": {"title": "Remove refs from ArrayRef arguments (#31845)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31845\n\nArrayRef is trivially copyable and should be passed by value. Removing\nunnecessary `&`s.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19278523\n\nPulled By: suo\n\nfbshipit-source-id: 026db693ea98d19246b02c48d49d1929ecb6478e", "pr_number": "31845", "files_changed": ["torch/csrc/jit/fuser/executor.cpp", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/passes/alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.h", "torch/csrc/jit/passes/bailout_graph.cpp", "torch/csrc/jit/passes/utils/memory_dag.cpp", "torch/csrc/jit/passes/utils/memory_dag.h", "torch/lib/c10d/Utils.hpp"], "labels": ["jit", "merged"]}, "2bac76969c": {"title": "Fix getConstant (#31012)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31012\n\n- getConstant should throw when the item is not found\n- add another getConstant which takes slot index as argument\n\nTest Plan:\ntest_class_type.cpp\n\nImported from OSS\n\nDifferential Revision: D18898418\n\nfbshipit-source-id: d3a23a4896fdbf5fa98e1c55c9c4d6205840014b", "pr_number": "31012", "files_changed": ["aten/src/ATen/core/jit_type.h", "test/cpp/jit/test_class_type.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/script/class_type.cpp", "torch/csrc/jit/script/python_sugared_value.cpp"], "labels": ["jit", "merged"]}, "6f62c311a1": {"title": "Add unsafeRemoveConstant for ClassType (#30787)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30787\n\nThis is needed when we fuse conv bn modules,\nwhere we need to rewrite a constant bias (None) of conv to an attribute\nbias of Tensor\n\nTest Plan:\nbuild/bin/test_jit\n\nImported from OSS\n\nDifferential Revision: D18846850\n\nfbshipit-source-id: 9fd5fe85d93d07226e180b75d2e068fe00ca25fe", "pr_number": "30787", "files_changed": ["aten/src/ATen/core/jit_type.h", "test/cpp/jit/test_class_type.cpp", "torch/csrc/jit/script/class_type.cpp"], "labels": ["jit", "merged"]}, "ebe69236d1": {"title": "Expose class constant through `attr` and `setattr` in object (#29219)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29219\n\nWe added class constant in previous PRs, this PR allows access to\nclass constant in the object API\n\nTest Plan:\nbuild/bin/test_jit\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D18846851\n\nfbshipit-source-id: 888a6517d5f747d1f8ced283c0c2c30b2f6c72c6", "pr_number": "29219", "files_changed": ["test/cpp/api/serialize.cpp", "test/cpp/jit/test_module_api.cpp", "test/jit/test_class_type.py", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/script/object.h"], "labels": ["jit", "merged", "topic: quantize script"]}, "5579611544": {"title": "Enable foldbn tests (#29220)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29220\n\nSupport for accessing constant is added in previous\nPRs, this PR re-enables the foldbn tests\n\nTest Plan:\ntest_jit.py\n\nImported from OSS\n\nDifferential Revision: D18846848\n\nfbshipit-source-id: 90ceaf42539ffee80b984e0d8b2420da66c263c3", "pr_number": "29220", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged", "topic: quantize script"]}, "f362cd510d": {"title": "Move prim ops from JIT registration to C10 (#30612)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30612\n\nThe first version to move prim ops to c10 registration. After the reviewers are fine with the initial changes, more operators will be moved in the same style.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19237648\n\nPulled By: iseeyuan\n\nfbshipit-source-id: c5a519604efffb80564a556536f17d829f71d9f9", "pr_number": "30612", "files_changed": ["caffe2/CMakeLists.txt", "test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h", "tools/build_variables.py", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/register_mobile_ops.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/register_prim_ops_c10.cpp"], "labels": ["jit", "merged"]}, "502533cfe6": {"title": "Implement backend-agnostic rpc._wait_all_workers() utility (#30710)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30710\n\nWe need a backend-agnostic mechanism to do barrier-like operation before locally destroy RRef context and shutdown RPC Agent.\n\n- Sort worker names.\n- Elect the first name as the leader in the ordered worker names.\n- Followers reports therir intent to synchronize to the leader.\n- Leader also reports to itself, when `_wait_all_workers()` called.\n- If all workers report their intent to proceed, leader send the command to every one to proceed.\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork -- test_wait_all_workers\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_wait_all_workers$\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rref_leak\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rref_forward_chain\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_wait_all_workers\n\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_wait_all_workers$\n```\n\n# Stress runs\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\n# Debug\n\n```\nbuck test mode/dev-nosan caffe2/test:rpc_fork -- test_shutdown\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:dist_autograd_fork -- test_clean_context_during_backward\n\nbuck build mode/dev-nosan //caffe2/test:dist_autograd_fork\n\nbuck-out/gen/caffe2/test/dist_autograd_fork\\#binary.par -r test_clean_context_during_backward\n```\n\nhttps://our.intern.facebook.com/intern/testinfra/diagnostics/281475127895800.844424945328750.1575664368/\n\n```\nI1206 12:27:47.491420 185619 process_group_agent.cpp:211] Shutting down ProcessGroupAgent.\nI1206 12:27:47.493880 185630 process_group_agent.cpp:211] Shutting down ProcessGroupAgent.\nI1206 12:27:47.494526 185625 process_group_agent.cpp:211] Shutting down ProcessGroupAgent.\nI1206 12:27:47.495390 185636 process_group_agent.cpp:211] Shutting down ProcessGroupAgent.\nE1206 12:27:47.544198 185627 pair.cc:642] 1 --->>> 0, read ERROR: AsyncSocketException: Network error, type = Network error, errno = 104 (Connection reset by peer)\nE1206 12:27:47.544203 185633 pair.cc:642] 2 --->>> 0, read ERROR: AsyncSocketException: Network error, type = Network error, errno = 104 (Connection reset by peer)\nE1206 12:27:47.544210 185639 pair.cc:642] 3 --->>> 0, read ERROR: AsyncSocketException: Network error, type = Network error, errno = 104 (Connection reset by peer)\n```\nThis should mean the UDF in the request has been run, so Python proceeded and ran to `_agent.shutdown()`.\n\nWhile the RpcAgents on followers wanted to send back the response, but the leader has closed RPC.\n\nNeed to re-trigger \"pytorch_rpc-buck\" to reproduce the rare-seen issue.\n\nDifferential Revision: D18643137\n\nfbshipit-source-id: d669d4fc9ad65ed48bed1329a4eb1c32ba51323c", "pr_number": "30710", "files_changed": ["test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "9020d30fc9": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/folly/commit/d7f0e32081e0af0a248ac05dcd4545c5246e3ab3\nhttps://github.com/facebook/proxygen/commit/f2a603d2df05b61182c4bb11b4c7b25d1cdb9ac5\nhttps://github.com/facebook/wangle/commit/323a2bc3e5755738ad31eb9270d689d06ba0d430\nhttps://github.com/facebookincubator/katran/commit/04c07965ef88050aff4579dd53e417bbe219efc8\nhttps://github.com/facebookincubator/mvfst/commit/c179d38294751e8669cd100d391d3821a8a93400\nhttps://github.com/pytorch/fbgemm/commit/6fac956f22e08ad242cf457ac8a87df654596fe3\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 558f35dbf1adb3b45179629c61d77488e441d4e3", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "3f0b330736": {"title": "corrected keyword argument name in docs for Tensor.scatter (#31617)", "body": "Summary:\nSee https://github.com/pytorch/pytorch/issues/31601\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31617\n\nDifferential Revision: D19268872\n\nPulled By: mruberry\n\nfbshipit-source-id: 52f0213f4aab991fd549b7623556a2ced61631a6", "pr_number": "31617", "files_changed": ["torch/_tensor_docs.py"], "labels": ["merged"]}, "fde94e7556": {"title": "Provide async mode for local autograd engine. (#31230)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31230\n\nA major issue with distributed autograd currently is that we block an\nRPC thread when we call Engine::execute_with_graph_task.\n\nTo resolve this issue, I've made modifications to the local autograd engine\nsuch that `execute_with_graph_task` returns a Future instead. The `execute()`\nmethods for Engine::execute() and DistEngine::execute() still wait() on this\nFuture which ensures there is no change in behavior yet.\n\nIn follow up PRs we can modify the distributed autograd engine to take\nadvantage of this Future.\n\nCloses #26359\nghstack-source-id: 96298057\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18999709\n\nfbshipit-source-id: 388f54467fd2415a0acb7df17bd063aedc105229", "pr_number": "31230", "files_changed": ["test/test_utils.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_engine.h", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/utils/future.h"], "labels": ["merged"]}, "33430cf094": {"title": "Revert D18643137: Implement backend-agnostic rpc._wait_all_workers() utility", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18643137\n\nOriginal commit changeset: d669d4fc9ad6\n\nfbshipit-source-id: fe1f8ed77c1c5760638fef06e67ba100b86c33e9", "pr_number": null, "files_changed": ["test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/api.py"], "labels": []}, "1ba1799a66": {"title": "C++ added 3rd arg of false to BatchNorm/InstanceNorm register_parameter \u2026 (#31873)", "body": "Summary:\nFix for issue https://github.com/pytorch/pytorch/issues/31680\nC++ BatchNorm & InstanceNorm attempt to register undefined tensors when affine is false.\n\nFixes https://github.com/pytorch/pytorch/issues/31680\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31873\n\nDifferential Revision: D19287087\n\nPulled By: yf225\n\nfbshipit-source-id: 0d57f10c49083386919b703d72b520a73a8e9e7f", "pr_number": "31873", "files_changed": ["torch/csrc/api/include/torch/nn/modules/batchnorm.h"], "labels": ["merged", "open source"]}, "ddff014b79": {"title": "fixed scale_factor calculation for uint8 tensor (#31778)", "body": "Summary:\nWhen calling the add_images() method on the tensorboard SummaryWriter with a uint8 NCHW tensor, the tensor is incorrectly scaled, resulting in overflow behavior. This leads to incorrect images being displayed in tensorboard.\n\nIssue: https://github.com/pytorch/pytorch/issues/31459\n\nLocal Testing (ran this code with and without the PR changes and printed scale_factor):\n\nimport torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\nx=torch.tensor([[[[1, 2, 3], [4, 5, 6]]]], dtype=torch.uint8)\nwriter.add_images(\"images\", x)\n\nBefore- scale_factor: 255, After- scale_factor: 1\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31778\n\nDifferential Revision: D19289189\n\nPulled By: anjali411\n\nfbshipit-source-id: 350a1650337244deae4fd8f8b7fb0e354ae6986b", "pr_number": "31778", "files_changed": ["test/test_tensorboard.py", "torch/utils/tensorboard/_utils.py"], "labels": ["merged"]}, "1f2b6d632a": {"title": "Refactor tests in pytorch's test/dist_autograd_test.py file (#31803)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31803\n\nRefactored the following fairly similar functions:\n  1. `test_context_cleanup_tensor_with_grad`\n  2. `test_context_cleanup_tensor_no_grad`\n  3. `test_context_cleanup_no_tensors`\nby creating a helper function `context_cleanup_test_helper` that can be invoked with the appropriate arguments.\n\nTest Plan: Verified by running tests.\n\nDifferential Revision: D19269246\n\nfbshipit-source-id: bfb42b078ad56b97ceeecf0d68b4169768c2c453", "pr_number": "31803", "files_changed": ["test/dist_autograd_test.py"], "labels": ["fb-exported", "merged"]}, "c65305e991": {"title": "Add a check method for custom type tensor (#31290)", "body": "Summary:\nFor backend integration, backend (e.g. Glow) needs to check the content of the tensor to determine whether it is a legit byte tensor or some special packed format. This provides a convenient interface for that.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31290\n\nReviewed By: jackm321, qizzzh\n\nDifferential Revision: D19069684\n\nPulled By: yinghai\n\nfbshipit-source-id: 63360fa2c4d32695fe9767a40027d446d63efdd4", "pr_number": "31290", "files_changed": ["aten/src/ATen/cpp_custom_type_hack.h"], "labels": ["merged"]}, "492ca46e71": {"title": "Fix androidTest - exclude host tests from it", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31522\n\nTest Plan: Imported from OSS\n\nReviewed By: dreiss\n\nDifferential Revision: D19200861\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: a6024f3013398f9e0d237e06c984a20493d42f11", "pr_number": "31522", "files_changed": ["android/pytorch_android/build.gradle"], "labels": ["merged"]}, "78cba90a8c": {"title": "Enable constant folding for Reshape (#31054)", "body": "Summary:\nEnabled constant folding for onnx::Reshape\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31054\n\nReviewed By: hl475\n\nDifferential Revision: D18946951\n\nPulled By: houseroad\n\nfbshipit-source-id: 499e8bf5fb091a94f7a27cbdf4311a23b1a6e3d3", "pr_number": "31054", "files_changed": ["test/onnx/test_onnx_opset.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/onnx/test_utility_funs.py", "torch/csrc/jit/passes/onnx/constant_fold.cpp"], "labels": ["jit", "merged", "open source"]}, "112196fdee": {"title": "Fix index put (#31552)", "body": "Summary:\nThis change is required for cases like:\nx[1:] = data or x[:3] = data\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31552\n\nReviewed By: hl475\n\nDifferential Revision: D19238815\n\nPulled By: houseroad\n\nfbshipit-source-id: 56c9837d86b341ea92b0a71d55034ce189d12e6c", "pr_number": "31552", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.cpp"], "labels": ["jit", "merged", "open source"]}, "a9dae70bae": {"title": "Remove LibIRC logic from cmake. (#31152)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31152\n\nPer apaszke: I can't find any reasonable references to libIRC online, so\nI decided to remove this.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19262582\n\nPulled By: ezyang\n\nfbshipit-source-id: a1d47462427a3e0ca469062321d608e0badf8548", "pr_number": "31152", "files_changed": ["cmake/Modules/FindMKL.cmake"], "labels": ["merged"]}, "4ef9daf7b2": {"title": "Remove dead CAFFE2_LIBS variable (#31155)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31155\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19262584\n\nPulled By: ezyang\n\nfbshipit-source-id: 147ac5a9c36e813ea9a2f68b498880942d661be5", "pr_number": "31155", "files_changed": ["setup.py"], "labels": ["merged"]}, "58cffbff91": {"title": "Add missing TORCH_CUDA_API annotation to throw_nccl_error (#31157)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31157\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19262583\n\nPulled By: ezyang\n\nfbshipit-source-id: 8fb87b41ab53770329b38e1e2fe679fb868fee12", "pr_number": "31157", "files_changed": ["torch/csrc/cuda/nccl.h"], "labels": ["merged"]}, "5d80f63478": {"title": "no_grad, enable_grad: support for decorating generator functions (#31792)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/31497\n\nThis allows `torch.no_grad` and `torch.enable_grad` to be used as decorators for generator functions. In which case it disables/enables grad only inside the body of the generator and restores the context outside of the generator.\n\nhttps://github.com/pytorch/pytorch/issues/31497 doesn't include a complete reproducer but the included test with `torch.is_grad_enabled` show this is working where it failed before.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31792\n\nDifferential Revision: D19274971\n\nPulled By: albanD\n\nfbshipit-source-id: fde6d3fd95d76c8d324ad02db577213a4b68ccbe", "pr_number": "31792", "files_changed": ["test/test_autograd.py", "torch/autograd/grad_mode.py"], "labels": ["merged", "open source"]}, "34561dadcd": {"title": "Don't handle bias inside cudnn_convolution* (#31524)", "body": "Summary:\nCompared to cuDNN bias, PyTorch add has the following advantage:\n- faster, especially for backward (see: https://github.com/zasdfgbnm/things/blob/master/2019/conv-backward-profile.md)\n- handles 64bit indexing automatically\n- has less code, less maintenance effort\n\nngimel I submit this PR early so the CI could start building it. But I have not tested it locally yet (still waiting for compiling).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31524\n\nDifferential Revision: D19264244\n\nPulled By: ngimel\n\nfbshipit-source-id: cb483d378a6d8bce0a05c3643a796e544bd8e8f0", "pr_number": "31524", "files_changed": ["aten/src/ATen/native/ConvUtils.h", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cudnn/Conv.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "tools/autograd/derivatives.yaml"], "labels": ["merged", "open source"]}, "a561a8448b": {"title": "minor doc tweak to use mp.spawn in example (#30381)", "body": "Summary:\nPer pietern's comment in https://github.com/pytorch/pytorch/issues/30022, we can make this example launcher a bit simpler by using `torch.multiprocessing`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30381\n\nDifferential Revision: D19292080\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 018ace945601166ef3af05d8c3e69d900bd77c3b", "pr_number": "30381", "files_changed": ["docs/source/notes/distributed_autograd.rst"], "labels": ["merged"]}, "985fd970aa": {"title": "Enable BFloat16 support for Convolutions on ROCm (#30948)", "body": "Summary:\nThis PR adds bfloat16 support for convolutions on ROCm.\n\n- Intergrates MIOpen bfloat16 convolution support into PyTorch\n\n- Enables bfloat16 convolution for non-miopen paths, i.e THCUNN, native hip kernels\n\n- Enables bfloat16 type for probability distribution functions(this is included in this PR since conv unit tests use bfloat16 random number generators)\n\nNative cuda kernels for convolution and random functions will be compiled for CUDA as well.\n\niotamudelta bddppq\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30948\n\nDifferential Revision: D19274164\n\nPulled By: ezyang\n\nfbshipit-source-id: c0888a6ac72a2c5749b1ebb2195ac6f2209996be", "pr_number": "30948", "files_changed": ["aten/src/ATen/AccumulateType.h", "aten/src/ATen/Dispatch.h", "aten/src/ATen/miopen/Descriptors.cpp", "aten/src/ATen/miopen/Descriptors.h", "aten/src/ATen/miopen/Types.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu", "aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu", "aten/src/ATen/nn.yaml", "aten/src/ATen/nn_parse.py", "aten/src/THCUNN/SpatialConvolutionMM.cu", "aten/src/THCUNN/SpatialDepthwiseConvolution.cu", "aten/src/THCUNN/THCUNN.h", "aten/src/THCUNN/generic/SpatialConvolutionMM.cu", "aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu", "test/common_utils.py", "test/test_nn.py", "test/test_torch.py"], "labels": ["merged", "module: rocm", "open source"]}, "346a349111": {"title": "Update all instances of 1.4.0 -> 1.5.0 (#31785)", "body": "Summary:\nDone with:\n\n```\n\u276f sed -i 's/1\\.4\\.0/1.5.0/g' $(find -type f -not -path \"./third_party/*\")\n```\n\nThis was previously done in separate commits, but it would be beneficial to bump all included projects within this repository at the same time.\n\nOld bumps for reference:\n* [iOS]Update Cocoapods to 1.4.0: https://github.com/pytorch/pytorch/pull/30326\n* [android] Change nightly builds version to 1.4.0-SNAPSHOT: https://github.com/pytorch/pytorch/pull/27381\n* Roll master to 1.4.0: https://github.com/pytorch/pytorch/pull/27374\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31785\n\nDifferential Revision: D19277925\n\nPulled By: seemethere\n\nfbshipit-source-id: f72ad082f0566004858c9374879f4b1bee169f9c", "pr_number": "31785", "files_changed": [".circleci/scripts/binary_populate_env.sh", "android/README.md", "android/gradle.properties", "android/test_app/app/build.gradle", "ios/LibTorch.podspec", "version.txt"], "labels": ["merged"]}, "20c5dd59bd": {"title": "Add stub for transformer.py and MultiheadAttention Class. (#28396)", "body": "Summary:\nAdd stub for `transformer.py` and `class MultiheadAttention`. Add import for `transformer.py`  and `class MultiheadAttention` in `__init__.pyi.in`. I've tested the code hint in PyCharm and all works file.\nRelate issue: [https://github.com/pytorch/pytorch/issues/27842](https://github.com/pytorch/pytorch/issues/27842)\nezyang\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28396\n\nDifferential Revision: D19300287\n\nPulled By: ezyang\n\nfbshipit-source-id: 1a79d6518b5edd4643892c46a959108385c739ad", "pr_number": "28396", "files_changed": ["torch/nn/modules/__init__.pyi.in", "torch/nn/modules/activation.pyi.in", "torch/nn/modules/transformer.pyi.in"], "labels": ["merged", "open source"]}, "22044c6f7c": {"title": "Use TORCH_CHECK instead of AT_ASSERT in torch::cuda::gather() (#27456)", "body": "Summary:\nThe error message produced by AT_ASSERT() in gather() encouraged users to file a bug report (\"please report a bug to PyTorch...\"). The assertion should be a regular argument check since it can be triggered by passing tensors with different dimensionality, e.g. `torch.cuda.comm.gather([torch.rand(1, device='cuda'), torch.rand(1, 1, device='cuda')])`.\n\nSee: https://github.com/pytorch/pytorch/issues/26400\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27456\n\nDifferential Revision: D19300270\n\nPulled By: ezyang\n\nfbshipit-source-id: ec87d225e23445020b377521e0daccceb4748215", "pr_number": "27456", "files_changed": ["torch/csrc/cuda/comm.cpp"], "labels": ["merged", "module: cuda", "open source"]}, "809ee9d04c": {"title": "Enable personalized FC weight_init and sparse_emb weight_init (#31707)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31707\n\nChange the initialization value for FC weight init and sparse embedding lookup init.\n\nPrevious default initialization is uniform(-\\sqrt(1/input_dim), \\sqrt(1/input_dim)); Now pass into a flexible hyperparameter, say \\alpha into it, to change into uniform(-\\sqrt(\\alpha/input_dim), \\sqrt(\\alpha/input_dim));\n\nReviewed By: chonglinsun\n\nDifferential Revision: D18825615\n\nfbshipit-source-id: 4c5f2e07f2b3f5d642fd96d64dbf68892ebeb30b", "pr_number": "31707", "files_changed": ["caffe2/python/layers/fc.py", "caffe2/python/layers/fc_without_bias.py", "caffe2/python/layers/sparse_lookup.py"], "labels": ["fb-exported", "merged"]}, "3c7db5ccbc": {"title": "Don't unconditionally compile runJITCPPTests (#31236)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31236\n\nIt is not compiled on Windows\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19262581\n\nPulled By: ezyang\n\nfbshipit-source-id: 80bfa553333a946f00291aaca6ad26313caaa9e6", "pr_number": "31236", "files_changed": ["torch/CMakeLists.txt", "torch/csrc/jit/init.cpp"], "labels": ["jit", "merged"]}, "2f5eefe525": {"title": "Raise ValueError if CUDA device is specified without specifying the : (#29087)", "body": "Summary:\nFix for https://github.com/pytorch/pytorch/issues/19076\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29087\n\nDifferential Revision: D19298959\n\nPulled By: ezyang\n\nfbshipit-source-id: 878ea4840682012f07177d8d159a77c0e5afada6", "pr_number": "29087", "files_changed": ["test/test_cuda.py", "torch/serialization.py"], "labels": ["merged", "open source"]}, "4ee9c56218": {"title": "Support PyTorch ROCm CI on Ubuntu18.04 (#31886)", "body": "Summary:\nIn order to support Ubuntu18.04, some changes to the scripts are required.\n* install dependencies with -y flag\n* mark install noninteractive\n* install some required dependencies (gpg-agent, python3-distutils, libidn11)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31886\n\nDifferential Revision: D19300586\n\nPulled By: bddppq\n\nfbshipit-source-id: d7fb815a3845697ce63af191a5bc449d661ff1de", "pr_number": "31886", "files_changed": ["docker/caffe2/jenkins/common/install_clang.sh", "docker/caffe2/jenkins/common/install_python.sh", "docker/caffe2/jenkins/common/install_rocm.sh", "docker/caffe2/jenkins/ubuntu-rocm/Dockerfile"], "labels": ["merged", "module: rocm", "open source"]}, "5cc62f2913": {"title": "Ensure autograd callbacks are called only once for reentrant backward. (#31909)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31909\n\nhttps://github.com/pytorch/pytorch/pull/31230 introduced a bug where\nwe would end up calling `graph_task_post_processing` twice for reentrant\nbackward calls (once when we mark the future completed and then we we called\ngraph_task_post_processing in execute_with_graph_task).\n\nThis PR fixes the issues by verifying the future we return in that case is\ncompleted and we remove the call to graph_task_post_processing.\n\nIn addition to that I added a test that reproduced the problem and verified it\nis fixed by this PR.\nghstack-source-id: 96349102\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19296363\n\nfbshipit-source-id: dc01a4e95989709ad163bb0357b1d191ef5a4fb2", "pr_number": "31909", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/engine.cpp"], "labels": ["merged"]}, "0e5a6700cc": {"title": "Emit warning from deprecated torch function signatures (#31514)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/28430\n\nThe unpythonic signatures for functions such as `torch.addcdiv` are already seperated in [`deprecated.yaml`] and the signatures marked as deprecated in `PythonArgParser`. However, nothing was done with this information previously. So, this now emits a warning when the deprecated signatures are used.\n\nOne minor complication is that if all arguments are passed as keyword args then there is nothing to differentiate the deprecated overload. This can lead to false warnings being emitted. So, I've also modified `PythonArgParser` to prefer non-deprecated signatures.\n\n[`deprecated.yaml`]: https://github.com/pytorch/pytorch/blob/master/tools/autograd/deprecated.yaml\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31514\n\nDifferential Revision: D19298735\n\nPulled By: ezyang\n\nfbshipit-source-id: 03cb78af17658eaab9d577cd2497c6f413f07647", "pr_number": "31514", "files_changed": ["test/test_torch.py", "tools/autograd/gen_python_functions.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": ["merged", "open source", "triaged"]}, "bf8e1c0710": {"title": "Integrate async mode for autograd engine with distributed autograd. (#31508)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31508\n\nThis PR builds on top of https://github.com/pytorch/pytorch/pull/31230\nto ensure that distributed autograd doesn't block an RPC thread anymore during\nthe backward pass.\n\nI've also added a unit test where all ranks hammer rank 0 without about 60\nbackward calls (which would cause a deadlock earlier), but now such a test\npasses without any issues.\nghstack-source-id: 96345097\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19188749\n\nfbshipit-source-id: b21381b38175699afd0f9dce1ddc8ea6a220f589", "pr_number": "31508", "files_changed": ["test/dist_autograd_test.py", "test/dist_utils.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp"], "labels": ["merged"]}, "c888473b57": {"title": "Restructure docs organization and naming (#31849)", "body": "Summary:\n* Rename \u201cOther Languages\u201d \u2192 \u201cLanguage Bindings\u201d\n* Move the Community section to the bottom\n* Move \"Language Bindings\" above \"Python API\"\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31849\n\nDifferential Revision: D19290966\n\nPulled By: jlin27\n\nfbshipit-source-id: 30b579e032a9fb1636e4afc7bbbd85a2708f637d", "pr_number": "31849", "files_changed": ["docs/source/index.rst"], "labels": ["merged"]}, "74d69e296e": {"title": "Raise an error if torch.cat is given `out` as one of the input tensors (#30577)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30562 for both cpu and cuda.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30577\n\nDifferential Revision: D19298732\n\nPulled By: ezyang\n\nfbshipit-source-id: ea539c97493ee17d8f60b1134d100a44c8717578", "pr_number": "30577", "files_changed": ["aten/src/ATen/MemoryOverlap.cpp", "aten/src/ATen/MemoryOverlap.h", "aten/src/TH/generic/THTensor.cpp", "aten/src/THC/generic/THCTensorMath.cu", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "3a2757c682": {"title": "Fix tracing for modules with List[Tensor] as output (#31343)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31343\n\nFix an issue in TorchScript tracing for modules with `c10::List<at::Tensor>` as an output. TensorList was not supported properly.\n\nTest Plan: unit tests\n\nReviewed By: wanchaol\n\nDifferential Revision: D18850722\n\nfbshipit-source-id: 87a223104d1361fe754d55deceeb1e8bbcad629b", "pr_number": "31343", "files_changed": ["test/test_jit.py", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h"], "labels": ["fb-exported", "jit", "merged"]}, "bb279c5c63": {"title": "named tensor max pooling support", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31669\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19240348\n\nPulled By: glaringlee\n\nfbshipit-source-id: 004387aa753e4e41afdede66647abbb0bcbd9808", "pr_number": "31669", "files_changed": ["aten/src/ATen/native/DilatedMaxPool2d.cpp", "aten/src/ATen/native/DilatedMaxPool3d.cpp", "aten/src/ATen/native/Pooling.cpp", "aten/src/ATen/native/cuda/DilatedMaxPool2d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_namedtensor.py"], "labels": ["merged"]}, "ca72df06ae": {"title": "disable __torch_function__ overides for operators in torch.functional (#30839)", "body": "Summary:\nFor now I'm just removing the decorators from all of the currently overridable functions in `torch.functional`. This means they are no longer overridable, however this should fix the benchmark regressions reported in https://github.com/pytorch/pytorch/issues/30831. Moving forward we'll be looking at reducing the overhead of the python-level override mechanism and failing that, re-implementing all of these operators in C++.\n\ncc hl475\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30839\n\nDifferential Revision: D18838848\n\nPulled By: ezyang\n\nfbshipit-source-id: 22b8015d7b2f7a947f1ebc9632c998e081b48ad8", "pr_number": "30839", "files_changed": ["test/test_overrides.py", "torch/functional.py"], "labels": ["merged", "open source"]}, "114562cf93": {"title": "For torch::from_blob() add clue when memory is non-owned. (#31222)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31222\n\n - When constructing torch::from_blob() in the case where the deleter is a nop, switch to using a nullptr context in the DataPtr (with a nop deleter)\n\n - No real extra memory/cpu requirements here, actually saves a minor alloc.\n\nWhy? Trying to get a signal that a Tensor might contain non-owned memory from\ntorch::from_blob(), by detecting the nullptr context.\nghstack-source-id: 96336078\n\nTest Plan:\nbuck test mode/dev caffe2/test/cpp/api/...\n   buck test mode/dev-nosan caffe2/test/...\n\nDifferential Revision: D18992119\n\nfbshipit-source-id: 4eea642f82d0858b57fdfc6995364a760c10567d", "pr_number": "31222", "files_changed": ["aten/src/ATen/templates/Functions.h", "test/cpp/api/tensor.cpp", "tools/autograd/templates/variable_factories.h"], "labels": ["merged"]}, "8a0503b355": {"title": "Run a non-quiet submodule update to prevent timeouts on Circle CI (#31900)", "body": "Summary:\nAs in title, this PR will disable the `--quiet` flag used in the CI as a workaround to a timeout hitting Mac OS CI.  Circle CI works by timing out when no text has been printed for 10 min.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31900\n\nDifferential Revision: D19302899\n\nPulled By: bwasti\n\nfbshipit-source-id: 145647da983ee06f40794bda1abd580ea45a0019", "pr_number": "31900", "files_changed": [".circleci/scripts/binary_checkout.sh"], "labels": ["merged"]}, "227d1a43a4": {"title": "Revert D18838848: disable __torch_function__ overides for operators in torch.functional", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18838848\n\nOriginal commit changeset: 22b8015d7b2f\n\nfbshipit-source-id: fdaeffcd112990ed379782cf7216d3f1beeb2cb1", "pr_number": null, "files_changed": ["test/test_overrides.py", "torch/functional.py"], "labels": []}, "a730920a3d": {"title": "Make RRef leak detection always print a warning log (#31922)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31922\n\nFor better debugging, `test_rref_leak` failure in https://app.circleci.com/jobs/github/pytorch/pytorch/4135881, as per discussion in https://github.com/pytorch/pytorch/pull/31888.\n\nghstack-source-id: 96375261\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_wait_all_workers\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rref_leak\n```\n\n# Stress runs\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\nDifferential Revision: D19302814\n\nfbshipit-source-id: 51632aede98e01689f8bc0f266788a9b020daa15", "pr_number": "31922", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/rref_context.cpp"], "labels": ["merged"]}, "3c07eb33bb": {"title": "Better error for `torch::jit::load`ing a eager file (#31709)", "body": "Summary:\nThis adds a check to catch the case where someone `torch.save`s something then `torch::jit::load`s it in C++.\n\nRelevant for #31620\n](https://our.intern.facebook.com/intern/diff/19252172/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31709\n\nPulled By: driazati\n\nDifferential Revision: D19252172\n\nfbshipit-source-id: f2a9b4442647285418b2778306629b4ff77c15e5", "pr_number": "31709", "files_changed": ["test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests.h", "test/cpp/jit/tests_setup.py", "test/cpp/jit/torch_python_test.cpp", "torch/csrc/jit/import.cpp"], "labels": ["jit", "merged"]}, "6d1fa8296b": {"title": "Support tensors with empty shape in Java", "body": "Summary: These are valid tensors.\n\nTest Plan: New unit test.\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D19221362\n\nfbshipit-source-id: fa9af2fc539eb7381627b3d473241a89859ef2ba", "pr_number": null, "files_changed": ["android/pytorch_android/src/androidTest/java/org/pytorch/PytorchTestBase.java", "android/pytorch_android/src/main/java/org/pytorch/Tensor.java", "android/pytorch_android/test_asset.jit"], "labels": []}, "2d6a2c898c": {"title": "Support tensors with a storage offset in Java (#31584)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31584\n\nThese were returning incorrect data before.\n\nTest Plan: New unit test.\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D19221360\n\nfbshipit-source-id: b3f01de086857027f8e952a1c739f60814a57acd", "pr_number": "31584", "files_changed": ["android/pytorch_android/src/androidTest/java/org/pytorch/PytorchTestBase.java", "android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/test_asset.jit"], "labels": ["fb-exported", "merged"]}, "1b4d3d5748": {"title": "Properly return data from non-contiguous tensors in Java", "body": "Summary:\nThese were returning incorrect data before.  Now we make a contiguous copy\nbefore converting to Java.  Exposing raw data to the user might be faster in\nsome cases, but it's not clear that it's worth the complexity and code size.\n\nTest Plan: New unit test.\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D19221361\n\nfbshipit-source-id: 22ecdad252c8fd968f833a2be5897c5ae483700c", "pr_number": null, "files_changed": ["android/pytorch_android/src/androidTest/java/org/pytorch/PytorchTestBase.java", "android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/test_asset.jit"], "labels": []}, "4daa3dedbe": {"title": "Fix IValue.isList", "body": "Summary: I think this was wrong before?\n\nTest Plan: Not sure.\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D19221358\n\nfbshipit-source-id: 27e675cac15dde29e026305f4b4e6cc774e15767", "pr_number": null, "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/IValue.java"], "labels": []}, "c21f89970f": {"title": "Remove c++14-conditional constexpr (#30916)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30916\n\nThese macros said \"make it constexpr if we're in C++14\". Since we're now always C++14, we can just say \"constexpr\" isntead.\nghstack-source-id: 96369584\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869635\n\nfbshipit-source-id: f41751e4e26fad6214ec3a98db2d961315fd73ff", "pr_number": "30916", "files_changed": ["c10/macros/Macros.h", "c10/test/util/string_view_test.cpp", "c10/util/Array.h", "c10/util/ArrayRef.h", "c10/util/reverse_iterator.h", "c10/util/string_view.h"], "labels": ["merged"]}, "0dca9c30ca": {"title": "constexpr typeid improvements (#31312)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31312\n\nghstack-source-id: 96369343\n\nTest Plan: unit tests\n\nDifferential Revision: D19087198\n\nfbshipit-source-id: 7f9a7169f11973759b9ecabcc755c211d34e2742", "pr_number": "31312", "files_changed": ["c10/util/typeid.h"], "labels": ["merged"]}, "ab60cca488": {"title": "Make c10::util::get_fully_qualified_type_name() backwards compatible with clang 4 (#31351)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31351\n\nClang 4 needs the c10:: namespace specifier on fully_qualified_type_name_impl() to work correctly.\n\nAlso, let's add an error message for people using clang 3 and earlier, we don't support those compilers anymore but before this PR, they got a crappy message.\nghstack-source-id: 96380163\n\nTest Plan: testinprod\n\nDifferential Revision: D19135587\n\nfbshipit-source-id: c206b56240b36e5c207fb2b69c389bb39f1e62aa", "pr_number": "31351", "files_changed": ["c10/util/C++17.h", "c10/util/TypeIndex.h"], "labels": ["merged"]}, "9116f02beb": {"title": "Rename TORCH_DCHECK to TORCH_INTERNAL_ASSERT_DEBUG_ONLY (#31917)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31917\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19301480\n\nPulled By: ezyang\n\nfbshipit-source-id: fcce8868733965b9fbd326b4ec273135759df377", "pr_number": "31917", "files_changed": ["c10/test/util/exception_test.cpp", "c10/util/Exception.h", "c10/util/intrusive_ptr.h"], "labels": ["merged"]}, "6664703842": {"title": "Implement backend-agnostic rpc._wait_all_workers() utility (#31888)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31888\n\nWe need a backend-agnostic mechanism to do barrier-like operation before locally destroy RRef context and shutdown RPC Agent.\n\n- Sort worker names.\n- Elect the first name as the leader in the ordered worker names.\n- Followers reports therir intent to synchronize to the leader.\n- Leader also reports to itself, when `_wait_all_workers()` called.\n- If all workers report their intent to proceed, leader send the command to every one to proceed.\nghstack-source-id: 96386210\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_wait_all_workers\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rref_leak\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_wait_all_workers\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_worker_id\n```\n\n# Stress runs\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\nDifferential Revision: D19290954\n\nfbshipit-source-id: cdb22203c2f27b5e0d0ad5b2d3b279d438c22dcf", "pr_number": "31888", "files_changed": ["test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "ee817012b2": {"title": "Add more tests to the autograd wrt view and inplace (#31147)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31147\n\nThe goal here is to add more tests of the current behavior of the autograd to make sure no regressions are introduced when modifying it.\nDo let me know if you think of other corner cases I missed.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19301082\n\nPulled By: albanD\n\nfbshipit-source-id: 2cb07dcf99e56eb1f2c56a179796f2e6042d5a2d", "pr_number": "31147", "files_changed": ["test/test_autograd.py"], "labels": []}, "84dfa96f62": {"title": "Fix -Wundef warning in conversions.h", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31911\n\nTest Plan:\n* CI builds including GPU and OSS-build tests\n* The `defined(__HIP_DEVICE_COMPILE__) ` instance a few lines below is proof that this is a define/undef flag, not a define01 flag\n\nReviewed By: hlu1\n\nDifferential Revision: D19296560\n\nfbshipit-source-id: 1c45069aec534b0bf4a87751a74680675c985e06", "pr_number": "31911", "files_changed": ["caffe2/utils/conversions.h"], "labels": ["fb-exported", "merged"]}, "2a294aace6": {"title": "Remove memory ordering from LeftRight (#31026)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31026\n\nThis is error prone and probably wrong. Since we don't use LeftRight on the hot path anymore, let's remove this.\nghstack-source-id: 96369644\n\nTest Plan: none\n\nDifferential Revision: D18902165\n\nfbshipit-source-id: 7b9478cd7cc071f403d75da20c7c889c27248b5c", "pr_number": "31026", "files_changed": ["c10/util/LeftRight.h"], "labels": ["merged"]}, "f67851d69a": {"title": "Fix c10::util::get_fully_qualified_type_name for MSVC (#31313)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31313\n\nThis is a bugfix. The reason we couldn't enable the constexpr-ness for it before is that it was buggy,\nand without constexpr it crashed at runtime and not at compile time which seems to have passed our CI unfortunately...\nghstack-source-id: 96380160\n\nTest Plan: Now it works even when enabling constexpr for it\n\nDifferential Revision: D19087471\n\nfbshipit-source-id: 28be107389f4507d35d08eab4b089a405690529b", "pr_number": "31313", "files_changed": ["c10/test/util/TypeIndex_test.cpp", "c10/util/TypeIndex.h"], "labels": ["merged"]}, "f0072b3af5": {"title": "Remove C++11 compatibility from c10::optional (#30919)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30919\n\ndeletecode\nghstack-source-id: 96383227\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869641\n\nfbshipit-source-id: c08345d17a291cea3749af20473b6acddc78ab27", "pr_number": "30919", "files_changed": ["c10/util/Optional.h"], "labels": ["merged"]}, "c66ca74f03": {"title": "Add device debug info to CUDA build (#31929)", "body": "Summary:\nAlso print NVCC flags in the summary\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31929\n\nDifferential Revision: D19312079\n\nPulled By: ezyang\n\nfbshipit-source-id: cd20d5a385f61174c1907a9ad883c04de66ef037", "pr_number": "31929", "files_changed": ["CMakeLists.txt", "cmake/Summary.cmake"], "labels": ["merged", "open source"]}, "7f723cbd8a": {"title": "Revert D19290954: Implement backend-agnostic rpc._wait_all_workers() utility", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19290954\n\nOriginal commit changeset: cdb22203c2f2\n\nfbshipit-source-id: 2ae194a06a645e4f48879271eccf0588b0956cd3", "pr_number": null, "files_changed": ["test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/api.py"], "labels": []}, "54777b1e73": {"title": "Avoid reference invalidation in cuda SpectralOps' plan_caches (#31861)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31412\n\nThe root cause is `plan_caches` being resized in one thread while another holds a reference to an existing `CuFFTParamsLRUCache` which then becomes invalidated.\n\nI was able to reproduce the crash very reliably without this fix applied and no longer see it. Being a race condition, it's hard to say for sure though.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31861\n\nDifferential Revision: D19312314\n\nPulled By: ezyang\n\nfbshipit-source-id: 06e4561128d503f2d70cdfe1982be0f3db2a8cf8", "pr_number": "31861", "files_changed": ["aten/src/ATen/native/cuda/SpectralOps.cu"], "labels": ["merged", "open source"]}, "620060cb0c": {"title": "Quantized H Tangent function (#31031)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31031\n\nThis activation will be needed for the LSTM implementation.\nAlso includes the QNNPack implementation.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18903453\n\nPulled By: z-a-f\n\nfbshipit-source-id: 0050b1cebb1ddb179b7ecbcb114fe70705070f67", "pr_number": "31031", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/src/tanh.c", "aten/src/ATen/native/quantized/cpu/qnnpack/test/tanh-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qtanh.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/test_quantized.py"], "labels": ["merged"]}, "5dfcfeebb8": {"title": "Revert D19298735: Emit warning from deprecated torch function signatures", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19298735\n\nOriginal commit changeset: 03cb78af1765\n\nfbshipit-source-id: 304a6d4412f53a8fc822d36897c96815432e0f70", "pr_number": null, "files_changed": ["test/test_torch.py", "tools/autograd/gen_python_functions.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": []}, "462bfc7fe7": {"title": "docker hub image info (#31923)", "body": "Summary:\nresult: http://docker.pytorch.org/docker_hub.html\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31923\n\nDifferential Revision: D19316770\n\nPulled By: mingbowan\n\nfbshipit-source-id: 57f34d8983d26772bb0d310fa0a4085674c860e5", "pr_number": "31923", "files_changed": [".circleci/config.yml", ".circleci/ecr_gc_docker/Dockerfile", ".circleci/ecr_gc_docker/docker_hub.py", ".circleci/ecr_gc_docker/requirements.txt", ".circleci/validate-docker-version.py", ".circleci/verbatim-sources/docker_jobs.yml", ".circleci/verbatim-sources/workflows-ecr-gc.yml"], "labels": ["merged"]}, "c299cb05ef": {"title": "temporary fix for jit test backward compatibility issues", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31949\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19314763\n\nPulled By: albanD\n\nfbshipit-source-id: b5eff0ed53a371d260596ca85d914c8bddb0a8aa", "pr_number": "31949", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "1314f7f4f4": {"title": "Ensure the original grad_mode is restored during backward (#31884)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31884\n\nFix #31715\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19301076\n\nPulled By: albanD\n\nfbshipit-source-id: 2d20c01bfb6364fa96c8fe5aa5ce7ea39defa3ce", "pr_number": "31884", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/engine.cpp"], "labels": ["merged"]}, "8b4feff01d": {"title": "Use simd version for fp16 conversions (#31897)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31897\n\nPrevious version only use avx2. The _simd version uses avx512 if CPU is capable of that.\n\nTest Plan: Unitttest\n\nReviewed By: tracelogfb\n\nDifferential Revision: D19291499\n\nfbshipit-source-id: 3b1ee0ba756e5c9defbd5caf7f68982d9b2ca06c", "pr_number": "31897", "files_changed": ["caffe2/operators/half_float_ops.cc"], "labels": ["fb-exported", "merged"]}, "d2fdf140af": {"title": "Combine all the user inputs together and convert them to fp16 (#31898)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31898\n\nAtt\n\nReviewed By: tracelogfb\n\nDifferential Revision: D19291357\n\nfbshipit-source-id: 747ed5234ca042ceeaff2d094701ead7597ac3ee", "pr_number": "31898", "files_changed": ["caffe2/opt/custom/glow_net_transform.cc", "caffe2/opt/onnxifi_transformer.cc", "caffe2/opt/onnxifi_transformer.h", "caffe2/python/onnx/onnxifi.py", "caffe2/python/pybind_state.cc"], "labels": ["fb-exported", "merged"]}, "4f9d2f74e2": {"title": "Port softplus activation to Aten(CPU+CUDA) (#30504)", "body": "Summary:\nVitalyFedyunin, This PR is about port Softplus activation to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.Softplus()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    fwd_t = 0\n    bwd_t = 0\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) forward time is 0.06 (ms); backwad avg time is 0.12 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.18 (ms).\nCPU:\ninput size(128, 100) forward time is 1.16 (ms); backwad avg time is 0.69 (ms).\ninput size(128, 10000) forward time is 60.19 (ms); backwad avg time is 31.86 (ms).\n```\nAfter:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU:\ninput size(128, 100) forward time is 0.43 (ms); backwad avg time is 0.16 (ms).\ninput size(128, 10000) forward time is 1.65 (ms); backwad avg time is 0.83 (ms).\n```\n`OMP_NUM_THREADS=1:`\n```\nBefore:\ninput size(128, 100) forward time is 0.53 (ms); backwad avg time is 0.28 (ms).\ninput size(128, 10000) forward time is 51.33 (ms); backwad avg time is 25.48 (ms).\nAfter:\ninput size(128, 100) forward time is 0.44 (ms); backwad avg time is 0.16 (ms).\ninput size(128, 10000) forward time is 42.05 (ms); backwad avg time is 13.97 (ms).\n```\n\nFix https://github.com/pytorch/pytorch/issues/24633, https://github.com/pytorch/pytorch/issues/24634, https://github.com/pytorch/pytorch/issues/24766, https://github.com/pytorch/pytorch/issues/24767.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30504\n\nDifferential Revision: D19274913\n\nPulled By: ezyang\n\nfbshipit-source-id: 21b29e8459dcba5a040cc68333887b45a858328e", "pr_number": "30504", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/SoftPlus.cu", "aten/src/THCUNN/generic/SoftPlus.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/SoftPlus.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged", "open source"]}, "9ba6a768de": {"title": "Add op bitwise_or (#31559)", "body": "Summary:\nezyang ,  this PR add bitwise_or operator as https://github.com/pytorch/pytorch/pull/31104 .\nBenchmark script :\n```\nimport timeit\nimport torch\ntorch.manual_seed(1)\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__or__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a | b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\")', number=t))\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__ior__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a | b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n```\nDevice: **Tesla P100, skx-8180**\nCuda verison: **9.0.176**\n\nBefore:\n```\n__or__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.17616272252053022\ndevice: cpu, dtype: torch.uint8, 100000 times           0.17148233391344547\ndevice: cpu, dtype: torch.int16, 100000 times           0.17616403382271528\ndevice: cpu, dtype: torch.int32, 100000 times           0.17717823758721352\ndevice: cpu, dtype: torch.int64, 100000 times           0.1801931718364358\ndevice: cuda, dtype: torch.int8, 100000 times           1.270583058707416\ndevice: cuda, dtype: torch.uint8, 100000 times          1.2636413089931011\ndevice: cuda, dtype: torch.int16, 100000 times          1.2839747751131654\ndevice: cuda, dtype: torch.int32, 100000 times          1.2548385225236416\ndevice: cuda, dtype: torch.int64, 100000 times          1.2650810535997152\n__or__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.031136621721088886\ndevice: cpu, dtype: torch.uint8, 10000 times            0.030786747112870216\ndevice: cpu, dtype: torch.int16, 10000 times            0.02391665056347847\ndevice: cpu, dtype: torch.int32, 10000 times            0.024147341027855873\ndevice: cpu, dtype: torch.int64, 10000 times            0.024414129555225372\ndevice: cuda, dtype: torch.int8, 10000 times            0.12741921469569206\ndevice: cuda, dtype: torch.uint8, 10000 times           0.1249831635504961\ndevice: cuda, dtype: torch.int16, 10000 times           0.1283819805830717\ndevice: cuda, dtype: torch.int32, 10000 times           0.12591975275427103\ndevice: cuda, dtype: torch.int64, 10000 times           0.12655890546739101\n__ior__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.3908365070819855\ndevice: cpu, dtype: torch.uint8, 100000 times           0.38267823681235313\ndevice: cpu, dtype: torch.int16, 100000 times           0.38239253498613834\ndevice: cpu, dtype: torch.int32, 100000 times           0.3817988149821758\ndevice: cpu, dtype: torch.int64, 100000 times           0.3901665909215808\ndevice: cuda, dtype: torch.int8, 100000 times           1.4211318120360374\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4215159295126796\ndevice: cuda, dtype: torch.int16, 100000 times          1.4307750314474106\ndevice: cuda, dtype: torch.int32, 100000 times          1.4123614141717553\ndevice: cuda, dtype: torch.int64, 100000 times          1.4480243818834424\n__ior__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.06468924414366484\ndevice: cpu, dtype: torch.uint8, 10000 times            0.06442475505173206\ndevice: cpu, dtype: torch.int16, 10000 times            0.05267547257244587\ndevice: cpu, dtype: torch.int32, 10000 times            0.05286940559744835\ndevice: cpu, dtype: torch.int64, 10000 times            0.06211103219538927\ndevice: cuda, dtype: torch.int8, 10000 times            0.15332304500043392\ndevice: cuda, dtype: torch.uint8, 10000 times           0.15353196952492\ndevice: cuda, dtype: torch.int16, 10000 times           0.15300503931939602\ndevice: cuda, dtype: torch.int32, 10000 times           0.15274472255259752\ndevice: cuda, dtype: torch.int64, 10000 times           0.1512152962386608\n```\nAfter:\n```\n__or__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.2465507509186864\ndevice: cpu, dtype: torch.uint8, 100000 times           0.2472386620938778\ndevice: cpu, dtype: torch.int16, 100000 times           0.2469814233481884\ndevice: cpu, dtype: torch.int32, 100000 times           0.2535214088857174\ndevice: cpu, dtype: torch.int64, 100000 times           0.24855613708496094\ndevice: cuda, dtype: torch.int8, 100000 times           1.4351346511393785\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4434308474883437\ndevice: cuda, dtype: torch.int16, 100000 times          1.4520929995924234\ndevice: cuda, dtype: torch.int32, 100000 times          1.4456610176712275\ndevice: cuda, dtype: torch.int64, 100000 times          1.4580101007595658\n__or__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.029985425993800163\ndevice: cpu, dtype: torch.uint8, 10000 times            0.03024935908615589\ndevice: cpu, dtype: torch.int16, 10000 times            0.026356655173003674\ndevice: cpu, dtype: torch.int32, 10000 times            0.027377349324524403\ndevice: cpu, dtype: torch.int64, 10000 times            0.029163731262087822\ndevice: cuda, dtype: torch.int8, 10000 times            0.14540370367467403\ndevice: cuda, dtype: torch.uint8, 10000 times           0.1456305105239153\ndevice: cuda, dtype: torch.int16, 10000 times           0.1450125053524971\ndevice: cuda, dtype: torch.int32, 10000 times           0.1472016740590334\ndevice: cuda, dtype: torch.int64, 10000 times           0.14709716010838747\n__ior__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.27195510920137167\ndevice: cpu, dtype: torch.uint8, 100000 times           0.2692424338310957\ndevice: cpu, dtype: torch.int16, 100000 times           0.27726674638688564\ndevice: cpu, dtype: torch.int32, 100000 times           0.2815811652690172\ndevice: cpu, dtype: torch.int64, 100000 times           0.2852728571742773\ndevice: cuda, dtype: torch.int8, 100000 times           1.4743850827217102\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4766502184793353\ndevice: cuda, dtype: torch.int16, 100000 times          1.4774163831025362\ndevice: cuda, dtype: torch.int32, 100000 times          1.4749693805351853\ndevice: cuda, dtype: torch.int64, 100000 times          1.5772947426885366\n__ior__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.03614502027630806\ndevice: cpu, dtype: torch.uint8, 10000 times            0.03619729354977608\ndevice: cpu, dtype: torch.int16, 10000 times            0.0319912089034915\ndevice: cpu, dtype: torch.int32, 10000 times            0.03319283854216337\ndevice: cpu, dtype: torch.int64, 10000 times            0.0343862259760499\ndevice: cuda, dtype: torch.int8, 10000 times            0.1581476852297783\ndevice: cuda, dtype: torch.uint8, 10000 times           0.15974601730704308\ndevice: cuda, dtype: torch.int16, 10000 times           0.15957212820649147\ndevice: cuda, dtype: torch.int32, 10000 times           0.16002820804715157\ndevice: cuda, dtype: torch.int64, 10000 times           0.16129320487380028\n```\n\nFix  https://github.com/pytorch/pytorch/issues/24511, https://github.com/pytorch/pytorch/issues/24515, https://github.com/pytorch/pytorch/issues/24658, https://github.com/pytorch/pytorch/issues/24662.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31559\n\nDifferential Revision: D19315875\n\nPulled By: ezyang\n\nfbshipit-source-id: 4a3ca88fdafbeb796079687e676228111eb44aad", "pr_number": "31559", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/THC/THCTensorMathPairwise.cu", "aten/src/THC/THCTensorMathPointwise.cuh", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPairwise.h", "aten/src/THC/generic/THCTensorMathPointwise.cu", "aten/src/THC/generic/THCTensorMathPointwise.h", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "9a3cb1e859": {"title": "Move cauchy to Aten(CPU) (#31824)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/24684.\nBenchmark script :\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\n\ndef _time():\n    return time.time()\n\ndevice = \"cpu\"\n\n#warm up\nfor n in [10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=False, device=device)\n    for i in range(1000):\n        input.cauchy_()\n\nfor n in [1, 10, 100, 1000]:\n    fwd_t = 0\n    input = torch.randn(128, n, requires_grad=False, device=device)\n    for i in range(10000):\n        t1 = _time()\n        input.cauchy_()\n        t2 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n    fwd_avg = fwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.4f (ms).\" % (n, fwd_avg))\n```\nTest device: **skx-8180**.\nBefore:\n```\ninput size(128, 1) forward time is 0.0071 (ms).\ninput size(128, 10) forward time is 0.0596 (ms).\ninput size(128, 100) forward time is 0.5798 (ms).\ninput size(128, 1000) forward time is 5.8395 (ms).\n```\nAfter:\n```\ninput size(128, 1) forward time is 0.0070 (ms).\ninput size(128, 10) forward time is 0.0583 (ms).\ninput size(128, 100) forward time is 0.5714 (ms).\ninput size(128, 1000) forward time is 5.7674 (ms).\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31824\n\nDifferential Revision: D19314411\n\nPulled By: ezyang\n\nfbshipit-source-id: 58098546face3e5971b023f702cfe44ff1cccfbc", "pr_number": "31824", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorRandom.cpp", "aten/src/TH/generic/THTensorRandom.h"], "labels": ["merged", "open source"]}, "dedd16b418": {"title": "remove THConv code which never be used (#31879)", "body": "Summary:\nJust remove dead code in TH.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31879\n\nDifferential Revision: D19315818\n\nPulled By: ezyang\n\nfbshipit-source-id: dbeb2475e19e9ebf769df2649cc859c08d3d184d", "pr_number": "31879", "files_changed": ["aten/src/TH/CMakeLists.txt", "aten/src/TH/THTensor.h", "aten/src/TH/THTensorConv.cpp", "aten/src/TH/generic/THTensorConv.cpp", "aten/src/TH/generic/THTensorConv.h"], "labels": ["merged", "open source"]}, "8c59d48281": {"title": "Add doc previewing instructions (#31905)", "body": "Summary:\nStacked PRs\n * #31908 - Remove C++ docs contributing page\n * **#31905 - Add doc previewing instructions**\n\nThis adds some instructions on how to get started with Github pages you can show reviewers your documentation changes. Hopefully we can delete this eventually and build docs automatically on relevant PRs in CI.\n](https://our.intern.facebook.com/intern/diff/19296364/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31905\n\nPulled By: driazati\n\nDifferential Revision: D19296364\n\nfbshipit-source-id: df47fa1a8d7be029c3efcf6521298583ad9f7a95", "pr_number": "31905", "files_changed": ["CONTRIBUTING.md"], "labels": ["merged"]}, "09a22f3301": {"title": "Remove C++ docs contributing page (#31908)", "body": "Summary:\nStacked PRs\n * **#31908 - Remove C++ docs contributing page**\n * #31905 - Add doc previewing instructions\n\nWe should have 1 source of truth for contribution instructions (CONTRIBUTING.md).\nThis PR moves the instructions from the C++ doc pages there instead of having its\nown separate page.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31908\n\nPulled By: driazati\n\nDifferential Revision: D19296366\n\nfbshipit-source-id: c1daf004259342bd09e09dea3b80e34db47066ec", "pr_number": "31908", "files_changed": ["CONTRIBUTING.md", "docs/cpp/source/contributing.rst", "docs/cpp/source/index.rst"], "labels": ["merged"]}, "883fb5434a": {"title": "Use real argument names for Python functions (#29300)", "body": "Summary:\nThis hooks up `inspect` so that Python functions get their parameters\nnames attached instead of naming them `0, 1, 2, ...`. This also fixes\nissue #28537 where `ignore` functions were improperly typing `self`.\n](https://our.intern.facebook.com/intern/diff/19256434/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29300\n\nPulled By: driazati\n\nDifferential Revision: D19256434\n\nfbshipit-source-id: 6a1fe7bd0afab708b8439517798955d0abfeb44c", "pr_number": "29300", "files_changed": ["test/test_jit.py", "test/test_jit_py3.py", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/jit/__init__.py", "torch/jit/_recursive.py", "torch/jit/annotations.py"], "labels": ["jit", "merged"]}, "5cc49ed45f": {"title": "Document `IValue` (#31904)", "body": "Summary:\nThis is a first pass attempt at documenting `IValue` to help with problems like in #17165. Most users are probably concerned with\n * how to make an `IValue` that matches the input type to their graph (most of the constructors are pretty self explanatory, so as long as they are in the docs I think its enough)\n * how to extract the results after running their graph (there is a small note on the behavior of `.toX()` based on confusions we've had in the past)\n\nPreview:\nhttps://driazati.github.io/pytorch_doc_previews/31904/api/structc10_1_1_i_value.html#exhale-struct-structc10-1-1-i-value\n\nThere are also some random CSS fixes to clean up the style.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31904\n\nPulled By: driazati\n\nDifferential Revision: D19318733\n\nfbshipit-source-id: b29dae3349d5a7ea5a3b8e09cd23f7ff8434edb4", "pr_number": "31904", "files_changed": ["aten/src/ATen/core/ivalue.h", "docs/cpp/source/Doxyfile", "docs/cpp/source/_static/cpp_theme.css", "docs/cpp/source/conf.py"], "labels": ["merged"]}, "319cc21108": {"title": "Add AliasDb API For Changing Aliasing (#31501)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31501\n\nWe have a number of places in our code base where we should be checking if it's safe to change the alias relationship between two sets of values. This PR adds an api to Alias Db to consolidate the logic, and refactors Constant Pooling and `CSE` to use the new api. Next steps: add api usage in peephole.cpp where applicable.\n\nHappy to bikeshed `AliasDb::safeToChangeAliasingRelationship`. Previously I suggested `AliasDb::safeToIntroduceAliasing`, however that's not quite accurate, because this API also handles when it is unsafe to remove aliasing.\n\nAlternate suggestions: `safeToChangeAliasing`, `validToChangeAliasing`, `validToChangeAliasingRelationship`\n\nRelated:  https://github.com/pytorch/pytorch/issues/28360\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19254413\n\nPulled By: eellison\n\nfbshipit-source-id: 17f7f52ad2d1526d303132767cbbb32f8189ae15", "pr_number": "31501", "files_changed": ["test/cpp/jit/test_alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.h", "torch/csrc/jit/passes/common_subexpression_elimination.cpp", "torch/csrc/jit/passes/constant_pooling.cpp"], "labels": ["jit", "merged"]}, "8ecd3f783d": {"title": "check for object equality in constant pooling (#31800)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31800\n\nIf we know that two constants are the same object, we can ignore other constraints and pool them together. This fixes an issue introduced by the other PR where quantization relied on constant pooling happening for correctness.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19269499\n\nPulled By: eellison\n\nfbshipit-source-id: 9d4396125aa6899cb081863d463d4f024135cbf4", "pr_number": "31800", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "test/cpp/jit/test_constant_pooling.cpp", "test/test_jit.py", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/passes/constant_pooling.cpp"], "labels": ["jit", "merged"]}, "eb23171bce": {"title": "TensorIterator norm update (#31903)", "body": "Summary:\nspecial case for norm out where p == 2. Instead of calling `pow`,\nwe use multiplication as a faster code path.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31903\n\nDifferential Revision: D19312749\n\nPulled By: ngimel\n\nfbshipit-source-id: 73732b7b37a243a14438609784795b920271a0b5", "pr_number": "31903", "files_changed": ["aten/src/ATen/native/SharedReduceOps.h", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cuda/ReduceOpsKernel.cu", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/THC/THCTensorMathReduce.cuh", "aten/src/THC/generic/THCTensorMathReduce.cu"], "labels": ["merged", "open source"]}, "9e9ca6ec37": {"title": "add conversion functions to embedding tables (#31083)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31083\n\nadd (fp32/fp16)<->(int8 rowwise quantized fp32/fp16 scale biases)\n\nTest Plan:\nadded unit tests\nenhanced shape inference tests\n\nReviewed By: jspark1105\n\nDifferential Revision: D18920547\n\nfbshipit-source-id: 6b3d7cb93f9d1669ecf511817d73976177632891", "pr_number": "31083", "files_changed": ["caffe2/operators/fused_rowwise_8bit_conversion_ops.cc", "caffe2/operators/fused_rowwise_8bit_conversion_ops.h", "caffe2/perfkernels/fused_8bit_rowwise_conversion.cc", "caffe2/perfkernels/fused_8bit_rowwise_conversion.h", "caffe2/python/operator_test/shape_inference_test.py"], "labels": ["fb-exported", "merged"]}, "6d9a9e379d": {"title": "Fix segfault in caffe2 slice test (#31801)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31801\n\nTry to fix issue #30764\n\nTest Plan:\npython test/onnx/test_utility_funs.py TestUtilityFuns\n\nImported from OSS\n\nDifferential Revision: D19315046\n\nfbshipit-source-id: de3595969280e4ebe762cb098ff0891f8b5a9a90", "pr_number": "31801", "files_changed": ["caffe2/operators/quantized/int8_slice_op.h"], "labels": ["merged"]}, "0dbd5c0bfe": {"title": "Added torchvision tests as part of ORT tests (#31835)", "body": "Summary:\nAdded torchvision tests as part of ORT tests\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31835\n\nReviewed By: hl475\n\nDifferential Revision: D19278607\n\nPulled By: houseroad\n\nfbshipit-source-id: 18a6a85ce3019bcc9aee9517af1378964b585afd", "pr_number": "31835", "files_changed": [".jenkins/caffe2/test.sh", "test/onnx/test_pytorch_onnx_onnxruntime.py"], "labels": ["merged", "open source"]}, "8614860210": {"title": "Uniformly apply Windows logic in cpp_extensions everywhere (#31161)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31161\n\nPreviously, it wasn't necessary to specify `DT_NEEDED` in C++ extensions on Linux (aka pass `-l` flags) because all of the symbols would have already been loaded with `RTLD_GLOBAL`, so there wouldn't be any undefined symbols.  But when we switch to loading `_C` with `RTLD_LOCAL`, it's now necessary for all the C++ extensions to know what libraries to link with. The resulting code is clearer and more uniform, so it's wins all around.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19262578\n\nPulled By: ezyang\n\nfbshipit-source-id: a893cc96f2e9aad1c064a6de4f7ccf79257dec3f", "pr_number": "31161", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["merged"]}, "ddff4efa26": {"title": "Don't use RTLD_GLOBAL to load _C. (#31162)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31162\n\nThis should help us resolve a multitude of weird segfaults and crashes\nwhen PyTorch is imported along with other packages. Those would often\nhappen because libtorch symbols were exposed globally and could be used\nas a source of relocations in shared libraries loaded after libtorch.\n\nFixes #3059.\n\nSome of the subtleties in preparing this patch:\n\n* Getting ASAN to play ball was a pain in the ass. The basic problem is that when we load with `RTLD_LOCAL`, we now may load a library multiple times into the address space; this happens when we have custom C++ extensions. Since the libraries are usually identical, this is usually benign, but it is technically undefined behavior and UBSAN hates it. I sprayed a few ways of getting things to \"work\" correctly: I preload libstdc++ (so that it is seen consistently over all library loads) and added turned off vptr checks entirely. Another possibility is we should have a mode where we use RTLD_GLOBAL to load _C, which would be acceptable in environments where you're sure C++ lines up correctly. There's a long comment in the test script going into more detail about this.\n* Making some of our shared library dependencies load with `RTLD_LOCAL` breaks them. OpenMPI and MKL don't work; they play linker shenanigans to look up their symbols which doesn't work when loaded locally, and if we load a library with `RLTD_LOCAL` we aren't able to subsequently see it with `ctypes`. To solve this problem, we employ a clever device invented by apaszke: we create a dummy library `torch_global_deps` with dependencies on all of the libraries which need to be loaded globally, and then load that with `RTLD_GLOBAL`. As long as none of these libraries have C++ symbols, we can avoid confusion about C++ standard library.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: D19262579\n\nTest Plan: Imported from OSS\n\nPulled By: ezyang\n\nfbshipit-source-id: 06a48a5d2c9036aacd535f7e8a4de0e8fe1639f2", "pr_number": "31162", "files_changed": [".jenkins/pytorch/test.sh", "caffe2/CMakeLists.txt", "torch/__init__.py", "torch/_utils_internal.py", "torch/csrc/empty.c", "ubsan.supp"], "labels": ["merged"]}, "5d5f156558": {"title": "Revert D18903453: Quantized H Tangent function", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18903453\n\nOriginal commit changeset: 0050b1cebb1d\n\nfbshipit-source-id: 205978f71d5688d4068861f7cf2dff40fbb311c6", "pr_number": null, "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/src/tanh.c", "aten/src/ATen/native/quantized/cpu/qnnpack/test/tanh-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qtanh.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/test_quantized.py"], "labels": []}, "5c423cae72": {"title": "Add precision tests for CUDA half linspace+logspace (#31962)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31962\n\nI added precision tests for CUDA half, float, and double.\n\nThe precision for CUDA half seems bad, but I checked the numbers against\nprevious versions of pytorch. The output of CUDA Half linspace+logspace\nare exactly the same when compared with 1.2.0.\n\nTest Plan: - Run CI\n\nDifferential Revision: D19320182\n\nPulled By: zou3519\n\nfbshipit-source-id: 38d3d4dea2807875ed0b0ec2b93b19c10a289988", "pr_number": "31962", "files_changed": ["test/test_torch.py"], "labels": ["merged"]}, "5a76335aaa": {"title": "Move lshift to Aten (#31566)", "body": "Summary:\nVitalyFedyunin , this PR is about move lshift to Aten.\nBenchmark script :\n```\nimport timeit\nimport torch\ntorch.manual_seed(1)\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__lshift__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a << b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\")', number=t))\n        for dtype in ('torch.float32', 'torch.float64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a << b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randn({n}, dtype = {dtype}, device=\"{device}\"); b = torch.randn({n}, dtype = {dtype}, device=\"{device}\")', number=t))\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__ilshift__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a << b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n        for dtype in ('torch.float32', 'torch.float64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a << b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randn({n}, dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n```\nDevice: **Tesla P100, skx-8180**\nCuda verison: **9.0.176**\n\nBefore:\n```\n__lshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.31618343852460384\ndevice: cpu, dtype: torch.uint8, 100000 times           0.31258584931492805\ndevice: cpu, dtype: torch.int16, 100000 times           0.3140896391123533\ndevice: cpu, dtype: torch.int32, 100000 times           0.34389012958854437\ndevice: cpu, dtype: torch.int64, 100000 times           0.339566046372056\ndevice: cpu, dtype: torch.float32, 100000 times         0.4180623721331358\ndevice: cpu, dtype: torch.float64, 100000 times         0.4165227338671684\ndevice: cuda, dtype: torch.int8, 100000 times           1.7851383443921804\ndevice: cuda, dtype: torch.uint8, 100000 times          1.7842160519212484\ndevice: cuda, dtype: torch.int16, 100000 times          1.789359962567687\ndevice: cuda, dtype: torch.int32, 100000 times          1.7822618428617716\ndevice: cuda, dtype: torch.int64, 100000 times          1.7968465769663453\ndevice: cuda, dtype: torch.float32, 100000 times                1.8066061967983842\ndevice: cuda, dtype: torch.float64, 100000 times                1.8046843251213431\n__lshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.04618230368942022\ndevice: cpu, dtype: torch.uint8, 10000 times            0.04634759668260813\ndevice: cpu, dtype: torch.int16, 10000 times            0.040676115080714226\ndevice: cpu, dtype: torch.int32, 10000 times            0.04404774494469166\ndevice: cpu, dtype: torch.int64, 10000 times            0.04511771444231272\ndevice: cpu, dtype: torch.float32, 10000 times          0.6887832451611757\ndevice: cpu, dtype: torch.float64, 10000 times          0.5559549620375037\ndevice: cuda, dtype: torch.int8, 10000 times            0.17996764183044434\ndevice: cuda, dtype: torch.uint8, 10000 times           0.17970609478652477\ndevice: cuda, dtype: torch.int16, 10000 times           0.17873135022819042\ndevice: cuda, dtype: torch.int32, 10000 times           0.1781835313886404\ndevice: cuda, dtype: torch.int64, 10000 times           0.17846618220210075\ndevice: cuda, dtype: torch.float32, 10000 times         0.18056879844516516\ndevice: cuda, dtype: torch.float64, 10000 times         0.18132662680000067\n__ilshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.61110960226506\ndevice: cpu, dtype: torch.uint8, 100000 times           0.6333359787240624\ndevice: cpu, dtype: torch.int16, 100000 times           0.6345370784401894\ndevice: cpu, dtype: torch.int32, 100000 times           0.6470990972593427\ndevice: cpu, dtype: torch.int64, 100000 times           0.6587044578045607\ndevice: cpu, dtype: torch.float32, 100000 times         0.7269002720713615\ndevice: cpu, dtype: torch.float64, 100000 times         0.7217964073643088\ndevice: cuda, dtype: torch.int8, 100000 times           1.9880435159429908\ndevice: cuda, dtype: torch.uint8, 100000 times          1.986489498987794\ndevice: cuda, dtype: torch.int16, 100000 times          2.0059875370934606\ndevice: cuda, dtype: torch.int32, 100000 times          1.995262237265706\ndevice: cuda, dtype: torch.int64, 100000 times          1.9974954994395375\ndevice: cuda, dtype: torch.float32, 100000 times                2.00442770216614\ndevice: cuda, dtype: torch.float64, 100000 times                2.009664717130363\n__ilshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.08199594635516405\ndevice: cpu, dtype: torch.uint8, 10000 times            0.08096733782440424\ndevice: cpu, dtype: torch.int16, 10000 times            0.0734213450923562\ndevice: cpu, dtype: torch.int32, 10000 times            0.0769620593637228\ndevice: cpu, dtype: torch.int64, 10000 times            0.08650507684797049\ndevice: cpu, dtype: torch.float32, 10000 times          0.7196345143020153\ndevice: cpu, dtype: torch.float64, 10000 times          0.597336508333683\ndevice: cuda, dtype: torch.int8, 10000 times            0.19723015930503607\ndevice: cuda, dtype: torch.uint8, 10000 times           0.19754122477024794\ndevice: cuda, dtype: torch.int16, 10000 times           0.19710093270987272\ndevice: cuda, dtype: torch.int32, 10000 times           0.19611249305307865\ndevice: cuda, dtype: torch.int64, 10000 times           0.19750046730041504\ndevice: cuda, dtype: torch.float32, 10000 times         0.19680574722588062\ndevice: cuda, dtype: torch.float64, 10000 times         0.19689027685672045\n```\nAfter:\n```\n__lshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.3031281465664506\ndevice: cpu, dtype: torch.uint8, 100000 times           0.30772678554058075\ndevice: cpu, dtype: torch.int16, 100000 times           0.3088294789195061\ndevice: cpu, dtype: torch.int32, 100000 times           0.30907699652016163\ndevice: cpu, dtype: torch.int64, 100000 times           0.31315001379698515\ndevice: cpu, dtype: torch.float32, 100000 times         0.38823566399514675\ndevice: cpu, dtype: torch.float64, 100000 times         0.39300001971423626\ndevice: cuda, dtype: torch.int8, 100000 times           1.3225595457479358\ndevice: cuda, dtype: torch.uint8, 100000 times          1.31739442050457\ndevice: cuda, dtype: torch.int16, 100000 times          1.3198596313595772\ndevice: cuda, dtype: torch.int32, 100000 times          1.309600466862321\ndevice: cuda, dtype: torch.int64, 100000 times          1.3264533821493387\ndevice: cuda, dtype: torch.float32, 100000 times                1.3377520674839616\ndevice: cuda, dtype: torch.float64, 100000 times                1.3343619462102652\n__lshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.02718757465481758\ndevice: cpu, dtype: torch.uint8, 10000 times            0.02701799664646387\ndevice: cpu, dtype: torch.int16, 10000 times            0.025483975186944008\ndevice: cpu, dtype: torch.int32, 10000 times            0.025557605549693108\ndevice: cpu, dtype: torch.int64, 10000 times            0.026179466396570206\ndevice: cpu, dtype: torch.float32, 10000 times          0.0962932649999857\ndevice: cpu, dtype: torch.float64, 10000 times          0.1611471576616168\ndevice: cuda, dtype: torch.int8, 10000 times            0.13165222201496363\ndevice: cuda, dtype: torch.uint8, 10000 times           0.13358880020678043\ndevice: cuda, dtype: torch.int16, 10000 times           0.1342075066640973\ndevice: cuda, dtype: torch.int32, 10000 times           0.1328689968213439\ndevice: cuda, dtype: torch.int64, 10000 times           0.13336248509585857\ndevice: cuda, dtype: torch.float32, 10000 times         0.1345295710489154\ndevice: cuda, dtype: torch.float64, 10000 times         0.14084953162819147\n__ilshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.19080814253538847\ndevice: cpu, dtype: torch.uint8, 100000 times           0.18541878275573254\ndevice: cpu, dtype: torch.int16, 100000 times           0.19136024825274944\ndevice: cpu, dtype: torch.int32, 100000 times           0.1916898973286152\ndevice: cpu, dtype: torch.int64, 100000 times           0.1973192635923624\ndevice: cpu, dtype: torch.float32, 100000 times         0.2668355852365494\ndevice: cpu, dtype: torch.float64, 100000 times         0.24472137168049812\ndevice: cuda, dtype: torch.int8, 100000 times           1.3581306440755725\ndevice: cuda, dtype: torch.uint8, 100000 times          1.3522163443267345\ndevice: cuda, dtype: torch.int16, 100000 times          1.366145665757358\ndevice: cuda, dtype: torch.int32, 100000 times          1.3674909211695194\ndevice: cuda, dtype: torch.int64, 100000 times          1.3734915973618627\ndevice: cuda, dtype: torch.float32, 100000 times                1.3831533305346966\ndevice: cuda, dtype: torch.float64, 100000 times                1.396162535995245\n__ilshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.02847585454583168\ndevice: cpu, dtype: torch.uint8, 10000 times            0.02960751298815012\ndevice: cpu, dtype: torch.int16, 10000 times            0.028516249731183052\ndevice: cpu, dtype: torch.int32, 10000 times            0.02842544950544834\ndevice: cpu, dtype: torch.int64, 10000 times            0.029186096973717213\ndevice: cpu, dtype: torch.float32, 10000 times          0.0999628696590662\ndevice: cpu, dtype: torch.float64, 10000 times          0.16676222812384367\ndevice: cuda, dtype: torch.int8, 10000 times            0.13856443110853434\ndevice: cuda, dtype: torch.uint8, 10000 times           0.13766566663980484\ndevice: cuda, dtype: torch.int16, 10000 times           0.13652489613741636\ndevice: cuda, dtype: torch.int32, 10000 times           0.13678150344640017\ndevice: cuda, dtype: torch.int64, 10000 times           0.13749946560710669\ndevice: cuda, dtype: torch.float32, 10000 times         0.13879029918462038\ndevice: cuda, dtype: torch.float64, 10000 times         0.14587809145450592\n```\n\nFix https://github.com/pytorch/pytorch/issues/24510 #24514 https://github.com/pytorch/pytorch/issues/24657  https://github.com/pytorch/pytorch/issues/24661\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31566\n\nDifferential Revision: D19314251\n\nPulled By: ezyang\n\nfbshipit-source-id: 52df17b2c18ef1880374c6dbcf18fb1118086552", "pr_number": "31566", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/THC/THCTensorMathPairwise.cu", "aten/src/THC/THCTensorMathPointwise.cuh", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPairwise.h", "aten/src/THC/generic/THCTensorMathPointwise.cu"], "labels": ["merged", "open source"]}, "99b3f9cac4": {"title": "Move log_sigmoid to Aten(CPU) (#30958)", "body": "Summary:\nVitalyFedyunin, This PR is about port LogSigmoid activation to Aten:\nTest script:\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\n\ndef _time():\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.LogSigmoid()\n#warm up\nfor n in [1, 10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.randn(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [1, 10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.randn(128, n, device=device)\n    fwd_t = 0\n    bwd_t = 0\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\n**Before:**\n```\ninput size(128, 1) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.10 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 100) forward time is 0.90 (ms); backwad avg time is 0.09 (ms).\ninput size(128, 1000) forward time is 9.04 (ms); backwad avg time is 0.87 (ms).\n```\n**After:**\n```\ninput size(128, 1) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 100) forward time is 0.04 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 1000) forward time is 0.28 (ms); backwad avg time is 0.07 (ms).\n```\n**OMP_NUM_THREADS=1:**\n```\nBefore:\ninput size(128, 1) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.10 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 100) forward time is 0.88 (ms); backwad avg time is 0.10 (ms).\ninput size(128, 1000) forward time is 8.72 (ms); backwad avg time is 0.81 (ms).\nAfter:\ninput size(128, 1) forward time is 0.01 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 100) forward time is 0.07 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 1000) forward time is 0.63 (ms); backwad avg time is 0.15 (ms).\n```\n\nFix https://github.com/pytorch/pytorch/issues/24724, https://github.com/pytorch/pytorch/issues/24725.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30958\n\nDifferential Revision: D19275111\n\nPulled By: ezyang\n\nfbshipit-source-id: bbfe82e58fb27a4fb21c1914c6547a9050072e5c", "pr_number": "30958", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/THNN/generic/LogSigmoid.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged", "open source"]}, "e59e5ba5a3": {"title": "Move geometric to Aten(CPU) (#31878)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/24704.\nBenchmark script :\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\n\ndef _time():\n    return time.time()\n\ndevice = \"cpu\"\n\n#warm up\nfor n in [10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=False, device=device)\n    for i in range(1000):\n        input.geometric_(0.5)\n\nfor n in [1, 10, 100, 1000]:\n    fwd_t = 0\n    input = torch.randn(128, n, requires_grad=False, device=device)\n    for i in range(10000):\n        t1 = _time()\n        input.geometric_(0.5)\n        t2 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n    fwd_avg = fwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.4f (ms).\" % (n, fwd_avg))\n```\nTest device: **skx-8180**.\nBefore:\n```\ninput size(128, 1) forward time is 0.0092 (ms).\ninput size(128, 10) forward time is 0.0802 (ms).\ninput size(128, 100) forward time is 0.7994 (ms).\ninput size(128, 1000) forward time is 7.8403 (ms).\n```\nAfter:\n```\ninput size(128, 1) forward time is 0.0088 (ms).\ninput size(128, 10) forward time is 0.0781 (ms).\ninput size(128, 100) forward time is 0.7815 (ms).\ninput size(128, 1000) forward time is 7.7163 (ms).\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31878\n\nDifferential Revision: D19314510\n\nPulled By: ezyang\n\nfbshipit-source-id: 2d95bf9938c8becf280890acf9e37223ddd08a39", "pr_number": "31878", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorRandom.cpp", "aten/src/TH/generic/THTensorRandom.h"], "labels": ["merged", "open source"]}, "26f552a3d1": {"title": "Javadoc changes (#31956)", "body": "Summary:\n- Add Javadoc url in index.rst\n- Delete no longer needed java rst files\n- Remove intersphinx extension from conf.oy\n- Remove javasphinx from docs/requirements.txt\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31956\n\nDifferential Revision: D19320493\n\nPulled By: jlin27\n\nfbshipit-source-id: cc76b2a2acbe2ecdabcd3339e1cc3182f0c906ae", "pr_number": "31956", "files_changed": ["docs/requirements.txt", "docs/source/conf.py", "docs/source/index.rst", "docs/source/org/pytorch/DType.rst", "docs/source/org/pytorch/IValue.rst", "docs/source/org/pytorch/Module.rst", "docs/source/org/pytorch/Tensor-Tensor_float32.rst", "docs/source/org/pytorch/Tensor-Tensor_float64.rst", "docs/source/org/pytorch/Tensor-Tensor_int32.rst", "docs/source/org/pytorch/Tensor-Tensor_int64.rst", "docs/source/org/pytorch/Tensor-Tensor_int8.rst", "docs/source/org/pytorch/Tensor-Tensor_uint8.rst", "docs/source/org/pytorch/Tensor.rst", "docs/source/org/pytorch/TensorImageUtils.rst", "docs/source/org/pytorch/package-index.rst", "docs/source/org/pytorch/torchvision/package-index.rst", "docs/source/packages.rst"], "labels": ["merged"]}, "bc68a8745f": {"title": "Spelling fix in transformer docs", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31973\n\nDifferential Revision: D19330660\n\nPulled By: zou3519\n\nfbshipit-source-id: 29ea1e790a34f0241cb7aba85110f087cdc069ba", "pr_number": "31973", "files_changed": ["torch/nn/modules/transformer.py"], "labels": ["merged", "open source"]}, "cfdfdf70d7": {"title": "remove JSON dumping dependency (#30724)", "body": "Summary:\nFix for https://github.com/pytorch/pytorch/issues/19420\n\nSo after actually writing a C++ JSON dumping class I figured that\na faster and cleaner way would be simply rewrite the Python without\nthe JSON module since the JSON that we need to output is so simple.\n\nFor now I decided to not touch the `parse_cpu_trace` function since\nonly changing `export_chrome_trace` shows a 4x speedup.\n\nHere's the script I used for benchmarking:\n``` python\nimport time\nimport torch\n\nx = torch.ones(2, 2)\n\nstart = time.time()\nwith torch.autograd.profiler.profile() as prof:\n  for _ in range(10000):\n    x * x\n\nfor i in range(50):\n  prof.export_chrome_trace(\"trace.json\")\n\nstop = time.time()\n\nprint(stop-start)\n```\nmaster branch (using json dump) -> 8.07515025138855\nnew branch (without json dump) ->  2.0943689346313477\n\nI checked the trace file generated in the [test](https://github.com/pytorch/pytorch/blob/master/test/test_autograd.py#L2659)\nand it does work fine.\n\nPlease let me know what you think.\n\nIf you still insist on the C++ version I can send a new patch soon enough.\n\nCC ezyang rgommers\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30724\n\nDifferential Revision: D19298955\n\nPulled By: ezyang\n\nfbshipit-source-id: b0d7324ea5f90884ab8a00dd272f3aa3d9bc0427", "pr_number": "30724", "files_changed": ["torch/autograd/profiler.py"], "labels": ["merged", "open source"]}, "1296e2d55e": {"title": "C++ API parity: isinf (#31099)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/31021, port the legacy binding method of `isinf` to C++ therefore support JIT\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31099\n\nDifferential Revision: D19314733\n\nPulled By: yf225\n\nfbshipit-source-id: 5725c51d19c33b4fddd0fc9e7034078580bd534e", "pr_number": "31099", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/native_functions.yaml", "test/cpp/api/functional.cpp", "torch/_torch_docs.py", "torch/functional.py"], "labels": ["merged", "module: cpp", "open source"]}, "67c1d930eb": {"title": "Lock graph_task before writing leaf_streams. (#31995)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31995\n\nFixes #31906.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19331259\n\nPulled By: ezyang\n\nfbshipit-source-id: 5d24bf3555e632211a9b6f8e50ff241603c18b3d", "pr_number": "31995", "files_changed": ["torch/csrc/autograd/engine.cpp"], "labels": ["merged"]}, "2968faf154": {"title": "Update doc about output_differentiability keyword in derivatives.yaml", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31925\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19303833\n\nPulled By: albanD\n\nfbshipit-source-id: 291a9f122720844a5f8386b22cf6abc66ae86e4d", "pr_number": "31925", "files_changed": ["tools/autograd/derivatives.yaml"], "labels": ["merged"]}, "67ff051ddd": {"title": "Remove temporary fix for torchbind in BC check (#31982)", "body": "Summary:\nRemove the patch\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31982\n\nReviewed By: hl475\n\nDifferential Revision: D19333205\n\nPulled By: houseroad\n\nfbshipit-source-id: 1d16fd31ede7266789141238520d47b762a7a340", "pr_number": "31982", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "700d1c5cbc": {"title": "update CI script to take string docker image version (#31857)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31857\n\nAccording to mingbowan we will change to use string docker image\nversion because the tag is no longer an integer since we move the docker\nimage build job to circle CI:\nhttp://ossci-docker.s3-website.us-east-1.amazonaws.com/pytorch.html\n\nTest Plan: - with stacked PR\n\nDifferential Revision: D19282726\n\nPulled By: ljk53\n\nfbshipit-source-id: 7a12ae89a11cf15163b905734d50fed6dc98cb07", "pr_number": "31857", "files_changed": [".circleci/cimodel/data/caffe2_build_definitions.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/validate-docker-version.py"], "labels": ["merged"]}, "021e1e20c1": {"title": "Revert D19320493: Javadoc changes", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19320493\n\nOriginal commit changeset: cc76b2a2acbe\n\nfbshipit-source-id: 3b36dd2d2591acc60a06a421dd625c21adbe578a", "pr_number": null, "files_changed": ["docs/requirements.txt", "docs/source/conf.py", "docs/source/index.rst", "docs/source/org/pytorch/DType.rst", "docs/source/org/pytorch/IValue.rst", "docs/source/org/pytorch/Module.rst", "docs/source/org/pytorch/Tensor-Tensor_float32.rst", "docs/source/org/pytorch/Tensor-Tensor_float64.rst", "docs/source/org/pytorch/Tensor-Tensor_int32.rst", "docs/source/org/pytorch/Tensor-Tensor_int64.rst", "docs/source/org/pytorch/Tensor-Tensor_int8.rst", "docs/source/org/pytorch/Tensor-Tensor_uint8.rst", "docs/source/org/pytorch/Tensor.rst", "docs/source/org/pytorch/TensorImageUtils.rst", "docs/source/org/pytorch/package-index.rst", "docs/source/org/pytorch/torchvision/package-index.rst", "docs/source/packages.rst"], "labels": []}, "6abfa9ad8a": {"title": "Quantized H Tangent function (#31031)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31031\n\nThis activation will be needed for the LSTM implementation.\nAlso includes the QNNPack implementation.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19334280\n\nPulled By: z-a-f\n\nfbshipit-source-id: ae14399765a47afdf9b1e072d3967c24ff473e8d", "pr_number": "31031", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/src/tanh.c", "aten/src/ATen/native/quantized/cpu/qnnpack/test/tanh-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qtanh.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/test_quantized.py"], "labels": ["merged"]}, "62f93443e5": {"title": "Explain RPC behavior when using Tensor as arg or return value", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31968\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19321380\n\nPulled By: mrshenli\n\nfbshipit-source-id: e3431f1f02963cc8d8266a420ab03866106f26ac", "pr_number": "31968", "files_changed": ["docs/source/rpc.rst"], "labels": ["merged"]}, "4e84661139": {"title": "update llvmlite to 0.30.0 (#31858)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31858\n\nTrying to upgrade docker image but ran into the following error:\n\n```\nRunning test_nn ... [2020-01-04 18:05:12.537860]\nTraceback (most recent call last):\n  File \"test_nn.py\", line 45, in <module>\n    from common_cuda import TEST_CUDA, TEST_MULTIGPU, TEST_CUDNN, TEST_CUDNN_VERSION\n  File \"/var/lib/jenkins/workspace/test/common_cuda.py\", line 16, in <module>\n    import numba.cuda\n  File \"/opt/conda/lib/python3.6/site-packages/numba/__init__.py\", line 178, in <module>\n    _ensure_llvm()\n  File \"/opt/conda/lib/python3.6/site-packages/numba/__init__.py\", line 100, in _ensure_llvm\n    raise ImportError(msg)\nImportError: Numba requires at least version 0.30.0 of llvmlite.\nInstalled version is 0.28.0.\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19282923\n\nPulled By: ljk53\n\nfbshipit-source-id: bdeefbf4f6c0c97df622282f76e77eb1eadba436", "pr_number": "31858", "files_changed": [".circleci/docker/common/install_conda.sh"], "labels": []}, "8ea49e7a08": {"title": "add missing braces for format in rpc _to_worker_info (#31969)", "body": "Summary:\nThis was missing and resulted in the incorrect `name` passed into `_to_worker_info` not being printed out in the error message.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31969\n\nDifferential Revision: D19331927\n\nPulled By: rohan-varma\n\nfbshipit-source-id: e74d47daec3224c2d9b9da3c0a6404cfa67baf65", "pr_number": "31969", "files_changed": ["torch/distributed/rpc/api.py"], "labels": ["merged"]}, "b6f43afaca": {"title": "Fix tensordot allowing negative dims (#31954)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/31926\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31954\n\nDifferential Revision: D19331847\n\nPulled By: zou3519\n\nfbshipit-source-id: e30dd9517917c056a52be7d16f23247fe28f4e28", "pr_number": "31954", "files_changed": ["test/test_torch.py", "torch/functional.py"], "labels": ["merged", "open source"]}, "c6f41ae01b": {"title": "Fix and add more padding mode support for Conv (#31784)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/29712 #29668 , add arg checking, doc, and support for reflection and replication padding modes.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31784\n\nDifferential Revision: D19301974\n\nPulled By: ezyang\n\nfbshipit-source-id: a0ed4815c0c22e416b16e256bba04324e376b2f8", "pr_number": "31784", "files_changed": ["test/common_nn.py", "test/test_nn.py", "torch/nn/modules/conv.py", "torch/nn/modules/utils.py"], "labels": ["merged", "open source", "topic: bc-breaking"]}, "a201027e93": {"title": "Abstract atomic add calls (#31992)", "body": "Summary:\nInstead of a mixture of direct calls to library provided atomicAdd calls, such as float atomicAdd(float*, float) and calls provided internally, such as void atomicAdd(long*, long), abstract to one API void gpuAtomicAdd(T*, T) in THCAtomics.cuh for the PyTorch backend.\n\nThe advantage of this approach is that it allows us to more easily distinguish between capabiltiies of different platforms (and their versions). Additionally, the abstraction of void returning atomicAdds allows us to, in the future, support fast HW instructions on some platforms that will not return the previous value.\n\nCall sites that do not satisfy above conditions and are either highly platform specific (__half2 atomicAdd fast path in one operator) or require the return explicitly (some int atomicAdd invocations) are left untouched. The Caffe2 backend also remains untouched.\n\nWhile here, add a bunch of includes of THCAtomics.cuh that were missing before.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31992\n\nDifferential Revision: D19330220\n\nPulled By: ezyang\n\nfbshipit-source-id: d6ab73ec5168c77e328faeef6c6f48eefba00861", "pr_number": "31992", "files_changed": ["aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/GridSampler.cuh", "aten/src/ATen/native/cuda/KernelUtils.cuh", "aten/src/ATen/native/cuda/LossCTC.cu", "aten/src/ATen/native/cuda/ReflectionPad.cu", "aten/src/ATen/native/cuda/ReplicationPadding.cu", "aten/src/ATen/native/cuda/SortingRadixSelect.cuh", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/UpSample.cuh", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/THC/THCAtomics.cuh", "aten/src/THC/THCTensorIndex.cu", "aten/src/THC/THCTensorScatterGather.cu", "aten/src/THCUNN/SpatialClassNLLCriterion.cu", "docs/cpp/source/notes/tensor_basics.rst"], "labels": ["merged", "module: rocm", "open source"]}, "8098ae455c": {"title": "Move rshift to Aten (#31594)", "body": "Summary:\nVitalyFedyunin , this PR is about move rshift to Aten.\nBenchmark script :\n```\nimport timeit\nimport torch\ntorch.manual_seed(1)\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__rshift__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a >> b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\")', number=t))\n        for dtype in ('torch.float32', 'torch.float64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a >> b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randn({n}, dtype = {dtype}, device=\"{device}\"); b = torch.randn({n}, dtype = {dtype}, device=\"{device}\")', number=t))\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__irshift__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a >> b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n        for dtype in ('torch.float32', 'torch.float64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a >> b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randn({n}, dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n```\nDevice: **Tesla P100, skx-8180**\nCuda verison: **9.0.176**\n\nBefore:\n```\n__rshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.17183916084468365\ndevice: cpu, dtype: torch.uint8, 100000 times           0.16587729007005692\ndevice: cpu, dtype: torch.int16, 100000 times           0.16659130714833736\ndevice: cpu, dtype: torch.int32, 100000 times           0.17177579551935196\ndevice: cpu, dtype: torch.int64, 100000 times           0.17860156949609518\ndevice: cpu, dtype: torch.float32, 100000 times         0.23938780091702938\ndevice: cpu, dtype: torch.float64, 100000 times         0.22591270506381989\ndevice: cuda, dtype: torch.int8, 100000 times           1.2709560776129365\ndevice: cuda, dtype: torch.uint8, 100000 times          1.2692269310355186\ndevice: cuda, dtype: torch.int16, 100000 times          1.2785452520474792\ndevice: cuda, dtype: torch.int32, 100000 times          1.2733035255223513\ndevice: cuda, dtype: torch.int64, 100000 times          1.2785427365452051\ndevice: cuda, dtype: torch.float32, 100000 times                1.2980637094005942\ndevice: cuda, dtype: torch.float64, 100000 times                1.3062487514689565\n__rshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.03122080024331808\ndevice: cpu, dtype: torch.uint8, 10000 times            0.030290847644209862\ndevice: cpu, dtype: torch.int16, 10000 times            0.024531075730919838\ndevice: cpu, dtype: torch.int32, 10000 times            0.024743229150772095\ndevice: cpu, dtype: torch.int64, 10000 times            0.025563121773302555\ndevice: cpu, dtype: torch.float32, 10000 times          0.6707976600155234\ndevice: cpu, dtype: torch.float64, 10000 times          0.5344798369333148\ndevice: cuda, dtype: torch.int8, 10000 times            0.12768010422587395\ndevice: cuda, dtype: torch.uint8, 10000 times           0.12681372743099928\ndevice: cuda, dtype: torch.int16, 10000 times           0.12995595764368773\ndevice: cuda, dtype: torch.int32, 10000 times           0.12989260721951723\ndevice: cuda, dtype: torch.int64, 10000 times           0.12804713658988476\ndevice: cuda, dtype: torch.float32, 10000 times         0.13013121113181114\ndevice: cuda, dtype: torch.float64, 10000 times         0.1406280631199479\n__irshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.3805475188419223\ndevice: cpu, dtype: torch.uint8, 100000 times           0.36341007333248854\ndevice: cpu, dtype: torch.int16, 100000 times           0.36908434610813856\ndevice: cpu, dtype: torch.int32, 100000 times           0.3669992135837674\ndevice: cpu, dtype: torch.int64, 100000 times           0.37847711704671383\ndevice: cpu, dtype: torch.float32, 100000 times         0.4311870699748397\ndevice: cpu, dtype: torch.float64, 100000 times         0.44503832422196865\ndevice: cuda, dtype: torch.int8, 100000 times           1.4343859804794192\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4298221375793219\ndevice: cuda, dtype: torch.int16, 100000 times          1.4460898758843541\ndevice: cuda, dtype: torch.int32, 100000 times          1.4518025070428848\ndevice: cuda, dtype: torch.int64, 100000 times          1.4456725595518947\ndevice: cuda, dtype: torch.float32, 100000 times                1.4610810624435544\ndevice: cuda, dtype: torch.float64, 100000 times                1.4736663019284606\n__irshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.05944254994392395\ndevice: cpu, dtype: torch.uint8, 10000 times            0.058085592463612556\ndevice: cpu, dtype: torch.int16, 10000 times            0.05094402376562357\ndevice: cpu, dtype: torch.int32, 10000 times            0.050842881202697754\ndevice: cpu, dtype: torch.int64, 10000 times            0.06223891582340002\ndevice: cpu, dtype: torch.float32, 10000 times          0.7006897022947669\ndevice: cpu, dtype: torch.float64, 10000 times          0.5614962242543697\ndevice: cuda, dtype: torch.int8, 10000 times            0.1461706068366766\ndevice: cuda, dtype: torch.uint8, 10000 times           0.14335164614021778\ndevice: cuda, dtype: torch.int16, 10000 times           0.1448021186515689\ndevice: cuda, dtype: torch.int32, 10000 times           0.14513055887073278\ndevice: cuda, dtype: torch.int64, 10000 times           0.1439579650759697\ndevice: cuda, dtype: torch.float32, 10000 times         0.14666561130434275\ndevice: cuda, dtype: torch.float64, 10000 times         0.1540807681158185\n```\nAfter:\n```\n_rshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.16366520430892706\ndevice: cpu, dtype: torch.uint8, 100000 times           0.16091545950621367\ndevice: cpu, dtype: torch.int16, 100000 times           0.1659633992239833\ndevice: cpu, dtype: torch.int32, 100000 times           0.1682385364547372\ndevice: cpu, dtype: torch.int64, 100000 times           0.17289020214229822\ndevice: cpu, dtype: torch.float32, 100000 times         0.24359441827982664\ndevice: cpu, dtype: torch.float64, 100000 times         0.21783945057541132\ndevice: cuda, dtype: torch.int8, 100000 times           1.2517220517620444\ndevice: cuda, dtype: torch.uint8, 100000 times          1.260181212797761\ndevice: cuda, dtype: torch.int16, 100000 times          1.2681935774162412\ndevice: cuda, dtype: torch.int32, 100000 times          1.2764465296640992\ndevice: cuda, dtype: torch.int64, 100000 times          1.294325228780508\ndevice: cuda, dtype: torch.float32, 100000 times                1.3062216322869062\ndevice: cuda, dtype: torch.float64, 100000 times                1.303224254399538\n__rshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.027045012451708317\ndevice: cpu, dtype: torch.uint8, 10000 times            0.026978280395269394\ndevice: cpu, dtype: torch.int16, 10000 times            0.025594274513423443\ndevice: cpu, dtype: torch.int32, 10000 times            0.02593063935637474\ndevice: cpu, dtype: torch.int64, 10000 times            0.02668109256774187\ndevice: cpu, dtype: torch.float32, 10000 times          0.09746317192912102\ndevice: cpu, dtype: torch.float64, 10000 times          0.1644029449671507\ndevice: cuda, dtype: torch.int8, 10000 times            0.12530914042145014\ndevice: cuda, dtype: torch.uint8, 10000 times           0.12615622486919165\ndevice: cuda, dtype: torch.int16, 10000 times           0.12741118855774403\ndevice: cuda, dtype: torch.int32, 10000 times           0.1284919548779726\ndevice: cuda, dtype: torch.int64, 10000 times           0.12974756956100464\ndevice: cuda, dtype: torch.float32, 10000 times         0.13044228963553905\ndevice: cuda, dtype: torch.float64, 10000 times         0.13918257877230644\n__irshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.19456563983112574\ndevice: cpu, dtype: torch.uint8, 100000 times           0.190769555978477\ndevice: cpu, dtype: torch.int16, 100000 times           0.2002257639542222\ndevice: cpu, dtype: torch.int32, 100000 times           0.20456529594957829\ndevice: cpu, dtype: torch.int64, 100000 times           0.2043834924697876\ndevice: cpu, dtype: torch.float32, 100000 times         0.2832390898838639\ndevice: cpu, dtype: torch.float64, 100000 times         0.2582795573398471\ndevice: cuda, dtype: torch.int8, 100000 times           1.304957083426416\ndevice: cuda, dtype: torch.uint8, 100000 times          1.3216373259201646\ndevice: cuda, dtype: torch.int16, 100000 times          1.3238621400669217\ndevice: cuda, dtype: torch.int32, 100000 times          1.333009460940957\ndevice: cuda, dtype: torch.int64, 100000 times          1.3835567953065038\ndevice: cuda, dtype: torch.float32, 100000 times                1.4483617274090648\ndevice: cuda, dtype: torch.float64, 100000 times                1.4179155295714736\n__irshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.03196091763675213\ndevice: cpu, dtype: torch.uint8, 10000 times            0.03048650734126568\ndevice: cpu, dtype: torch.int16, 10000 times            0.03048624936491251\ndevice: cpu, dtype: torch.int32, 10000 times            0.030591044574975967\ndevice: cpu, dtype: torch.int64, 10000 times            0.031246556900441647\ndevice: cpu, dtype: torch.float32, 10000 times          0.10918692220002413\ndevice: cpu, dtype: torch.float64, 10000 times          0.18057993799448013\ndevice: cuda, dtype: torch.int8, 10000 times            0.13614848721772432\ndevice: cuda, dtype: torch.uint8, 10000 times           0.130373639985919\ndevice: cuda, dtype: torch.int16, 10000 times           0.1332557238638401\ndevice: cuda, dtype: torch.int32, 10000 times           0.1331850504502654\ndevice: cuda, dtype: torch.int64, 10000 times           0.1363008264452219\ndevice: cuda, dtype: torch.float32, 10000 times         0.1370363561436534\ndevice: cuda, dtype: torch.float64, 10000 times         0.1442740885540843\n```\nFix https://github.com/pytorch/pytorch/issues/24512 #24516  https://github.com/pytorch/pytorch/issues/24659  https://github.com/pytorch/pytorch/issues/24663\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31594\n\nDifferential Revision: D19346542\n\nPulled By: ezyang\n\nfbshipit-source-id: 37dd00b86898810b850cf4769c3af8aea6d4596b", "pr_number": "31594", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/THC/THCTensorMathPairwise.cu", "aten/src/THC/THCTensorMathPointwise.cuh", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPairwise.h", "aten/src/THC/generic/THCTensorMathPointwise.cu"], "labels": ["merged", "open source"]}, "c5a362a96d": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/hhvm/hsl/commit/b14a430062ad95c378765899f955457f3454f2f9\nhttps://github.com/facebook/fb303/commit/c1c54260184257158d2b3feb3fc276045120c054\nhttps://github.com/facebook/fbthrift/commit/42d18a93c4a411cef3367a35f88c48637bfa533e\nhttps://github.com/facebook/fbzmq/commit/a4e11e872173de7824a70f977c75d5ca7fbe9f2d\nhttps://github.com/facebook/folly/commit/25c971b0c39abc1a7a1b449e5338e1b49916da30\nhttps://github.com/facebook/litho/commit/b2ea65322fd3b6f41541940e30034695ed41c4c0\nhttps://github.com/facebook/mcrouter/commit/e86573b6ded555668a4b6b6215f3e702c7e4e801\nhttps://github.com/facebook/proxygen/commit/31d721301cd410079355e4ab2a46864d57818ce5\nhttps://github.com/facebook/rocksdb/commit/687119aeaf3799e599976330cfe673a8c4a81511\nhttps://github.com/facebook/wangle/commit/25cad9547dfe6c2f8dbac093872643204dcb43b5\nhttps://github.com/facebookincubator/fizz/commit/428862c04540c35881d800630ef30bcf6d7d24f3\nhttps://github.com/facebookincubator/katran/commit/95640f80d874942439337e5d5bcffcff4559b240\nhttps://github.com/facebookincubator/mvfst/commit/0e4db05b37545ee2f1684ffb8eb0321698659da8\nhttps://github.com/facebookincubator/profilo/commit/5cb83de9cc2b7270eb8004361c8a61020dfad250\nhttps://github.com/pytorch/fbgemm/commit/4fdb800074307aeb996f48746ce1341fa6db90c6\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: bcd533c540c1170844dbf2b23538d72c95a0d304", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "f995ec2076": {"title": "Remove qconfig_dict in top level eager mode quantization API (#31972)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31972\n\nSince eager mode quantization requires many user modifications, we can't\nconsistently quantize a given model by just changing qconfig_dict, therefore\nthe top level `qconfig_dict` is not that useful.\nfixes: https://github.com/pytorch/pytorch/issues/31549\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19330691\n\nfbshipit-source-id: 8aee6e5249e0c14e8a363ac1a83836e88887cd7d", "pr_number": "31972", "files_changed": ["torch/quantization/quantize.py"], "labels": ["merged"]}, "ab5eb65e74": {"title": "gate torch_global_deps with BUILD_SHARED_LIBS flag (#32011)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32011\n\nRun into build problem with Ninja + code analysis build as follows:\n```\nThe install of the torch_global_deps target requires changing an RPATH from\nthe build tree, but this is not supported with the Ninja generator unless\non an ELF-based platform.\n```\n\nSeems we don't need build the target for static build mode?\n\nVerified code analyzer works with the patch.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19336818\n\nPulled By: ljk53\n\nfbshipit-source-id: 37f45a9392c45ce92c1df40d739b23954e50a13a", "pr_number": "32011", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["merged"]}, "03ff3eb94d": {"title": "skip TEST_DILL on Python2 (#32027)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32027\n\nThe test was added in #30985 for #28313. Seems the fix only works for\nPython3 but doesn't work on Python2. The current Python2 CI docker image\ndoesn't have `dill` module installed at all so it's not captured.\n\nI'm trying to build and push new CI docker image which has `dill` installed\nand I verified it's the latest version 0.3.1.1 but the fix doesn't seem\nto work and blocks me from upgrading image version. It works for Python3\ndocker image though...\n\nHere is a succeeded job with old image (no dill installed):\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/4192688\n\nHere is a failed job with new image (dill installed):\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/4192679\n\nThis PR bypasses the test for Py2 to unblock docker image change. We\ncan figure out a proper fix for Py2 later.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19341451\n\nPulled By: ljk53\n\nfbshipit-source-id: d5768de8cbaf1beba8911da76f4942b8f210f2d2", "pr_number": "32027", "files_changed": ["test/common_utils.py"], "labels": ["merged"]}, "16b8ca56b6": {"title": "update docker image version (#31848)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31848\n\nTrigger docker image build and bump up docker image version.\n\nTest Plan: - Check tag at: http://ossci-docker.s3-website.us-east-1.amazonaws.com/pytorch.html\n\nDifferential Revision: D19282725\n\nPulled By: ljk53\n\nfbshipit-source-id: a27b2831a92ff54d80ccbae0f18dadff0469254c", "pr_number": "31848", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-ecr-gc.yml", ".circleci/verbatim-sources/workflows-nightly-android-binary-builds.yml", ".circleci/verbatim-sources/workflows-pytorch-ge-config-tests.yml", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml"], "labels": ["merged"]}, "346005d3ed": {"title": "integrate op dependency analysis process into CI", "body": "Summary:\nCustom build and internal build will depend on the analysis result so\nlet's make sure it doesn't break.\n\nTested locally with LLVM-5.0, LLVM-7 and LLVM-8.\n\nTest Plan: - check CI result\n\nDifferential Revision: D18894637\n\nPulled By: ljk53\n\nfbshipit-source-id: 657854e4bed85a84907e3b6638d158823a56ec80", "pr_number": null, "files_changed": [".circleci/config.yml", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml", ".jenkins/pytorch/build-mobile-code-analysis.sh", ".jenkins/pytorch/build.sh", "tools/code_analyzer/build.sh"], "labels": []}, "c5af0afdcb": {"title": "catch exceptions in ProcessGroupAgent::enqueueSend and report them. (#31023)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31023\n\nAdds support to catch exceptions in ProcessGroupAgent::enqueueSend and\nreport them in the future by marking the future as completed with an exception\nindicating the error. An example of when this could happen is if the receiving\nside aborts when the sender is sending the message, previously, we would hang\nuntil the timeout is hit, and the original exception would be lost.\nghstack-source-id: 96498386\n\nTest Plan: Added a relevant unit test: `test_sender_exceptions` in rpc_test.py\n\nDifferential Revision: D18901981\n\nfbshipit-source-id: 08de26936c4ad45b837219a247088cbea644c04c", "pr_number": "31023", "files_changed": ["test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": ["merged"]}, "28c1258f18": {"title": "Scale init for batch-norm and layer-norm (#31983)", "body": "Summary:\nPer discussion with Fei Tian, we need to add a `scale_init_value` to scale down the output of normalization such as batch-norm and layer-norm.\n\nCurrently we have `sparse_normalization_options` to normalize embedding pooling output. By default, scale = 1.0, we found it's better to set scale from 0.025 to 0.1 https://fb.quip.com/MiKUAibEaYhH\n\nBesides, I am removing the tags from normalizers because it makes more sense to calculate norm ops in distributed trainers, not ps.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31983\n\nTest Plan:\nTesting LN and BN after sum-pooling --\nbaseline f160348514\nLN: f160348609\nBN: f160348710\n\n{F226106518}\n\nLayer norm after sum-pooling fwd_net https://fburl.com/sa4j207n\nLayer norm after dot-prod fwd_net https://fburl.com/twggwyvb\n\n## Unit Tests\nTesting normalization after pooling\n```\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test_4 -- test_sparse_pooling_batch_normalization\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test_4 -- test_dense_sparse_pooling_batch_normalization\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test_4 -- test_sparse_pooling_layer_normalization\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test_4 -- test_dense_sparse_pooling_layer_normalization\n```\n\nTesting normalization after dot-prod\n```\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test -- test_last_layer_use_batch_norm\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test -- test_last_layer_use_layer_norm\n```\n\nDifferential Revision: D19277618\n\nPulled By: SilunWang\n\nfbshipit-source-id: ea323e33e3647ba55d2e808ef09d94ad7b45b934", "pr_number": "31983", "files_changed": ["caffe2/python/layers/batch_normalization.py", "caffe2/python/layers/layer_normalization.py", "caffe2/python/normalizer.py"], "labels": ["fb-exported", "merged"]}, "638e4ad8b9": {"title": "Updated function definition for torch.mode and torch.median in torch docs (#32003)", "body": "Summary:\nIssue: https://github.com/pytorch/pytorch/issues/32002\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32003\n\nDifferential Revision: D19334306\n\nPulled By: anjali411\n\nfbshipit-source-id: fe6a7cc7295b2d582a0b528f353ec64d9085e8c5", "pr_number": "32003", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged"]}, "b6cee03e29": {"title": "C++ tensor indexing: add Slice / TensorIndex (#30424)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30424\n\n`at::indexing::TensorIndex` is used for converting C++ tensor indices such as `{None, \"...\", Ellipsis, 0, true, {1, None, 2}, torch::tensor({1, 2})}` into its equivalent `std::vector<TensorIndex>`, so that further tensor indexing operations can be performed using the supplied indices.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18695902\n\nPulled By: yf225\n\nfbshipit-source-id: d73e14a411cdbec815866b02e75ffd71a9186e89", "pr_number": "30424", "files_changed": ["aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/Indexing.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/TensorIndexing.cpp", "aten/src/ATen/native/TensorIndexing.h", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/Indexing.cu", "test/cpp/api/CMakeLists.txt", "test/cpp/api/tensor_indexing.cpp"], "labels": ["merged"]}, "20e5c90d82": {"title": "accept url query when rank or wolrd_size is specified (#32016)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32016\n\nThe previously logic will raise exception when there is query in url when rank or world_size is specified\nThe fix will parse the url and stitch rank and world_size into url.query and regenerate the url.\n\nTest Plan: f161291877\n\nDifferential Revision: D19337929\n\nfbshipit-source-id: 6bb3a07716dda5233553804000b706052ff18db8", "pr_number": "32016", "files_changed": ["torch/distributed/rendezvous.py"], "labels": ["fb-exported", "merged"]}, "927c2a02b0": {"title": "enable autograd profiler to work with RPC and RRef. (#31381)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31381\n\nThis PR adds support for being able to profile both sync and async RPCs, so that users can use the autograd profiler and be able to view metrics such as RPC latency and number of calls in the profiler output.\n\nThe way this is implemented is by using the existing `RecordFunction` class provided by the autograd profiler. We create a `RecordFunction` instance when sending an RPC, if autograd profiling is enabled. We also invoke the starting callbacks on this `RecordFunction` instance, this does things such as start the CPU timer.  This instance is then persisted across the lifetime of the RPC by attaching it to the `Future` created by the RPC. When the RPC is finished (i.e. when `future->markComplete()` is called), we run the `RecordFunction` instance's end callbacks, which among other things, stops the timer so that we get the correct RPC latency.\n\nThe `RecordFunction` and relevant callbacks in `profiler.cpp` are modified slightly to support running end callbacks from a different thread (which is needed since futures are marked as completed by a different thread than the main RPC thread). By default, the autograd profiler uses a `thread_local` list of `Events` and `thread_id`. However, since we'd like to run the `RecordFunction`'s callbacks from a different thread, we would like to access the list of `Events` created by the original thread. This is done by attaching the `thread_id` for the event to the `RecordFunction`, and then looking up the event with that thread in `all_event_lists` (see the changes in `profiler.cpp`). To ensure that the original behavior does not change in the profiler, this described behavior is only run when a user calls `setOverrideThreadId()` on the `RecordFunction` object.\nghstack-source-id: 96527291\n\nTest Plan: Added a unit test.\n\nDifferential Revision: D19053322\n\nfbshipit-source-id: 9a27a60c809fc4fdb16fa5d85085f3b6b21abfbb", "pr_number": "31381", "files_changed": ["test/rpc_test.py", "test/test_autograd.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler.cpp", "torch/csrc/autograd/profiler.h", "torch/csrc/autograd/record_function.cpp", "torch/csrc/autograd/record_function.h", "torch/csrc/distributed/autograd/utils.cpp", "torch/csrc/distributed/autograd/utils.h", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_functions.h", "torch/distributed/rpc/api.py", "torch/distributed/rpc/internal.py"], "labels": ["merged"]}, "46f32e136a": {"title": "Revert \"Support PyTorch ROCm CI on Ubuntu18.04 (#31886)\" (#31946)", "body": "Summary:\nThis reverts commit 4ee9c562188ae930cb2520cfce7805f55acaf968.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31946\n\nDifferential Revision: D19368391\n\nPulled By: bddppq\n\nfbshipit-source-id: 63d032a5256ff4da7247fb1092be314c5b133eb6", "pr_number": "31946", "files_changed": ["docker/caffe2/jenkins/common/install_clang.sh", "docker/caffe2/jenkins/common/install_python.sh", "docker/caffe2/jenkins/common/install_rocm.sh", "docker/caffe2/jenkins/ubuntu-rocm/Dockerfile"], "labels": ["merged", "module: rocm", "open source"]}, "14593f077f": {"title": "remove list specialization from ivalue (#30734)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30734\n\nWhat are specialized lists?\n\nThe IValues that hold List[int], List[Tensor], and List[AnythingElse] are different C++ types.\ne.g. List[int] has a std::vector<int> while List[AnythingElse] holds a std::vector<IValue>.\n\nWhy do we have specialized lists?\n\nWhen we first created the JIT we needed to bind the ATen C++ API which has std::vector<int>,\nstd::vector<Tensor> as inputs. The easiest way to match this API was to make our IValues contain\nthese same types. Conversion was just unwrapping the IValue, very easy and cheap.\n\nWhat is the problem with specialized lists?\n\nWe end up with significant special cases through the compiler. Other types like Dict are not\nspecialized. So in the Pickler, for instance, there is a single piece of logic to handle\ntheir serialization. For Lists, we end up with multiple cases. Furthermore, it doesn't\nmatch Python, leading to problems along translation boundaries. Our pickle serialization\nis slightly different than python, so it is harder to load objects from our IValue serialization\nas Python values.\n\nThey also make it harder to provide an easy-to-use user API. We'd like to match pybind11 for C++\nbindings to TorchScript. This would entail having a single torch::List class (untemplated)\nthat can be used to construct inputs. This is made much harder if the underlying ivalue needs\nto be different depending on the type inside the list. The ideal case would be to have a constructor like\n\n```\ntemplate<typename T>\nList(std::vector<T> foo);\n```\n\nIt would then set up the type tags correctly based on type T, without the need for passing tags.\n\nDo specialized lists improve perf?\n\nNot in a way we have been able to measure. Our major concern initially was having to translate\na std::vector<IValue> to std::vector<int> to call ATen functions. This was especially a concern\nfor aten::_convolution which takes a number of mostly-constant lists of integers. However,\nwhen we measure the effect of actually having to do this conversion for an aten::_convolution,\nit does not take measurable time (benchmark results below).\nThis is true even if you use a trivial convolution (e.g. 1x1x1), and comment out the actual convolution code.\n\nWhat are the issues removing them?\n\nThis PR removes list specialization but keeps the serialization format, and IValue APIs almost exactly\nthe same. The only visible change is that toTensorListRef and family have turned into toTensorVector\nbecause they now return by value a copy of the list as a vector.\n\nFurther PRs can then clean up the complexity issues that arose from speclization. This will likely\ninvolve removing the isTensorList/isIntList functions, and refactoring the code that used them to\nwork generically. At some point we will also change serialization to no longer write specialized\nlists in the pickle binary. This is forward incompatible, so will go in its own PR.\n\nBenchmark:\n```\nimport torch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\n\nclass MnistNet(nn.Module):\n    def __init__(self):\n        super(MnistNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 1, kernel_size=1)\n        self.conv2 = nn.Conv2d(1, 1, kernel_size=1)\n\n    def forward(self, x):\n        for i in range(10):\n            x = F.relu(self.conv1(x))\n            x = F.relu(self.conv2(x))\n        return x\n\nmodel = MnistNet()\nx = torch.rand(1, 1, 1, 1)\nr = torch.jit.trace(model, x )\nr(x)\nr(x)\nr(x)\nr(x)\nprint(torch.jit.last_executed_optimized_graph())\n\nwhile True:\n    b = time.time()\n    for i in range(100):\n        r(x)\n    e = time.time()\n    print(e - b)\n```\n\nResults (no observable difference):\n\n```\nBefore (actual conv)\n0.13251137733459473\n0.13260436058044434\n0.13276338577270508\n0.1327497959136963\n0.13250041007995605\n0.13270330429077148\n0.13290190696716309\n0.13265132904052734\n0.13274288177490234\n0.1326758861541748\n0.13253355026245117\n0.13254785537719727\n0.13260746002197266\n0.13285017013549805\n0.13264012336730957\n0.132490873336792\n0.13280034065246582\n0.13243484497070312\n0.1325232982635498\n0.1326127052307129\n0.13264131546020508\n0.13274383544921875\n0.13298296928405762\n0.1326909065246582\n-------------------\nAfter (actual conv)\n0.13127517700195312\n0.13150334358215332\n0.13092470169067383\n0.13102364540100098\n0.13134360313415527\n0.13155555725097656\n0.13314104080200195\n0.13151955604553223\n0.13160037994384766\n0.1315293312072754\n0.13137340545654297\n0.13148093223571777\n0.131455659866333\n0.1327371597290039\n0.13134026527404785\n0.13152337074279785\n0.13151192665100098\n0.13165974617004395\n0.13403725624084473\n0.13251852989196777\n0.13135504722595215\n0.1315624713897705\n0.1317615509033203\n0.1314380168914795\n0.13157200813293457\n--------------------\n\nThe following replace the convolution operator with a no-op, to show\nthat even if the conv op was made faster, then we still would not see\na difference:\n\nBefore (fake conv)\n0.0069539546966552734\n0.0069522857666015625\n0.007120847702026367\n0.007344722747802734\n0.007689952850341797\n0.007932662963867188\n0.00761723518371582\n0.007501363754272461\n0.007532835006713867\n0.007141828536987305\n0.007174253463745117\n0.007114410400390625\n0.007071495056152344\n------------------\nAfter (fake conv)\n0.007458209991455078\n0.007337093353271484\n0.007268190383911133\n0.007313251495361328\n0.007306575775146484\n0.007468700408935547\n0.0073091983795166016\n0.007308483123779297\n0.007538318634033203\n0.007356882095336914\n0.007464170455932617\n0.007372140884399414\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18814702\n\nPulled By: zdevito\n\nfbshipit-source-id: 0371c73b63068fdc12f24b801371ea90f23531a6", "pr_number": "30734", "files_changed": ["aten/src/ATen/core/Dict_inl.h", "aten/src/ATen/core/List.h", "aten/src/ATen/core/List_inl.h", "aten/src/ATen/core/List_test.cpp", "aten/src/ATen/core/boxing/boxing.h", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/jit_type.h", "caffe2/core/operator.h", "caffe2/operators/experimental/c10/cpu/filler_cpu.cc", "test/cpp/jit/test_ivalue.cpp", "test/cpp/jit/test_misc.cpp", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/register_mobile_ops.cpp", "torch/csrc/jit/passes/peephole.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/passes/utils/check_alias_annotation.cpp", "torch/csrc/jit/pickler.cpp", "torch/csrc/jit/pickler.h", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/register_c10_ops.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/register_special_ops.cpp", "torch/csrc/jit/script/function_schema_parser.cpp", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h", "torch/csrc/jit/unpickler.cpp", "torch/csrc/jit/unpickler.h", "torch/csrc/jit/vararg_functions.h"], "labels": ["jit", "merged"]}, "77c2c78e01": {"title": "Fix typographical error in torch.triu docstring (#32067)", "body": "Summary:\nbelow --> above\n\nFixes https://github.com/pytorch/pytorch/issues/32032\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32067\n\nDifferential Revision: D19355788\n\nPulled By: zou3519\n\nfbshipit-source-id: dc7a2538a78cd11e72d47ad923ef50599a5a87e2", "pr_number": "32067", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "e74a215ade": {"title": "Changed clip_grad_norm_ total_norm calculation (#32020)", "body": "Summary:\nRedefines the computation of the total_norm to increase performance as shown in https://github.com/pytorch/pytorch/issues/31474.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32020\n\nDifferential Revision: D19353309\n\nPulled By: ngimel\n\nfbshipit-source-id: bf7530dcd39f56614a211b5f21445864d4f2e875", "pr_number": "32020", "files_changed": ["torch/nn/utils/clip_grad.py"], "labels": ["merged", "open source"]}, "4002fec509": {"title": "Display NVCC version in CI for convenience to look at", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32069\n\nDifferential Revision: D19372943\n\nPulled By: ezyang\n\nfbshipit-source-id: c78e5779d4139e42df1f235db65d8c0399ffa1a2", "pr_number": "32069", "files_changed": [".jenkins/pytorch/build.sh"], "labels": ["merged", "open source"]}, "9a4219eb39": {"title": "Install complete set of headers for ROCm build (#32076)", "body": "Summary:\nThis PR adds a more complete list of pytorch header files to be installed at build time. It also fixes one instance of including a header from local src directory instead of installed directory.\nA more complete set of headers enable other modules to correctly work with pyTorch built for ROCm.\n\ncc: ezyang bddppq iotamudelta\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32076\n\nDifferential Revision: D19372933\n\nPulled By: ezyang\n\nfbshipit-source-id: 3b5f3241c001fa05ea448c359a706ce9a8214aa0", "pr_number": "32076", "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h", "setup.py"], "labels": ["merged", "open source"]}, "8e93159fb6": {"title": "CUDA 8 cleanup (#32013)", "body": "Summary:\nCUDA 8 is no longer supported\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32013\n\nDifferential Revision: D19372963\n\nPulled By: ezyang\n\nfbshipit-source-id: e584d7d5d5908933221ea4400234b3e6e7c32e7a", "pr_number": "32013", "files_changed": [".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/docker/common/install_conda.sh", ".circleci/verbatim-sources/workflows-docker-builder.yml", ".jenkins/caffe2/build.sh", ".jenkins/pytorch/multigpu-test.sh", ".jenkins/pytorch/test.sh", "docker/caffe2/ubuntu-16.04-cuda8-cudnn6-all-options/Dockerfile", "docker/caffe2/ubuntu-16.04-cuda8-cudnn7-all-options/Dockerfile"], "labels": ["merged", "open source"]}, "695c4f1bab": {"title": "Fix a typo in function name: liner -> linear", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32068\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19373360\n\nPulled By: nairbv\n\nfbshipit-source-id: 7696300b5c1dbcd7991fda3311d68807b2960982", "pr_number": "32068", "files_changed": ["aten/src/ATen/native/Normalization.cpp"], "labels": ["merged"]}, "5988d36f58": {"title": "Fix cumprod error for tensors with zero elements (#32070)", "body": "Summary:\nCurrently cumprod crashes for tensors with non-empty dimensions but with zero elements, which could happen when some dimension is zero. This commit fixes the error by checking both dim() and numel() in cumprod backward\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32070\n\nDifferential Revision: D19373200\n\nPulled By: ezyang\n\nfbshipit-source-id: d8ecde33f3330b40a7c611f6faa3b1d707ef2a9a", "pr_number": "32070", "files_changed": ["test/test_torch.py", "tools/autograd/templates/Functions.cpp"], "labels": ["merge-this-please", "merged"]}, "a3dd44653f": {"title": "Fix typo in config script to re-enable libtorch build and test in macOS CI (#32072)", "body": "Summary:\nCurrently, libtorch build and test are not running in macOS CI. This PR fixes the issue.\n\n**Test Plan:**\nCheck that libtorch build and test are running again in macOS CI.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32072\n\nDifferential Revision: D19373615\n\nPulled By: yf225\n\nfbshipit-source-id: 28686ef5895358a2b60db46b1946f21c58c6a18e", "pr_number": "32072", "files_changed": [".jenkins/pytorch/common.sh", ".jenkins/pytorch/macos-test.sh"], "labels": ["merged"]}, "1f34801460": {"title": "More robust mangling (#31978)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31978\n\nCurrently we keep a `mangleIndex_` that's intenral to compilation unit and\njust increment the index when we found the original name is mangled, this doesn't\nguarantee the new name is not defined.\nThis PR fixes the problem by querying whether the new name is defined or not.\nfixes: https://github.com/pytorch/pytorch/issues/31268\n\nTest Plan:\nfixes the issue\n\nImported from OSS\n\nDifferential Revision: D19350535\n\nfbshipit-source-id: fe3262b2838d4208ab72e2cd4a5970b3a792ae86", "pr_number": "31978", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/compiler.cpp"], "labels": ["jit", "merged"]}, "d97413eb7a": {"title": "Change python/cpp docs CI to use a CPU-only image (#32102)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32102\n\nPreviously, the docs CI depended on our CUDA xenial py3 build. This\nmeant that the turnaround time to get signal for docs was very slow\n(I've seen builds that go as much as 3 hours).\n\nFortunately, the docs CI do not (and should not!) rely on CUDA. This\nPR changes it so that the docs CI runs on a CPU-only machine.\n\nFixes #29995\n\nTest Plan:\n- Check CI status on this PR by reading logs for the python and cpp docs\nbuilds.\n- I built the docs locally, once for CPU, and once for CUDA, and\nverified (via diff) that the pages were exactly the same)\n\nDifferential Revision: D19374078\n\nPulled By: zou3519\n\nfbshipit-source-id: 3eb36f692c3c0632d2543d3439c822d51a87b809", "pr_number": "32102", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml"], "labels": ["merged"]}, "d53ce5e4cd": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/b5718e35c8d4cade764e8da36e699915a5ca8dce\nhttps://github.com/facebook/folly/commit/e1af1b05501448f6a26d91d1dba3724c3aea3c0f\nhttps://github.com/facebook/litho/commit/8a34e7f444164c8ff5523db406aeb996d9f80aa3\nhttps://github.com/facebook/proxygen/commit/e9e70ade5bdb35a86fbb699dbf2fe7afbe2e2ab9\nhttps://github.com/facebook/wangle/commit/d9e693ece017ba5ac3de4c5302d638bd5048f650\nhttps://github.com/facebookincubator/fizz/commit/329347c63c1e04607d73325fa42ee752eb624f38\nhttps://github.com/facebookincubator/katran/commit/671b5aa06432f9177a66a52c5c2247ffc3181305\nhttps://github.com/facebookincubator/mvfst/commit/7f3bb0bf37a51aea2869af4325319e921e095158\nhttps://github.com/facebookincubator/profilo/commit/6207e92b9be0d138c07e3bc4fde457184918fc67\nhttps://github.com/pytorch/fbgemm/commit/d4b95d87d48cbab0e29498fcdd2b967c8e49af6c\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: 3c9131bdee0bf8a8ca5c679a95e8ff8a6f805762", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "701ca68882": {"title": "Docs entry for the `is_quantized`", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32075\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19353861\n\nPulled By: z-a-f\n\nfbshipit-source-id: 4249216ac9a4af354a251c62181d65bc14cbfd3e", "pr_number": "32075", "files_changed": ["docs/source/tensors.rst", "torch/_tensor_docs.py"], "labels": ["merged"]}, "632d6fc583": {"title": "Revert D19373615: Fix typo in config script to re-enable libtorch build and test in macOS CI", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19373615\n\nOriginal commit changeset: 28686ef58953\n\nfbshipit-source-id: 432b04adfd9d010e1965846a386f117ebc80e013", "pr_number": null, "files_changed": [".jenkins/pytorch/common.sh", ".jenkins/pytorch/macos-test.sh"], "labels": []}, "f003008d6e": {"title": "Allow TCPStore to pick a port to bind to. (#31674)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31674\n\nThe motivation of this PR was to fix the problem where we would see\n\"Address already in use\" issues for TCPStoreTest due to port conflicts. To\nresolve this:\n\n1. We can now pass in port 0 for TCPStore and retrieve the port it actually\nbound to using a new getPort() API.\n2. Added a `wait` flag to TCPStore constructor indicating whether or not it\nshould wait for workers (defaults to true).\n3. Made `waitForWorkers` a public API to ensure that we can construct TCPStore\nwithout waiting and wait for workers separately. This helps in TCPStoreTest to\nensure we can retrieve the port and pass it to the client stores.\nghstack-source-id: 96486845\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19240947\n\nfbshipit-source-id: 7b1d1cb2730209fac788764845f1dbbe73d75d9b", "pr_number": "31674", "files_changed": ["torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/TCPStore.hpp", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": ["merged"]}, "470c496eb2": {"title": "use cholesky_inverse to compute precision matrix (#32092)", "body": "Summary:\nResolves a long-standing TODO. :D\n\nI also fix the docs of lowrank_mvn which is raised at [forum](https://discuss.pytorch.org/t/lowrankmultivariatenormal-example-raises-valueerror/65381).\n\ncc vishwakftw\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32092\n\nDifferential Revision: D19373912\n\nPulled By: ezyang\n\nfbshipit-source-id: b13129d7c30e87c6f8a6ced86601762a3f5c5624", "pr_number": "32092", "files_changed": ["torch/distributions/lowrank_multivariate_normal.py", "torch/distributions/multivariate_normal.py"], "labels": ["merge-this-please", "merged", "open source"]}, "c474952b5d": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/1f8321394d059496726a165ef02561513f1dc8bd\nhttps://github.com/facebook/fbthrift/commit/024c1d0b43c7332329d9a99c5c9fe837b47b3664\nhttps://github.com/facebook/fbzmq/commit/1d57089fc3858d72099dca1abf9a482647a67fa0\nhttps://github.com/facebook/folly/commit/3c6f1f782c0b97f2c1cd375cc1e4291b7b00980d\nhttps://github.com/facebook/mcrouter/commit/21a27b0f8efa73b11ea70bdec3f7b03af45d43e0\nhttps://github.com/facebook/proxygen/commit/23bb716b629eeda4641b6ccdbdfe9fed01daf16a\nhttps://github.com/facebook/rocksdb/commit/894c6d21aff74d1edd2a355f52f2315d5ba58a60\nhttps://github.com/facebook/wangle/commit/e3e241d7003622987d0644f93182d4dc7d9bfd3f\nhttps://github.com/facebookincubator/fizz/commit/ac4e11d84a7063780fbb2d50764a13d2284cf81b\nhttps://github.com/facebookincubator/katran/commit/c35803ad688b9d11b67d7cbd7b383b46148a2439\nhttps://github.com/facebookincubator/mvfst/commit/647388f265fda37227e852368c52fe915e612e42\nhttps://github.com/facebookincubator/profilo/commit/50a32886308a727c02cc2c1a46d3265dc55dee43\nhttps://github.com/pytorch/fbgemm/commit/b197f0c95acf3b9988dedc2deff2a60029489794\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: 1807ac876a126d221c257edbd4732f9a1240e869", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "a472f0201f": {"title": "Added support for Dim operation in ONNX export (#31928)", "body": "Summary:\nWhile ONNX does not currently directly support the Dim operation on a\ntensor, we can provide the same functionality with two ONNX operations.\nThis allows us to support Dim for all opsets. It may be adventageous to\nadd support for Dim into a future ONNX opset, and use that for more\nefficient code.\nWhile testing dim op found that there is an issue with empty blocks\nwithing if statements. Modified graph generation to prevent generation\nof empty if blocks.\n\nFixes https://github.com/pytorch/pytorch/issues/27569\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31928\n\nReviewed By: hl475\n\nDifferential Revision: D19376602\n\nPulled By: houseroad\n\nfbshipit-source-id: 111682b058a5341f5cca6c1a950c83ae412a4c6c", "pr_number": "31928", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/expect/TestOperators.test_dim.expect", "test/onnx/test_operators.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_conditionals.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_conditionals.h", "torch/onnx/symbolic_opset9.py", "torch/onnx/utils.py"], "labels": ["jit", "merged", "open source", "triaged"]}, "62b1a5f846": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/2156e4892433044836029e07d533c1485268783b\nhttps://github.com/facebook/fbthrift/commit/8c5b4af317141bad7c60fabb1f713bc8507d7917\nhttps://github.com/facebook/fbzmq/commit/be69716784365245b0323768f4795ca4a7caf6b2\nhttps://github.com/facebook/proxygen/commit/4f76ad1fab968f5863417adc84b9f4b68ba8dd30\nhttps://github.com/facebook/wangle/commit/0b12b2f13cad4e3187b68729c25244a76b746c3b\nhttps://github.com/facebookincubator/fizz/commit/0449b53cb1ca806098f866927226d4bc0b752ee5\nhttps://github.com/facebookincubator/katran/commit/1481689822f328f7318f95516060174dbc511c1d\nhttps://github.com/facebookincubator/mvfst/commit/43ffa9bbf04c1784d96c8a13fac0d9b6fa33d5db\nhttps://github.com/pytorch/fbgemm/commit/787d6b6c93b9e5c68bfbc24b44f734d23491b4ca\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: b0080fd1a4c26efbe8f26245fbba7740fbac08f3", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "26621d101f": {"title": "remove simple .data from torch/nn", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31482\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19303185\n\nPulled By: albanD\n\nfbshipit-source-id: 610eae096bab24a7b9f651b9af2e3ecd19df55b0", "pr_number": "31482", "files_changed": ["torch/autograd/gradcheck.py", "torch/testing/__init__.py"], "labels": ["merged"]}, "c036fbdc5c": {"title": "remove .data from torch/jit", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31480\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19303244\n\nPulled By: albanD\n\nfbshipit-source-id: ec66b32353f2f9b16072185ecde3ae8abbe09a35", "pr_number": "31480", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "77c78b7d28": {"title": "remove .data from torch/nn doc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31481\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19303242\n\nPulled By: albanD\n\nfbshipit-source-id: 4f650df9e9e302a299175967bcc6e30a5099fa2a", "pr_number": "31481", "files_changed": ["torch/nn/functional.py", "torch/nn/modules/module.py"], "labels": ["merged"]}, "8d472bab6b": {"title": "Make torch.backends.mkldnn usable without import", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32055\n\nDifferential Revision: D19373220\n\nPulled By: ezyang\n\nfbshipit-source-id: 50ab3ff70fc893c81123419c4d3cf2e3e48a0a93", "pr_number": "32055", "files_changed": ["torch/__init__.py"], "labels": ["merge-this-please", "merged", "open source"]}, "5f1a881cb8": {"title": "Add private user tensor type IDs for experimentation. (#31830)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31830\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19330312\n\nPulled By: ezyang\n\nfbshipit-source-id: fe2e53e732e946088e983ec45fed2393436f0517", "pr_number": "31830", "files_changed": ["c10/core/TensorTypeId.h"], "labels": ["merged"]}, "dbd737158b": {"title": "support torch script call over rpc (#30063)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30063\n\nThis diff makes following changes:\n1. Providing a new set of python rpc privated APIs, they can accept an annotated TorchScript call and this call can be serialized, deserialized and executed in C++ without GIL. These privated APIs will be binded to JIT in the future, and they are different from public APIs as future JIT binded private APIs will be able to accept qualified_name, not callables. These private APIs are subject to be deprecated once JIT supports torch script function to be a JIT type.\n\nAlso, these APIs require torch script function to be defined and annotated by users in python land, it can not be script class/module constructor or class/module methods.\n\n2. This diff also allows public rpc APIs to accept an annotated TorchScript call and execute code path that above private APIs ran on. Therefore if users invoke an annotated TorchScript call over RPC, this call can be serialized, deserialized and executed in C++ without GIL as well.\n\n3. The above private APIs call a newly defined C++ function to make rpc torch script call to be serialized, deserialized and executed in C++ land. This C++ function returns an ivalue::Future. so that in follow up diff this C++ function can be called when these privated APIs are binded to JIT.\n\n4. script_call.cpp/.h and request_callback_impl.cpp files are refactored accordingly so that torch script call and builtin call can share same message type and codes.\n\n5. refactored deserializeResponse() and added a new utility to deserizalize response to IValue\n\nghstack-source-id: 96638829\n\nTest Plan: unit test\n\nDifferential Revision: D18482934\n\nfbshipit-source-id: bd82a0d820c47a8e45b2e7c616eca06573f7d7ea", "pr_number": "30063", "files_changed": ["test/dist_autograd_test.py", "test/rpc_test.py", "tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.h", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/script_call.cpp", "torch/csrc/distributed/rpc/script_call.h", "torch/csrc/distributed/rpc/script_functions.cpp", "torch/csrc/distributed/rpc/script_functions.h", "torch/csrc/distributed/rpc/script_remote_call.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "1487582ba7": {"title": "Switch important CI from CUDA 9 to 10.1 (#31951)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31427\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31951\n\nDifferential Revision: D19393566\n\nPulled By: ezyang\n\nfbshipit-source-id: 06f9637791494a453d3fbef765840dc9f9805196", "pr_number": "31951", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-ecr-gc.yml", ".circleci/verbatim-sources/workflows-nightly-android-binary-builds.yml", ".circleci/verbatim-sources/workflows-pytorch-ge-config-tests.yml", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml", ".jenkins/pytorch/build.sh", ".jenkins/pytorch/common.sh", ".jenkins/pytorch/multigpu-test.sh", ".jenkins/pytorch/test.sh"], "labels": ["merged", "open source"]}, "fa60e1150d": {"title": "Fix tensor^tensor derivative for 0 base entries", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32062\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19394259\n\nPulled By: agolynski\n\nfbshipit-source-id: 836525e03573af838511ad5b4cc87ec2c1536a5e", "pr_number": "32062", "files_changed": ["test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/templates/Functions.cpp"], "labels": ["merged"]}, "b783a75aa3": {"title": "Fix scalar^tensor derivative for scalars that are zero", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32063\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19394258\n\nPulled By: agolynski\n\nfbshipit-source-id: 3eed0f9cc1b8c677c6948c927d007044be67fe7f", "pr_number": "32063", "files_changed": ["test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/templates/Functions.cpp"], "labels": ["merged"]}, "0664c6bbfd": {"title": "Add ccls cache to gitignore (#31437)", "body": "Summary:\n`ccls` [puts a cache](https://github.com/MaskRay/ccls/wiki/Customization#cachedirectory) in the working directory by default, this PR adds it to gitignore so git doesn't pick it up\n](https://our.intern.facebook.com/intern/diff/19165007/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31437\n\nPulled By: driazati\n\nDifferential Revision: D19165007\n\nfbshipit-source-id: 41012eb0ece2df60b8566d7929710b154c38ee66", "pr_number": "31437", "files_changed": [".gitignore"], "labels": ["merged"]}, "61e509b992": {"title": "Skip un-runnable tests (#31965)", "body": "Summary:\n`test_init_ops` calls `orthogonal_` which fails without lapack (this test was just missing a skip condition)\n\nThe cpp tests would fail with a `undefined symbol` error if run with `BUILD_TESTS=0`, so this PR skips them if that flag is `0`\n](https://our.intern.facebook.com/intern/diff/19320064/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31965\n\nPulled By: driazati\n\nDifferential Revision: D19320064\n\nfbshipit-source-id: d1dcd36714107688ded25a414e8969abe026bd03", "pr_number": "31965", "files_changed": ["test/jit/test_unsupported_ops.py", "test/jit/unsupported_ops.py", "test/test_jit.py", "torch/CMakeLists.txt", "torch/csrc/jit/init.cpp"], "labels": ["jit", "merged"]}, "b0ac425dc4": {"title": "Emit warning from deprecated torch function signatures (#32009)", "body": "Summary:\nContinuation of https://github.com/pytorch/pytorch/issues/31514, fixes https://github.com/pytorch/pytorch/issues/28430\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32009\n\nTest Plan:\nI verified that the deprecation warnings only occur once on a relevant workflow. Built with:\n\n```\nbuck build mode/opt //vision/fair/detectron2/tools:train_net\n```\n\nRan with:\n\n```\nDETECTRON2_ENV_MODULE=detectron2.fb.env ~/local/train_net.par --config-file configs/quick_schedules/retinanet_R_50_FPN_instant_test.yaml --num-gpus 1 SOLVER.IMS_PER_BATCH 2\n```\n\nInspected log:\n\n```\n[01/14 07:28:13 d2.engine.train_loop]: Starting training from iteration 0\nbuck-out/opt/gen/caffe2/generate-code=python_variable_methods.cpp/python_variable_methods.cpp:1299: UserWarning: This overload of add is deprecated:\nadd(Number alpha, Tensor other)\nConsider using one of the following signatures instead:\nadd(Tensor other, Number alpha)\nbuck-out/opt/gen/caffe2/generate-code=python_variable_methods.cpp/python_variable_methods.cpp:1334: UserWarning: This overload of add_ is deprecated:\nadd_(Number alpha, Tensor other)\nConsider using one of the following signatures instead:\nadd_(Tensor other, Number alpha)\n[01/14 07:28:25 d2.utils.events]: eta: 0:00:10  iter: 19  total_loss: 1.699  loss_cls: 1.185  loss_box_reg: 0.501  time: 0.5020  data_time: 0.0224  lr: 0.000100  max_mem: 3722M\n[01/14 07:28:35 fvcore.common.checkpoint]: Saving checkpoint to ./output/model_final.pth\n```\n\nDifferential Revision: D19373523\n\nPulled By: ezyang\n\nfbshipit-source-id: 75756de129645501f43ecc4e3bf8cc0f78c40b90", "pr_number": "32009", "files_changed": ["test/common_utils.py", "test/test_torch.py", "tools/autograd/gen_python_functions.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": ["merged", "open source"]}, "2bb9dbeffa": {"title": "omit constexpr with nvcc on clang (#32149)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32149\n\nThis is an attempt at clarifying some of the preprocessor boolean logic that was getting more and more complicated. The previous logic used constexpr with nvcc on clang; which we were getting compiler failures on in ovrsource with mode/linux/* (based on platform007).\n\nTest Plan:\novrsource xplat/caffe2 compiles\nfbsource sandcastle green\n\nDifferential Revision: D19385409\n\nfbshipit-source-id: 60a02bae9854388b87510afdd927709673a6c313", "pr_number": "32149", "files_changed": ["c10/util/TypeIndex.h"], "labels": ["fb-exported", "merged"]}, "4a26bb9b18": {"title": "Suppress pip logs (#31912)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31912\n\n### Summary\n\nClean up the logs from pip-install.\n\n### Test Plan\n\n- Don't break the iOS simulator build\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19395526\n\nPulled By: xta0\n\nfbshipit-source-id: a638a209cab801ce90c8615e7ea030b1ab0939f3", "pr_number": "31912", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml"], "labels": ["merged"]}, "51a34545e9": {"title": "Revert D18482934: support torch script call over rpc", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18482934\n\nOriginal commit changeset: bd82a0d820c4\n\nfbshipit-source-id: ca5e50fb0a883ee311aeb310198d84ad28062158", "pr_number": null, "files_changed": ["test/dist_autograd_test.py", "test/rpc_test.py", "tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.h", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/script_call.cpp", "torch/csrc/distributed/rpc/script_call.h", "torch/csrc/distributed/rpc/script_functions.cpp", "torch/csrc/distributed/rpc/script_functions.h", "torch/csrc/distributed/rpc/script_remote_call.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py"], "labels": []}, "9bf0479b65": {"title": "Fix the passing-by-ref constructor of OperatorName. (#32170)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32170\n\nStack from [ghstack](https://github.com/ezyang/ghstack):\nChange the overload name from passing by const ref to by value and move.\n* **#32170 Fix the passing-by-ref constructor of OperatorName.**\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19396225\n\nPulled By: iseeyuan\n\nfbshipit-source-id: e946c47647e1f8d23d7565cfe93f487845e7f24c", "pr_number": "32170", "files_changed": ["aten/src/ATen/core/operator_name.h"], "labels": ["merged"]}, "ecc3497172": {"title": "Update Gemfile (#32147)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32147\n\n### Summary\n\nGot some security warnings regarding the ruby dependencies. This diff updates the packages in Gemfile.\n\n```\nGitHub has detected that a package defined in the ios/TestApp/Gemfile.lock file of the pytorch/pytorch repository contains a security vulnerability.\n\nPackage name: excon\nAffected versions: < 0.71.0\nFixed in version: 0.71.0\nSeverity: LOW\n\nIdentifier(s):\nGHSA-q58g-455p-8vw9\nCVE-2019-16779\n```\n\n### Test Plan\n\n- Won't affect the existing iOS CI jobs\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19400087\n\nPulled By: xta0\n\nfbshipit-source-id: 34b548d136cfd6b68fcc53bf0b243461bd7afd64", "pr_number": "32147", "files_changed": ["ios/TestApp/Gemfile.lock"], "labels": ["merged"]}, "f3b67bf750": {"title": "Fix frontend kwarg defualts error (#32146)", "body": "Summary:\nThis was not tested before, fixes #32139 (which was actually a false positive, functions with kwargs but without defaults on those kwargs are supported). This PR adds testing for both cases and cleans up the error reporting.\n](https://our.intern.facebook.com/intern/diff/19385828/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32146\n\nPulled By: driazati\n\nDifferential Revision: D19385828\n\nfbshipit-source-id: 5eab74df6d02f8e1d7ec054cafb44f909f9d637e", "pr_number": "32146", "files_changed": ["test/test_jit_py3.py", "torch/jit/frontend.py"], "labels": ["jit", "merged"]}, "f6f1e0aef5": {"title": "Automatic update of fbcode/onnx to 65020daafa9183c769938b4512ce543fd5740f8f (#32125)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32125\n\nPrevious import was 57ebc587fcf3913b4be93653b0dd58c686447298\n\nIncluded changes:\n- **[65020daa](https://github.com/onnx/onnx/commit/65020daa)**: better error message for undefined inputs (#2540) <Yuxin Wu>\n- **[8afff0e9](https://github.com/onnx/onnx/commit/8afff0e9)**: bump ORT version (#2538) <Lu Fang>\n- **[3d9ca57e](https://github.com/onnx/onnx/commit/3d9ca57e)**: fix name of directory (#2537) <Prasanth Pulavarthi>\n- **[df8fa2c9](https://github.com/onnx/onnx/commit/df8fa2c9)**: Repository guidelines (#2539) <Prasanth Pulavarthi>\n- **[49cc2f02](https://github.com/onnx/onnx/commit/49cc2f02)**: Update CircleCI job to use Python3.6 (#2527) <bddppq>\n- **[25ff79a4](https://github.com/onnx/onnx/commit/25ff79a4)**: Fix wrong model version, it's not 12 (the onnx_opset_version()), not 11 (the opset version of the latest stable), but 10 (#2478) <daquexian>\n- **[7cebaed5](https://github.com/onnx/onnx/commit/7cebaed5)**: Fix Windows py3.5 CI (#2529) <bddppq>\n- **[eddae00e](https://github.com/onnx/onnx/commit/eddae00e)**: Correct the order of arguments of InferShapes (#2500) <Shinichiro Hamaji>\n- **[41b5afe6](https://github.com/onnx/onnx/commit/41b5afe6)**: Include <ostream> in common/status.h (#2519) <Casey Carter>\n- **[423f1977](https://github.com/onnx/onnx/commit/423f1977)**: add 8 bit support to maxpool op (#2510) <Ashwini Khade>\n- **[78593c2f](https://github.com/onnx/onnx/commit/78593c2f)**: add 8 bit support to reducemin and reducemax ops (#2516) <Ashwini Khade>\n\nTest Plan: cont build\n\nReviewed By: benoitsteiner\n\nDifferential Revision: D19380034\n\nfbshipit-source-id: ddce8450864a611773b2a32e2f0254c9bb6b6906", "pr_number": "32125", "files_changed": ["caffe2/python/onnx/tests/onnx_backend_test.py", "third_party/onnx"], "labels": ["fb-exported", "merged"]}, "2bd179147a": {"title": "Fix typo in config script to re-enable libtorch build and test in macOS CI (#32072)", "body": "Summary:\nCurrently, libtorch build and test are not running in macOS CI. This PR fixes the issue.\n\n**Test Plan:**\nCheck that libtorch build and test are running again in macOS CI.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32072\n\nDifferential Revision: D19391909\n\nPulled By: yf225\n\nfbshipit-source-id: 1ab345b099869f78e1124f1a8bd185fa51371b6a", "pr_number": "32072", "files_changed": ["test/cpp/api/tensor_indexing.cpp"], "labels": ["merged"]}, "02c3493a84": {"title": "Fix an invalid peephole transformation if input/output values are written to (#28455)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/28360\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28455\n\nDifferential Revision: D19374601\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 622f24b40aba03e79e55a6b8d25d88417f7d8bad", "pr_number": "28455", "files_changed": ["test/cpp/jit/test_peephole_optimize.cpp", "test/test_jit.py", "test/test_jit_fuser.py", "torch/csrc/jit/passes/peephole.cpp"], "labels": ["jit", "merged"]}, "8dc67a014f": {"title": "Add cummax", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32169\n\nDifferential Revision: D19393236\n\nPulled By: anjali411\n\nfbshipit-source-id: 5dac6b0a4038eb48458d4a0b253418daeccbb6bc", "pr_number": "32169", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/common_methods_invocations.py", "test/test_namedtensor.py", "test/test_namedtuple_return_api.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/templates/Functions.cpp", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged"]}, "7572501d40": {"title": "move ProcessGroupGlooTest to gtest (#32133)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32133\n\nWe should do this to better debug the test.\n\nDifferential Revision: D19375479\n\nfbshipit-source-id: 8c2bf61bae605a38252bb793b091ade479bea11a", "pr_number": "32133", "files_changed": ["torch/lib/c10d/test/CMakeLists.txt", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["merged"]}, "02f09a1bbd": {"title": "Implement backend-agnostic rpc._wait_all_workers() utility (#32190)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32190\n\nWe need a backend-agnostic mechanism to do barrier-like operation before locally destroy RRef context and shutdown RPC Agent.\n\n- Sort worker names.\n- Elect the first name as the leader in the ordered worker names.\n- Followers reports therir intent to synchronize to the leader.\n- Leader also reports to itself, when `_wait_all_workers()` called.\n- If all workers report their intent to proceed, leader send the command to every one to proceed.\nghstack-source-id: 96693296\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_wait_all_workers\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rref_leak\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn\n\nbuck-out/gen/caffe2/test/rpc_spawn\\#binary.par -r test_wait_all_workers\nbuck-out/gen/caffe2/test/rpc_spawn\\#binary.par -r test_rref_leak\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_wait_all_workers\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_worker_id\n```\n\n# Stress runs\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\nDifferential Revision: D19399908\n\nfbshipit-source-id: 1dee607cd49adafe88534621a1c85e2736e2f595", "pr_number": "32190", "files_changed": ["test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "7ad03855dc": {"title": "Fix 'template' keyword warning with clang-cl and clang.exe (#32104)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32104\n\nFixes these warnings:\n```\nxplat\\caffe2\\caffe2Windows#header-mode-symlink-tree-only,headers\\caffe2\\operators\\quantized\\int8_conv_op.h(96,17): warning: use 'template' keyword to treat 'data' as a dependent template name\n            W.t.data<uint8_t>(),\n                ^\n                template\nxplat\\caffe2\\caffe2Windows#header-mode-symlink-tree-only,headers\\caffe2\\operators\\quantized\\int8_conv_op.h(97,17): warning: use 'template' keyword to treat 'data' as a dependent template name\n            B.t.data<int32_t>(),\n                ^\n                template\n```\n\nTest Plan: Tested locally with clang-cl and CI for other toolchains\n\nReviewed By: boguscoder\n\nDifferential Revision: D19353563\n\nfbshipit-source-id: c28afb8c1ad72fd77ef82556ba89fcf09100d1f9", "pr_number": "32104", "files_changed": ["caffe2/operators/quantized/int8_conv_op.h"], "labels": ["fb-exported", "merged"]}, "879620e85e": {"title": "[caffe2] fix how np.clip is used in lengths_reducer_fused_{4,8}_rowwise_ops_test (#32086)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32086\n\nnp.clip(1, num_indices // 2, 10) -> np.clip(num_indices // 2, 1, 10)\nAlso change batchsize -> num_rows to match with what the variable actually does\n\nTest Plan: CI\n\nReviewed By: hx89\n\nDifferential Revision: D19361521\n\nfbshipit-source-id: 9ce864c7d7da046dc606afa5207da677ccf80f52", "pr_number": "32086", "files_changed": ["caffe2/python/lengths_reducer_fused_8bit_rowwise_ops_test.py"], "labels": ["fb-exported", "merged"]}, "c70bb0a4f8": {"title": "Fixes to prim ops (#32179)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32179\n\nTensors are used as keys in dictionaries, so we need to annotate that key insertion into a dictionary inserts the key into the wildcard set. Also fixes bug with `listCopyAndSort` not copying the input list.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19397555\n\nPulled By: eellison\n\nfbshipit-source-id: 17acdc22ff5e2dda44fd25c80450396f5592095e", "pr_number": "32179", "files_changed": ["test/jit/test_list_dict.py", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/script/schema_type_parser.cpp", "torch/csrc/jit/script/schema_type_parser.h"], "labels": ["jit", "merged"]}, "4dce482acb": {"title": "dict type unification fix (#32185)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32185\n\nPreviously we would unify the contained types of dictionaries, however this breaks type safety.\n```\ntorch.jit.script\ndef test(input: Dict[str, None], cond):\n    if cond:\n        out = input\n    else:\n        out: {\"1\": 1}\n    out[\"hi\"] = 3\n```\n\nThis would only occur if a dictionary is being re-assigned across an if condition with different contained types, which is pretty unlikely. I tested `model_backward_compatibility` for all fb models and this didn't break anything. This PR is a precursor to alias analysis changes.\n\nAlso fixes `Future` type unification. Because `Future` is an immutable type, it is okay to unify the contained type.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19398585\n\nPulled By: eellison\n\nfbshipit-source-id: ebc8812cdf5b6dba37b1cfbc2edc7d8c467b258c", "pr_number": "32185", "files_changed": ["aten/src/ATen/core/type.cpp", "test/cpp/jit/test_jit_type.cpp"], "labels": ["jit", "merged"]}, "19bbb4fccb": {"title": "Stop building documentation in pytorch_linux_xenial_cuda*_build (#32187)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32187\n\nFixes #32058. Previously we would build documentation during the pytorch\nlinux cuda build. We don't actually need to do this because we have a\ndedicated python_doc_build job that builds the docs. With this change,\nthe CUDA build should run ~10 minutes faster, giving devs faster signal.\n\nTest Plan: - Check the CUDA (10.1) build on this PR, make sure it doesn't build the docs.\n\nDifferential Revision: D19400417\n\nPulled By: zou3519\n\nfbshipit-source-id: e8fb2b818146f33330e06760377a9afbc18a71ed", "pr_number": "32187", "files_changed": [".jenkins/pytorch/build.sh", "test/test_jit.py"], "labels": ["merged"]}, "ef0f96e92f": {"title": "[pytorch][PR] update comment in autograd.h for locking (#32222)", "body": "Summary:\nJust update the comment to make it accurate.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32222\n\nDifferential Revision: D19410428\n\nPulled By: albanD\n\nfbshipit-source-id: ad13596382613c2728e674a47049ea4f563964b9", "pr_number": "32222", "files_changed": ["torch/csrc/autograd/engine.h"], "labels": ["merged"]}, "05088da8e9": {"title": "[pytorch][PR] Fixed error in sample code of documentation (#31682)", "body": "Summary:\n\"in_features\" and \"out_features\" are not defined. Possibly a typo. They should be \"input_features\" and \"output_features\" instead\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31682\n\nDifferential Revision: D19251685\n\nPulled By: zou3519\n\nfbshipit-source-id: ac9e524e792a1853a16e8876d76b908495d8f35e", "pr_number": "31682", "files_changed": ["docs/source/notes/extending.rst"], "labels": ["merged", "open source"]}, "8c3ee9f2ba": {"title": "[Python] Deprecate use of scipy.misc.logsumexp and scipy.misc.comb (#32209)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32209\n\n* Deprecate use of scipy.misc.logsumexp and scipy.misc.comb.\n* Removed in 1.0.0 https://docs.scipy.org/doc/scipy-1.1.0/reference/generated/scipy.misc.logsumexp.html and https://docs.scipy.org/doc/scipy-1.2.1/reference/generated/scipy.misc.comb.html\n* Use scipy.special.logsumexp and scipy.special.comb instead.\n* This diff updates most usages of except those in experimental folders.\n* This diff does NOT fix existing lint/code/TARGETS issues.\n* This diff does NOT autoformat codes.\n\nTest Plan: sandcastle auto unittests\n\nDifferential Revision: D19406460\n\nfbshipit-source-id: 2103fa0d674d9671a0175f4ce54b3c887d22f04e", "pr_number": "32209", "files_changed": ["caffe2/python/operator_test/crf_test.py"], "labels": ["fb-exported", "merged"]}, "62b06b9fae": {"title": "Rename TensorTypeId to DispatchKey (#32154)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32154\n\nTensorTypeId -> DispatchKey\n\tc10/core/TensorTypeId.h -> c10/core/DispatchKey.h\n\tc10/core/TensorTypeId.cpp -> c10/core/DispatchKey.cpp\n\tTensorTypeId::* -> DispatchKey::*\n\tTensorTypeId type_id -> DispatchKey dispatch_key\n\t\ttype_id -> dispatch_key\n\tTensorTypeId::NumTensorIds -> DispatchKey::NumDispatchKeys\n\tRealTensorTypeId -> RealDispatchKey\nTensorTypeSet -> DispatchKeySet\n\tTensorTypeIds -> DispatchKeys\n\tc10/core/TensorTypeSet.h -> c10/core/DispatchKeySet.h\n\tc10/core/TensorTypeSet.cpp -> c10/core/DispatchKeySet.cpp\n\ttype_set() -> key_set()\n\ttype_set_ -> key_set_\n\ttypeSet -> keySet\nExcludeTensorTypeIdGuard -> ExcludeDispatchKeyGuard\nIncludeTensorTypeIdGuard -> IncludeDispatchKeyGuard\nLocalTensorTypeSet -> LocalDispatchKeySet\n\tc10/core/impl/LocalTensorTypeSet.h -> c10/core/impl/LocalDispatchKeySet.h\n\tc10/core/impl/LocalTensorTypeSet.cpp -> c10/core/impl/LocalDispatchKeySet.cpp\n\ttls_local_tensor_type_set -> tls_local_dispatch_key_set\n\ttls_is_tensor_type_id_excluded -> tls_is_dispatch_key_excluded\n\ttls_set_tensor_type_id_excluded -> tls_set_dispatch_key_excluded\n\ttls_is_tensor_type_id_included -> tls_is_dispatch_key_included\n\ttls_set_tensor_type_id_included -> tls_set_dispatch_key_included\nMultiDispatchTensorTypeSet -> MultiDispatchKeySet\n\tmulti_dispatch_tensor_type_set -> multi_dispatch_key_set\ntensorTypeIdToBackend -> dispatchKeyToBackend\nbackendToTensorTypeId -> backendToDispatchKey\ninitForTensorTypeSet -> initForDispatchKeySet\ninferred_type_set -> inferred_key_set\ncomputeTensorTypeId -> computeDispatchKey\nPODLocalTensorTypeSet raw_local_tensor_type_set -> PODLocalDispatchKeySet raw_local_dispatch_key_set\nget_default_tensor_type_id -> get_default_dispatch_key\ninferred_type_id -> inferred_dispatch_key\nactual_type_id -> actual_dispatch_key\ntypeSetToDispatchKey_ -> dispatchKeySetToDispatchKey_\nget_type_id() -> get_dispatch_key()\nlegacyExtractTypeId -> legacyExtractDispatchKey\nextractTypeId -> extractDispatchKey\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19398900\n\nPulled By: pbelevich\n\nfbshipit-source-id: 234ad19f93d33e00201b61e153b740a339035776", "pr_number": "32154", "files_changed": ["aten/src/ATen/OpaqueTensorImpl.h", "aten/src/ATen/SparseTensorImpl.cpp", "aten/src/ATen/SparseTensorImpl.h", "aten/src/ATen/Utils.h", "aten/src/ATen/core/LegacyTypeDispatch.h", "aten/src/ATen/core/VariableFallbackKernel.cpp", "aten/src/ATen/core/boxing/kernel_function_legacy_test.cpp", "aten/src/ATen/core/boxing/kernel_function_test.cpp", "aten/src/ATen/core/boxing/kernel_functor_test.cpp", "aten/src/ATen/core/boxing/kernel_lambda_legacy_test.cpp", "aten/src/ATen/core/boxing/kernel_lambda_test.cpp", "aten/src/ATen/core/boxing/kernel_stackbased_test.cpp", "aten/src/ATen/core/boxing/test_helpers.h", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/dispatch/DispatchTable.h", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/op_registration/op_registration.cpp", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/TypeProperties.cpp", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp", "aten/src/ATen/native/quantized/cpu/qadd.cpp", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "aten/src/ATen/native/quantized/cpu/qconcat.cpp", "aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qconv_unpack.cpp", "aten/src/ATen/native/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_unpack.cpp", "aten/src/ATen/native/quantized/cpu/qmul.cpp", "aten/src/ATen/native/quantized/cpu/qpool.cpp", "aten/src/ATen/native/quantized/cpu/qrelu.cpp", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/quantized/QTensorImpl.cpp", "aten/src/ATen/quantized/QTensorImpl.h", "aten/src/ATen/quantized/Quantizer.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/test/backend_fallback_test.cpp", "aten/src/ATen/test/extension_backend_test.cpp", "aten/src/ATen/test/xla_tensor_test.cpp", "aten/src/TH/generic/THTensor.cpp", "aten/src/THC/generic/THCTensor.cpp", "c10/core/Backend.h", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "c10/core/QEngine.h", "c10/core/QScheme.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "c10/core/TensorOptions.h", "c10/core/TensorTypeId.cpp", "c10/core/TensorTypeId.h", "c10/core/TensorTypeSet.cpp", "c10/core/TensorTypeSet.h", "c10/core/UndefinedTensorImpl.cpp", "c10/core/impl/LocalDispatchKeySet.cpp", "c10/core/impl/LocalDispatchKeySet.h", "c10/core/impl/LocalTensorTypeSet.cpp", "c10/core/impl/LocalTensorTypeSet.h", "c10/test/core/DispatchKeySet_test.cpp", "c10/test/core/TensorTypeSet_test.cpp", "caffe2/core/export_caffe2_op_to_c10.h", "caffe2/core/tensor.h", "caffe2/operators/experimental/c10/cpu/add_cpu.cc", "caffe2/operators/experimental/c10/cpu/averaged_loss_cpu.cc", "caffe2/operators/experimental/c10/cpu/batch_gather_cpu.cc", "caffe2/operators/experimental/c10/cpu/batch_matmul_cpu.cc", "caffe2/operators/experimental/c10/cpu/cast_cpu.cc", "caffe2/operators/experimental/c10/cpu/concat_cpu.cc", "caffe2/operators/experimental/c10/cpu/enforce_finite_cpu.cc", "caffe2/operators/experimental/c10/cpu/expand_dims_cpu.cc", "caffe2/operators/experimental/c10/cpu/fc_cpu.cc", "caffe2/operators/experimental/c10/cpu/filler_cpu.cc", "caffe2/operators/experimental/c10/cpu/flatten_cpu.cc", "caffe2/operators/experimental/c10/cpu/mul_cpu.cc", "caffe2/operators/experimental/c10/cpu/relu_cpu.cc", "caffe2/operators/experimental/c10/cpu/sigmoid_cpu.cc", "caffe2/operators/experimental/c10/cpu/sigmoid_cross_entropy_with_logits_cpu.cc", "caffe2/operators/experimental/c10/cpu/sparse_lengths_sum_cpu.cc", "caffe2/operators/experimental/c10/cpu/stop_gradient_cpu.cc", "test/cpp_extensions/complex_registration_extension.cpp", "test/cpp_extensions/msnpu_extension.cpp", "test/mobile/op_deps/simple_ops.cpp", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/python_torch_functions.cpp", "tools/autograd/templates/python_variable_methods.cpp", "torch/csrc/DynamicTypes.cpp", "torch/csrc/Module.cpp", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/python_legacy_variable.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/jit/mobile/register_mobile_ops.cpp", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp", "torch/csrc/tensor/python_tensor.cpp", "torch/csrc/tensor/python_tensor.h", "torch/csrc/utils/python_arg_parser.h", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_new.h", "torch/csrc/utils/tensor_types.cpp"], "labels": ["jit", "merged", "topic: bc-breaking"]}, "4314620ba0": {"title": "[jit] Module clone work with shared ClassType (#31970)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31970\n\nNow that the ClassType can be shared among different module instances, we'll\npreserve the sharing in clone as well, that is if the original module has\na ClassType that is shared, we'll clone this ClassType once and share it between\ndifferent module instances as well.\n\nTest Plan:\nbuild/test/test_jit\n\nImported from OSS\n\nDifferential Revision: D19406251\n\nfbshipit-source-id: 2881c695f6e718e5432040a3817cf187a62017bf", "pr_number": "31970", "files_changed": ["test/cpp/jit/test_module_api.cpp", "test/cpp/jit/tests.h", "test/test_quantization.py", "torch/csrc/jit/script/module.cpp", "torch/csrc/jit/script/module.h"], "labels": ["jit", "merged"]}, "0392e8384b": {"title": "Fix simple typo: whos -> whose (#31288)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/31287\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31288\n\nDifferential Revision: D19166753\n\nPulled By: zou3519\n\nfbshipit-source-id: da31ad323b8fafa7cbc502fda4e2eb6e02facfb6", "pr_number": "31288", "files_changed": ["caffe2/python/core.py"], "labels": ["merged", "open source"]}, "3d01e3d16f": {"title": "Notify other threads before running callbacks (#31713)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31713\n\n- In case the callbacks are heavy/slow, the other threads should be able to start work on the value of the future after the current thread moves the value and unlock the mutex.\n- `completed()` is not inlined. Avoid function call overhead.\n\nghstack-source-id: 96694593\n\nTest Plan: tdb\n\nDifferential Revision: D5624371\n\nfbshipit-source-id: 5762e6e894d20108ec9afedd1a6e64bcd97ee3fe", "pr_number": "31713", "files_changed": ["torch/csrc/utils/future.h"], "labels": ["merged"]}, "3363ca20a7": {"title": "example_outputs Doc Edit (#31826)", "body": "Summary:\ntorch.onnx.export docs contain two descriptions for 'example_outputs' arg.\nSo combined the information for it with the description with the parameters.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31826\n\nDifferential Revision: D19274928\n\nPulled By: zou3519\n\nfbshipit-source-id: cbcce0a79c51784c1d7aa8981aab8aac118ca9b4", "pr_number": "31826", "files_changed": ["torch/onnx/__init__.py"], "labels": ["merged", "open source"]}, "81048c41ab": {"title": "remove simple .data from torch/nn", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31482\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19303243\n\nPulled By: albanD\n\nfbshipit-source-id: 5afdfeb4b8382c09b9ec65acd545148ed76d4285", "pr_number": "31482", "files_changed": ["torch/nn/modules/batchnorm.py", "torch/nn/modules/container.py", "torch/nn/modules/module.py"], "labels": ["merged"]}, "74621ca926": {"title": "Add allgather_base as per our discussion re: ProcessGroup interface. (#31892)", "body": "Summary:\nIntroduce ProcessGroup::allgather_base. No implementation yet: plan to add it one PG backend at a time in a follow up.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31892\n\nTest Plan: No functional changes, no tests yet.\n\nDifferential Revision: D19290739\n\nPulled By: agolynski\n\nfbshipit-source-id: c2f4947d2980995724c539de7c6d97618e1ba11a", "pr_number": "31892", "files_changed": ["torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/ProcessGroupMPI.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/ProcessGroupRoundRobin.cpp", "torch/lib/c10d/ProcessGroupRoundRobin.hpp"], "labels": ["fb-exported", "merged"]}, "322f34b245": {"title": "Adding DDP Design Note", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32158\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19405980\n\nPulled By: mrshenli\n\nfbshipit-source-id: 808ef1c71b637546f8872375bf1828967b1a5a60", "pr_number": "32158", "files_changed": ["docs/source/notes/ddp.rst"], "labels": ["merged"]}, "a5161c7022": {"title": "Update out-of-date comment on Docker image updates. (#32224)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32224\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19416878\n\nPulled By: ezyang\n\nfbshipit-source-id: 0205d0635658a3328128dcaad94bbbef505342be", "pr_number": "32224", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/header-section.yml"], "labels": ["merged"]}, "01010f5705": {"title": "Add comments to torch::nn::ConvTranspose{1,2,3}d modules explaining how to use them in a Sequential module (#32223)", "body": "Summary:\nFollowing changes in https://github.com/pytorch/pytorch/pull/31005.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32223\n\nDifferential Revision: D19415328\n\nPulled By: yf225\n\nfbshipit-source-id: f6f74f10ba3b5cc7e1a92f8b02ea4c9747018ae8", "pr_number": "32223", "files_changed": ["torch/csrc/api/include/torch/nn/modules/conv.h"], "labels": ["merged"]}, "4460a86cd6": {"title": "Support op registration if name starts with underscore (_) (#32017)", "body": "Summary:\nThis is required for rehistering torchvision::_new_empty_tensor op\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32017\n\nReviewed By: hl475\n\nDifferential Revision: D19399606\n\nPulled By: houseroad\n\nfbshipit-source-id: 43e1f2d78d2a0310af347b42f7e9b54cd503a20d", "pr_number": "32017", "files_changed": ["torch/onnx/utils.py"], "labels": ["merged", "open source"]}, "ffc8e255c4": {"title": "Sort export w/ negative axes (#31971)", "body": "Summary:\nFixing export of Sort on negative axes\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31971\n\nReviewed By: hl475\n\nDifferential Revision: D19325874\n\nPulled By: houseroad\n\nfbshipit-source-id: 18ab2bf39221970c8ab65a1355f5759f88faa54f", "pr_number": "31971", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_helper.py"], "labels": ["merged", "open source", "triaged"]}, "8b4c695e47": {"title": "Added cons folding for ONNX mul, div, sqrt ops (#32077)", "body": "Summary:\nAn example of a model with such leaf nodes is faster_rcnn model. This PR helps optimizing onnx ops.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32077\n\nReviewed By: hl475\n\nDifferential Revision: D19399622\n\nPulled By: houseroad\n\nfbshipit-source-id: 35c628c6f1514b79f1bcf7982c25f0f4486f8941", "pr_number": "32077", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/test_utility_funs.py", "torch/csrc/jit/passes/onnx/constant_fold.cpp"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "31b7d0873c": {"title": "Add File existence checking (#32208)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32208\n\n### Summary\n\nSince the master branch will generate `libtorch_cpu.a`, which is different from the release branch. This PR will skip the missing libs before archiving them.\n\n### Test Plan\n\n- don't break the nightly build\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19420042\n\nPulled By: xta0\n\nfbshipit-source-id: fb28df17b7e95d5c7fdf5f3a21bece235d7be17c", "pr_number": "32208", "files_changed": [".circleci/scripts/binary_ios_upload.sh"], "labels": ["merged"]}, "de5821d291": {"title": "Torchscript print to logcat (#31456)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31456\n\nExternal request https://discuss.pytorch.org/t/jit-android-debugging-the-model/63950\n\nBy default torchscript print function goes to stdout. For android it is not seen in logcat by default.\nThis change propagates it to logcat.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19171405\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: f9c88fa11d90bb386df9ed722ec9345fc6b25a34", "pr_number": "31456", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp"], "labels": ["merged"]}, "104b2c610b": {"title": "Tensor prep from image in native (#31426)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31426\n\nTensor convertion from YUV image is moved to native with optimizations to eliminate branching inside loop, no variables declaration, less ops.\n\nPerf stat from local devices - measuring converting 320x240 image from camera to 1,3,224,224 tensor;\nLegend:\nJava - current java impl\nJavaOpt - current java impl + the same optimizations with no if/else in for, declare variables outside of for, inlining etc.\nC - C impl\n\n```\nNexus 5\nJavaOpt N:25 avg:119.24 min: 87 max:177 p10:102 p25:105 p50:115 p75:127 p90:150\n      C N:25 avg: 17.24 min: 14 max: 39 p10: 14 p25: 15 p50: 15 p75: 16 p90: 23\n   Java N:25 avg:139.96 min: 70 max:214 p10: 89 p25:110 p50:139 p75:173 p90:181\navg C vs JavaOpt 6.91x\n\nPixel 3 XL\nJavaOpt N:19 avg: 16.11 min: 12 max: 19 p10: 14 p25: 15 p50: 16 p75: 18 p90: 19\n      C N:19 avg:  5.79 min:  3 max: 10 p10:  4 p25:  5 p50:  6 p75:  6 p90:  9\n   Java N:19 avg: 16.21 min: 12 max: 20 p10: 14 p25: 15 p50: 16 p75: 18 p90: 20\navg C vs JavaOpt 2.78x\n\nFull build with 4 abis inside:\nPixel 3 XL\nJavaOpt N:25 avg: 18.84 min: 16 max: 24 p10: 16 p25: 17 p50: 18 p75: 20 p90: 22\n      C N:25 avg:  7.96 min:  5 max: 10 p10:  7 p25:  7 p50:  8 p75:  9 p90:  9\navg C vs JavaOpt 2.36x\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19165429\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 3b54e545f6fbecbc5bb43216aca81061e70bd369", "pr_number": "31426", "files_changed": ["android/build.gradle", "android/pytorch_android/build.gradle", "android/pytorch_android_torchvision/CMakeLists.txt", "android/pytorch_android_torchvision/build.gradle", "android/pytorch_android_torchvision/src/main/cpp/pytorch_vision_jni.cpp", "android/pytorch_android_torchvision/src/main/java/org/pytorch/torchvision/TensorImageUtils.java"], "labels": ["merged"]}, "90c65b81c3": {"title": "Define `repr()` on IValues (#32232)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32232\n\nPreviously, we were using `operator<<` as the default way of printing\nIValue constants during serialization. The semantics of `operator<<`\nwere ill-defined; and this bit us in particular with strings and lack of\nquoting.\n\nThis PR defines the role of `operator<<`: much like Python `str()`, it\nis intended to produce a human-readable-ish representation for\ndebugging purposes.\n\nThis PR also defines a new `repr()` function on IValue that is intended\nto produce a valid Python expression that can be used to recreate an\nobject with the same value. `repr()` is not defined on all IValue kinds\n(notably tensors!) for this reason.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19417036\n\nPulled By: suo\n\nfbshipit-source-id: c102d509eaf95a28b6a62280bc99ca6f09603de5", "pr_number": "32232", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "test/test_jit.py", "torch/csrc/jit/passes/python_print.cpp"], "labels": ["jit", "merged"]}, "89c6e18c43": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/9915834ced3fdfd5ca33969d079a7f0cae796219\nhttps://github.com/facebook/fbthrift/commit/3cdb0d61d604d094a4fe42a3bcd47a2df2cdec86\nhttps://github.com/facebook/fbzmq/commit/93a4e9f4ccb77372f1929ad238eb87257798b449\nhttps://github.com/facebook/folly/commit/dafd4506838033583a390249ba0cf9cf4c6b9ffd\nhttps://github.com/facebook/mcrouter/commit/b5d5670e40c43543bb22f80fa817a86f69ded77f\nhttps://github.com/facebook/proxygen/commit/bab52dcc84ae58c074acdf976c2b61554ba83a00\nhttps://github.com/facebook/rocksdb/commit/d2b4d42d4b7aeade8375a44cb193ba6e4786f34a\nhttps://github.com/facebook/wangle/commit/83479196c337e4d2c46eccfb2cd56931fb13d2eb\nhttps://github.com/facebookincubator/fizz/commit/f2ec66095ac8c1eed2d7d7a16f3a64b492b0b14e\nhttps://github.com/facebookincubator/katran/commit/99561fee3b770e2ef2f4eb66758f1e26f32252a9\nhttps://github.com/facebookincubator/mvfst/commit/eacaa4f35d2e3d806ce64a24a281baf59849c9ca\nhttps://github.com/facebookincubator/profilo/commit/4ce4667b20fe8fcce60c1ce288c68737fd435003\nhttps://github.com/pytorch/fbgemm/commit/89291814cc4c949fbf133cd109424bccb23c02a7\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: 2a3c90f0a7615441dae746b18b9048cfddf0f4de", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "851a7e861b": {"title": "Add CAFFE2_API to video decoding functions (#31187)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31132\nAlso closes old issue https://github.com/pytorch/pytorch/issues/11735\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31187\n\nDifferential Revision: D19147172\n\nPulled By: pbelevich\n\nfbshipit-source-id: e959058eec3489061f431fbecc99ded0d4dc1704", "pr_number": "31187", "files_changed": ["caffe2/video/video_decoder.h"], "labels": ["merged", "open source"]}, "f3b62d4b1c": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/191bbb106995a9ce5c98b70990bd9a3df9a7b067\nhttps://github.com/facebook/fbthrift/commit/9d5a6e33e385724bc454cced4b519d9d9d399c7a\nhttps://github.com/facebook/fbzmq/commit/2bdfe1544a0c06673fafa60caab711673c9e94df\nhttps://github.com/facebook/proxygen/commit/1600bee8ded79b77802a98eb21b7fac5ad479b1e\nhttps://github.com/facebook/rocksdb/commit/b7f1b3e51cf516ce8bb9348053717c96ee97cc14\nhttps://github.com/facebook/wangle/commit/3220376f13923575eeb3f45dbc969c6ab074b33d\nhttps://github.com/facebookincubator/fizz/commit/1ba747dfb46bd243a67d972a2f111cce8adf84cc\nhttps://github.com/facebookincubator/katran/commit/0d5b08cbfcc7f6410cd39f9e7758695026ddb19b\nhttps://github.com/facebookincubator/mvfst/commit/481179a38e59dabb7d46081179829f9d8a977e07\nhttps://github.com/pytorch/fbgemm/commit/9bc4f9c40f278b568c2f9502080f97245b261e4f\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: 79135519c3449c2b77ff1ca7d4f13724e2390f6e", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "d75b6b3f9d": {"title": "Support shape inference and lowering of SparseLengthsWeightedSumFused4BitRowwise (#32257)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32257\n\nPull Request resolved: https://github.com/pytorch/glow/pull/4018\n\natt.\n\nTest Plan:\nUnit tests:\n```\nbuck test glow:masterCaffe2ImporterTest -- caffe2.SparseLengthsSumFused4BitRowwise\nbuck test caffe2/caffe2/opt:bound_shape_inference_test\n```\n\nReviewed By: jfix71\n\nDifferential Revision: D19389014\n\nfbshipit-source-id: 5f6863443adee5d3bf7a50a105866441eefb9560", "pr_number": "32257", "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc"], "labels": ["fb-exported", "merged"]}, "7df5dc2775": {"title": "Creating callUnboxedWithDispatchKey method (#32198)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32198\n\ncreating a method called \"callUnboxedWithDispatchKey\".\n\nAlso adding tests to make sure it works.\n\nTest Plan: buck test mode/dev //caffe2:ATen-core-test\n\nDifferential Revision: D19402815\n\nfbshipit-source-id: b206cf04b1216fbbd5b54ac79aef495cb0c1be06", "pr_number": "32198", "files_changed": ["aten/src/ATen/core/boxing/test_helpers.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp"], "labels": ["fb-exported", "merged"]}, "bab87e4b60": {"title": "reimplement __torch_function__ overrides for torch.functional using inline logic (#32194)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30831.\n\nThis improves the performance of operators in the `torch.functional` namespace that are overridable by `__torch_function__` implementations when supplied with `Tensor` operands.\n\nRunning the split benchmark in various configurations produces the following timings:\n\n<details>\n<summary>Expand for timings on <code>master</code> </summary>\n\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M8_N8_parts2_cpu\n# Input: M: 8, N: 8, parts: 2, device: cpu\nForward Execution Time (us) : 3.340\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M8_N8_parts2_cuda\n# Input: M: 8, N: 8, parts: 2, device: cuda\nForward Execution Time (us) : 3.333\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M256_N512_parts2_cpu\n# Input: M: 256, N: 512, parts: 2, device: cpu\nForward Execution Time (us) : 3.366\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M256_N512_parts2_cuda\n# Input: M: 256, N: 512, parts: 2, device: cuda\nForward Execution Time (us) : 3.385\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M512_N512_parts2_cpu\n# Input: M: 512, N: 512, parts: 2, device: cpu\nForward Execution Time (us) : 3.468\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M512_N512_parts2_cuda\n# Input: M: 512, N: 512, parts: 2, device: cuda\nForward Execution Time (us) : 3.416\n```\n</details>\n\n<details>\n<summary>Expand for timings with this pull request applied</summary>\n\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M8_N8_parts2_cpu\n# Input: M: 8, N: 8, parts: 2, device: cpu\nForward Execution Time (us) : 2.261\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M8_N8_parts2_cuda\n# Input: M: 8, N: 8, parts: 2, device: cuda\nForward Execution Time (us) : 2.223\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M256_N512_parts2_cpu\n# Input: M: 256, N: 512, parts: 2, device: cpu\nForward Execution Time (us) : 2.237\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M256_N512_parts2_cuda\n# Input: M: 256, N: 512, parts: 2, device: cuda\nForward Execution Time (us) : 2.218\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M512_N512_parts2_cpu\n# Input: M: 512, N: 512, parts: 2, device: cpu\nForward Execution Time (us) : 2.259\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M512_N512_parts2_cuda\n# Input: M: 512, N: 512, parts: 2, device: cuda\nForward Execution Time (us) : 2.234\n```\n\n</details>\n\n<details>\n<summary>Expand for timings on <code>master</code> with <code>__torch_function__</code> dispatch disabled </summary>\n\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M8_N8_parts2_cpu\n# Input: M: 8, N: 8, parts: 2, device: cpu\nForward Execution Time (us) : 2.180\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M8_N8_parts2_cuda\n# Input: M: 8, N: 8, parts: 2, device: cuda\nForward Execution Time (us) : 2.172\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M256_N512_parts2_cpu\n# Input: M: 256, N: 512, parts: 2, device: cpu\nForward Execution Time (us) : 2.171\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M256_N512_parts2_cuda\n# Input: M: 256, N: 512, parts: 2, device: cuda\nForward Execution Time (us) : 2.146\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M512_N512_parts2_cpu\n# Input: M: 512, N: 512, parts: 2, device: cpu\nForward Execution Time (us) : 2.175\n\n# Benchmarking PyTorch: split\n# Mode: Eager\n# Name: split_M512_N512_parts2_cuda\n# Input: M: 512, N: 512, parts: 2, device: cuda\nForward Execution Time (us) : 2.152\n```\n\n</details>\n\nSo at least on the machine I'm testing on, this brings the overhead down to less than 100 ns. For comparison, the overhead for `__array_function__` in NumPy is about 850 ns on the same machine.\n\n<details>\n<summary>Expand for timings for NumPy <code>__array_function__</code> dispatch </summary>\n\n```\nIn [1]: import numpy as np\n\nIn [2]: %timeit np.mean([1])\n8.89 \u00b5s \u00b1 17.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\nIn [3]: %timeit np.mean._implementation([1])\n8.04 \u00b5s \u00b1 28.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n```\n\nSee [the implementation in NumPy](https://github.com/numpy/numpy/blob/master/numpy/core/overrides.py#L195) for why this measures `__array_function__` overhead.\n\n</details>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32194\n\nDifferential Revision: D19410396\n\nPulled By: ezyang\n\nfbshipit-source-id: ada788a4399c81cd7eb2d548aa04a2459e96634a", "pr_number": "32194", "files_changed": ["test/onnx/expect/TestOperators.test_frobenius_norm.expect", "test/onnx/expect/TestOperators.test_meshgrid.expect", "test/onnx/expect/TestOperators.test_unique.expect", "test/test_overrides.py", "torch/_overrides.py", "torch/functional.py"], "labels": ["merged", "open source"]}, "14548c2d5b": {"title": "out variant for native_batch_norm forward (#29192)", "body": "Summary:\nThis is dealing with forward of native BatchNorm CUDA impl to support inplace operation. The larger issue: https://github.com/pytorch/pytorch/issues/26288\n\nezyang\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29192\n\nDifferential Revision: D19410370\n\nPulled By: ezyang\n\nfbshipit-source-id: a6889c96bdd848f3a1cb2d943d06e054d22fb7ab", "pr_number": "29192", "files_changed": ["aten/src/ATen/native/cuda/Normalization.cu", "aten/src/ATen/native/cuda/Normalization.cuh", "aten/src/ATen/native/native_functions.yaml"], "labels": ["merged", "open source", "triaged"]}, "f94aab45fd": {"title": "Logical condition reduction (#32201)", "body": "Summary:\nx || ( !x  &&  y )  <=>  to x || y\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32201\n\nDifferential Revision: D19429334\n\nPulled By: ezyang\n\nfbshipit-source-id: 044dc46c2d9a7e180aa1795703c0097b0c7c3585", "pr_number": "32201", "files_changed": ["caffe2/video/video_decoder.cc"], "labels": ["merged", "open source"]}, "cd99b3706a": {"title": "Pin Pillow to latest and use a torchvision that works with it (#32290)", "body": "Summary:\nFollow on from https://github.com/pytorch/pytorch/pull/31777, as suggested in https://github.com/pytorch/pytorch/pull/31777#issuecomment-575166543.\n\nPillow 7.0.0 removed `PILLOW_VERSION` and `__version__` should be used instead.\n\ntorchvision 0.5.0 switched from using `PILLOW_VERSION` to `__version__`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32290\n\nDifferential Revision: D19430280\n\nPulled By: mrshenli\n\nfbshipit-source-id: be8d6317a4948d71e818adeafe61dfe567df5601", "pr_number": "32290", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml"], "labels": ["merged", "open source"]}, "b26ee54176": {"title": "For ppc64le, stop presenting the python 2.7 builds (we will no longer\u2026 (#32315)", "body": "Summary:\nFor ppc64le, we no longer plan to run regular builds on Python 2.7, and we wish to stop\npublicizing the build status for those two builds (ppc64le/CPU and ppc64le/GPU each on py27).\n\nThis pull request simply removes the build status links for these two builds, replacing them\nwith a generic dash character (consistent with other un-publicized builds within the table).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32315\n\nDifferential Revision: D19435939\n\nPulled By: soumith\n\nfbshipit-source-id: c9f31e7acba83e42f6a758ac011bbef36fd8aaa0", "pr_number": "32315", "files_changed": ["README.md"], "labels": ["merged", "open source"]}, "8746f90cf6": {"title": "Fix weight backward for cudnn conv of large tensor (#31889)", "body": "Summary:\nThis is the last PR for https://github.com/pytorch/pytorch/issues/22496\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31889\n\nDifferential Revision: D19431371\n\nPulled By: ngimel\n\nfbshipit-source-id: 754fa91d49ad03549cb07aa30dde34bf9e851302", "pr_number": "31889", "files_changed": ["aten/src/ATen/native/cudnn/Conv.cpp", "test/test_nn.py"], "labels": ["merged", "open source", "triaged"]}, "7b7390778c": {"title": "Make an assert on a hotpath trigger only in DEBUG mode. (#32117)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32117\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19392949\n\nPulled By: ezyang\n\nfbshipit-source-id: 7f579e45d49bddeab36b8dd1a90c83224a368ac8", "pr_number": "32117", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction_impl.h"], "labels": ["merged"]}, "36d09197ab": {"title": "Move error reporting code out-of-line from header. (#32118)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32118\n\nThis reduces code size and makes the calling function more likely to inline.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19392950\n\nPulled By: ezyang\n\nfbshipit-source-id: 5e3829cca5604407229f93c2486eb9a325581ea2", "pr_number": "32118", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": ["merged"]}, "b85dbe8f7b": {"title": "Out-of-line construction of OperatorName. (#32121)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32121\n\nThis reduces code size in the call sites of this function (of which\nthere are many: one for every operator call) since we no longer have\nto construct std::string at the site.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19392951\n\nPulled By: ezyang\n\nfbshipit-source-id: 8bc43d46ba635380ff9f8989f7557fdd74b552cf", "pr_number": "32121", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/function_wrapper.py"], "labels": ["merged"]}, "34c751c263": {"title": "Eliminate exception throwing code from dispatch call sites (#32168)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32168\n\nWe move the exception raising into the function, saving us a\nbig pile of instructions for raising the stack.\n\nAfter this stack of changes, the compiler is willing to inline, e.g.,\n`c10::KernelFunction::callUnboxed<at::Tensor, at::Tensor const&>(c10::OperatorHandle const&, at::Tensor const&) const::__func__`\n(whereas previously it refused to do so.)\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19392948\n\nPulled By: ezyang\n\nfbshipit-source-id: d5edab00cae48444b308e74438a17a421532c08f", "pr_number": "32168", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/function_wrapper.py"], "labels": ["merged"]}, "8c8bd79f32": {"title": "Add CI scripts for Custom Build (#32316)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32316\n\n### Summary\n\nSince the Custom Build has been released in 1.4.0, it's time setup CI. To do that, we need\n\n1.  Add a python script to generate the yaml file\n2. Add new build scripts to circle CI (arm64 only).\n\n### Test Plan\n\n- Don't break the current iOS CIs\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19437362\n\nPulled By: xta0\n\nfbshipit-source-id: 395e27a582c43663af88d11b1ef974a4687e672c", "pr_number": "32316", "files_changed": ["ios/TestApp/custom_build/custom_build.py", "ios/TestApp/custom_build/mobilenetv2.yaml"], "labels": ["merged"]}, "53708e21ed": {"title": "classic fixed-point liveness", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31724\n\nDifferential Revision: D19426570\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 3387dfb25e6e9456d5d0517eac1d2e44e61d6813", "pr_number": "31724", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/bailout_graph.cpp", "torch/csrc/jit/passes/liveness.cpp"], "labels": ["jit", "merged"]}, "c2761490fc": {"title": "Enhancing the test (#32321)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32321\n\nUpdating the test to test more meaningful sematics\n\nTest Plan:\n[xintchen@devvm6308.prn2 ~/fbsource/fbcode] buck test mode/dev //caffe2:ATen-core-test -- 'OperatorRegistrationTest\\.whenRegisteringCPUTensorType_thenCanOnlyCallUnboxedWithCPUTensorIdDispatchKey'\nBuilding: finished in 0.4 sec (100%) 517/517 jobs, 0 updated\n  Total time: 0.5 sec\nTrace available for this run at /tmp/testpilot.20200116-132729.2541763.log\nTestPilot test runner for Facebook. See https://fburl.com/testpilot for details.\nTestpilot build revision e5f315ebe0508d11fc281fa4b4f7b43d2ef1c003 fbpkg 67e8eb96914f400db234fd9af70fdcde at Wed Jan 15 23:38:32 2020 by twsvcscm from /usr/local/fbprojects/packages/testinfra.testpilot/762/t.par\nDiscovering tests\nRunning 1 tests\nStarted new test run: https://our.intern.facebook.com/intern/testinfra/testrun/6192449492430045\n      \u2713 caffe2:ATen-core-test - OperatorRegistrationTest.whenRegisteringCPUTensorType_thenCanOnlyCallUnboxedWithCPUTensorIdDispatchKey 0.002 1/1 (passed)\nFinished test run: https://our.intern.facebook.com/intern/testinfra/testrun/6192449492430045\nSummary (total time 1.15s):\n  PASS: 1\n  FAIL: 0\n  SKIP: 0\n  FATAL: 0\n  TIMEOUT: 0\n  OMIT: 0\n\nDifferential Revision: D19436345\n\nfbshipit-source-id: c1f2383d62627aa4507616b8905ceb42ac563e9d", "pr_number": "32321", "files_changed": ["aten/src/ATen/core/op_registration/op_registration_test.cpp"], "labels": ["fb-exported", "merged"]}, "b79030d6c8": {"title": "remove unused code after refactoring optimizations into profiling-sensitive and profiling-insensitive (#32106)", "body": "Summary:\nAfter we removed `Specialize_AutogradZero` from the optimization pipeline of the simple executor mode, we don't need to mark any inputs as undefined in `autodiff`. Also, `needsGradient` in `graph_executor.cpp` never runs on graph with profiling information, so I removed that code as well.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32106\n\nDifferential Revision: D19374238\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 4223d3efe3c904a55a28471e5ae9593017ce3e07", "pr_number": "32106", "files_changed": ["torch/csrc/jit/autodiff.cpp", "torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/profiling_graph_executor_impl.cpp"], "labels": ["jit", "merged"]}, "ef5ae4823a": {"title": "Register RoIAlignRotated with C10", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30785\n\nReviewed By: wat3rBro\n\nDifferential Revision: D18415056\n\nfbshipit-source-id: e00376bec948309d53f2172697cd477449f769b2", "pr_number": "30785", "files_changed": ["caffe2/operators/roi_align_rotated_op.cc", "caffe2/operators/roi_align_rotated_op.cu", "caffe2/operators/roi_align_rotated_op.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": ["fb-exported", "merged"]}, "91bdb872ce": {"title": "fix spelling mistake: excpected -> expected", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28817\n\nDifferential Revision: D18544562\n\nPulled By: dgisser\n\nfbshipit-source-id: 51f728e807f9c4bb30f58585d5b6f436cb880153", "pr_number": "28817", "files_changed": ["caffe2/python/layers/concat.py", "caffe2/python/layers/pairwise_similarity.py", "caffe2/python/layers/split.py"], "labels": ["merged"]}, "7a9c920bac": {"title": "add lock for ncclCommAbort (#31901)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31901\n\nncclCommAbort is not thread safe, so adding a lock for it\nghstack-source-id: 96829715\n\nTest Plan: unit tests\n\nDifferential Revision: D19293869\n\nfbshipit-source-id: 711b4a07605d6e5a81577247d2f90a78041c1809", "pr_number": "31901", "files_changed": ["torch/lib/c10d/NCCLUtils.hpp"], "labels": ["merged"]}, "904ab092c2": {"title": "fix testSend and testRecv in ProcessGroupGlooTest (#32134)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32134\n\nThese tests weren't written in the most correct way and were often\nflaky. It was tricky to identify these tests as flaky until we moved this file\nto use gtest.\n\nThe gist of the issue is that the test previously would not coordinate sends\nand recvs properly. For example, we created a single thread to test an\nabortRecv and a successful recv. A separate sender thread was used to send 2\nmessages. What could go wrong here is that the first send could successfully\ncomplete, resulting in the receiving end processing the message before it gets\nthe abort signal. In this case we would have an error in the test.\nghstack-source-id: 96806879\n\nDifferential Revision: D19379395\n\nfbshipit-source-id: 24782ccaf6e6ec6b445378b29d5f10f901e0dee6", "pr_number": "32134", "files_changed": ["torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["merged"]}, "61a2b34113": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/2d9c2bb4010bee5a28b688f1b239d8f390391d4d\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: ea12c419c4bab8ce60793deecb10a8ead086a4d5", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "4968bc2450": {"title": "cap the maximum depth of bailout chains at 1 (#32073)", "body": "Summary:\nThis is another implementation of the maximum bailout depth.\nThe first version was implemented in https://github.com/pytorch/pytorch/pull/31521\nThis one has advantages that\n* the bailout depth only exists in `CodeImpl` which seems to be an appropriate place to keep it in.\n* threading many objects is reduced to threading through CodeImpl and getPlanFor\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32073\n\nDifferential Revision: D19443432\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 898384bb2308a1532a50a33d9e05cfca504711e6", "pr_number": "32073", "files_changed": ["torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/graph_executor.h", "torch/csrc/jit/graph_executor_impl.h", "torch/csrc/jit/interpreter.cpp", "torch/csrc/jit/interpreter.h", "torch/csrc/jit/profiling_graph_executor_impl.cpp", "torch/csrc/jit/profiling_graph_executor_impl.h"], "labels": ["jit", "merged"]}, "6a5a55d573": {"title": "use gtest asserts in ProcessGroupGlooTest instead of other checks (#32138)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32138\n\nI personally prefer `throw std::runtime_error(\"BOOM\")`, but we should\nprobably have asserts here now that it is gtest. Also ensures that the correct\nexceptions are thrown by the `testSignal` tests.\nghstack-source-id: 96811000\n\nDifferential Revision: D19382905\n\nfbshipit-source-id: 1b00dd70524d03c8bd6f48715baa5070a7985467", "pr_number": "32138", "files_changed": ["torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["merged"]}, "78d8f691ad": {"title": "Don't dispatch to integral types in smooth_l1_kernel", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32333\n\nDifferential Revision: D19442787\n\nPulled By: ngimel\n\nfbshipit-source-id: 9578483202614d7406eceb13cbf15b253c04f237", "pr_number": "32333", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu"], "labels": ["merged", "open source"]}, "5b815d980e": {"title": "Added cummin", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32238\n\nDifferential Revision: D19416791\n\nPulled By: anjali411\n\nfbshipit-source-id: 5aadc0a7a55af40d76f444ab7d7d47ec822f55a5", "pr_number": "32238", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/common_methods_invocations.py", "test/test_namedtensor.py", "test/test_namedtuple_return_api.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/templates/Functions.cpp", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged"]}, "8c1268aad3": {"title": "Use default scale/zero_point in fake_quantize module instead of None (#32318)", "body": "Summary:\nDistributed data parallel can not broadcast None so when we prepare the model for QAT and trying to save the model it will error out.\nfixes: https://github.com/pytorch/pytorch/issues/32082\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32318\n\nDifferential Revision: D19434801\n\nPulled By: jerryzh168\n\nfbshipit-source-id: ee70abe4c3dcdd3506fb7dd0316aee2fb1705469", "pr_number": "32318", "files_changed": ["torch/quantization/fake_quantize.py"], "labels": ["merged"]}, "7732924501": {"title": "Delete unused bernoulli_Tensor from THTensorRandom.h", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32328\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19448736\n\nPulled By: pbelevich\n\nfbshipit-source-id: 92380ca1e0c0ac88d100e6fba8d216a46d0b181e", "pr_number": "32328", "files_changed": ["aten/src/TH/generic/THTensorRandom.h"], "labels": ["merged"]}, "aa61d1ee85": {"title": "Add a new job to support custom build (#32323)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32323\n\n### Summary\n\nSince we have released the custom build in 1.4.0, it's time to setup a CI for that. This PR adds a new iOS job to the iOS builds. To save time, It only runs the arm64 build.\n\n### Test Plan\n\n- Don't break any iOS jobs\n- Custom Build works.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19451342\n\nPulled By: xta0\n\nfbshipit-source-id: 9de305c004fc795710ecf01d436ef4792c07760c", "pr_number": "32323", "files_changed": [".circleci/config.yml", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/pytorch-build-params.yml", ".circleci/verbatim-sources/workflows-pytorch-ios-builds.yml", "ios/TestApp/TestApp/Benchmark.mm"], "labels": ["merged"]}, "4e69352713": {"title": "Add 64bit atomic fetch add (#32354)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32354\n\nadding int_64 version of AtomicFetchAdd\n\nReviewed By: bwasti\n\nDifferential Revision: D19434349\n\nfbshipit-source-id: b2358e8c5c6b7cd7e7b21de974b4ee1b5258fcf4", "pr_number": "32354", "files_changed": ["caffe2/operators/atomic_ops.cc", "caffe2/python/operator_test/atomic_ops_test.py"], "labels": ["fb-exported", "merged"]}, "e133d8be3b": {"title": "Fix ASAN / potential segfault in quantized Tensor memory allocations.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29882\n\nDifferential Revision: D18522039\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 1fdc68491aa2ac176633b9ecc3ee78c9175a97aa", "pr_number": "29882", "files_changed": ["aten/src/ATen/quantized/Quantizer.cpp"], "labels": ["merged"]}, "df514fd8c0": {"title": "C++ C2/Glow operator unittest", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32258\n\nTest Plan:\n```\n buck test glow/fb/test/numerics:fp16_op_test\n```\n\nReviewed By: bddppq\n\nDifferential Revision: D19401786\n\nfbshipit-source-id: 1382b5208be6172d3e6f768dedad7ebec31cffc9", "pr_number": "32258", "files_changed": ["caffe2/core/test_utils.cc", "caffe2/core/test_utils.h"], "labels": ["fb-exported", "merged"]}, "e7bc1663bd": {"title": "fix unchecked cast alias analysis (#32309)", "body": "Summary:\nUnchecked cast just refines the type of a value, the value stays the same, so the output should alias the input.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32309\n\nDifferential Revision: D19439037\n\nPulled By: eellison\n\nfbshipit-source-id: fe6902d0d9a5a9ef5e9c13e1dbd056576d8c327e", "pr_number": "32309", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/alias_analysis.cpp"], "labels": ["jit", "merged"]}, "9b6ec61bfd": {"title": "exposing CPU/GPU Copy ops (#32248)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32248\n\nexpose CPU/GPU copy ops\n\nTest Plan: buck test mode/dev-nosan caffe2/caffe2/python/operator_test:torch_integration_test\n\nReviewed By: houseroad\n\nDifferential Revision: D19405856\n\nfbshipit-source-id: 1df4aa202e26647cb81e9fe7e4478e594a5f7f3e", "pr_number": "32248", "files_changed": ["caffe2/core/export_caffe2_op_to_c10.h", "caffe2/operators/copy_op.cc", "caffe2/operators/copy_op.cu", "caffe2/operators/copy_op.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": ["fb-exported", "merged"]}, "5a58c16722": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/29aba0a28715b89ef60c338ffa1db574e60fdf35\nhttps://github.com/facebook/fbthrift/commit/37a97eb4de2596310339fcc1520c7e5dada37ab5\nhttps://github.com/facebook/fbzmq/commit/0efdd5729236427074842bb91c9b4687e6721a69\nhttps://github.com/facebook/folly/commit/6d886fc7ebe4a7cb55c7733f5d0ec2d85e7062bb\nhttps://github.com/facebook/proxygen/commit/2e5854752afb8068fc0fbc6b736790260167d56d\nhttps://github.com/facebook/wangle/commit/931d1c643bf4fa57fcdb3ca695ae643b39066476\nhttps://github.com/facebookincubator/fizz/commit/781986ef716d85c66584612d2d1e261772f85699\nhttps://github.com/facebookincubator/katran/commit/2e6d2903d7cfec77b7d2f878f2add87e354352f1\nhttps://github.com/facebookincubator/mvfst/commit/e04348ff63f56ff791336ecfd037193f1bd9f822\nhttps://github.com/pytorch/fbgemm/commit/e8650fd5601e28783f64f5a38541e6d562125375\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: abd7ee4aaec8401b2c885335940773a0655b4496", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "bdd5e15437": {"title": "skip testExceptions in ProcessGroupGloo if built with TSAN (#32242)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32242\n\nTSAN and fork don't play well together, so skip this test if we're\nbuilding under TSAN. It will still run in other modes.\n\nDifferential Revision: D19416113\n\nfbshipit-source-id: 7e88d63a843356372160c2524c05e8fd1706553e", "pr_number": "32242", "files_changed": ["torch/lib/c10d/test/ProcessGroupGlooTest.cpp", "torch/lib/c10d/test/TestUtils.hpp"], "labels": ["merged"]}, "7e3c438913": {"title": "Renaming IValue List functions (#32093)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32093\n\ntoGenericListRef -> toListRef\nisGenericList -> isList\ntoGenericList -> toList\ntoXListRef -> toXVector\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D19369767\n\nPulled By: zdevito\n\nfbshipit-source-id: 4f0078f95b83e6586524c03f7bcf206722fdd9ae", "pr_number": "32093", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "aten/src/ATen/core/Dict_inl.h", "aten/src/ATen/core/List.h", "aten/src/ATen/core/List_inl.h", "aten/src/ATen/core/boxing/kernel_function_legacy_test.cpp", "aten/src/ATen/core/boxing/kernel_function_test.cpp", "aten/src/ATen/core/boxing/kernel_functor_test.cpp", "aten/src/ATen/core/boxing/kernel_lambda_legacy_test.cpp", "aten/src/ATen/core/boxing/kernel_lambda_test.cpp", "aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "caffe2/core/operator.cc", "caffe2/core/operator.h", "test/cpp/jit/test_ivalue.cpp", "test/test_cpp_api_parity.py", "tools/jit/gen_jit_dispatch.py", "tools/jit/templates/register_aten_ops.cpp", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/import_legacy.cpp", "torch/csrc/jit/mobile/register_mobile_ops.cpp", "torch/csrc/jit/passes/utils/check_alias_annotation.cpp", "torch/csrc/jit/pickler.cpp", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/register_c10_ops.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/register_special_ops.cpp", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/unpickler.cpp"], "labels": ["jit", "merged"]}, "c8ca70e39d": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebookincubator/fizz/commit/54b290f00ff8a1e1bc12957f97d41b7f32b36268\nhttps://github.com/facebookincubator/mvfst/commit/e8df50310d5d883660b409d2e484b6e05235ce3d\nhttps://github.com/pytorch/fbgemm/commit/ef5c9efe120d1e8b5b263ebe37be8cb0c9583cc2\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: 7b6dc88d40e8fd8c396d4d12846db43b0fb4258c", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "f326045b37": {"title": "Fix typos, via a Levenshtein-type corrector (#31523)", "body": "Summary:\nShould be non-semantic.\n\nUses https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines to find likely typos, with https://github.com/bwignall/typochecker to help automate the checking.\n\nUses an updated version of the tool used in https://github.com/pytorch/pytorch/pull/30606 .\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31523\n\nDifferential Revision: D19216749\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7fd489cb9a77cd7e4950c1046f925d57524960ea", "pr_number": "31523", "files_changed": ["CMakeLists.txt", "CODEOWNERS", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/core/boxing/kernel_lambda.h", "aten/src/ATen/core/function_schema.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cuda/CUDAGenerator.cpp", "aten/src/ATen/cuda/nvrtc_stub/ATenNVRTC.h", "aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h", "aten/src/ATen/miopen/Descriptors.h", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/LossMultiMargin.cpp", "aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/Unfold3d.cpp", "aten/src/ATen/native/UpSample.h", "aten/src/ATen/native/cpu/DistanceOpsKernel.cpp", "aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/ATen/native/cuda/GridSampler.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/PersistentSoftmax.cuh", "aten/src/ATen/native/cuda/SoftMax.cu", "aten/src/ATen/native/cuda/SortingKthValue.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/cuda/UpSample.cuh", "aten/src/ATen/native/cudnn/RNN.cpp", "aten/src/ATen/native/mkl/SpectralOps.cpp", "aten/src/ATen/native/mkldnn/Conv.cpp", "aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "aten/src/ATen/native/quantized/cpu/qmul.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/src/requantization/fp32-neon.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/requantization/fp32-psimd.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/requantization/precise-scalar.c", "aten/src/ATen/native/quantized/cpu/qnnpack_utils.h", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "aten/src/TH/THGeneral.h.in", "aten/src/TH/THStorage.h", "aten/src/TH/THTensor.hpp", "aten/src/TH/generic/THVectorDispatch.cpp", "aten/src/THC/THCIntegerDivider.cuh", "aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu", "benchmarks/framework_overhead_benchmark/C2Module.py", "benchmarks/operator_benchmark/README.md", "benchmarks/operator_benchmark/benchmark_caffe2.py", "benchmarks/operator_benchmark/benchmark_core.py", "benchmarks/operator_benchmark/pt/qrnn_test.py", "binaries/convert_and_benchmark.cc", "binaries/convert_image_to_tensor.cc", "c10/core/StorageImpl.h", "c10/test/util/registry_test.cpp", "caffe2/contrib/gloo/allreduce_ops.cc", "caffe2/contrib/gloo/allreduce_ops.h", "caffe2/contrib/gloo/allreduce_ops_gpu.cc", "caffe2/contrib/gloo/broadcast_ops.h", "caffe2/contrib/gloo/reduce_scatter_ops.h", "caffe2/contrib/opencl/OpenCL/cl.hpp", "caffe2/contrib/playground/checkpoint.py", "caffe2/contrib/tensorrt/tensorrt_tranformer.cc", "caffe2/core/blob_serialization.h", "caffe2/core/common.h", "caffe2/core/context_gpu.cu", "caffe2/core/memonger.cc", "caffe2/core/net.h", "caffe2/core/net_async_tracing.cc", "caffe2/core/net_simple_refcount.cc", "caffe2/core/nomnigraph/include/nomnigraph/Representations/NeuralNet.h", "caffe2/core/operator.h", "caffe2/core/operator_schema.h", "caffe2/core/scope_guard.h", "caffe2/core/static_tracepoint_elfx86.h", "caffe2/core/workspace.h", "caffe2/experiments/python/net_construct_bench.py", "caffe2/image/image_input_op.h", "caffe2/mobile/contrib/libopencl-stub/include/CL/cl.hpp", "caffe2/mobile/contrib/nnapi/nnapi.cc", "caffe2/onnx/backend.cc", "caffe2/onnx/onnx_exporter.cc", "caffe2/onnx/onnx_exporter.h", "caffe2/onnx/torch_ops/defs.cc", "caffe2/operators/activation_ops_cudnn.h", "caffe2/operators/batch_bucketize_op.cc", "caffe2/operators/batch_matmul_op.cc", "caffe2/operators/bisect_percentile_op.cc", "caffe2/operators/box_with_nms_limit_op.h", "caffe2/operators/conv_op_cudnn.cc", "caffe2/operators/crf_viterbi_op.cc", "caffe2/operators/fused_rowwise_random_quantization_ops.cc", "caffe2/operators/gather_op.h", "caffe2/operators/generate_proposals_op.h", "caffe2/operators/h_softmax_op.cc", "caffe2/operators/heatmap_max_keypoint_op.cc", "caffe2/operators/lengths_reducer_rowwise_8bit_ops.h", "caffe2/operators/load_save_op_util.cc", "caffe2/operators/op_utils_cudnn.h", "caffe2/operators/pool_op_util.cc", "caffe2/operators/reservoir_sampling.cc", "caffe2/operators/rnn/recurrent_network_executor_incl.h", "caffe2/operators/segment_reduction_op.h", "caffe2/operators/sparse_normalize_op.cc", "caffe2/operators/string_ops.h", "caffe2/operators/stump_func_op.cc", "caffe2/operators/summarize_op.cu", "caffe2/operators/tile_op.h", "caffe2/operators/utility_ops.cu", "caffe2/opt/backend_cutting.cc", "caffe2/opt/backend_transformer_base.h", "caffe2/opt/custom/glow_net_transform.cc", "caffe2/opt/onnxifi_transformer.cc", "caffe2/opt/onnxifi_transformer.h", "caffe2/opt/tvm_transformer.h", "caffe2/perfkernels/adagrad.h", "caffe2/predictor/emulator/benchmark.cc", "caffe2/proto/caffe2.proto", "caffe2/python/checkpoint.py", "caffe2/python/crf.py", "caffe2/python/dataio_test.py", "caffe2/python/examples/imagenet_trainer.py", "caffe2/python/functional.py", "caffe2/python/helpers/fc.py", "caffe2/python/layers/batch_normalization.py", "caffe2/python/layers/feature_sparse_to_dense.py", "caffe2/python/layers/functional.py", "caffe2/python/layers/layer_normalization.py", "caffe2/python/layers/random_fourier_features.py", "caffe2/python/layers/select_record_by_context.py", "caffe2/python/layers/tags.py", "caffe2/python/mkl/rewrite_graph.py", "caffe2/python/modeling/initializers.py", "caffe2/python/modeling/parameter_sharing.py", "caffe2/python/net_drawer.py", "caffe2/python/onnx/ONNXOpCoverage.md", "caffe2/python/operator_test/batch_bucketize_op_test.py", "caffe2/python/operator_test/box_with_nms_limit_op_test.py", "caffe2/python/operator_test/elementwise_op_broadcast_test.py", "caffe2/python/operator_test/gather_ops_test.py", "caffe2/python/operator_test/heatmap_max_keypoint_op_test.py", "caffe2/python/operator_test/one_hot_ops_test.py", "caffe2/python/operator_test/pooling_test.py", "caffe2/python/operator_test/recurrent_network_test.py", "caffe2/python/pipeline.py", "caffe2/python/regularizer.py", "caffe2/python/rnn_cell.py", "caffe2/python/schema.py", "caffe2/python/trt/transform.py", "caffe2/python/visualize.py", "caffe2/python/workspace.py", "caffe2/quantization/server/dnnlowp.cc", "caffe2/quantization/server/norm_minimization.cc", "caffe2/sgd/clip_tensor_op.cc", "caffe2/utils/map_utils.h", "caffe2/utils/math_cpu.cc", "caffe2/utils/proto_utils.h", "caffe2/utils/signal_handler.h", "caffe2/utils/threadpool/WorkersPool.h", "caffe2/utils/zmq_helper.h", "caffe2/video/video_input_op.h", "cmake/Modules/FindvecLib.cmake", "cmake/public/cuda.cmake", "docs/source/hub.rst", "docs/source/jit.rst", "docs/source/notes/extending.rst", "docs/source/notes/windows.rst", "ios/TestApp/benchmark/setup.rb", "modules/detectron/group_spatial_softmax_op.cu", "modules/detectron/select_smooth_l1_loss_op.cc", "modules/detectron/softmax_focal_loss_op.cu", "scripts/xcode_build.rb", "test/common_device_type.py", "test/common_distributed.py", "test/common_utils.py", "test/cpp/api/rnn.cpp", "test/cpp/jit/test_irparser.cpp", "test/dist_autograd_test.py", "test/hypothesis_utils.py", "test/jit/test_data_parallel.py", "test/onnx/debug_embed_params.py", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_cpp_extensions.py", "test/test_distributions.py", "test/test_numba_integration.py", "test/test_utils.py", "tools/autograd/gen_autograd.py", "tools/autograd/gen_python_functions.py", "tools/autograd/gen_variable_type.py", "tools/pyi/gen_pyi.py", "torch/autograd/__init__.pyi", "torch/csrc/api/include/torch/data/datasets/base.h", "torch/csrc/api/include/torch/data/datasets/stateful.h", "torch/csrc/api/include/torch/data/iterator.h", "torch/csrc/api/include/torch/nn/modules/container/sequential.h", "torch/csrc/autograd/profiler.cpp", "torch/csrc/distributed/autograd/context/container.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/utils.h", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/constants.h", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export_module.cpp", "torch/csrc/jit/fuser/codegen.cpp", "torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/import_source.cpp", "torch/csrc/jit/init.cpp", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/operator.cpp", "torch/csrc/jit/passes/alias_analysis.cpp", "torch/csrc/jit/passes/bailout_graph.cpp", "torch/csrc/jit/passes/batch_mm.cpp", "torch/csrc/jit/passes/guard_elimination.cpp", "torch/csrc/jit/passes/onnx.cpp", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization.h", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/script/concrete_module_type.h", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/script/lexer.h", "torch/csrc/jit/script/python_sugared_value.h", "torch/csrc/jit/script/schema_matching.cpp", "torch/csrc/jit/script/schema_matching.h", "torch/csrc/jit/script/string_to_type.cpp", "torch/csrc/jit/unpickler.h", "torch/csrc/tensor/python_tensor.cpp", "torch/csrc/utils/throughput_benchmark-inl.h", "torch/csrc/utils/variadic.h", "torch/distributed/distributed_c10d.py", "torch/distributed/launch.py", "torch/distributed/rpc/__init__.py", "torch/distributions/transforms.py", "torch/hub.py", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/multiprocessing/reductions.py", "torch/nn/modules/_functions.py", "torch/nn/modules/adaptive.py", "torch/nn/modules/instancenorm.py", "torch/nn/utils/prune.py", "torch/onnx/operators.py", "torch/utils/checkpoint.py", "torch/utils/cpp_extension.py", "torch/utils/data/_utils/worker.py", "torch/utils/file_baton.py"], "labels": ["jit", "merged", "open source"]}, "5bc44fb6ea": {"title": "TensorIterator unrolling and vectorized load - step 0, 1 (#31974)", "body": "Summary:\nThis is step 0 and 1 for  https://github.com/pytorch/pytorch/issues/31975:\n\n- Old code is moved to namespace `legacy`\n- New `elementwise_kernel` and `launch_kernel` added to namespace `modern`, they only support 1d contiguous case for now\n- In `gpu_kernel_impl`, dispatch to the new code if the problem is trivial 1d contiguous.\n\nIn terms of performance, this PR affect elementwise operators on contiguous tensors. The performance is improved slightly (up to 8%) for medium size tensors on Volta.\n\n## compiled code\nSee https://github.com/zasdfgbnm/things/blob/master/2020Q1/disassembly-elementwise.ipynb\n\nWe can see that, previously, the add kernel compiles to\n```\n\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 71\n        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;\n        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;\n        /*0020*/                   S2R R0, SR_TID.X ;\n\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 73\n        /*0030*/                   S2R R3, SR_CTAID.X ;\n        /*0040*/                   IMAD R0, R3, 0x200, R0 ;\n\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 76\n        /*0050*/                   ISETP.GE.AND P0, PT, R0, c[0x0][0x160], PT ;\n        /*0060*/               P0 EXIT ;\n\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 110\n        /*0070*/                   IMAD R3, R0.reuse, c[0x0][0x194], RZ ;\n        /*0080*/                   IMAD R6, R0, c[0x0][0x198], RZ ;\n        /*0090*/                   IADD3 R4, P0, R3.reuse, c[0x0][0x178], RZ ;\n        /*00a0*/                   IADD3 R2, P1, R6.reuse, c[0x0][0x180], RZ ;\n        /*00b0*/                   LEA.HI.X.SX32 R5, R3, c[0x0][0x17c], 0x1, P0 ;\n        /*00c0*/                   LEA.HI.X.SX32 R3, R6, c[0x0][0x184], 0x1, P1 ;\n        /*00d0*/                   LDG.E.SYS R5, [R4] ;\n        /*00e0*/                   LDG.E.SYS R2, [R2] ;\n\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 77\n        /*00f0*/                   IMAD R0, R0, c[0x0][0x190], RZ ;\n        /*0100*/                   IADD3 R6, P0, R0, c[0x0][0x170], RZ ;\n        /*0110*/                   LEA.HI.X.SX32 R7, R0, c[0x0][0x174], 0x1, P0 ;\n\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 110\n        /*0120*/                   FFMA R9, R2, c[0x0][0x1a0], R5 ;\n\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 170\n        /*0130*/                   STG.E.SYS [R6], R9 ;\n\t//## File \"/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh\", line 81\n        /*0140*/                   EXIT ;\n.L_16826:\n        /*0150*/                   BRA `(.L_16826);\n        /*0160*/                   NOP;\n        /*0170*/                   NOP;\n.L_29063:\n```\nNow it compiles to\n```\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 210\n        /*0000*/                   MOV R1, c[0x0][0x28] ;\n        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;\n        /*0020*/                   S2R R6, SR_CTAID.X ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 217\n        /*0030*/                   MOV R7, 0x4 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 208\n        /*0040*/                   S2R R3, SR_TID.X ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 210\n        /*0050*/                   LEA R6, R6, R3, 0x8 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 225\n        /*0060*/                   IADD3 R2, R6.reuse, 0x40, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 217\n        /*0070*/                   IMAD.WIDE R4, R6.reuse, R7.reuse, c[0x0][0x190] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 225\n        /*0080*/                   IADD3 R3, R6, 0x80, RZ ;\n        /*0090*/                   ISETP.GE.AND P1, PT, R2, c[0x0][0x160], PT ;\n        /*00a0*/                   ISETP.GE.AND P0, PT, R6.reuse, c[0x0][0x160], PT ;\n        /*00b0*/                   ISETP.GE.AND P2, PT, R3, c[0x0][0x160], PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 217\n        /*00c0*/                   IMAD.WIDE R2, R6.reuse, R7, c[0x0][0x188] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 225\n        /*00d0*/                   IADD3 R14, R6, 0xc0, RZ ;\n        /*00e0*/                   ISETP.GE.AND P3, PT, R14, c[0x0][0x160], PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 228\n        /*00f0*/              @!P1 LDG.E.SYS R11, [R4+0x100] ;\n        /*0100*/              @!P0 LDG.E.SYS R0, [R2] ;\n        /*0110*/              @!P0 LDG.E.SYS R9, [R4] ;\n        /*0120*/              @!P1 LDG.E.SYS R8, [R2+0x100] ;\n        /*0130*/              @!P2 LDG.E.SYS R10, [R2+0x200] ;\n        /*0140*/              @!P2 LDG.E.SYS R13, [R4+0x200] ;\n        /*0150*/              @!P3 LDG.E.SYS R12, [R2+0x300] ;\n        /*0160*/              @!P3 LDG.E.SYS R15, [R4+0x300] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n        /*0170*/                   IMAD.WIDE R6, R6, R7, c[0x0][0x180] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 191\n        /*0180*/                   FFMA R9, R9, c[0x0][0x168], R0 ;\n        /*0190*/                   FFMA R11, R11, c[0x0][0x168], R8 ;\n        /*01a0*/                   FFMA R13, R13, c[0x0][0x168], R10 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n        /*01b0*/              @!P0 STG.E.SYS [R6], R9 ;\n        /*01c0*/              @!P1 STG.E.SYS [R6+0x100], R11 ;\n        /*01d0*/              @!P2 STG.E.SYS [R6+0x200], R13 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 191\n        /*01e0*/                   FFMA R15, R15, c[0x0][0x168], R12 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 244\n        /*01f0*/               P3 EXIT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 245\n        /*0200*/                   STG.E.SYS [R6+0x300], R15 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 248\n        /*0210*/                   EXIT ;\n.L_727:\n        /*0220*/                   BRA `(.L_727);\n        /*0230*/                   NOP;\n        /*0240*/                   NOP;\n        /*0250*/                   NOP;\n        /*0260*/                   NOP;\n        /*0270*/                   NOP;\n.L_32233:\n```\n\n## benchmark\n\nThe benchmark is for add kernel on Volta.\n\nSee https://github.com/zasdfgbnm/things/blob/master/2020Q1/benchmark-unroll.ipynb\n\nFor tensors of size from 2^20 to 2^30, previously we had\n```\n1.5.0a0+dedd16b\ndedd16b4181cae81e37e978cd3bf24c1ba35ca05\n33 \u00b5s \u00b1 31.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n48.7 \u00b5s \u00b1 75 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n78.9 \u00b5s \u00b1 122 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n140 \u00b5s \u00b1 51.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n261 \u00b5s \u00b1 71.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n506 \u00b5s \u00b1 159 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n993 \u00b5s \u00b1 189 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.96 ms \u00b1 139 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n3.9 ms \u00b1 955 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n7.79 ms \u00b1 187 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\nNow we have\n```\n1.5.0a0+b1a239b\nb1a239be8d529e89875fe47cd09964ef3a9516ac\n30.4 \u00b5s \u00b1 18 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n45.2 \u00b5s \u00b1 46.5 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n75 \u00b5s \u00b1 476 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n134 \u00b5s \u00b1 192 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n253 \u00b5s \u00b1 354 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n489 \u00b5s \u00b1 138 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n961 \u00b5s \u00b1 431 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.91 ms \u00b1 578 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n3.8 ms \u00b1 88.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n7.57 ms \u00b1 763 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\nIt is slightly better.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31974\n\nDifferential Revision: D19450765\n\nPulled By: ngimel\n\nfbshipit-source-id: 79601bfceb5da84ff87384ba8193793eb4095a2e", "pr_number": "31974", "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/Loops.cuh"], "labels": ["merged", "open source"]}, "0ac31a99be": {"title": "run code analysis against mobile interpreter (#32276)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32276\n\nInclude mobile interpreter in mobile code analysis pass, which has some\nmanually registered ops in temporary namespaces.\n\nThe mobile interpreter is still under development and these ops will be\nremoved in the future. This is a temporary step for internal build\nexperiment.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19426818\n\nPulled By: ljk53\n\nfbshipit-source-id: 507453dc801e5f93208f1baea12400beccda9ca5", "pr_number": "32276", "files_changed": ["CMakeLists.txt", "caffe2/CMakeLists.txt", "tools/code_analyzer/build.sh"], "labels": ["merged"]}, "824e649d40": {"title": "Specify requires_grad for Parameter replica so it's not always set to True by default (#32356)", "body": "Summary:\nThis is the proposed fix for issue https://github.com/pytorch/pytorch/issues/32018\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32356\n\nDifferential Revision: D19450648\n\nPulled By: mrshenli\n\nfbshipit-source-id: c63eeb6e9f5a87ebe613dd7013907559f295a7ea", "pr_number": "32356", "files_changed": ["torch/nn/parallel/replicate.py"], "labels": ["merged", "open source"]}, "10c2bd35af": {"title": "Fix cudnn channels_last descriptors problem (#31952)", "body": "Summary:\nThis is to append fixes to https://github.com/pytorch/pytorch/issues/31783 so we can pull the fixes in without breaking tests.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31952\n\nDifferential Revision: D19433839\n\nPulled By: ngimel\n\nfbshipit-source-id: 5b3d2f0b2a86aacd1d100dd86996ee0d63e5ee92", "pr_number": "31952", "files_changed": ["aten/src/ATen/cudnn/Descriptors.cpp", "aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/native/ConvUtils.h", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cudnn/Conv.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/memory_format_test.cpp", "c10/core/MemoryFormat.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "test/test_nn.py"], "labels": ["merged", "open source", "triaged"]}, "25e62ebac9": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/9b13f58aa1b1a5a65f21cf9a80f8552f5c07ff60\nhttps://github.com/facebook/folly/commit/044b292accb454838008f0fe88eea0c78c9af27e\nhttps://github.com/pytorch/fbgemm/commit/e1f67bbf3da31ca8fc5f4f506d4791cd8883b448\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: 21df26f60f436eb8c1766f66afac4a0d93dd33d1", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "8c40a78277": {"title": "Back out \"Calling JITed 8 Bit Fused SLS in FBGEMM from C2\" (#32381)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32381\nOriginal commit changeset: 0dfa936eb503\n\n\"Facebook\"\nTemporary remedy for SEV :\nhttps://our.intern.facebook.com/intern/sevmanager/view/s/193726\n\nTest Plan: Run CI tests\n\nReviewed By: jspark1105\n\nDifferential Revision: D19458382\n\nfbshipit-source-id: 731790f96b341ade5e70ff13e4b0b5fafad0fea6", "pr_number": "32381", "files_changed": ["caffe2/operators/lengths_reducer_fused_8bit_rowwise_ops.h"], "labels": ["fb-exported", "merged"]}, "53429680d5": {"title": "Remove stray `@script` (#32235)", "body": "Summary:\nThis should be covered under recursive script now\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32235\n\nPulled By: driazati\n\nDifferential Revision: D19414889\n\nfbshipit-source-id: 85f8132401dbe44c9dbaef7c0350110f90eb9843", "pr_number": "32235", "files_changed": ["torch/jit/quantized.py"], "labels": ["jit", "merged"]}, "61ee8c972f": {"title": "porting scatter_add to ATen (CPU) (#31662)", "body": "Summary:\nFixes [https://github.com/pytorch/pytorch/issues/24758](https://github.com/pytorch/pytorch/issues/24758).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31662\n\nDifferential Revision: D19440824\n\nPulled By: ngimel\n\nfbshipit-source-id: b13443cfcc8bcb9ec21f1cddb5c6fbc0ef4bb0f2", "pr_number": "31662", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/ScatterGatherShapeChecks.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/cpu/ScatterGatherKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.h", "test/test_torch.py"], "labels": ["merged", "open source", "topic: porting", "triaged"]}, "ceffdbd217": {"title": "Temporary workaround for BC test due to schema parser changes", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32324\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19438085\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 3dd2586e73c890a7bdadd6cbb3df2c186f93199d", "pr_number": "32324", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "c7fdf5b251": {"title": "Remove __torch__ from custom class qualname", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32301\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19431645\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 198522a1641cb9f90fa4c614da4ca4162fadf456", "pr_number": "32301", "files_changed": ["torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/python_custom_class.cpp", "torch/csrc/jit/script/schema_type_parser.cpp", "torch/custom_class.h"], "labels": ["jit", "merged"]}, "c7078a1ce8": {"title": "Fix returning instance of custom class from method", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32312\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19433511\n\nPulled By: jamesr66a\n\nfbshipit-source-id: f048d5f60eaba992ee42fea2d318a59b3a156578", "pr_number": "32312", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "test/cpp/jit/test_custom_class.cpp", "test/test_jit.py"], "labels": ["jit", "merged"]}, "1ecad2bb2b": {"title": "Test passing custom class instance to bound method", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32320\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19437335\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 8f5166dbe6fc5704b12b6224932460b12be0d39b", "pr_number": "32320", "files_changed": ["test/cpp/jit/test_custom_class.cpp", "test/test_jit.py"], "labels": ["jit", "merged"]}, "58234c0254": {"title": "support torch script call over rpc (#32197)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32197\n\nThis is to reland https://github.com/pytorch/pytorch/pull/30063, the main change is to match a general exception and grep \"pickle\" error word in \"test_script_functions_not_supported\" unit test, as Python 3.5 and Python 3.6 throw different types of errors with different error message for the rpc call in the unit test.\n[test all]This diff makes following changes:\n1. Providing a new set of python rpc privated APIs, they can accept an annotated TorchScript call and this call can be serialized, deserialized and executed in C++ without GIL. These privated APIs will be binded to JIT in the future, and they are different from public APIs as future JIT binded private APIs will be able to accept qualified_name, not callables. These private APIs are subject to be deprecated once JIT supports torch script function to be a JIT type.\n\nAlso, these APIs require torch script function to be defined and annotated by users in python land, it can not be script class/module constructor or class/module methods.\n\n2. This diff also allows public rpc APIs to accept an annotated TorchScript call and execute code path that above private APIs ran on. Therefore if users invoke an annotated TorchScript call over RPC, this call can be serialized, deserialized and executed in C++ without GIL as well.\n\n3. The above private APIs call a newly defined C++ function to make rpc torch script call to be serialized, deserialized and executed in C++ land. This C++ function returns an ivalue::Future. so that in follow up diff this C++ function can be called when these privated APIs are binded to JIT.\n\n4. script_call.cpp/.h and request_callback_impl.cpp files are refactored accordingly so that torch script call and builtin call can share same message type and codes.\n\n5. refactored deserializeResponse() and added a new utility to deserizalize response to IValue\n\nghstack-source-id: 96879167\nghstack-source-id: 96879167\n\nTest Plan: unit test\n\nDifferential Revision: D19402374\n\nfbshipit-source-id: 04efcc7c167d08a6503f29efe55e76f2be4b2c5e", "pr_number": "32197", "files_changed": ["test/dist_autograd_test.py", "test/rpc_test.py", "tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.h", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/script_call.cpp", "torch/csrc/distributed/rpc/script_call.h", "torch/csrc/distributed/rpc/script_functions.cpp", "torch/csrc/distributed/rpc/script_functions.h", "torch/csrc/distributed/rpc/script_remote_call.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "7fbfb7eef2": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/ea6039a6c98f089b7d5b4455715effbf492deb80\nhttps://github.com/facebook/fbthrift/commit/0d30b8e0fc3191b18d16e1ebb1d7db74dc39b082\nhttps://github.com/facebook/fbzmq/commit/7acedd4723f1997d51638f583bee061abff3b58b\nhttps://github.com/facebook/folly/commit/4db6e3b78569d72dd2c11a13ba508daa02c97fac\nhttps://github.com/facebook/proxygen/commit/cd898afb5e249266789f76951ca1e8ded5a09d5f\nhttps://github.com/facebook/wangle/commit/cf5dd1120450ffe81be83f51396231907cfec325\nhttps://github.com/facebookincubator/fizz/commit/08bdcfd87ed0b382956c6c1ee3ba01e2b48dab1d\nhttps://github.com/facebookincubator/katran/commit/fc84c09b8f104bb3b1497ff97132d39789b37ed1\nhttps://github.com/facebookincubator/mvfst/commit/454d37976b88605aa3ff7cfc7f8f735d385e0bea\nhttps://github.com/pytorch/fbgemm/commit/a22e6b8cb480dadfdada25188c50d65acd39f649\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: b87550b26e69216be2a8e40870a6e7dab825261c", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "b543e3cd6f": {"title": "support empty batch in group normalization (#32401)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32401\n\nhttps://github.com/pytorch/pytorch/issues/12013\n\nTest Plan: buck test mode/dev-nosan //caffe2/test:nn -- 'test_GroupNorm_empty'\n\nDifferential Revision: D19463720\n\nfbshipit-source-id: 8ae44590fc5eeb1adc69a2345d7cc2187d3307ac", "pr_number": "32401", "files_changed": ["aten/src/ATen/native/Normalization.cpp", "test/test_nn.py"], "labels": ["fb-exported", "merged"]}, "ecbf6f99e6": {"title": "Removed unused weight update in prepack. Moved zero point update to (#32254)", "body": "Summary:\nqlinear/qconv to be consistent with data update.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32254\n\nDifferential Revision: D19422929\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 595a4f7d6fde4978c94f3e720ec8645f3f2bdb7a", "pr_number": "32254", "files_changed": ["aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp"], "labels": ["merged"]}, "418ebc827b": {"title": "Build: Respect USE_CUDNN=0, even if cudnn is found (#32404)", "body": "Summary:\nCurrently, setting `USE_CUDNN=0` has no effect and any cudnn library found on your system will be used anyway. This is especially problematic when your system has multiple CUDA versions installed, and you are building with a version that lacks a matching cudnn. CMake will find any other cudnn versions and you end up with both CUDA versions added to your compiler include paths.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32404\n\nDifferential Revision: D19499425\n\nPulled By: ezyang\n\nfbshipit-source-id: a9b3f6f9dc22033481c3c1c5999b1a7ef98468cb", "pr_number": "32404", "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/test/CMakeLists.txt", "cmake/public/cuda.cmake"], "labels": ["merged", "open source"]}, "a2641e6005": {"title": "Make type of `Tensor.type()` more specific (#32353)", "body": "Summary:\nFixes the following issue:\n\n```\n$ cat test.py\nimport torch\n\nt = torch.tensor(1.5)\nt.type(torch.float32)[None]\n\n$ mypy test.py\ntest.py:4: error: Invalid index type \"None\" for \"Union[str, Tensor]\"; expected type \"Union[int, slice]\"\nFound 1 error in 1 file (checked 1 source file)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32353\n\nDifferential Revision: D19499388\n\nPulled By: ezyang\n\nfbshipit-source-id: 715111e934aea020b20f850d27e32c4f70b82572", "pr_number": "32353", "files_changed": ["tools/pyi/gen_pyi.py"], "labels": ["merged", "open source"]}, "0c03304bdf": {"title": ".circleci: Only run macos libtorch on master (#32378)", "body": "Summary:\nThese jobs were taking forver to run so we decided it's only really\nworth it to run it on master.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32378\n\nDifferential Revision: D19499301\n\nPulled By: seemethere\n\nfbshipit-source-id: 22cac5b5baee84e44607a16daeb77048cb0f5974", "pr_number": "32378", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows-binary-builds-smoke-subset.yml"], "labels": ["merged"]}, "cc2d5b15ad": {"title": "F.normalize uses clamp_min_ inplace (#32360)", "body": "Summary:\nWe don't care about autograd when `out!=None` anyways\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32360\n\nDifferential Revision: D19452402\n\nPulled By: colesbury\n\nfbshipit-source-id: c54775289f8a700019ca61e951d59ff4894ac980", "pr_number": "32360", "files_changed": ["torch/nn/functional.py"], "labels": ["merged", "open source"]}, "1177191c8e": {"title": "Synchronize with ShipIt.", "body": "Signed-off-by: Edward Z. Yang <ezyang@fb.com>", "pr_number": null, "files_changed": ["docs/cpp/source/index.rst", "docs/source/community/persons_of_interest.rst"], "labels": []}, "9ce25cce91": {"title": "add an option to record time spent waiting for GIL (#30842)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30842\n\nWe'd like to profile the time spent on GIL acqusiition to debug\nperformance issues.\n\nTest Plan: Unit tests pass.\n\nDifferential Revision: D18837590\n\nfbshipit-source-id: 925968f71c5fb96b8cd93f1eab4647602d2617d1", "pr_number": "30842", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "c13df8b688": {"title": "Fix cusparse version check (#32405)", "body": "Summary:\nThe current version check doesn't use proper lexicographic comparison and so will break for future versions of cuSPARSE with `CUSPARSE_VER_MAJOR > 10` and `CUSPARSE_VER_MINOR < 2`. Also, my cusparse headers for CUDA 9 don't seem to include version macros at all, so added `if !defined` to be explicit about that.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32405\n\nDifferential Revision: D19499412\n\nPulled By: ezyang\n\nfbshipit-source-id: 1593bf1e5a4aae8b75bb3b350d016cc6c3b9c009", "pr_number": "32405", "files_changed": ["aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cu"], "labels": ["merged", "open source"]}, "9482683065": {"title": "Remove dead includes in caffe2/test", "body": "Reviewed By: ezyang\n\nDifferential Revision: D19273220\n\nfbshipit-source-id: 3dfc3388914e60611c84472e3fc529f5b5e40534", "pr_number": null, "files_changed": ["test/cpp/api/any.cpp", "test/cpp/api/functional.cpp", "test/cpp/api/module.cpp", "test/cpp/api/modules.cpp", "test/cpp/api/nn_utils.cpp", "test/cpp/api/serialize.cpp", "test/cpp/jit/test_fuser.cpp", "test/cpp/jit/test_ivalue.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/rpc/test_wire_serialization.cpp"], "labels": []}, "e4f43bf7a5": {"title": "Set rpath for JNI library on Mac (#32247)", "body": "Summary:\nWithout this, dlopen won't look in the proper directory for dependencies\n(like libtorch and fbjni).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32247\n\nTest Plan:\nBuild libpytorch_jni.dylib on Mac, replaced the one from the libtorch\nnightly, and was able to run the Java demo.\n\nDifferential Revision: D19501498\n\nPulled By: dreiss\n\nfbshipit-source-id: 13ffdff9622aa610f905d039f951ee9a3fdc6b23", "pr_number": "32247", "files_changed": ["android/pytorch_android/CMakeLists.txt"], "labels": ["merged"]}, "839fe714de": {"title": "Fix BC test after TorchBind cahnges (#32429)", "body": "Summary:\nIt was broken by https://github.com/pytorch/pytorch/issues/32320. Let's be on the safe side and just whitelist all testing ops\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32429\n\nDifferential Revision: D19501016\n\nPulled By: dzhulgakov\n\nfbshipit-source-id: 9cc1d363edb4579905bee1976a2b57255ce41738", "pr_number": "32429", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "ec4be4e58c": {"title": "Redundant condition (#32396)", "body": "Summary:\nOptimize expression: 'A || (!A && B)' <=> 'A || B'\n\nA: relErr <= maxRelErr\n!A : relErr > maxRelErr\nB: absErr <= absErrForRelErrFailure\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32396\n\nDifferential Revision: D19499370\n\nPulled By: ezyang\n\nfbshipit-source-id: c19bdcb2d4e7ff7806a8cd181c6e7e9e276b9979", "pr_number": "32396", "files_changed": ["caffe2/operators/conv_transpose_op_mobile_test.cc"], "labels": ["merged", "open source"]}, "f86d6c6afd": {"title": "Enhance NCCL watchdog to acitvely abort communicators for timed out ops. (#32338)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32338\n\nTimed out ops could linger around if the user doesn't actually call\n`wait()` on that OP. As result, to fix this I've introduced the following\nfunctionality in this PR:\n\n1. Keep track of all outstanding work in ProcessGroupNCCL.\n2. Enhance NCCL watchdog to sweep through all outstanding work and perform the\nfollowing operations:\n  i.   If the work has timed out, abort all communicators for that work and\n       remove them from the cache.\n  ii.  If the communicators for the work receive an error, abort the\n       communicators and remove them from the cache.\n  iii. If the work has completed (successfully/unsuccessfully), remove it from\n       the list of outstanding work.\nghstack-source-id: 96895704\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19401625\n\nfbshipit-source-id: 8f6f277ba2750a1e1aa03cdbc76e8c11862e7ce5", "pr_number": "32338", "files_changed": ["test/test_c10d.py", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp"], "labels": ["merged"]}, "9e853e7090": {"title": "Revert \"Temporary workaround for BC test due to schema parser changes\" (#32441)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32441\n\nThis reverts commit ceffdbd2179e7dafdc6407909a00f4267db040de.\n\nTest Plan: Imported from OSS\n\nReviewed By: houseroad\n\nDifferential Revision: D19500043\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 3bd22c55e4a81ff8b89d27f6e7438e3bdfc18606", "pr_number": "32441", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "bc6005281b": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/47e0b9b97e19c34dc15a6abf0e8ed93063870ce8\nhttps://github.com/facebook/folly/commit/6d225aaf95b58baf2420efec7f4c570a2d426395\nhttps://github.com/pytorch/fbgemm/commit/ab4da8f60a0194f04c55aa4c9b74c5c175bd1172\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 27bcdf08b6f5e47a5c948e094aca26bf67a6fb66", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "e1d97025ee": {"title": "QNNPACK: Add support for dynamic quantization.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31896\n\nTest Plan: Added new tests to QNNPACK's test suite to cover the new use case.  All new tests are passing.\n\nReviewed By: supriyar\n\nDifferential Revision: D19443250\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: fa7b1cffed7266a3c198eb591d709f222141a152", "pr_number": "31896", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt", "aten/src/ATen/native/quantized/cpu/qnnpack/generate-wrapper.py", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm/4x4c2-dq-sse2.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm/4x8-dq-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm/4x8-dq-neon.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm/8x8-dq-aarch64-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/qnnpack/pack.h", "aten/src/ATen/native/quantized/cpu/qnnpack/src/qnnpack/params.h", "aten/src/ATen/native/quantized/cpu/qnnpack/src/qnnpack/q8gemm.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/gemm-microkernel-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/q8gemm.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/wrappers/q8gemm/4x4c2-dq-sse2.c", "aten/src/ATen/native/quantized/cpu/qnnpack/wrappers/q8gemm/4x8-dq-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/wrappers/q8gemm/4x8-dq-neon.c", "aten/src/ATen/native/quantized/cpu/qnnpack/wrappers/q8gemm/8x8-dq-aarch64-neon.S"], "labels": ["merged"]}, "0ed04bfdf6": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/40b08129cfd2aed6dba56d10d8cea4ac0ef6932e\nhttps://github.com/facebook/proxygen/commit/8cd8d286e68a06968b80dd5a6d8e150392b87aea\nhttps://github.com/facebook/rocksdb/commit/d305f13e2124132863267eb49b2a08ede679d2c4\nhttps://github.com/pytorch/fbgemm/commit/2957bd45f19d8fa2d185e26b7ada5a394c5ba5b4\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 3b76eb7c8b6b5cf617aca7bd143e1ee404c4f0ed", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "be6ffac1b6": {"title": "Adagrad optimizer - updated step function, added param_groups, state to optimizers", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29335\n\nDifferential Revision: D19449382\n\nPulled By: anjali411\n\nfbshipit-source-id: ee238801ed9cdf15a80f2ce31cc4aab8ba582aea", "pr_number": "29335", "files_changed": ["test/cpp/api/optim.cpp", "test/cpp/api/serialize.cpp", "torch/csrc/api/include/torch/optim/adagrad.h", "torch/csrc/api/include/torch/optim/optimizer.h", "torch/csrc/api/include/torch/optim/serialize.h", "torch/csrc/api/include/torch/serialize/input-archive.h", "torch/csrc/api/src/optim/adagrad.cpp", "torch/csrc/api/src/optim/optimizer.cpp", "torch/csrc/api/src/serialize/input-archive.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "0b606a4a7c": {"title": "Enhace DispatchStub to be thread safe from a TSAN point of view. (#32148)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32148\n\nTSAN would complain about multiple threads reading and writing to the\n`cpu_dispatch_ptr` without any sort of synchronization. Although, this is a\nvalid issue from a TSAN point of view, there wasn't a correctness issue since\nboth threads would compute the same value.\n\nIn order to fix this, I've used std::atomic for cpu_dispatch_ptr with relaxed\nordering guarantees.\nghstack-source-id: 96989435\n\nTest Plan: Verify the TSAN tests pass.\n\nDifferential Revision: D19386082\n\nfbshipit-source-id: 1ff0893e02529eddd06b2855d9565edf1bbf1196", "pr_number": "32148", "files_changed": ["aten/src/ATen/native/DispatchStub.h"], "labels": ["merged"]}, "7fdc6cb74e": {"title": "Fix test_data_parallel name errors and add to run_test.py (#32428)", "body": "Summary:\nWhile working on https://github.com/pytorch/pytorch/issues/31768 and trying to add tests for `DataParallel`, I discovered that:\n- `test_data_parallel.py` can't be run through `run_test.py`\n- running it with `pytest` fails with many name errors\n\n`test_data_parallel.py` seems to have been split from `test_nn.py` in https://github.com/pytorch/pytorch/issues/28297 but not in a state where it can actually be run. Presumably `DataParallel` hasn't been tested by CI in the time since.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32428\n\nDifferential Revision: D19499345\n\nPulled By: ezyang\n\nfbshipit-source-id: f9b748a99a5c85fc6675c22506cf10bbfd9c8a4d", "pr_number": "32428", "files_changed": ["test/run_test.py", "test/test_data_parallel.py"], "labels": ["merged", "open source"]}, "4973695268": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/d45f7b4f0972951c2548e918c0bc167f397815b3\nhttps://github.com/facebook/rocksdb/commit/e6e8b9e8718698b334d18fa8f5ab6db30b147c53\nhttps://github.com/facebookincubator/katran/commit/da618022d26b0786d4a090f38006db9ae584f2cb\nhttps://github.com/pytorch/fbgemm/commit/2df47f519a6c896b7c418a8a94aae9c07ba7285c\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: c4af09e70a56d11e845150ba3d90a570a3758e51", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "64de93d8e7": {"title": "Move log_normal to Aten(CPU) (#31854)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/24723.\nBenchmark script :\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\n\ndef _time():\n    return time.time()\n\ndevice = \"cpu\"\n\n#warm up\nfor n in [10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=False, device=device)\n    for i in range(1000):\n        input.log_normal_()\n\nfor n in [1, 10, 100, 1000]:\n    fwd_t = 0\n    input = torch.randn(128, n, requires_grad=False, device=device)\n    for i in range(10000):\n        t1 = _time()\n        input.log_normal_()\n        t2 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n    fwd_avg = fwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.4f (ms).\" % (n, fwd_avg))\n```\nTest Device: skx-8180.\nBefore:\n```\ninput size(128, 1) forward time is 0.0114 (ms).\ninput size(128, 10) forward time is 0.1021 (ms).\ninput size(128, 100) forward time is 1.0081 (ms).\ninput size(128, 1000) forward time is 10.1831 (ms).\n```\nAfter:\n```\ninput size(128, 1) forward time is 0.0108 (ms).\ninput size(128, 10) forward time is 0.0969 (ms).\ninput size(128, 100) forward time is 0.9804 (ms).\ninput size(128, 1000) forward time is 9.6131 (ms).\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31854\n\nDifferential Revision: D19314586\n\nPulled By: pbelevich\n\nfbshipit-source-id: 2ea1d9a2c505e36aca9e609b52ccb3e8caf2ba8f", "pr_number": "31854", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorRandom.cpp", "aten/src/TH/generic/THTensorRandom.h"], "labels": ["merged", "open source"]}, "60b6c99aa7": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/proxygen/commit/d2ee8a1a3fc0bceee0dae34de37d1e23a8383977\nhttps://github.com/pytorch/fbgemm/commit/a1543b168df44c4722fa545746aaaa7cf9660f6d\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: a1394f1c4a48920d3ce1403c70351e2c56eaecf0", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "44b270d892": {"title": "`insert_quant_dequant` pass support shared class types (#31408)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31408\n\nWe'll error out when a graph is quantized with different QSchemes.\nThis only occurs when we have two modules that have same types (e.g. two Conv2d modules initialized with\nsame arguments) and quantized with two configs that would produce different quantized graphs, for example\nper tensor affine and per channel affine. This is a rare case, so it should be OK to skip for now.\nActual support will come later.\n\nTest Plan:\ntest_jit.py, test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D19162366\n\nfbshipit-source-id: 798f06d0ddef0c8458237ce88b62159cc77eec8b", "pr_number": "31408", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/script/module.h"], "labels": ["jit", "merged"]}, "0d610b4821": {"title": "Remove the support of build options like NO_*, WITH_* (#32447)", "body": "Summary:\nWe will now use USE_*, BUILD_* consistently. The backward compatibility\nfor NO_* and WITH_* is hereby removed in this commit, as promised in the\ncomment (next release is beyond Feb 20):\n\n    # Before we run the setup_helpers, let's look for NO_* and WITH_* variables and hotpatch environment with the USE_*\n    # equivalent The use of NO_* and WITH_* is deprecated and will be removed in Feb 20, 2020.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32447\n\nDifferential Revision: D19515536\n\nPulled By: ezyang\n\nfbshipit-source-id: 2f2c51e6d4674af690b190a1f0397b8f596b6a15", "pr_number": "32447", "files_changed": ["tools/setup_helpers/env.py"], "labels": ["merged", "open source"]}, "248f6d0485": {"title": "Implement backend fallback fallthrough (#32439)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32439\n\nThis adds c10::fallthrough_kernel which is a special boxed function which\ncan be used to implement fallthrough behavior at a dispatch key.  A fallthrough\nkernel will redispatch to the next valid dispatch key.  It is implemented\nin such a way that it costs no more to fallthrough than it does to go\nstraight to the actual implementation of the kernel.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: D19503886\n\nTest Plan: Imported from OSS\n\nPulled By: ezyang\n\nfbshipit-source-id: 6ee05bd815c4ef444e612d19f62312dbb76f2787", "pr_number": "32439", "files_changed": ["aten/src/ATen/Utils.h", "aten/src/ATen/core/boxing/KernelFunction.cpp", "aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.cpp", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/dispatch/DispatchTable.h", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/test/backend_fallback_test.cpp"], "labels": ["merged"]}, "8abaa322da": {"title": "fix torch.eq() doc entry (#32399)", "body": "Summary:\nfix `torch.eq()` entry example to match the current output (boolean, instead of uint8)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32399\n\nDifferential Revision: D19498104\n\nPulled By: ezyang\n\nfbshipit-source-id: e7ec1263226766a5c549feed16d22f8f172aa1a3", "pr_number": "32399", "files_changed": ["torch/_torch_docs.py"], "labels": ["merge-this-please", "merged", "open source"]}, "e37a24b044": {"title": "Always return a new tensor from nn.functional.pad (#32350)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31734\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32350\n\nDifferential Revision: D19501845\n\nPulled By: ezyang\n\nfbshipit-source-id: ea79496d23dc0016f3caa233c53d283b08f60371", "pr_number": "32350", "files_changed": ["aten/src/ATen/native/ConstantPadNd.cpp", "test/test_nn.py"], "labels": ["merged", "open source"]}, "c342c354a9": {"title": "Put sparse all reduce results to input tensors (#32226)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32226\n\nright now if users call torch.dist.all_reduce() on dense tensors, outputs are put in input tensors. but if users call torch.dist.all_reduce() on sparse tensors, outputs are neither returned explicitly to users nor are put in input tensors.\n\nTo make torch.dist.all_reduce() API have same behavior on both dense tensors and sparse tensors, this diff is made to make torch.dist.all_reduce() on sparse tensors to put output in input tensors as well. This is acheived by simply calling input_sparse.copy_(output_sparse), see PR https://github.com/pytorch/pytorch/pull/9005 that implemented copy_ for sparse tensors.\n\nclose #31413\nghstack-source-id: 96984228\n\nTest Plan: unit test\n\nDifferential Revision: D19192952\n\nfbshipit-source-id: 2dd31dc057f20cc42b44b9e55df864afa2918c33", "pr_number": "32226", "files_changed": ["test/common_distributed.py", "test/test_c10d.py", "test/test_distributed.py", "torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["merged"]}, "b77c25dec0": {"title": "Fix dll load logic for Python 3.8 on Windows (#32215)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31181 and https://github.com/pytorch/pytorch/pull/31162#discussion_r362495611.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32215\n\nDifferential Revision: D19501869\n\nPulled By: ezyang\n\nfbshipit-source-id: 363824e52d2592ad968ecf1df345aa4c0daff915", "pr_number": "32215", "files_changed": ["caffe2/python/__init__.py", "test/test_jit.py", "test/test_torch.py", "torch/__init__.py", "torch/backends/cudnn/__init__.py", "torch/cuda/__init__.py"], "labels": ["merged", "open source"]}, "1c017f0c14": {"title": "Migrate max and min (binary) from TH to ATen. (#30851)", "body": "Summary:\nTH implementation will be removed after the unary max and min are\nmigrated.\n\nBenchmark: (Debian 10, Release build, gcc 7.4, no turbo)\n\n```python\nimport timeit\nfor device in ('cpu', 'cuda'):\n    print(f'device: {device}')\n    for op in ('max', 'min'):\n        for dtype in ('torch.double', 'torch.float', 'torch.int16',\n'torch.int32', 'torch.int64'):\n            for n, t in [(10_000, 200000),\n                        (100_000, 20000)]:\n                print(f'torch.{op}(a, b), numel() == {n} for {t} times,\ndtype={dtype}')\n                print(timeit.timeit(f'torch.{op}(a)' +\n(';torch.cuda.synchronize()' if device == 'cuda' else ''),\n                                    setup=f'import torch; a =\ntorch.arange({n}, dtype={dtype}); b = torch.ones({n}, 0, dtype={dtype})\n* ({n} / 2)', number=t))\n    print()\n```\n\nBefore:\n\n```\ndevice: cpu\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n2.241763713000182\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.7138833169992722\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.2183356810000987\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.7031846980007685\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.7704679510006827\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.289198366999699\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.7937613740014058\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.2930124340000475\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8032857640009752\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.2908709189996443\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n1.8829010000008566\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.2994690759987861\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n1.8037853410005482\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.2929310759991495\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.8075240359994496\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.2932477679987642\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.7868400779989315\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.2885970789993735\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8389664830010588\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.29402057399966\n\ndevice: cuda\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n4.787109836999662\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.842438002999188\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n3.429616614999759\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.835390076999829\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.940423873000327\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.4108991760003846\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.9318018840003788\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4168134739993548\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n2.9610764919998473\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4189234130008117\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n2.960172712999338\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.4162539499993727\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.8985912560001452\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.4113489299998037\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.9160250799995993\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.4128787690005993\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.8806865219994506\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4086357010000938\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n2.9362181240012433\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4151225870009512\n\n```\n\nAfter:\n\n```\ndevice: cpu\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n2.2685823729998447\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.72004808300062\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.212242640000113\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.7089235590001408\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.7767087259999244\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.2916517639996528\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.8265984959998605\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.3002885240002797\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8084679720004715\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.3012119999993956\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n1.8800218449996464\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.3060645710002063\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.4905043950002437\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.9126290209997023\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.7972335520007618\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.2918074379995232\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.8047651860006226\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.2992197730000044\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8526509560006161\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.3030709570002728\n\ndevice: cuda\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n4.700986622000528\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.8415469050005413\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n3.3051693249999516\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.8321999460004008\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.8086475109994353\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.405110773999695\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.913458047999484\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4236377289998927\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n2.9386842409994642\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4230227469997772\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n3.0341797270002644\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.4289592409995748\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n3.6091147850002017\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n2.036691903999781\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.8256167649997224\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.4078955400000268\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.8631781489993955\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4210130069996012\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n3.0112479260005784\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4297719679998409\n\n```\n\nSolve partly https://github.com/pytorch/pytorch/issues/24594 #24595\n\nClose https://github.com/pytorch/pytorch/issues/25016\n\nContinuing https://github.com/pytorch/pytorch/issues/27185\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30851\n\nDifferential Revision: D19515694\n\nPulled By: ezyang\n\nfbshipit-source-id: 1764897f912d6ae24b0c361f19a1aacf96e0826e", "pr_number": "30851", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryCompareKernel.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["merged", "open source"]}, "510a122d27": {"title": "add missing align_corners annotation (#32492)", "body": "Summary:\nadds the missing annotation in grid_sample and affine_grid functional\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32492\n\nDifferential Revision: D19516550\n\nPulled By: ezyang\n\nfbshipit-source-id: 064c8c99bf6eae6744237c0b151b3ce4c82ada96", "pr_number": "32492", "files_changed": ["torch/nn/functional.pyi.in"], "labels": ["merged", "open source"]}, "8e689378c7": {"title": "Move some of the helper functions for public use (#32202)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32202\n\nMove some helper functions in ModuleUseDeduper for public use\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19508034\n\nfbshipit-source-id: 2e8e05eff6f3bbcfe6936598371e4afa72f9b11f", "pr_number": "32202", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "e184a8843c": {"title": "Fix comparisions for ConcreteModuleType (#32256)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32256\n\nPreviously two unrelated modules loaded from torch.jit.load\nwould compare equal because we only considered their data_ attributes which\nare initialized blank in torch.jit.load. This changes ConcreteModuleType\nto distinguish when the data_ attribute is blank vs when it is empty.\n\nThis replaces the poisoned logic.\nghstack-source-id: 96755797\n\nTest Plan: oss\n\nDifferential Revision: D19423055\n\nfbshipit-source-id: 79d6a50a3731c6eeb8466ba2a93702b49264bba0", "pr_number": "32256", "files_changed": ["test/jit/test_type_sharing.py", "torch/csrc/jit/script/concrete_module_type.cpp"], "labels": ["jit", "merged"]}, "adf0916606": {"title": "Add str[] float[] constants resubmit", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31791\n\nTest Plan: Imported from OSS\n\nReviewed By: driazati\n\nDifferential Revision: D19439513\n\nPulled By: eellison\n\nfbshipit-source-id: a04c7401687b051f0d4fb4794963931ebe004194", "pr_number": "31791", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/cpp/jit/test_constant_propagation.cpp", "test/cpp/jit/test_irparser.cpp", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_jit.py", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/ir.cpp"], "labels": ["jit", "merged"]}, "b01d824a78": {"title": "improve mayContainAlias (#31839)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31839\n\nThere are a number of improvements that can be made to `mayContainAlias`, which I would like to do in follow ups. For now, this is an easy one.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19439516\n\nPulled By: eellison\n\nfbshipit-source-id: 0042fb7eaae6cfb4916bf95dc38280517a4bd987", "pr_number": "31839", "files_changed": ["test/cpp/jit/test_alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.cpp"], "labels": ["jit", "merged"]}, "69492ad6ac": {"title": "remove tuple logic in constant propagation (#31840)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31840\n\nThe next PR in this stack makes tuples insertable as constants, so we can remove special handling of tuples in constant propagation.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19439515\n\nPulled By: eellison\n\nfbshipit-source-id: c58f153157f1d4eee4c1242decc4f36e41c1aa05", "pr_number": "31840", "files_changed": ["test/cpp/jit/test_constant_propagation.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/passes/constant_propagation.cpp"], "labels": ["jit", "merged"]}, "38d122eca9": {"title": "implement tuple constants (#31841)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31841\n\nAdd Tuple Constants to JIT. The constraint here is that all elements of a tuple must themself be insertable as a a constant. Previously tuples were special cased in constant propagation, but now that there are more passes that are inserted constants, such as freezing, we should just have tuples be representable as constants.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19439514\n\nPulled By: eellison\n\nfbshipit-source-id: 3810ba08ee349fa5598f4b53ea64525996637b1a", "pr_number": "31841", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "test/cpp/jit/test_irparser.cpp", "test/test_jit.py", "torch/csrc/jit/attributes.h", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/init.cpp", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/ir.h", "torch/csrc/jit/node_hashing.cpp", "torch/csrc/jit/passes/lower_tuples.cpp", "torch/csrc/jit/passes/onnx.cpp"], "labels": ["jit", "merged"]}, "b7c6277c53": {"title": "Adding QConfigTypePtrMap (#32203)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32203\n\nThe type is needed for allowing multiple qconfig configurations for shared\nClassType, see next PR for more details\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19508027\n\nfbshipit-source-id: a3df29dab3038bfa88c55dda98a3e8a78e99e5a1", "pr_number": "32203", "files_changed": ["torch/csrc/jit/passes/quantization.h", "torch/csrc/jit/script/module.h"], "labels": ["jit", "merged"]}, "43eb931c0f": {"title": "Remove mis-exposed abort API on ProcessGroup", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32292\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19430252\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4ec594e1be54afe774bdcecc0f1c9bda2edf5e0d", "pr_number": "32292", "files_changed": ["torch/csrc/distributed/c10d/init.cpp"], "labels": ["merged"]}, "faffd2141a": {"title": "Corrected logical boolean expression (#32249)", "body": "Summary:\nChanged bitwise & to logical && in the boolean expression.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32249\n\nDifferential Revision: D19501586\n\nPulled By: eellison\n\nfbshipit-source-id: afe374cfc9661182703cc82810d9cb735fbb8180", "pr_number": "32249", "files_changed": ["torch/csrc/jit/script/schema_matching.cpp"], "labels": ["jit", "merged", "open source"]}, "14e0bec9f2": {"title": "[caffe2] remove unnecessary np.set_printoptions and fix test errors (#32475)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32475\n\nAs title\n\nTest Plan: CI\n\nReviewed By: houseroad\n\nDifferential Revision: D19508778\n\nfbshipit-source-id: fd9ad63607535980505d155f3e3c3b7c6b95daf7", "pr_number": "32475", "files_changed": ["caffe2/python/operator_test/matmul_op_test.py"], "labels": ["fb-exported", "merged"]}, "556c0b063d": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/87b81e7cb2e17d6cb2289d678decd9311136ab28\nhttps://github.com/facebook/folly/commit/3a9a0976f2537ed66a465bf30ec2038a7a92d636\nhttps://github.com/facebook/litho/commit/9294f3b2faeded509b6fb0c2780b4bf4d4e6d763\nhttps://github.com/facebook/proxygen/commit/c8addc5ad4ebf73a2dbb8a00e0d9e68dfdf12cd7\nhttps://github.com/facebookincubator/profilo/commit/9a9f1a849a33248fa4d7f06a100cfa73257de233\nhttps://github.com/pytorch/fbgemm/commit/27cb280170fbf530033c4d0123e063e2f8bb50f3\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 73beec64bf9c17fa6c42dd09ea85350e8c9c66ea", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "193ac31441": {"title": "[jit] Enable IValue to hold a PyObject (#32491)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32491\n\nThis PR enables IValue to be able to hold a pure PyObject by adding a\nnew enum tag, a new jit_type to denote PyObject existance in IValue and\nthe JIT type system. We don't and not plan to expose this to user.\n\nThis is the basic piece that enable ivalue to be adopted broader like\nmaking RRef always hold IValue, it might also simplify some compiler\nlogic\nghstack-source-id: 97039980\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19502234\n\nfbshipit-source-id: 90be001706d707d376cfbea25980fd82980df84a", "pr_number": "32491", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/test_jit.py", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/python_ir.cpp", "torch/csrc/jit/python_ivalue.h", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/unpickler.cpp"], "labels": ["jit", "merged"]}, "4bdfc71421": {"title": "Fix race condition for to() backward that spans devices (#31930)", "body": "Summary:\nWhile putting finishing touches on the gradient scaling PR (https://github.com/pytorch/pytorch/pull/26512), I discovered my multi-GPU test (which uses `to()` to transfer tensors between devices) was intermittently failing with bad numerics.  I knew it was going to be [a weird case from the start](https://www.imdb.com/title/tt8946378/quotes/qt4868203) and spent a week descending into madness.  It turns out, for backward ops that create gradients on a different device from the device on whose stream the op is executed, the streaming backward synchronizations in [input_buffer.cpp](https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/input_buffer.cpp#L46-L83) do not properly tell later ops to wait on the population/creation of those gradients.  For example, a cross-device `to()` backward (CopyBackward Node) enqueues a cudaMemcpyAsync on the current stream of the source (incoming gradient's) device, then [syncs getCurrentCUDAStream on the destination device with the cudaMemcpyAsync](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Copy.cu#L76).  However, `input_buffer.cpp` in such cases ([case (3)](https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/input_buffer.cpp#L77-L81)) was not properly telling `opt_consumer_stream` to wait on the current stream of the destination device (`var`'s device).\n\nCircumstances needed to repro in current master (see [my test](https://github.com/pytorch/pytorch/compare/master...mcarilli:backward_to_race_fix#diff-e68a7bc6ba14f212e5e7eb3727394b40R1901)):\n- 2 devices, with non-default streams used for forward-pass ops on both devices (which is the default behavior in test_cuda.py)\n- A `to()` that transfers a tensor requiring grad from one device to another\n- A backward pass that routes back through to()'s backward (aka CopyBackward).\n\nUnder these circumstances, backward ops following CopyBackward on CopyBackward's destination device (aka the original forward-pass source device) race with the device-to-device transfer, and execute using partially-transferred data.\n\nThe present PR fixes the race condition and ensures that later ops wait on the CopyBackward transfer.  This PR should also make streaming backward safe for other backward ops that span devices, as long as they play nice and populate any new gradients they create using the \"current stream\" of the device(s) on which they create those gradients.\n\nThere are a couple minor issues where I'm not sure of the best approach:\n- Should we guard onto the var's device for the entire body of InputBuffer::add?\n- I'm fairly sure we need to `recordStream` on `var` if the consumer stream is different from the stream on which (we expect) `var` was created, but calling `c10::cuda::CUDACachingAllocator::recordStream` in input_buffer.cpp might break CPU-only builds.  I couldn't find a different API call to record streams that seemed CPU-build-agnostic.  Could I wrap the call with a macro?\n\nThanks to mruberry for helpful suggestions and also the organization/naming of the stream pool and streaming backward code that allowed me to (just barely) wrap my head around the issue.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31930\n\nDifferential Revision: D19517617\n\nPulled By: mruberry\n\nfbshipit-source-id: 183d5460aefa5d27366b465b0473b80ec80fa044", "pr_number": "31930", "files_changed": ["test/test_cuda.py", "torch/csrc/autograd/input_buffer.cpp"], "labels": ["merged", "open source"]}, "685f090ac8": {"title": "[Rowwise Pruning][c2 op] Add Quantile Op (#32448)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32448\n\nUsing binary search to compute the value for the given quantile among the input tensors.\n\nTest Plan: Newly added unittests;\n\nReviewed By: jspark1105\n\nDifferential Revision: D19487604\n\nfbshipit-source-id: 0dc6627b78d1310ac35b3f1d53b89cc89a697ece", "pr_number": "32448", "files_changed": ["caffe2/operators/quantile_op.cc", "caffe2/operators/quantile_op.h", "caffe2/python/operator_test/quantile_test.py"], "labels": ["fb-exported", "merged"]}, "e735395fc6": {"title": "[caffe2] use 2-stage EmbeddingSpMDM interface (#32271)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32271\n\nUse the 2-stage EmbeddingSpMDM interface in D19425982 to reduce the overhead of code cache lookup and lock contention.\nFix an issue in sparse_lengths_sum_benchmarks generating empty indices when average length is small like 1.\n\nTest Plan: CI\n\nReviewed By: dskhudia\n\nDifferential Revision: D19425987\n\nfbshipit-source-id: d5c5f0d46e0072403901809c31d516fa0f4b9b31", "pr_number": "32271", "files_changed": ["caffe2/operators/lengths_reducer_fused_8bit_rowwise_ops.h", "caffe2/python/operator_test/sparse_lengths_sum_benchmark.py"], "labels": ["fb-exported", "merged"]}, "f050b16dd9": {"title": "Move pytorch distributed tests to separate folder for contbuild. (#30445)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30445\n\nCreate distributed and rpc directories under caffe/test for better management\nof unit tests.\n\nDifferential Revision: D18702786\n\nfbshipit-source-id: e9daeed0cfb846ef68806f6decfcb57c0e0e3606", "pr_number": "30445", "files_changed": [".jenkins/pytorch/multigpu-test.sh", ".jenkins/pytorch/test.sh", ".jenkins/pytorch/win-test-helpers/test_python_all_except_nn.bat", ".jenkins/pytorch/win-test-helpers/test_python_nn.bat", "setup.py", "test/common_cuda.py", "test/common_device_type.py", "test/common_distributed.py", "test/common_methods_invocations.py", "test/common_nn.py", "test/common_quantization.py", "test/common_quantized.py", "test/common_utils.py", "test/data/network1.py", "test/data/network2.py", "test/dist_autograd_test.py", "test/dist_optimizer_test.py", "test/dist_utils.py", "test/distributed/rpc/test_dist_autograd_spawn.py", "test/distributed/rpc/test_dist_optimizer_spawn.py", "test/distributed/rpc/test_rpc_spawn.py", "test/distributed/test_c10d.py", "test/distributed/test_c10d_spawn.py", "test/distributed/test_data_parallel.py", "test/distributed/test_distributed.py", "test/distributed/test_nccl.py", "test/expect/__init__.py", "test/expecttest.py", "test/hypothesis_utils.py", "test/jit/test_async.py", "test/jit/test_autodiff_subgraph_slicing.py", "test/jit/test_builtins.py", "test/jit/test_class_type.py", "test/jit/test_custom_operators.py", "test/jit/test_data_parallel.py", "test/jit/test_export_modes.py", "test/jit/test_list_dict.py", "test/jit/test_logging.py", "test/jit/test_models.py", "test/jit/test_module_interface.py", "test/jit/test_recursive_script.py", "test/jit/test_type_sharing.py", "test/jit/test_unsupported_ops.py", "test/jit_utils.py", "test/onnx/export_onnx_tests_generator.py", "test/onnx/test_operators.py", "test/onnx/test_pytorch_common.py", "test/rpc_agent_test_fixture.py", "test/rpc_test.py", "test/run_test.py", "test/test_autograd.py", "test/test_c10d.py", "test/test_c10d_spawn.py", "test/test_cpp_api_parity.py", "test/test_cpp_extensions.py", "test/test_cuda.py", "test/test_cuda_primary_ctx.py", "test/test_data_parallel.py", "test/test_dataloader.py", "test/test_dist_autograd_spawn.py", "test/test_dist_optimizer_spawn.py", "test/test_distributed.py", "test/test_distributions.py", "test/test_expecttest.py", "test/test_fake_quant.py", "test/test_function_schema.py", "test/test_indexing.py", "test/test_jit.py", "test/test_jit_disabled.py", "test/test_jit_fuser.py", "test/test_jit_py3.py", "test/test_logging.py", "test/test_mkldnn.py", "test/test_module/__init__.py", "test/test_module/future_div.py", "test/test_module/no_future_div.py", "test/test_multiprocessing.py", "test/test_multiprocessing_spawn.py", "test/test_namedtensor.py", "test/test_nccl.py", "test/test_nn.py", "test/test_numba_integration.py", "test/test_optim.py", "test/test_overrides.py", "test/test_qat.py", "test/test_quantization.py", "test/test_quantized.py", "test/test_quantized_models.py", "test/test_quantized_nn_mods.py", "test/test_quantized_tensor.py", "test/test_rpc_spawn.py", "test/test_sparse.py", "test/test_tensorboard.py", "test/test_throughput_benchmark.py", "test/test_torch.py", "test/test_type_hints.py", "test/test_type_info.py", "test/test_type_promotion.py", "test/test_utils.py", "torch/testing/_internal/__init__.py", "torch/testing/_internal/common_cuda.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_nn.py", "torch/testing/_internal/common_quantization.py", "torch/testing/_internal/common_quantized.py", "torch/testing/_internal/common_utils.py", "torch/testing/_internal/data/__init__.py", "torch/testing/_internal/data/network1.py", "torch/testing/_internal/data/network2.py", "torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/__init__.py", "torch/testing/_internal/distributed/rpc/__init__.py", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/dist_optimizer_test.py", "torch/testing/_internal/distributed/rpc/rpc_agent_test_fixture.py", "torch/testing/_internal/distributed/rpc/rpc_test.py", "torch/testing/_internal/expecttest.py", "torch/testing/_internal/hypothesis_utils.py", "torch/testing/_internal/jit_utils.py", "torch/testing/_internal/test_module/__init__.py", "torch/testing/_internal/test_module/future_div.py", "torch/testing/_internal/test_module/no_future_div.py"], "labels": ["merged", "module: tests"]}, "21d475e20d": {"title": "[gloo] Skip registry warning (#31126)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31126\n\nGloo device creator registry is throwing warning that confuses users - https://fb.workplace.com/groups/1405155842844877/permalink/3217491788277931/\nCreate C10_DEFINE_SHARED_REGISTRY_WITHOUT_WARNING API to skip such warning\n\nTest Plan:\n{F224342749}\n\nTested both `C10_DEFINE_SHARED_REGISTRY` and `C10_DEFINE_SHARED_REGISTRY_WITHOUT_WARNING`.\nMake sure nothing breaks\n\nReviewed By: d4l3k\n\nDifferential Revision: D18904783\n\nfbshipit-source-id: 0e0065d530956249a18325d4ed3cb58dec255d4c", "pr_number": "31126", "files_changed": ["c10/util/Registry.h", "torch/lib/c10d/GlooDeviceFactory.cpp"], "labels": ["fb-exported", "merged"]}, "02aa3ba331": {"title": "Raise error for code that risk deadlock (#32295)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32295\n\nFix for https://github.com/pytorch/pytorch/issues/32045\n\nCalling into the engine with the GIL can deadlock because:\n- worker thread initialization acquires the GIL\n- Any Node / hook can be a python function that will acquire the GIL\n\nThe choice was made here to raise an error as one of the advantage of using cpp extensions with python is to be able to release the GIL. So we prefer to educate users to do it rather than doing it under the hook.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19430979\n\nPulled By: albanD\n\nfbshipit-source-id: e43f57631885f12e573da0fc569c03a943cec519", "pr_number": "32295", "files_changed": ["test/test_cpp_extensions.py", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/utils/python_compat.h"], "labels": ["merged"]}, "ea7bebb7fe": {"title": "[PyTorch BC] Clean up the whitelist for PyTorch Op BC check (#32523)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32523\n\nremove stale items\n\nTest Plan: cont build\n\nReviewed By: hl475\n\nDifferential Revision: D19526918\n\nfbshipit-source-id: ee7392ae84e5ddf88284020775119e59c9b6533e", "pr_number": "32523", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["fb-exported", "merged"]}, "583bb97618": {"title": "[quant][graphmode] Default to non-inplace in graph mode quantization API (#32204)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32204\n\natt\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19508030\n\nfbshipit-source-id: 94814c3c126a196f3938f944abfa5ae2a24d8dde", "pr_number": "32204", "files_changed": ["torch/quantization/_quantize_script.py"], "labels": ["merged"]}, "9af5a97b1d": {"title": "Fix nll_loss to support empty tensors on GPU (#31491)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31491\n\nFixes #31472\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19537231\n\nPulled By: pbelevich\n\nfbshipit-source-id: 20a43251a0f68a7a3557dd8234daee2d4814e5dd", "pr_number": "31491", "files_changed": ["aten/src/THCUNN/ClassNLLCriterion.cu", "aten/src/THCUNN/SpatialClassNLLCriterion.cu", "aten/src/THCUNN/generic/ClassNLLCriterion.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu", "test/test_nn.py", "torch/testing/_internal/common_device_type.py"], "labels": ["merged"]}, "b6b8620871": {"title": "Add unit test on export_opnames with interface. (#31531)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31531\n\nAs suggested by suo , add unit test on torch.jit.export_opnames with interface. A submodule is annotated as interface and assigned to an instance, and then re-assigned to another instance. Make sure the operator names are also updated.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19539129\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 71a76ae7790cdd577618ca278afdb132727f08dc", "pr_number": "31531", "files_changed": ["test/test_jit_py3.py"], "labels": ["merged"]}, "db02a4e4ce": {"title": "Support 3D attention mask in MultiheadAttention. (#31996)", "body": "Summary:\nSupport a 3D attention mask for MultiheadAttention. If `attn_mask` has the batch dimension, it will not be unsqueezed. Fix https://github.com/pytorch/pytorch/issues/30678\nRelevant issues/pr:\nhttps://github.com/pytorch/pytorch/pull/25359\nhttps://github.com/pytorch/pytorch/issues/29520\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31996\n\nDifferential Revision: D19332816\n\nPulled By: zhangguanheng66\n\nfbshipit-source-id: 3448af4b219607af60e02655affe59997ad212d9", "pr_number": "31996", "files_changed": ["test/test_nn.py", "torch/nn/functional.py", "torch/nn/modules/activation.py"], "labels": ["merged"]}, "ef94496b36": {"title": "[JIT] throw if no self arg on ignored methods (#32503)", "body": "Summary:\nThere was a user who did this and it would seg fault.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32503\n\nDifferential Revision: D19538481\n\nPulled By: eellison\n\nfbshipit-source-id: dc3752028b9eff6ac88c025e8a2b5f8fd44ce32f", "pr_number": "32503", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/python_sugared_value.cpp"], "labels": ["jit", "merged"]}, "d234626267": {"title": "[quant][graphmode] Support quantizing shared ClassType with different qconfigs (#32205)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32205\n\nto be filled\n\nTest Plan:\npython test_jit.py\n\nImported from OSS\n\nDifferential Revision: D19508031\n\nfbshipit-source-id: cbf03d34e52eae62595c34fde6ec645cb6744ad9", "pr_number": "32205", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "49cd83d735": {"title": "no more build_pytorch_libs.sh/.bat (#32319)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/12918\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32319\n\nDifferential Revision: D19544272\n\nPulled By: soumith\n\nfbshipit-source-id: dd32fa61efa78af908f21c7e54cb6484bf895e54", "pr_number": "32319", "files_changed": ["tools/README.md"], "labels": ["merged", "open source"]}, "ad4fba0ce4": {"title": "Only run test_conv_large and test_conv_transposed_large_cuda on 32GB device (#32473)", "body": "Summary:\nFor some reason, these two tests start to fail on 16GB Volta on Linux...\n\nAlso fixes https://github.com/pytorch/pytorch/issues/31650\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32473\n\nDifferential Revision: D19538314\n\nPulled By: ngimel\n\nfbshipit-source-id: 266195f19d8cf76b035795e0e318c152ae72adc2", "pr_number": "32473", "files_changed": ["test/test_nn.py"], "labels": ["merged", "open source"]}, "dbd29e5668": {"title": "[JIT] Passing custom class as arg (#32260)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32260\n\nThis makes it so you can actually pass the custom class as an arg to ScriptFunctions\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19424252\n\nPulled By: jamesr66a\n\nfbshipit-source-id: c3530186619655781dedbea03c2ad321aaff1cb8", "pr_number": "32260", "files_changed": ["test/test_jit.py", "torch/_ops.py", "torch/csrc/jit/custom_class.h", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/python_custom_class.cpp", "torch/csrc/jit/python_custom_class.h", "torch/csrc/jit/script/compilation_unit.h", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/script/python_sugared_value.cpp"], "labels": ["jit", "merged"]}, "7e14c420ae": {"title": "[JIT] Test __getstate__ and __setstate__ for custom bound C++ classes", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32470\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19508250\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 481299fb3c18fa874c2a1d2993984bb6b3193bac", "pr_number": "32470", "files_changed": ["test/cpp/jit/test_custom_class.cpp", "test/test_jit.py"], "labels": ["jit", "merged"]}, "ae42e232ce": {"title": "[JIT] Fix custom class method binding for const methods", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32471\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19508249\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 3a0bce6845072bb03567049a73b9982b54d8daf9", "pr_number": "32471", "files_changed": ["test/cpp/jit/test_custom_class.cpp", "torch/custom_class.h"], "labels": ["jit", "merged"]}, "69f9bf8893": {"title": "[JIT] Support returning tuple from custom bound C++ method", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32477\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19509927\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 7d407150402cc19344c3ec3b4a27b3d7c464e8ac", "pr_number": "32477", "files_changed": ["test/cpp/jit/test_custom_class.cpp", "test/test_jit.py", "torch/custom_class.h"], "labels": ["jit", "merged"]}, "8ed1dd528e": {"title": "[JIT] Add torch.classes.load_library", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32508\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19525175\n\nPulled By: jamesr66a\n\nfbshipit-source-id: b9f07113f551bdfb56d49d24d12989be2b8fc7e4", "pr_number": "32508", "files_changed": ["torch/_classes.py"], "labels": ["merged"]}, "6745bfc31c": {"title": "Revert \"Remove __torch__ from custom class qualname\" (#32514)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32514\n\nThis reverts commit c7fdf5b251c6fecd5d78b4f33d30bd77ca3f841c.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19525532\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 126f4e87250a2ac739bd7aa161a0f7b39f143d38", "pr_number": "32514", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "test/test_jit.py", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/python_custom_class.cpp", "torch/csrc/jit/script/schema_type_parser.cpp", "torch/custom_class.h"], "labels": ["jit", "merged"]}, "4cd6b5cda6": {"title": "[quant] Re-enable test_nested that has different qconfig for shared ClassType (#32206)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32206\n\natt\n\nTest Plan:\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D19508028\n\nfbshipit-source-id: 5de3c2ef17de146feca03d7135a7e04f393de398", "pr_number": "32206", "files_changed": ["test/test_quantization.py"], "labels": ["merged"]}, "d2f66083c5": {"title": "porting gather to ATen using TensorIterator with multithreading support. (#32425)", "body": "Summary:\nFixes [https://github.com/pytorch/pytorch/issues/24702](https://github.com/pytorch/pytorch/issues/24702).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32425\n\nDifferential Revision: D19538265\n\nPulled By: ngimel\n\nfbshipit-source-id: 78821a16b6948916e956a04f984e0956f86cf582", "pr_number": "32425", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/LegacyDefinitions.cpp", "aten/src/ATen/native/ScatterGatherShapeChecks.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/cpu/ScatterGatherKernel.cpp", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.h"], "labels": ["merged", "open source", "topic: porting"]}, "6f146e1768": {"title": "[JIT] Remove capsule type handling of node hashing (#32540)", "body": "Summary:\nCapsule Type doesn't appear in the IR, it is purely used in the runtime. So we should not have to handle it node hashing... Let's see if this breaks anything.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32540\n\nDifferential Revision: D19541357\n\nPulled By: eellison\n\nfbshipit-source-id: 905ed9f89cf6d03b45ddb4fde02adfa149b477f8", "pr_number": "32540", "files_changed": ["aten/src/ATen/core/jit_type.h", "torch/csrc/jit/node_hashing.cpp"], "labels": ["jit", "merged"]}, "ef2d4e67d1": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/08e28edc08dea3b96bc5eab84c10efecee580133\nhttps://github.com/facebook/folly/commit/6884ecfc6724b30f3f54899889f309f81650e125\nhttps://github.com/facebook/mcrouter/commit/685144514fc59139189b75f7a1c3387a992670e2\nhttps://github.com/pytorch/fbgemm/commit/ed665880aa9b017b04af40193a22bcc933ddabad\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 7b19dca06ad7e8751de21efc48f5eada37b446fb", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "b474c351dd": {"title": "[rpc] Remove template on RRef and add Type to RRef creation (#30630)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30630\n\nThis remove template and all the specializations it have in rpc, we\nuniversally use IValue as the inner value since we support making python\nobject to be hold inside IValue.\n\nThis will also ensure that we have the correct type information when\ncreating the RRef, we use the return type from the schema when creating\nuserRRef and OwnerRRef, it will enable IValue to always have the correct\ntype if the IValue is the RRef object (next PR)\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19502235\n\nfbshipit-source-id: 0d5decae8a9767e0893f3b8b6456b231653be3c5", "pr_number": "30630", "files_changed": ["torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h"], "labels": ["merged"]}, "3ada2e0d64": {"title": "[pytorch][embeddingbag] Parallelize the EmbeddingBag operator (#4049)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/glow/pull/4049\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27477\n\nWe would like to add the intra-op parallelization support for the EmbeddingBag operator.\n\nThis should bring speedup for the DLRM benchmark:\nhttps://github.com/pytorch/pytorch/pull/24385\n\nBenchmark code:\n```\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport torch\nimport time\n\neb = torch.nn.EmbeddingBag(1000000, 64, mode='sum')\n\ninput = torch.LongTensor(1500).random_(0, 1000000)\noffsets = torch.zeros(64, dtype=torch.int64)\n\nniter = 10000\ns = time.time()\nfor _ in range(niter):\n    out = eb(input, offsets)\ntime_per_iter = (time.time() - s) / niter\nprint('time_per_iter', time_per_iter)\nprint('GB/s', (input.numel() * 64 * 4 + out.numel() * 4) / time_per_iter / 1e9)\n```\n\nThe following results are single core on Skylake T6:\n- Before our change (with the original caffe2::EmbeddingLookup)\ntime_per_iter 6.313693523406982e-05\nGB/s 6.341517821789133\n\n- After our change using the EmbeddingLookupIdx API which takes the offsets instead of lengths.\ntime_per_iter 5.7627105712890626e-05\nGB/s 6.947841559053659\n\n- With Intel's PR: https://github.com/pytorch/pytorch/pull/24385\ntime_per_iter 7.393271923065185e-05\nGB/s 5.415518381664018\n\nFor multi-core performance, because Clang doesn't work with OMP, I can only see the single-core performance on SKL T6.\nghstack-source-id: 97124557\n\nTest Plan:\nWith D16990830:\n```\nbuck run mode/dev //caffe2/caffe2/perfkernels:embedding_bench\n```\n\nWith D17750961:\n```\nbuck run mode/opt //experimental/jianyuhuang/embeddingbag:eb\nbuck run mode/opt-lto //experimental/jianyuhuang/embeddingbag:eb\n```\n\nOSS test\n```\npython run_test.py -i nn -- TestNNDeviceTypeCPU.test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu\n```\n\nBuck test\n```\nbuck test mode/dev-nosan //caffe2/test:nn -- \"test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu\"\n\nOMP_NUM_THREADS=3 buck test mode/opt -c pytorch.parallel_backend=tbb //caffe2/test:nn -- \"test_EmbeddingBag_per_sample_weights_and_new_offsets\"  --print-passing-details\n```\n\nGenerate the AVX2 code for embedding_lookup_idx_avx2.cc:\n```\npython hp_emblookup_codegen.py --use-offsets\n```\n\nDifferential Revision: D17768404\n\nfbshipit-source-id: 8dcd15a62d75b737fa97e0eff17f347052675700", "pr_number": "27477", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/native_functions.yaml", "caffe2/perfkernels/embedding_lookup_idx.cc", "caffe2/perfkernels/embedding_lookup_idx.h", "caffe2/perfkernels/embedding_lookup_idx_avx2.cc", "caffe2/perfkernels/hp_emblookup_codegen.py", "test/onnx/expect/TestOperators.test_embedding_bags.expect", "test/test_nn.py", "tools/autograd/derivatives.yaml", "torch/csrc/api/include/torch/nn/functional/embedding.h", "torch/csrc/api/include/torch/nn/options/embedding.h", "torch/csrc/api/src/nn/modules/embedding.cpp", "torch/nn/functional.py", "torch/nn/modules/sparse.py", "torch/nn/modules/sparse.pyi.in", "torch/onnx/symbolic_opset9.py"], "labels": ["caffe2", "merged", "module: operators"]}, "8fd3eaed25": {"title": "[jit] Fix dict type serialization (#32569)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32569\n\nIf the dict's contained types cannot be inferred from its contents (for\nexample, `Dict[str, Tensor]` vs. `Dict[str, Optional[Tensor]]`), we must\nexplicitly annotate the type.\n\nAlso this removes some special handling that omits annotations on empty\ncontainers that have the default type. It makes the code more complex\nfor not too much value, and was wrong for dicts anyway.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19551016\n\nPulled By: suo\n\nfbshipit-source-id: c529b112e72c10f509a6bc0f5876644caa1be967", "pr_number": "32569", "files_changed": ["test/test_jit_py3.py", "torch/csrc/jit/passes/python_print.cpp"], "labels": ["jit", "merged"]}, "f0c85571ed": {"title": "docker: Refactor Dockerfile process for official images (#32515)", "body": "Summary:\n## Commit Message:\n\nRefactors Dockerfile to be as parallel as possible with caching and adds a new Makefile to build said Dockerfile.\n\nAlso updated the README.md to reflect the changes as well as updated some of the verbage around running our latest Docker images.\n\nAdds the new Dockerfile process to our CircleCI workflows\n\n## How to build:\n\nBuilding the new images is pretty simple, just requires `docker` > 18.06 since the new build process relies on `buildkit` caching and multi-stage build resolving.\n\n### Development images\nFor `runtime` images:\n```\nmake -f docker.Makefile runtime-image\n```\n\nFor `devel` images:\n```\nmake -f docker.Makefile devel-image\n```\n\nBuilds are tagged as follows:\n```bash\ndocker.io/${docker_user:-whoami}/pytorch:$(git describe --tags)-${image_type}\n```\n\nExample:\n```\ndocker.io/seemethere/pytorch:v1.4.0a0-2225-g9eba97b61d-runtime\n```\n\n### Official images\n\nOfficial images are the ones hosted on [`docker.io/pytorch/pytorch`](https://hub.docker.com/r/pytorch/pytorch)\n\nTo do official images builds you can simply add set the `BUILD_TYPE` variable to `official` and it will do the correct build without building the local binaries:\n\nExample:\n```\nmake -f docker.Makefile BUILD_TYPE=official runtime-image\n```\n\n## How to push:\n\nPushing is also super simple (And will automatically tag the right thing based off of the git tag):\n\n```\nmake -f docker.Makefile runtime-push\nmake -f docker.Makefile devel-push\n```\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32515\n\nDifferential Revision: D19558619\n\nPulled By: seemethere\n\nfbshipit-source-id: a06b25cd39ae9890751a60f8f36739ad6ab9ac99", "pr_number": "32515", "files_changed": ["Dockerfile", "README.md", "docker.Makefile"], "labels": ["merged"]}, "7d0f0b62de": {"title": "API for testing bailouts (#32518)", "body": "Summary:\nThis API seems to be quite useful to make sure all bailouts in a graph are triggered. I used it for testing torchvision models and I was wondering if this might be something we might actually want to have? zdevito\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32518\n\nDifferential Revision: D19553147\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 7542c99051588b622091aec6d041c70731ca5d26", "pr_number": "32518", "files_changed": ["test/test_jit.py", "torch/csrc/jit/init.cpp", "torch/csrc/jit/instruction.h", "torch/csrc/jit/interpreter.cpp", "torch/csrc/jit/interpreter.h"], "labels": ["jit", "merged"]}, "ef5637f85e": {"title": "[jit] allow compilation using optional modules (#32539)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32539\n\nBefore: if something in `_modules` was `None`, we would barf. This is\nincorrect because it's allowed for users to put `None` there, in case a\nmodule is optional.\n\nThis case ought to be handled correctly during scripting. Fixes https://github.com/pytorch/pytorch/issues/32469\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19552346\n\nPulled By: suo\n\nfbshipit-source-id: aba7fdc19fd84d195c81cdaca8a75013a8626a8b", "pr_number": "32539", "files_changed": ["test/jit/test_recursive_script.py", "torch/csrc/jit/python_ir.cpp", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "fd1a4f18ee": {"title": "[pytorch] update code analyzer build.sh to handle srcs with same name (#32525)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32525\n\nBefore calling static code analyzer we need link all bitcode files into\na single module. Current approach is a bit hacky: cmake still calls \"ar\"\nto pack bitcode files into archives, then we manually unpack these\narchives and call llvm-link.\n\nTurns out libtorch_cpu.a contains a few files with same name, e.g.:\n```\naten/src/ATen/native/SoftMax.cpp\naten/src/ATen/native/mkldnn/SoftMax.cpp\n```\n\n\"ar x\" will only keep one of them and cause inaccurate analysis result.\n\nUse this temporary hack to workaround the problem. Ideally should merge\nthis step into cmake (e.g. directly calling llvm-link to produce target\noutput?).\n\nDifferential Revision: D19530533\n\nPulled By: ljk53\n\nfbshipit-source-id: 94b292c241abaaa0ff4a23059882abdc3522971e", "pr_number": "32525", "files_changed": ["tools/code_analyzer/build.sh"], "labels": ["merged"]}, "fe3eb09da5": {"title": "[quant] Re-enable fold_convbn in quantize_script (#32302)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32302\n\natt\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19508035\n\nfbshipit-source-id: 2ac26585396ec8a115acd0e1d7ccb84098a76824", "pr_number": "32302", "files_changed": ["torch/quantization/_quantize_script.py"], "labels": ["merged"]}, "d2bda53f6d": {"title": "[quant][graphmode] Call _jit_pass_dedup_module_ueses in quantize_script (#32303)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32303\n\natt\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19508029\n\nfbshipit-source-id: 468ed53fc8bb3c8fdf5d79aea186949e64be711a", "pr_number": "32303", "files_changed": ["torch/quantization/_quantize_script.py"], "labels": ["merged"]}, "9e59244b53": {"title": "fix view listing in autograd codegen (#32044)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32044\n\nFix the list of views in the codegen:\n- Move `narrow` out of the autograd functions since it's now implemented with slice.\n- Add `split_with_sizes` that was missing from the list\n- Remove special formulas for both `split` and `split_with_sizes`. Both used not to be considered as views. When they are, all the rnn code breaks because it uses them in an invalid way. The generic formula will generate one `narrow` Node for each output. Which is always valid.\n\nThe diff for the generated code can be found [here](https://github.com/pytorch/pytorch/compare/16eff6e...albanD:06d6e85) (outdated for last commit)\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19409648\n\nPulled By: albanD\n\nfbshipit-source-id: 5ebc4c978af500403f7f008c0231b7db0cabab26", "pr_number": "32044", "files_changed": ["aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_autograd.py", "torch/csrc/autograd/variable.cpp"], "labels": ["merged"]}, "3ab30753e9": {"title": "Make autogen functions correct for multiple outputs and views (#31990)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31990\n\nThis PR does three things:\n- Add a new `allow_rebase_history` flag to the differentiable views. If set, trying to rebase their history will raise an error.\n- Make sure that the codegen functions verify this flag before doing inplace operations so that they fail before doing the inplace modification.\n- Make sure the codegen functions set this flag properly when we don't support rebasing the history of the output.\n\nThe codegen change can be found [here](https://github.com/pytorch/pytorch/commit/4bf180caa0e655111b036c8b0419da89fc75671b).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19409649\n\nPulled By: albanD\n\nfbshipit-source-id: a2b41c2d231e952ecfe162bdb6bad620ac595703", "pr_number": "31990", "files_changed": ["test/cpp/api/init.cpp", "test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h"], "labels": ["merged"]}, "573a30270c": {"title": "[pytorch] Minor: boilerplate to propagate errors in request_callback_impl (#32556)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32556\n\nOut of caution, avoid assuming that there's never a failure in a couple of\nrequest_calback_impl case handlers, but rather propagate the error.\nghstack-source-id: 97128697\n\nTest Plan: buck test mode/dev-nosan caffe2/test/...\n\nDifferential Revision: D19544685\n\nfbshipit-source-id: 67c55626960bd42a5b0dec7841e8ba44ab059eb9", "pr_number": "32556", "files_changed": ["torch/csrc/distributed/rpc/request_callback_impl.cpp"], "labels": ["merged"]}, "2bfd33b4ab": {"title": "[refactor] Adding FoldConvBatchNorm2dHelper (#32374)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32374\n\nMoving all fold conv bn code to a class to prepare for making\nit work with shared ClassType\n\nTest Plan:\ncompiles\n\nImported from OSS\n\nDifferential Revision: D19508032\n\nfbshipit-source-id: 4e9cf714111305d2b5474d4506507078f69f0c84", "pr_number": "32374", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "9e0ce72e9e": {"title": "[pytorch] change op dependency output to use double-quoted strings (#32464)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32464\n\nChanged to double quoted strings to make FB linter happy.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19507859\n\nPulled By: ljk53\n\nfbshipit-source-id: fa70535c7fbea73214b3b0efb0532184b5ee6854", "pr_number": "32464", "files_changed": ["tools/code_analyzer/op_dependency.cpp"], "labels": ["merged"]}, "52f8f031ac": {"title": "add diag into pt operator microbenchmark (#32597)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32597\n\nCurrently, there is no benchmark test about diag operator. This diff will add one into the suite.\n\nTest Plan:\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: diag\n# Mode: Eager\n# Name: diag_dim1_M64_N64_diagonal0_outTrue_cpu\n# Input: dim: 1, M: 64, N: 64, diagonal: 0, out: True, device: cpu\nForward Execution Time (us) : 28.496\n\n# Benchmarking PyTorch: diag\n# Mode: Eager\n# Name: diag_dim2_M128_N128_diagonal-10_outFalse_cpu\n# Input: dim: 2, M: 128, N: 128, diagonal: -10, out: False, device: cpu\nForward Execution Time (us) : 45.179\n\n# Benchmarking PyTorch: diag\n# Mode: Eager\n# Name: diag_dim1_M256_N256_diagonal20_outTrue_cpu\n# Input: dim: 1, M: 256, N: 256, diagonal: 20, out: True, device: cpu\nForward Execution Time (us) : 49.009\n```\n\nReviewed By: mingzhe09088\n\nDifferential Revision: D19564024\n\nfbshipit-source-id: 828a3e0e0e06810a77eb5ddb734efd30e4a63acf", "pr_number": "32597", "files_changed": ["benchmarks/operator_benchmark/benchmark_all_other_test.py", "benchmarks/operator_benchmark/pt/diag_test.py"], "labels": ["fb-exported", "merged"]}, "91f10a1de1": {"title": "[quant][graphmode][refactor] Better API for fold_convbn (#32380)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32380\n\nWe'll clone the module first and then fold conv bn and return a new\nmodule\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19508033\n\nfbshipit-source-id: 328e91a2c9420761c904a7f2b62dab4cfaaa31ac", "pr_number": "32380", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization.h", "torch/quantization/_quantize_script.py"], "labels": ["jit", "merged"]}, "6412ca3ce9": {"title": "duplicate symbols with AT_PARALLEL_OPENMP=0 (#32568)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32568\n\nexplicitly disabling openmp actually causes it to be used.\n\nTest Plan: CI passes\n\nReviewed By: ilia-cher\n\nDifferential Revision: D19549732\n\nfbshipit-source-id: 767b92148f47a1450ded46e101cd3d9b331a5d40", "pr_number": "32568", "files_changed": ["aten/src/ATen/ParallelOpenMP.cpp"], "labels": ["fb-exported", "merged"]}, "666472a38d": {"title": "[docs] Change fut.wait() to torch.jit._wait(fut) in jit overview docs (#32336)", "body": "Summary:\nIt looks like the jit Future does not have a `wait()` anymore and this throws an error when trying to run this code.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32336\n\nDifferential Revision: D19559922\n\nPulled By: rohan-varma\n\nfbshipit-source-id: a5aa67990595e98e0682a20cf5aced17c2ae85bb", "pr_number": "32336", "files_changed": ["torch/csrc/jit/docs/OVERVIEW.md"], "labels": ["jit", "merged"]}, "e7edc5f20e": {"title": "[jit] Cloning constants in ClassType (#32371)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32371\n\nAfter we add constants to ClassType, we didn't update clone to\nclone the constants, this PR adds the support\nfixes: https://github.com/pytorch/pytorch/issues/32368\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D19564378\n\nfbshipit-source-id: dbb13fb889d6ea9291034313b1f3c9aff4748bda", "pr_number": "32371", "files_changed": ["torch/csrc/jit/script/module.cpp"], "labels": ["jit", "merged"]}, "1218a16aae": {"title": "[pytorch][refactor] Explicitly use auto* for pointers (#32548)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32548\n\nAs Title says.\nghstack-source-id: 97175523\n\nTest Plan: CI\n\nDifferential Revision: D19541893\n\nfbshipit-source-id: 96dce6964e6a89393d4159401a59672f041f51d3", "pr_number": "32548", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp"], "labels": ["merged"]}, "59dbece371": {"title": "Fix iterator for ncclCommWatchdog. (#32571)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32571\n\nThe watchdog thread would erase an element and call `it--` (implicitly\nrelying on `it++` in the for loop to position correctly). Although, `it--`\nwould cause undefined behavior if the iterator is pointing to begin(). As a\nresult, I've modified the logic to update the iterator appropriately.\n\nI've also enhanced the watchdog thread to catch and log exceptions.\nghstack-source-id: 97150763\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19551365\n\nfbshipit-source-id: 426835819ad8d467bccf5846b04d14442a342f78", "pr_number": "32571", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["merged"]}, "169541871a": {"title": "Add operator support for dynamic quant on mobile (#32479)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32479\n\nRun dynamic quantization on mobile (similar to FBGEMM). Currently only implemented on linear operator\n\nTest Plan:\npython test/test_quantized.py TestDynamicQuantizedLinear.test_qlinear\n\nImported from OSS\n\nDifferential Revision: D19542980\n\nfbshipit-source-id: c9f6e5e8ded4d62ae0f2ed99e478c8307dde22ed", "pr_number": "32479", "files_changed": ["aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt", "aten/src/ATen/native/quantized/cpu/qnnpack/include/qnnpack_func.h", "aten/src/ATen/native/quantized/cpu/qnnpack/src/fc-dynamic-run.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/src/init.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/qnnpack/params.h", "aten/src/ATen/native/quantized/cpu/quant_utils.h", "test/test_quantized.py", "torch/testing/_internal/common_quantized.py"], "labels": ["merged"]}, "e0ffe72649": {"title": "[aten] fix shadowing variable warning (#32573)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32573\n\nFix the following warning\n```\ncaffe2/aten/src/ATen/ParallelOpenMP.h:36:9: warning: declaration of \u2018num_threads\u2019 shadows a previous local [-Wshadow=compatible-local]\n     int64_t num_threads = omp_get_num_threads();\n         ^~~~~~~~~~~\ncaffe2/aten/src/ATen/ParallelOpenMP.h:29:9: note: shadowed declaration is here\n   int64_t num_threads = omp_in_parallel() ? 1 : omp_get_max_threads();\n         ^~~~~~~~~~~\n```\n\nTest Plan: CI\n\nReviewed By: ilia-cher\n\nDifferential Revision: D19552578\n\nfbshipit-source-id: b8388de1aaa2bb7676b777c93b8ba9c25f5a3d51", "pr_number": "32573", "files_changed": ["aten/src/ATen/ParallelOpenMP.h"], "labels": ["fb-exported", "merged"]}, "6ad9e5c70d": {"title": "Support TorchScript call over remote API (RRef) (#32466)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32466\n\nIt's a follow-up work of https://github.com/pytorch/pytorch/pull/32197.\n\nIn https://github.com/pytorch/pytorch/pull/32197, `rpc.sync_rpc(..) `and `rpc.rpc_async(..)` support taking a TorchScript annotated Python function as the user function for RPC.\n\nThis PR extend along this direction by making `rpc.remote(..)` support taking a TorchScript annotated Python function as well.\n\nghstack-source-id: 97211168\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork -- test_script_function_exception\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork\n\nbuck-out/gen/caffe2/test/distributed/rpc/rpc_fork\\#binary.par -r test_script_function_exception\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:dist_autograd_fork -- test_backward_simple_script_call\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:dist_autograd_fork\n\nbuck-out/gen/caffe2/test/distributed/rpc/dist_autograd_fork\\#binary.par -r test_backward_simple_script_call\n```\n\nDifferential Revision: D19440633\n\nfbshipit-source-id: d37f6dcdc0b80d35ac7bcba46ad6f9b831c3779b", "pr_number": "32466", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/script_call.cpp", "torch/csrc/distributed/rpc/script_call.h", "torch/csrc/distributed/rpc/script_functions.cpp", "torch/csrc/distributed/rpc/script_functions.h", "torch/csrc/distributed/rpc/script_remote_call.cpp", "torch/csrc/distributed/rpc/script_remote_call.h", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "bd20274e8f": {"title": "[caffe2] use JIT'ed fp32 SLS (#32413)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32413\n\nUse JIT'ed fp32 SLS in Caffe2 operators\n\nTest Plan: CI\n\nReviewed By: jianyuh\n\nDifferential Revision: D19460555\n\nfbshipit-source-id: 4f29d34523efb6ea1e4c324cc8c93c96990c6aad", "pr_number": "32413", "files_changed": ["caffe2/operators/lengths_reducer_ops.h"], "labels": ["fb-exported", "merged"]}, "0afe195046": {"title": "[pytorch] move type_derived_methods out of anonymous namespace (#32275)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32275\n\nCurrently TypeDerived (e.g. `CPUType::`) methods are declared and\ndefined in anonymous namespace as they are only called from c10\ndispatcher - except for STATIC_DISPATCH mode, where they can be directly\ncalled from Functions.h.\n\nWe plan to generate c10 op registration into separate files for internal\nxplat/BUCK build, thus we need declare these methods in non-anonymous\nnamespace.\n\nI feel it's easier to simply change it unconditionally, unless there are\nsome side effect I'm not aware of - `TypeDefault::` methods are in\nnon-anonymous namespace anyway.\nghstack-source-id: 97214789\n\nTest Plan: - CI\n\nDifferential Revision: D19426692\n\nPulled By: ljk53\n\nfbshipit-source-id: 44aebba15f5e88ef4acfb623844f61d735016959", "pr_number": "32275", "files_changed": ["aten/src/ATen/templates/SparseTypeDerived.cpp", "aten/src/ATen/templates/TypeDerived.cpp", "aten/src/ATen/templates/TypeDerived.h"], "labels": ["merged"]}, "69283388ca": {"title": "[pytorch] codegen flags to whitelist op registrations / generate to separate files (#32451)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32451\n\nThis PR adds a few new parameters to ATen codegen script:\n\n```\n1. op_registration_whitelist\nCan be used to filter op registrations for selective build;\n\n2. type_whitelist\nCan be used to filter types (CPUType, CUDAType, ...) for selective build;\n\n3. per_op_registration\nWhen set it will group function registrations by op name and write to separate files;\n```\n\n1 & 2 are introduced for mobile custom build without relying on static dispatch;\n3 is introduced to solve custom build with multi-library / multi-model (needed by FB\ninternal build - see more details: https://fb.quip.com/ZVh1AgOKW8Vv).\n\nThese flags should work independently with each other (and independent to USE_STATIC_DISPATCH).\n\nNot setting them should have no effect compared to master.\nghstack-source-id: 97214788\n\nTest Plan: - tested all 3 params with FB internal build changes.\n\nDifferential Revision: D19427919\n\nfbshipit-source-id: a381fe5f768fe2e9196563787f08eb9f18316e83", "pr_number": "32451", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen.py", "aten/src/ATen/templates/PerOpRegistration.cpp"], "labels": ["merged"]}, "320d1a1573": {"title": "Fix wrong typing (torch/nn/parameter.pyi) (#32617)", "body": "Summary:\nA constructor of `nn.Parameter` has default values on `data` and `requires_grad`, but in type stub, there are no default values.\n\nResolve https://github.com/pytorch/pytorch/issues/32481\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32617\n\nDifferential Revision: D19571397\n\nPulled By: ngimel\n\nfbshipit-source-id: fd14298aa472b7575221229cecf5a56f8c84f531", "pr_number": "32617", "files_changed": ["torch/nn/parameter.pyi"], "labels": ["merged", "open source"]}, "5fd037ce44": {"title": "Fix MagmaInitializesCorrectly_CUDA by using an invertible matrix (#32547)", "body": "Summary:\nThis test case had been using the tensor\n\n```\n1  2  3  4\n5  6  7  8\n9  10 11 12\n13 14 15 16\n```\n\nwhich is not an invertible tensor and causes the test case to fail, even if magma gets initialized just fine. This change uses a tensor that is invertible, and the inverse doesn't include any elements that are close to zero to avoid floating point rounding errors.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32547\n\nDifferential Revision: D19572316\n\nPulled By: ngimel\n\nfbshipit-source-id: 1baf3f8601b2ba69fdd6678d7a3d86772d01edbe", "pr_number": "32547", "files_changed": ["test/cpp/api/tensor_cuda.cpp"], "labels": ["merged", "open source"]}, "3bbb36e02d": {"title": "Update linspace types (#32218)", "body": "Summary:\nChanges the linspace functions to be more consistent as requested in https://github.com/pytorch/pytorch/issues/31991. The code has also been updated to avoid an early rounding error; the line `scalar_t step = (scalar_end - scalar_start) / static_cast<static_t>(steps-1)` can result in `step = 0` for integer scalars, and this gives unintended results. I examined the new output using\n```\nimport torch\n\ntypes = [torch.uint8, torch.int8, torch.short, torch.int, torch.long, torch.half, torch.float, torch.double]\n\nprint('Testing linspace:')\nfor type in types:\n    print(type, torch.linspace(-2, 2, 10, dtype=type))\n```\nwhich returns\n```\nTesting linspace:\ntorch.uint8 tensor([254, 254, 254, 255, 255,   0,   0,   1,   1,   2], dtype=torch.uint8)\ntorch.int8 tensor([-2, -2, -2, -1, -1,  0,  0,  1,  1,  2], dtype=torch.int8)\ntorch.int16 tensor([-2, -2, -2, -1, -1,  0,  0,  1,  1,  2], dtype=torch.int16)\ntorch.int32 tensor([-2, -2, -2, -1, -1,  0,  0,  1,  1,  2], dtype=torch.int32)\ntorch.int64 tensor([-2, -2, -2, -1, -1,  0,  0,  1,  1,  2])\ntorch.float16 tensor([-2.0000, -1.5557, -1.1113, -0.6670, -0.2227,  0.2227,  0.6660,  1.1113,\n         1.5547,  2.0000], dtype=torch.float16)\ntorch.float32 tensor([-2.0000, -1.5556, -1.1111, -0.6667, -0.2222,  0.2222,  0.6667,  1.1111,\n         1.5556,  2.0000])\ntorch.float64 tensor([-2.0000, -1.5556, -1.1111, -0.6667, -0.2222,  0.2222,  0.6667,  1.1111,\n         1.5556,  2.0000], dtype=torch.float64)\n```\nwhich is the expected output: `uint8` overflows as it should, and the result of casting from a floating point to an integer is correct.\n\nThis PR does not change the logspace function.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32218\n\nDifferential Revision: D19544224\n\nPulled By: ngimel\n\nfbshipit-source-id: 2bbf2b8552900eaef2dcc41b6464fc39bec22e0b", "pr_number": "32218", "files_changed": ["aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/cuda/RangeFactories.cu", "test/test_torch.py"], "labels": ["merged", "open source"]}, "90a259e1e2": {"title": "Add warning regarding pickle insecurity on torch.load documentation (#32593)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31875\n\nAdded a small warning box based on the one presented on the [pickle](https://docs.python.org/3/library/pickle.html) module regarding the safety issues of unpickling files. i.e., unwanted code execution.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32593\n\nDifferential Revision: D19572292\n\nPulled By: ngimel\n\nfbshipit-source-id: 69e7de390133ea77bddcadcd5b6820193c8abcc9", "pr_number": "32593", "files_changed": ["torch/serialization.py"], "labels": ["merged", "module: docs", "open source"]}, "12d5933969": {"title": "Bug fix of norm minimization for dev mode (#31462)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31462\n\nFix the divide by zero issue in norm minimization in dev mode\n\nTest Plan: buck run mode/dev vision/video_modeling/classification/tools:test_octGloRe_quantization -- --test_data=/mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/xray_video/deep_vision_video_yufei_test_data_fcc_v4p2_10.csv --output_dir /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/xray_video/octGloRe --load_model_path=/mnt/vol/gfsfblearner-oregon/flow/data/2019-10-15/e2681db8-e4f5-4b70-ae18-45bf0b8fbfbc/train_model_epoch0_inputcount0_final.mdl --dataset_name=\"FCC V4P2\" --num_labels=1099 --column_handle=\"handle\" --clip_per_video=1 --num_groups=24 --width_per_group=2 --batch_size=32 --histogram_file=/mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/xray_video/octGloRe/hist_octGloRe_final_24x2_fcc_v4p2_1clip_f144586257_nullfix_100k_compiled.hist --int8_model_type=\"pb\"  --int8_predict_net_path=\"reproduce_octGloRe_final_24x2_predict_net_int8_l2approx_wminmax_from_mdl.pb\" --int8_init_net_path=\"reproduce_octGloRe_final_24x2_init_net_int8_l2approx_wminmax_from_mdl.pb\" --weight_quant=\"l2_approx\" --activation_quant=\"l2_approx\"  --print_model --int8_model_saved --num_iter 10\n\nReviewed By: jspark1105\n\nDifferential Revision: D19172591\n\nfbshipit-source-id: 994a20e3364b0dc33623a11281e0bdbc2e06159d", "pr_number": "31462", "files_changed": ["caffe2/quantization/server/norm_minimization.cc"], "labels": ["fb-exported", "merged"]}, "8fbe1ccd16": {"title": "faster bailout tests (#32266)", "body": "Summary:\nReduces the overhead of `prim::BailOut` nodes.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32266\n\nDifferential Revision: D19503336\n\nPulled By: Krovatkin\n\nfbshipit-source-id: daa0c373f0fa17edd689600b75e7e4ba98b4670a", "pr_number": "32266", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "torch/csrc/jit/interpreter.cpp"], "labels": ["jit", "merged"]}, "1e5aead35b": {"title": "Make cuda search process of cpp extension quiet (#32620)", "body": "Summary:\nFixes https://discuss.pytorch.org/t/error-with-cpp-extentions/67559.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32620\n\nDifferential Revision: D19576164\n\nPulled By: soumith\n\nfbshipit-source-id: 076229322375774bec03ef2632fc233000c15391", "pr_number": "32620", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["merged", "open source", "triaged"]}, "5b321a0985": {"title": "[rpc] make handling of FORWARD_AUTOGRAD_REQ in request_callback_impl (#32476)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32476\n\nThis makes the handling of FORWARD_AUTOGRAD_REQ in request_callback\nnonblocking. Processing this message requires unwrapping the message with\nautograd information, processing the original message, and sending back the\nmessage with autograd information wrapped. This makes the processing the\noriginal message nonblocking by grabbing a future to it and marking the parent\nfuture as completed when this one completes.\nghstack-source-id: 97221251\n\nTest Plan: `test_rpc_spawn.py` and `test_dist_autograd_spawn.py` both pass.\n\nDifferential Revision: D19509501\n\nfbshipit-source-id: 84ad2f9c5305ed11ed9bb0144b1aaf5f8698cd2b", "pr_number": "32476", "files_changed": ["torch/csrc/distributed/rpc/request_callback_impl.cpp"], "labels": ["merged"]}, "957a07ffbd": {"title": "[ROCm] Enable Caffe2 video operators for ROCm", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32610\n\nDifferential Revision: D19580129\n\nPulled By: ezyang\n\nfbshipit-source-id: 16d620173dcc231068e041d599aa09c94e677a9e", "pr_number": "32610", "files_changed": ["caffe2/video/CMakeLists.txt"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "19bb496a0d": {"title": "Enable mkldnn on windows (#31355)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/15982.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31355\n\nDifferential Revision: D19428979\n\nPulled By: ezyang\n\nfbshipit-source-id: bee304c5913e70e8dead3098e9796051861cd666", "pr_number": "31355", "files_changed": ["caffe2/ideep/operators/momentum_sgd_op.cc", "caffe2/ideep/operators/operator_fallback_ideep.h", "cmake/Modules/FindMKLDNN.cmake", "cmake/Summary.cmake", "third_party/ideep"], "labels": ["merge-this-please", "merged", "open source", "triaged"]}, "602394e996": {"title": "verify input sizes for instance norm and group norm (#29082)", "body": "Summary:\nFix for https://github.com/pytorch/pytorch/issues/19250\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29082\n\nDifferential Revision: D19373507\n\nPulled By: ezyang\n\nfbshipit-source-id: 231a79280f4cd7db2c26218a60869356a124bf77", "pr_number": "29082", "files_changed": ["test/test_autograd.py", "test/test_nn.py", "torch/nn/functional.py", "torch/nn/functional.pyi.in"], "labels": ["merged", "open source", "triaged"]}, "ca9dc67094": {"title": "0-dim batch size input for interpolate. (#32400)", "body": "Summary:\nThis PR adds support for 0-dim batch size input for `torch.nn.functional.interpolate` for various modes of interpolation.\n\nFixes part of gh-12013\n\nCC: rgommers  ezyang\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32400\n\nDifferential Revision: D19557090\n\nPulled By: ezyang\n\nfbshipit-source-id: 6822f148bb47bfbcacb5e03798bf2744f24a2a32", "pr_number": "32400", "files_changed": ["aten/src/ATen/native/UpSample.h", "aten/src/ATen/native/cuda/UpSample.cuh", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleNearest1d.cu", "aten/src/ATen/native/cuda/UpSampleNearest2d.cu", "aten/src/ATen/native/cuda/UpSampleNearest3d.cu", "torch/testing/_internal/common_nn.py"], "labels": ["merged", "open source"]}, "5ac2593d4f": {"title": "[ROCm] Adjust elementwise_kernel settings on ROCm (#32609)", "body": "Summary:\nRecent PR https://github.com/pytorch/pytorch/issues/31974 and upcoming PR https://github.com/pytorch/pytorch/issues/32383 are changing the behavior of the elementwise_kernel infrastructure on CUDA.\n\nIn order to stay in sync, change the nd-loop behavior to match ROCm and CUDA for now. Once the full rework is done, the ROCm settings will likely diverge again.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32609\n\nDifferential Revision: D19580121\n\nPulled By: ezyang\n\nfbshipit-source-id: 4c8dcf6db3ac973e48ece6a665615cfe7d7cb764", "pr_number": "32609", "files_changed": ["aten/src/ATen/native/cuda/Loops.cuh"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "e36cbb8f2f": {"title": "Fixes moving after weight norm application (#32563)", "body": "Summary:\nThis PR updates how RNNs handle their \"flat weights.\" In particular, it allows for only some flat weights to be \"materialized\" when apply is called, and it updates the flattening behavior to only apply if all flat weights are (1) materialized, (2) share a dtype and (3) are acceptable to cuDNN.\n\nOne test is modified and another created to test these changes. One practical effect of this change is that weight norm can be successfully applied to a module BEFORE that module is moved to an accelerator. Previously doing so would throw an error.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32563\n\nDifferential Revision: D19562258\n\nPulled By: mruberry\n\nfbshipit-source-id: 4fef006e32cdfd8e3e3d519fc2ab5fc203dd7b36", "pr_number": "32563", "files_changed": ["test/test_nn.py", "torch/nn/modules/rnn.py"], "labels": ["merged"]}, "64323ae177": {"title": "Back out \"Use simd version for fp16 conversions\" (#32640)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32640\n\nOriginal commit changeset: 3b1ee0ba756e\n\nReverting according to https://our.intern.facebook.com/intern/diff/D19291499/?transaction_id=1347995678706116&dest_fbid=465672071047258\n\nTest Plan: unittests.\n\nReviewed By: jspark1105, jianyuh\n\nDifferential Revision: D19576708\n\nfbshipit-source-id: bec92318523498067935234ab702c925ece71da6", "pr_number": "32640", "files_changed": ["caffe2/operators/half_float_ops.cc"], "labels": ["fb-exported", "merged"]}, "f0917dce7f": {"title": "Revert D19562258: [pytorch][PR] Fixes moving after weight norm application", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19562258\n\nOriginal commit changeset: 4fef006e32cd\n\nfbshipit-source-id: 62e40de19331a61f4a65b7371460fe7dc28f23ea", "pr_number": null, "files_changed": ["test/test_nn.py", "torch/nn/modules/rnn.py"], "labels": []}, "f6c46df856": {"title": "Adding native qconcat", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32252\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19422889\n\nPulled By: z-a-f\n\nfbshipit-source-id: 23dd5f50009cc4c46b36c39ae1168b57f9a977a4", "pr_number": "32252", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/qconcat.cpp"], "labels": ["merged"]}, "897b6908d4": {"title": "Kill THIntegerTensor, THDenseTensor, THDenseIndexTensor. (#32599)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32599\n\nthese aren't used anymore.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19565655\n\nPulled By: gchanan\n\nfbshipit-source-id: c0da31365df7342352f9850ae2a2e1e611a6886b", "pr_number": "32599", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/THNN/THNN.h"], "labels": ["merged"]}, "57519bd829": {"title": "Revert \"Fix iterator for ncclCommWatchdog. (#32571)\" (#32649)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32649\n\nThis reverts commit 59dbece3716775c3e6f3a428f73fbf1bde8fac4f.\n\nRevert \"Enhance NCCL watchdog to acitvely abort communicators for timed out ops. (#32338)\"\n\nThis reverts commit f86d6c6afd0e981642d20b4269837334ec46c140.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19584224\n\nPulled By: ezyang\n\nfbshipit-source-id: 6cc0ad56ba1f3aec5b48db44e8c6c24c8105db4a", "pr_number": "32649", "files_changed": ["test/distributed/test_c10d.py", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp"], "labels": ["merged"]}, "389b9c180b": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/9ae8cbb0a1ee7ec2ec6f520ca9cc3c4e884daa4c\nhttps://github.com/facebook/rocksdb/commit/986df37135214d91e543f43a14d32a228b8d6fbb\nhttps://github.com/pytorch/fbgemm/commit/ef4d11b6e125dd09f5d69f6fc999454a516dab6e\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 04e7a5ad02cb412ef36672ec30e10a898c525232", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "812b1ad869": {"title": "[quantization] FP16 dynamic quantized Linear", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32331\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19441158\n\nPulled By: jamesr66a\n\nfbshipit-source-id: c04247ffe707be68718c486c31bc6c6040f7dc11", "pr_number": "32331", "files_changed": ["aten/src/ATen/native/quantized/cpu/fbgemm_utils.h", "aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_unpack.cpp", "test/test_quantization.py", "torch/nn/intrinsic/quantized/modules/linear_relu.py", "torch/nn/quantized/dynamic/modules/linear.py", "torch/nn/quantized/modules/linear.py", "torch/quantization/quantize.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged"]}, "39987de9e4": {"title": "[vulkan][caffe2] Add logging for descriptor extensions, fp16 storage", "body": "Summary:\n`fbcode/caffe2/caffe2/mobile/contrib/libvulkan-stub/BUCK` changes comment:\n\nlibvulkan-stub contains vulkan headers `VK_HEADER_VERSION 29`\n\nfbandroid uses ndk r17 that includes vulkan `VK_HEADER_VERSION 76`\nwhich contains defines for extensions that we need.\n\n`(\"include\", \"**/*.h\"),` -> `(\"include\", \"*.h\"),` means that ndk vulkan headers to use.\n\nFor fp16_storage logging need to add boilerplate for `vkGetPhysicalDeviceFeatures2KHR`\n\nTest Plan:\nscuba employees device_event\n\nlogcat getVulkanInfo().\n```\ninstance ext.name:VK_KHR_surface\ninstance ext.name:VK_KHR_android_surface\ninstance ext.name:VK_EXT_swapchain_colorspace\ninstance ext.name:VK_KHR_get_surface_capabilities2\ninstance ext.name:VK_EXT_debug_report\ninstance ext.name:VK_KHR_device_group_creation\ninstance ext.name:VK_KHR_external_fence_capabilities\ninstance ext.name:VK_KHR_external_memory_capabilities\ninstance ext.name:VK_KHR_get_physical_device_properties2\ninstance ext.name:VK_KHR_external_semaphore_capabilities\ndevice ext.name:VK_KHR_incremental_present\ndevice ext.name:VK_EXT_hdr_metadata\ndevice ext.name:VK_KHR_shared_presentable_image\ndevice ext.name:VK_GOOGLE_display_timing\ndevice ext.name:VK_KHR_push_descriptor\ndevice ext.name:VK_KHR_image_format_list\ndevice ext.name:VK_EXT_queue_family_foreign\ndevice ext.name:VK_ANDROID_external_memory_android_hardware_buffer\ndevice ext.name:VK_KHR_external_semaphore_fd\ndevice ext.name:VK_KHR_external_fence_fd\ndevice ext.name:VK_KHR_external_memory_fd\ndevice ext.name:VK_KHR_external_memory\ndevice ext.name:VK_KHR_swapchain\ndevice ext.name:VK_KHR_external_semaphore\ndevice ext.name:VK_KHR_driver_properties\ndevice ext.name:VK_KHR_sampler_mirror_clamp_to_edge\ndevice ext.name:VK_KHR_multiview\ndevice ext.name:VK_KHR_relaxed_block_layout\ndevice ext.name:VK_KHR_maintenance1\ndevice ext.name:VK_KHR_maintenance3\ndevice ext.name:VK_KHR_maintenance2\ndevice ext.name:VK_EXT_global_priority\ndevice ext.name:VK_KHR_get_memory_requirements2\ndevice ext.name:VK_KHR_descriptor_update_template\ndevice ext.name:VK_KHR_bind_memory2\ndevice ext.name:VK_KHR_shader_draw_parameters\ndevice ext.name:VK_KHR_dedicated_allocation\ndevice ext.name:VK_KHR_create_renderpass2\ndevice ext.name:VK_KHR_draw_indirect_count\ndevice ext.name:VK_KHR_sampler_ycbcr_conversion\ndevice ext.name:VK_KHR_device_group\ndevice ext.name:VK_KHR_external_fence\ndevice ext.name:VK_KHR_variable_pointers\ndevice ext.name:VK_EXT_sampler_filter_minmax\ndevice ext.name:VK_KHR_storage_buffer_storage_class\nVULKAN_SYMBOL_WRAPPER_LOAD_INSTANCE_SYMBOL(vkGetPhysicalDeviceFeatures2KHR) res=1\nmChipsetInfoUtilInfo.getVulkanInfo():{vk_driver_version=2149056512, vk_device_id=100859905, vk_extension_descriptor_update_template=1, vk_api_version=4198487, vk_support_fp16_storage=0, vk_platform_dlopen=success, vk_shader_int16=1, vk_device_type=1, vk_shader_float64=0, vk_extension_push_descriptor=1, vk_shader_int64=0, vk_wrapper_init=true, vk_vendor_id=20803, vk_max_compute_shared_memory_size=32768, vk_device_name=Adreno (TM) 630, vk_max_compute_work_group_invocations=1024, vk_device_count=1}\n```\n\nReviewed By: dreiss\n\nDifferential Revision: D19564664\n\nfbshipit-source-id: 908b34bdcc24d9b03ecc185edbc5cfb6e7aa27c9", "pr_number": null, "files_changed": ["caffe2/mobile/contrib/libvulkan-stub/include/libvulkan-stub.h", "caffe2/mobile/contrib/libvulkan-stub/src/libvulkan-stub.c"], "labels": []}, "1695915371": {"title": "Make _wait_all_workers() support being called for multiple times (#32624)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32624\n\nWe need this PR to resolve the issue mentioned in https://github.com/pytorch/pytorch/issues/31325#issuecomment-574918917.\n\nThe solution is for each `_wait_all_workers()` call, there is a sequence ID added, to identify different calls.\nghstack-source-id: 97277591\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork -- test_wait_all_workers\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork\n\nbuck-out/gen/caffe2/test/distributed/rpc/rpc_fork\\#binary.par -r test_wait_all_workers\n```\n\nDifferential Revision: D5739520\n\nfbshipit-source-id: a64131e09c365179624700514422f5375afe803f", "pr_number": "32624", "files_changed": ["torch/distributed/rpc/api.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "1217c9b364": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/3f156207e8a6583d88999487e954320dc18955e6\nhttps://github.com/facebook/folly/commit/135cff30a54b77523ff404a269a960ad981ff8df\nhttps://github.com/facebook/rocksdb/commit/7aa66c704f71d74ff97090caee1d29c4dff22a21\nhttps://github.com/facebookincubator/katran/commit/1dc41366445c0a1eef50e7329c5ea3069763c91b\nhttps://github.com/pytorch/fbgemm/commit/9166d9f76755274970e77dc1386bcdd1bd91c9a4\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: fb27e09060ecb4278b4002c02bce48fe9f4dc361", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "5c8535d5b0": {"title": "Make C++ RpcAgent::currentRPCAgent_ the source of truth of current RPC Agent (#32633)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32633\n\nThere were 2 sources of current RPC agent.\n\n- One is in Python world, `torch.distributedrpc.api._agent`.\n- The other is in C++ world, `RpcAgent::defaultRpcAgent_`\n\nSetting Python `_agent` to `None`, does not necessarily reset the C++ `defaultRpcAgent_` to `nullptr`.\n\ni.e.\n```\n torch.distributedrpc.api._agent = None\n```\ndoes not translate to\n```\nRpcAgent::defaultRpcAgent_ = nullptr\n```\n\nThis PR is to remove this ambiguity, and use the C++ pointer as source of truth.\n\nThe solution is to leverage a pybind11 behavior that it implicitly casts C++ `shared_ptr<RpcAgent>(nullptr)` to Python `None`.\nghstack-source-id: 97293315\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork -- test_duplicate_name\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork\n\nbuck-out/gen/caffe2/test/distributed/rpc/rpc_fork\\#binary.par -r test_process_group_debug_info\n```\n\n```\nbuck test mode/dev-nosan //caffe2/torch/fb/distributed/pytorch/tests:test_remote_module\n\nbuck test mode/dev-nosan //caffe2/torch/fb/distributed/modules/tests:test_sharded_embedding\n\nbuck test mode/dev-nosan //caffe2/torch/fb/distributed/modules/tests:test_sharded_pairwise_attention_pooling\n\nbuck test mode/dev-nosan //caffe2/torch/fb/distributed/pytorch/tests:test_rpc\n```\n\nDifferential Revision: D5733066\n\nfbshipit-source-id: b3e6032ee975f19ca556497edbbf40b517b25be8", "pr_number": "32633", "files_changed": ["torch/csrc/distributed/autograd/context/container.cpp", "torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp", "torch/csrc/distributed/autograd/utils.cpp", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/script_functions.cpp", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py", "torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "db8ce7ea2d": {"title": "Back out \"Make autogen functions correct for multiple outputs and views\" (#32681)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32681\n\nOriginal commit changeset: a2b41c2d231e\n\nTest Plan: fb and oss tests\n\nReviewed By: hudeven\n\nDifferential Revision: D19591864\n\nfbshipit-source-id: 7068b5563e37bc9a5d415fd535c73fd9d71fe131", "pr_number": "32681", "files_changed": ["test/cpp/api/init.cpp", "test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h"], "labels": ["fb-exported", "merged"]}, "666e5430f8": {"title": "Clean up mvlgamma doc (including a weird way to link to reference) (#32667)", "body": "Summary:\nIntentionally left blank\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32667\n\nDifferential Revision: D19594683\n\nPulled By: ezyang\n\nfbshipit-source-id: 5a6eb0a74f569d3c0db2a35e0ed4b329792a18e4", "pr_number": "32667", "files_changed": ["torch/_torch_docs.py"], "labels": ["merge-this-please", "merged", "module: docs", "open source"]}, "b9f764b1c7": {"title": "Use the C++ current RpcAgent pointer to eliminate the unnecessary argument passing from Python world (#32635)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32635\n\nWith the source of truth of current RPC agent moved to C++ world, there is no point of passing current RPC agent from Python world to C++ world.\nghstack-source-id: 97293316\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork\n\nbuck-out/gen/caffe2/test/distributed/rpc/rpc_fork\\#binary.par -r test_process_group_debug_info\n```\n\nDifferential Revision: D5703519\n\nfbshipit-source-id: ef7c28bdb1efd293eb6cafe0b0fca7ca80fa08a6", "pr_number": "32635", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_functions.h", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "d68592a440": {"title": "[JIT] Fix classes as attributes in recursive scripting", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32594\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19562951\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 3d5491c1c23456f107390a78be16da687de951e6", "pr_number": "32594", "files_changed": ["test/jit/test_recursive_script.py", "test/test_jit.py", "torch/csrc/jit/pybind_utils.h", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "b3848c568e": {"title": "Fix flaky test_nccl_timeout. (#32653)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32653\n\nThis test was flaky since the watchdog thread could abort the\ncommunicator instead of the thread calling `wait()`. As a result, we could\nactually see `NCCL error` instead of `Operation timed out` on the user end.\nghstack-source-id: 97250714\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19583003\n\nfbshipit-source-id: 5c07326d1a16f214dcdbabed97ca613e0a5b42b9", "pr_number": "32653", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["merged"]}, "8e4161517e": {"title": "div_kernel: throw when dividing by integer zero (#32629)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/327\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32629\n\nDifferential Revision: D19595782\n\nPulled By: ezyang\n\nfbshipit-source-id: f5bbb298f150efe63a698e8a0b53a84871d16560", "pr_number": "32629", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "test/test_torch.py"], "labels": ["merged", "open source"]}, "63170431f9": {"title": "[jit] fix segfault on missing getstate (#32642)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32642\n\nPreviously, if we defined `__setstate__` but not `__getstate__`, we\nwould segfault. This PR turns that into a comprehensible error message\n(and improves another error message as well).\n\nFixes https://github.com/pytorch/pytorch/issues/25886\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19596463\n\nPulled By: suo\n\nfbshipit-source-id: dbe76bc36bc747d65fb0223184c009e0e9ba072c", "pr_number": "32642", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/compiler.cpp"], "labels": ["jit", "merged"]}, "9a2691f2fc": {"title": "Fix spelling errors", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32673\n\nDifferential Revision: D19597118\n\nPulled By: pietern\n\nfbshipit-source-id: f88c1da7548fcee141ed248f5f49d25c1d639955", "pr_number": "32673", "files_changed": ["benchmarks/README.md", "benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py", "c10/core/TensorImpl.h", "caffe2/mobile/contrib/nnapi/NeuralNetworks.h", "caffe2/operators/assert_op.cc", "caffe2/operators/rnn/recurrent_network_executor.h", "test/test_quantized_tensor.py", "third_party/miniz-2.0.8/ChangeLog.md", "torch/csrc/jit/autodiff.cpp", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/lib/c10d/Utils.hpp"], "labels": ["jit", "merged", "open source"]}, "e24ce0e524": {"title": "Kill some more unused code in function_wrapper.py", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32600\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19565654\n\nPulled By: gchanan\n\nfbshipit-source-id: 993c3dc5467639a7690109d07911951a165a412f", "pr_number": "32600", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/nn_parse.py"], "labels": ["merged"]}, "c64dec1993": {"title": "Python binding to export bytecode format for lite interpreter (#32621)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32621\n\nExport the \"_save_for_mobile\" method to Python so that the bytecode format for lite interpreter can be added or updated to the original script model.\n\nIt's the first step of python binding for lite interpreter, as discussed in this [internal post](https://fb.workplace.com/groups/1144215345733672/permalink/1478900738931796/) and offline.\n\nNext step is to export the load_for_mobile and run method of mobile module, so that users could verify the mobile model from Python.\n\nTest: use the following python script to display the bytecode part of the updated model file.\n```\n#!/usr/bin/env python3\nimport sys\nimport pickle\nimport pprint\nimport zipfile\n\nclass FakeObject(object):\n    def __init__(self, module, name, args):\n        self.module = module\n        self.name = name\n        self.args = args\n        self.state = None\n\n    def __repr__(self):\n        state_str = \"\" if self.state is None else f\"(state={self.state!r})\"\n        return f\"{self.module}.{self.name}{self.args!r}{state_str}\"\n\n    def __setstate__(self, state):\n        self.state = state\n\nclass FakeClass(object):\n    def __init__(self, module, name):\n        self.module = module\n        self.name = name\n        self.__new__ = self.fake_new\n\n    def __repr__(self):\n        return f\"{self.module}.{self.name}\"\n\n    def __call__(self, *args):\n        return FakeObject(self.module, self.name, args)\n\n    def fake_new(self, *args):\n        return FakeObject(self.module, self.name, args)\n\nclass DumpUnpickler(pickle._Unpickler):\n    def find_class(self, module, name):\n        return FakeClass(module, name)\n\n    def persistent_load(self, pid):\n        return FakeObject(\"pers\", \"obj\", (pid,))\n\ndef main(argv):\n    zfile = zipfile.ZipFile(argv[1])\n    names = [i for i in zfile.namelist() if \"bytecode.pkl\" in i]\n    if not names:\n        print(\"bytecode.pkl not found.\")\n        return\n    with zfile.open(names[0], \"r\") as handle:\n        value = DumpUnpickler(handle).load()\n    pprint.pprint(value)\n\nif __name__ == \"__main__\":\n    sys.exit(main(sys.argv))\n\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19596359\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 19a4a771320f95217f5b0f031c2c04db7b4079a8", "pr_number": "32621", "files_changed": ["torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "0dc38be407": {"title": "consider FAIL_GUARD while counting indices for GUARDs (#32672)", "body": "Summary:\nThis handles a corner case when a user schedules second bailout after the first one and the first one doesn't fire.\nAlternatively, we could go back to the implementation that uses a hash set to remember the indices of bailouts that need to fire.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32672\n\nDifferential Revision: D19596872\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 41dcc374cd2501ac20a9892fb31a9c56d6640258", "pr_number": "32672", "files_changed": ["torch/csrc/jit/interpreter.cpp"], "labels": ["jit", "merged"]}, "da390914bd": {"title": ".circleci: Add workflows for Python 3.8 (#31948)", "body": "Summary:\nDone by just editing `.circleci/cimodel/data/dimensions.py` to include `3.8` and then regenerated using `.circleci/regenerate.sh`\n\ncc kostmo, mingbowan, ezyang, soumith\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31948\n\nDifferential Revision: D19602069\n\nPulled By: seemethere\n\nfbshipit-source-id: ac57fde9d0c491c7d948a3f5944c3cb324d403c0", "pr_number": "31948", "files_changed": [".circleci/cimodel/data/dimensions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/docker/common/install_travis_python.sh", ".circleci/docker/ubuntu/Dockerfile", ".circleci/verbatim-sources/workflows-docker-builder.yml", "docker/pytorch/Dockerfile"], "labels": ["merged"]}, "1719da13f9": {"title": "[JIT] Support for registering C++ lambdas as methods on custom C++ class", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32553\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19543269\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 7e566650295e9d1c4f2f716470e061308a6210a0", "pr_number": "32553", "files_changed": ["test/cpp/jit/test_custom_class.cpp", "test/test_jit.py", "torch/custom_class.h"], "labels": ["jit", "merged"]}, "06c19263d3": {"title": "[JIT] Serialize attributes and types in ClassType serialization", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32555\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19544737\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 2256cfba414a850cdc986bb5872dd4cb177b456c", "pr_number": "32555", "files_changed": ["torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/script/string_to_type.cpp"], "labels": ["jit", "merged"]}, "34ccfba403": {"title": "[JIT] Include custom_class.h in torch/script.h", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32586\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19558716\n\nfbshipit-source-id: be540d8ed7de0834e64be89ae621ae50befc83b0", "pr_number": "32586", "files_changed": ["torch/script.h"], "labels": ["merged"]}, "465ebd58ba": {"title": "[JIT] pickle serialization for custom bound classes", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32604\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19566633\n\nfbshipit-source-id: 9387d3ff45cbd6ccde49ce190a52859481cc301c", "pr_number": "32604", "files_changed": ["c10/util/TypeTraits.h", "caffe2/CMakeLists.txt", "test/cpp/jit/test_custom_class.cpp", "test/test_jit.py", "torch/csrc/jit/pickler.cpp", "torch/csrc/jit/pybind_utils.h", "torch/custom_class.h", "torch/custom_class_detail.h"], "labels": ["jit", "merged"]}, "0ea65d63cf": {"title": "[JIT] Fix stateful lambda stuff and simplify code in custom C++ binding API", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32658\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19584701\n\nPulled By: jamesr66a\n\nfbshipit-source-id: d556c7db2f32900eb1122348402789b59516a7d7", "pr_number": "32658", "files_changed": ["torch/custom_class.h", "torch/custom_class_detail.h"], "labels": ["merged"]}, "6e7e595c1d": {"title": "[rpc][easy] remove redundant test in rpc_test.py (#32588)", "body": "Summary:\nBoth `test_wait_all_workers` and `test_wait_all_workers_and_shutdown` test the same pattern of initialize RPC, call `_wait_all_workers`, and `rpc.shutdown(graceful=False)`.\n\n`test_wait_all_workers` seems to be more thorough since it tests one worker driving and the others waiting on it as well.\n\nWe shouldn't have duplicate test so removing this `test_wait_all_workers_and_shutdown`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32588\n\nDifferential Revision: D19566294\n\nPulled By: rohan-varma\n\nfbshipit-source-id: b69519d169b3964649d47ad75532bda5de538241", "pr_number": "32588", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "9de3208449": {"title": "[rpc][flaky-tests] fix for test_handle_send_exceptions and (#32656)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32656\n\nFixes these flaky tests.\n\nTest Plan: Run the test 500 times and verify that it succeeds every time.\n\nDifferential Revision: D19584453\n\nfbshipit-source-id: 07cbc4914211f274182ac0fa74bb5ef6d43392d1", "pr_number": "32656", "files_changed": ["torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "ffdcbadeaa": {"title": "Minor refactoring to improve code reuse (#32675)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32675\n\nIt's good to have one location to do the mapping.\n\nTest Plan: Everything still runs.\n\nReviewed By: amylittleyang\n\nDifferential Revision: D19590354\n\nfbshipit-source-id: d8c0d14e4bdf27da3e13bd4d161cd135d6e3822b", "pr_number": "32675", "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc", "caffe2/opt/custom/fakefp16_transform.h"], "labels": ["fb-exported", "merged"]}, "0327e75e14": {"title": "Back out \"[caffe2] use JIT'ed fp32 SLS\" (#32711)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32711\n\nOriginal commit changeset: 4f29d34523ef\n\nTest Plan: CI\n\nDifferential Revision: D19603967\n\nfbshipit-source-id: af3f647fff416a84290a42217747948bac4d73c6", "pr_number": "32711", "files_changed": ["caffe2/operators/lengths_reducer_ops.h"], "labels": ["merged"]}, "2060e0a9dd": {"title": "Split serialization tests to their own file (#32241)", "body": "Summary:\nStacked PRs\n * #32244 - Make zip serialization the default\n * **#32241 - Split serialization tests to their own file**\n\nThis makes them all easier to run as a batch. This PR is just a code move / fixing up imports. There are still some serialization tests in `test_torch.py` as part of `TestDeviceType`.\n](https://our.intern.facebook.com/intern/diff/19415826/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32241\n\nPulled By: driazati\n\nDifferential Revision: D19415826\n\nfbshipit-source-id: a3f6cfe1626ff2f9b9631c409bf525bd32e4639b", "pr_number": "32241", "files_changed": ["test/run_test.py", "test/test_serialization.py", "test/test_torch.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged"]}, "ee60cd9124": {"title": "Back out \"fix view listing in autograd codegen\" (#32720)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32720\n\nOriginal commit changeset: 5ebc4c978af5\n\nTest Plan: existing tests\n\nReviewed By: chenyangyu1988\n\nDifferential Revision: D19603336\n\nfbshipit-source-id: 56051a716c4eedf49cfe7367ff447b4b9c5429ea", "pr_number": "32720", "files_changed": ["aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_autograd.py", "torch/csrc/autograd/variable.cpp"], "labels": ["fb-exported", "merged"]}, "e74e1ccc47": {"title": "Use direct vector indexing in Object::getSlot() instead of at(). (#31627)", "body": "Summary:\nThis method is pretty hot.  In an internal workload, this single\ncall to at() accounted for ~2% of overall cycles.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31627\n\nReviewed By: yinghai\n\nDifferential Revision: D19607779\n\nPulled By: qizzzh\n\nfbshipit-source-id: 1684919049a35fdad686d8396c7dce7243ab92d4", "pr_number": "31627", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["merged"]}, "99228086a6": {"title": "Added missing period in README.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32723\n\nDifferential Revision: D19607256\n\nPulled By: mlacayo\n\nfbshipit-source-id: 2993014d4d90fa26acd5bc01ed7494cc43a29a62", "pr_number": "32723", "files_changed": ["README.md"], "labels": ["merged"]}, "43d31ae4c3": {"title": "Added ONNX model checker to ONNX export (#32298)", "body": "Summary:\nIncluded the ONNX model checker code in the ONNX export\nthis will force onnx checker to run for all models that get exported.\nThis should help with validating exported models.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32298\n\nReviewed By: hl475\n\nDifferential Revision: D19538251\n\nPulled By: houseroad\n\nfbshipit-source-id: eb20b124fe59200048f862ddaf20f6c59a0174d5", "pr_number": "32298", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export.h", "torch/csrc/jit/script/init.cpp", "torch/onnx/__init__.py", "torch/onnx/utils.py"], "labels": ["jit", "merged", "module: onnx", "open source"]}, "6f7d5bb3e1": {"title": "Temporarily disable the test_quantized_rnn test (#32742)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32742\n\nAs Title says (Check https://github.com/pytorch/pytorch/issues/32644).\nghstack-source-id: 97352793\n\nTest Plan: CI\n\nDifferential Revision: D19611029\n\nfbshipit-source-id: 9f4a155c909f419e41c1d7078eb2796dd17cedd2", "pr_number": "32742", "files_changed": ["test/test_quantization.py"], "labels": ["merged"]}, "1f78bd0774": {"title": "[caffe2] Early error throwing for currupted embeddings", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32717\n\nReviewed By: xianjiec\n\nDifferential Revision: D19604954\n\nfbshipit-source-id: c02eccf048c0dba3f66d729ab1fda50f3cacef63", "pr_number": "32717", "files_changed": ["caffe2/operators/gather_ranges_to_dense_op.h", "caffe2/python/operator_test/gather_ranges_op_test.py"], "labels": ["fb-exported", "merged"]}, "18aab32959": {"title": "Move exponential_ from TH to Aten (CPU) (#32501)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32501\n\nThis diff will address https://github.com/pytorch/pytorch/issues/24699\n\nWe ask the input `lambda` to be >= 0 to be same as https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.exponential.html#numpy-random-exponential. This does not exist in the previous implementation.\n\nBenchmark I am using PT operator microbenchmark\n```\n================================================================================\nBefore the change, Program Output:\n================================================================================\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: exponential_\n# Mode: Eager\n# Name: exponential__M512_N512_cpu\n# Input: M: 512, N: 512, device: cpu\nForward Execution Time (us) : 21311.746\n\n================================================================================\nAfter the change, Program Output:\n================================================================================\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: exponential_\n# Mode: Eager\n# Name: exponential__M512_N512_cpu\n# Input: M: 512, N: 512, device: cpu\nForward Execution Time (us) : 20919.914\n\n================================================================================\n```\n\nTest Plan: Sandcastle and Github tests\n\nReviewed By: BIT-silence\n\nDifferential Revision: D19518700\n\nfbshipit-source-id: 0e79cb6a999c1278eb08b0d94cf61b119c85a36c", "pr_number": "32501", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/core/DistributionsHelper.h", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorRandom.cpp", "aten/src/TH/generic/THTensorRandom.h", "test/test_torch.py"], "labels": ["fb-exported", "merged", "topic: bc-breaking"]}, "02f055ffd9": {"title": "Add mapping for FbFCPacked in fakefp16 transform", "body": "Summary: ATT. Since the infra is there.\n\nTest Plan: run it\n\nReviewed By: amylittleyang\n\nDifferential Revision: D19605250\n\nfbshipit-source-id: c68be4d7963afa4fa5f8f60c90f1913605eae516", "pr_number": null, "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc"], "labels": []}, "25d33a2ee8": {"title": "[JIT] Use Type Level Granularity in Alias Analysis Wildcards (#32251)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32251\n\nPreviously wildcard sets were associated by TypeKind, meaning all Lists were in one alias set, all Classes were in one alias set, etc. We can improve analysis by bucketing wildcard sets by TypePtr instead. Any two mutable types which can unify should be in the same wildcard set bucket.\n\nThis also allows us do much simpler `mayContainAlias` analysis, and also improves `analyzeConservative` analysis because now we can recurse through all contained memory locations and mark writes, instead of just recursing only level deep in contained elements.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19563263\n\nPulled By: eellison\n\nfbshipit-source-id: 371a37d1a8596abc6c53f41c09840b6c140ea362", "pr_number": "32251", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_misc.cpp", "tools/build_variables.py", "torch/csrc/jit/passes/alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.h", "torch/csrc/jit/passes/utils/memory_dag.h", "torch/csrc/jit/type_hashing.cpp", "torch/csrc/jit/type_hashing.h"], "labels": ["jit", "merged"]}, "c729614997": {"title": "[JIT] Improve May Contain Alias Using Contained Elements (#32326)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32326\n\nNow that we have type-level granularity we can improve `mayContainAlias` queries. Each new values is initialized as containing the wildcard set of each contained mutable type. Whenever a value is added to a container it is set to the wildcard set. Now, to check if any two values contain overlapping values, we can just check if the `containedMemoryLocations` of two sets overlap.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19563262\n\nPulled By: eellison\n\nfbshipit-source-id: c6d7489749c14b2054a6d50ef75baca699ada471", "pr_number": "32326", "files_changed": ["test/cpp/jit/test_alias_analysis.cpp", "test/jit/test_models.py", "torch/csrc/jit/passes/alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.h", "torch/csrc/jit/type_hashing.cpp", "torch/csrc/jit/type_hashing.h"], "labels": ["jit", "merged"]}, "62d652f922": {"title": "replaces .at with [] in getSlot (#32677)", "body": "Summary:\nper title. cc qizzzh\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32677\n\nDifferential Revision: D19596094\n\nPulled By: ngimel\n\nfbshipit-source-id: 06177b9e12d203d84b541205437ef2ad51db0fac", "pr_number": "32677", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["merged"]}, "fd850685da": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/b81d0657df33a4012d01a8b9d5583583d0d0f17c\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 82d39025e331083e58c0d0cc9b47985e590bb289", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "5e2311033e": {"title": "fix windows build (#32762)", "body": "Summary:\nremove windows visibility macro\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32762\n\nDifferential Revision: D19616367\n\nPulled By: eellison\n\nfbshipit-source-id: d824162fe92bff4cb2b1a170312cd14b6d7bd99d", "pr_number": "32762", "files_changed": ["torch/csrc/jit/type_hashing.h"], "labels": ["jit", "merged"]}, "594cadeb8f": {"title": "Make sure temporary vectors are properly initialized in avx2 code (#32722)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32722\n\nChecked using [this](https://godbolt.org/z/uAaE9R) that it gives the correct assembly.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19610012\n\nPulled By: albanD\n\nfbshipit-source-id: 4d1cb812951ae03d412a0fba3c80730f0d286e1f", "pr_number": "32722", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_complex_double.h", "aten/src/ATen/cpu/vec256/vec256_complex_float.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/cpu/vec256/vec256_int.h"], "labels": ["merged"]}, "c35ca84eee": {"title": "Get rid of some unused THGenerate*Type defines. (#32657)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32657\n\nThe goal here is to simplify the codegen enough that we can just handwrite the bindings, so anything in here is \"bad\".\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19584521\n\nPulled By: gchanan\n\nfbshipit-source-id: 93005b178228c52a1517e911adde2e2fe46d66a5", "pr_number": "32657", "files_changed": ["aten/src/TH/THGenerateBFloat16Type.h", "aten/src/TH/THGenerateBoolType.h", "aten/src/TH/THGenerateByteType.h", "aten/src/TH/THGenerateCharType.h", "aten/src/TH/THGenerateDoubleType.h", "aten/src/TH/THGenerateFloatType.h", "aten/src/TH/THGenerateHalfType.h", "aten/src/TH/THGenerateIntType.h", "aten/src/TH/THGenerateLongType.h", "aten/src/TH/THGenerateShortType.h"], "labels": ["merged"]}, "c7bf4d22fe": {"title": "added exception args to the returned error message (#32693)", "body": "Summary:\naddresses https://github.com/pytorch/pytorch/issues/32692\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32693\n\nDifferential Revision: D19606757\n\nPulled By: mrshenli\n\nfbshipit-source-id: 79fc09f8bb6a33e1b73ce0bbc45387544c7adc1b", "pr_number": "32693", "files_changed": ["torch/nn/modules/module.py"], "labels": ["merged", "open source"]}, "8bc889e502": {"title": "Fix crash of SobolEngine if default tensor type is cuda (#32496)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32496\n\nAddresses https://github.com/pytorch/pytorch/issues/32494\n\nTest Plan:\n```\nimport torch\nfrom torch.quasirandom import SobolEngine\n\ntorch.set_default_tensor_type(torch.cuda.FloatTensor)\nse = SobolEngine(3)\n```\n\nReviewed By: 2timesjay\n\nDifferential Revision: D19517571\n\nfbshipit-source-id: 02eb499ffbd4260474d348e9bb536fb8c36c2c31", "pr_number": "32496", "files_changed": ["torch/quasirandom.py"], "labels": ["fb-exported", "merged"]}, "cbb744f00f": {"title": "apply linter to rpc test files (#32659)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32659\n\nApplies linter to RPC test files so that we can use linter shortcuts\nwithout getting unnecessary changes to the whole file.\nghstack-source-id: 97361237\n\nTest Plan: No actual changes.\n\nDifferential Revision: D19584742\n\nfbshipit-source-id: a11ce74ee0e2817e6f774fff7c39bcab06e99307", "pr_number": "32659", "files_changed": ["torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/dist_optimizer_test.py", "torch/testing/_internal/distributed/rpc/rpc_agent_test_fixture.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "d119de8abd": {"title": "Deduplication of type casting codes (#32730)", "body": "Summary:\nThese codes are implemented twice at different places by different people, we should merge them together.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32730\n\nDifferential Revision: D19622023\n\nPulled By: ezyang\n\nfbshipit-source-id: a9cbda31428b335bf28a7e4050f51f58e787b94f", "pr_number": "32730", "files_changed": ["aten/src/ATen/detail/ScalarTypeConversions.h", "aten/src/ATen/native/TensorIterator.h", "c10/core/Scalar.h", "c10/util/Half.h", "c10/util/TypeCast.h"], "labels": ["merged", "open source"]}, "642c9ef922": {"title": "More code fakefp16 mapping unification", "body": "Summary: ATT\n\nReviewed By: amylittleyang\n\nDifferential Revision: D19597036\n\nfbshipit-source-id: deed61945884fb4b01d058f3c72c75f5a937a41c", "pr_number": null, "files_changed": ["caffe2/opt/custom/fakefp16_transform.h", "caffe2/python/pybind_state.cc"], "labels": []}, "b1c85dd916": {"title": "Custom RNG DispatchKey (#32325)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32325\n\nThe purpose of this PR is to enable PyTorch dispatching on `at::Generator*` parameters and demonstrate how it can be used in cpp extensions to implement custom RNG.\n1. `CustomRNGKeyId` value added to DispatchKey enum and `DispatchKeySet key_set_` added to `at::Generator`\n2. The overloaded `operator()(at::Generator* gen)` added to MultiDispatchKeySet.\n3. The existing CPUGenerator and CUDAGenerator class are supplied with CPUTensorId and CUDATensorId dispatch keys\n4. The implementation of CPU's `cauchy_kernel`(as an example, because it's already moved to ATen) was templatized and moved to `ATen/native/cpu/DistributionTemplates.h` to make it available for cpp extensions\n5. Minor CMake changes to make native/cpu tensors available for cpp extensions\n6. RegisterCustomRNG test that demonstrates how CustomCPUGenerator class can be implemented and how custom_rng_cauchy_ native function can be registered to handle Tensor::cauchy_ calls.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19604558\n\nPulled By: pbelevich\n\nfbshipit-source-id: 2619f14076cee5742094a0be832d8530bba72728", "pr_number": "32325", "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/CPUGenerator.cpp", "aten/src/ATen/CPUGenerator.h", "aten/src/ATen/core/DistributionsHelper.h", "aten/src/ATen/core/Generator.cpp", "aten/src/ATen/core/Generator.h", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/cuda/CUDAGenerator.cpp", "aten/src/ATen/native/cpu/DistributionTemplates.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/rng_test.cpp", "c10/core/DispatchKey.h", "setup.py"], "labels": ["merged"]}, "2471ddc96c": {"title": "Improved speed of frobenous norm for non-complex dtype (#30871)", "body": "Summary:\nIn-tree changes to pytorch to support complex numbers are being submitted here.\nOut-of-tree support for CUDA complex numbers is here: [pytorch-cuda-strided-complex extension](https://gitlab.com/pytorch-complex/pytorch-cuda-strided-complex)\n\nChanges:\n[x] Fixed performance issue raise in https://github.com/pytorch/pytorch/issues/30704 so that non-complex numbers do not call `conj()` and `real()`.\n[x] Fixed tensor_to_numpy() conversion likely broken by a `checkBackend()` in https://github.com/pytorch/pytorch/issues/27064.\n[x] Fixed some ReduceOps and TensorCompare Ops that recently added a `checkBackend()`.\n    - `checkBackend()` is replaced with a device type check and a layout check.\n    - This ensures the ComplexCPU Type ID is supported.\n[x] Added AVX support for complex `exp()`, as requested in https://github.com/pytorch/pytorch/issues/755\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30871\n\nDifferential Revision: D19200726\n\nPulled By: ezyang\n\nfbshipit-source-id: d7e1be0b0a89c5d6e5f4a68ce5fcd2adc5b88277", "pr_number": "30871", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_complex_double.h", "aten/src/ATen/cpu/vec256/vec256_complex_float.h", "aten/src/ATen/native/Fill.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/UnaryOps.cpp", "torch/csrc/utils/tensor_numpy.cpp"], "labels": ["merged", "open source"]}, "eab99ab08e": {"title": "[android] fbjni DoNotStrip annotation for oss native methods (#32567)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32567\n\nAs a first change to support proguard.\neven if these methods could be not called from java, on jni level we register them and this registration will fail if methods are stripped.\n\nAdding DoNotStrip to all native methods that are registered in OSS.\n\nAfter integration of consumerProguardFiles in fbjni that prevents stripping by proguard DoNotStrip it will fix errors with proguard on.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19624684\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: cd7d9153e9f8faf31c99583cede4adbf06bab507", "pr_number": "32567", "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/NativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java"], "labels": ["merged"]}, "b5d8982ae2": {"title": "clean up GIL usuage (#32748)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32748\n\nThis is to follow up PR #30630, we need to have GIL when calling jit::toPyObject(), for some binded functions need to be taged with GIL release if underneath C++ codes requires GIL. so\n1. pyRef::to_here() and pyRef::local_value() added GIL\n2. pyRef::pickle and pyRef::unpickle() added GIL release tag\n3. in request_callback_impl, also added GIL as needed\n4. for typeParser, use cached jitCompilationUnit_, also clean it up in cleanUp() function\nghstack-source-id: 97373011\n\nTest Plan: unit test\n\nDifferential Revision: D19612337\n\nfbshipit-source-id: 4d09f9b52ba626545ae7d31fea6b671301ed3890", "pr_number": "32748", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.cpp"], "labels": ["merged"]}, "8693164acb": {"title": "Randomize xla port (#32718)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/30717\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32718\n\nDifferential Revision: D19607998\n\nPulled By: ailzhang\n\nfbshipit-source-id: 81ba9c7c71988a64cdc8fa5500967509657438fe", "pr_number": "32718", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["merged"]}, "e84f9d9d0c": {"title": "Fix TensorProtosDBInput AttributeError (#32274)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/6794\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32274\n\nDifferential Revision: D19621889\n\nPulled By: ezyang\n\nfbshipit-source-id: 1bdd042b6421a2798c7f1e9030dfc6dfc1246989", "pr_number": "32274", "files_changed": ["caffe2/python/model_helper.py"], "labels": ["merged", "open source"]}, "50d82f5122": {"title": "Make VC++ version a parametrizable option for Windows CI. (#32043)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32043\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19621910\n\nPulled By: ezyang\n\nfbshipit-source-id: dce00a56ff679548fd9f467661c3c54c71a3dd4e", "pr_number": "32043", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/pytorch-build-params.yml", ".circleci/verbatim-sources/pytorch-job-specs.yml", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat"], "labels": ["merge-this-please", "merged"]}, "8cb05e72c6": {"title": "Port BCELoss to ATen to increase accuracy (#31365)", "body": "Summary:\nFixes issue https://github.com/pytorch/pytorch/issues/24933\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31365\n\nDifferential Revision: D19557712\n\nPulled By: ezyang\n\nfbshipit-source-id: 3ae78c949b2f6c21b294d986d28e09daa9b0c526", "pr_number": "31365", "files_changed": ["aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/cuda/Loss.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/BCECriterion.cu", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/generic/BCECriterion.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/BCECriterion.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp", "test/test_nn.py"], "labels": ["merged", "open source", "topic: bc-breaking", "triaged"]}, "3b47922855": {"title": "Improve documentation in dispatcher; remove unnecessary optional (#32533)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32533\n\nApplies renames based on comments in #32439.  I also updated some\nother documentation and variable names while I was at it.\n\nFixes #32435.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19579854\n\nPulled By: ezyang\n\nfbshipit-source-id: 85021a92a2a84501f49ee5c16318f81f5df64f8d", "pr_number": "32533", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.cpp", "aten/src/ATen/core/boxing/test_helpers.h", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.cpp", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/dispatch/DispatchTable.h", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": ["merged"]}, "5ffa1efa52": {"title": "Add missing C10_API to dispatch key TLS setter/getters (#32557)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32557\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19579853\n\nPulled By: ezyang\n\nfbshipit-source-id: 45f83a7a5ead0344e4c13526abb5fafdedaed4a4", "pr_number": "32557", "files_changed": ["c10/core/impl/LocalDispatchKeySet.h"], "labels": ["merged"]}, "c7df28a2a3": {"title": "Delete copy/move constructors on these RAII guards. (#32727)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32727\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19621858\n\nPulled By: ezyang\n\nfbshipit-source-id: 5112c849252478d8249de4f8c8c5a2d6caf60672", "pr_number": "32727", "files_changed": ["c10/core/impl/LocalDispatchKeySet.h"], "labels": ["merged"]}, "b371eab8c7": {"title": "Expunge last two sites of resize_dim (#32112)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32112\n\nIt turns out we already removed these from the CPU version; copy\nthe changes over.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19579874\n\nPulled By: ezyang\n\nfbshipit-source-id: e40efbf94e128fd81421b227b76dd9c9c0256d96", "pr_number": "32112", "files_changed": ["aten/src/TH/generic/THTensor.cpp", "aten/src/THC/THCTensor.cpp"], "labels": ["merged"]}, "8c6f52ac24": {"title": "Delete resize_dim() (#32114)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32114\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19579876\n\nPulled By: ezyang\n\nfbshipit-source-id: d09a231ba891403a06eae0c2203e0ad7dd6d3a12", "pr_number": "32114", "files_changed": ["aten/src/ATen/OpaqueTensorImpl.h", "aten/src/ATen/SparseTensorImpl.cpp", "aten/src/ATen/SparseTensorImpl.h", "c10/core/TensorImpl.h"], "labels": ["merged"]}, "3ee6673e99": {"title": "Refreshing numel on a stride update is pointless. (#32116)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32116\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19579875\n\nPulled By: ezyang\n\nfbshipit-source-id: 00393c9dc101967c79231bfae36b23b7b80135fb", "pr_number": "32116", "files_changed": ["c10/core/TensorImpl.h"], "labels": ["merged"]}, "c47c78d0bf": {"title": "Revert D19597036: More code fakefp16 mapping unification", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19597036\n\nOriginal commit changeset: deed61945884\n\nfbshipit-source-id: c057e57810a99464aefb00b645613ecd6a7c5533", "pr_number": null, "files_changed": ["caffe2/opt/custom/fakefp16_transform.h", "caffe2/python/pybind_state.cc"], "labels": []}, "8b187e8f2a": {"title": "Fix ivalue_inl.h:353:29: warning: comparison of unsigned expression >= 0 is always true (#32778)", "body": "Summary:\n`slot` is unsigned integer which is `always >= 0`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32778\n\nDifferential Revision: D19625789\n\nPulled By: ngimel\n\nfbshipit-source-id: c92c35c65d4372be934283e87aeba99e9e0ef353", "pr_number": "32778", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["merged", "open source"]}, "9357b91180": {"title": "Remove -Werror from test/cpp_extensions/setup.py (#32704)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32704\n\n-Werror is too aggressive check for test cpp extensions because it fails even on deprecation warnings which is are included from core codebase.\n\nFixes #32136\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19620190\n\nPulled By: pbelevich\n\nfbshipit-source-id: 0e91566eb5de853559bb59e68a02b0bb15e7341b", "pr_number": "32704", "files_changed": ["test/cpp_extensions/setup.py", "test/test_cpp_extensions.py"], "labels": ["merged"]}, "fc2ff7912f": {"title": "[quantization] Remove incorrect fp16 dynamic linear/relu op", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32774\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19624471\n\nPulled By: jamesr66a\n\nfbshipit-source-id: eb6cb11fabf2ddd5edf345aff35b86b83c3af94c", "pr_number": "32774", "files_changed": ["aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp"], "labels": ["merged"]}, "b565d9b356": {"title": "Logspace fixes (#32744)", "body": "Summary:\nReopening of PR https://github.com/pytorch/pytorch/issues/32631 with `viable/strict` base for testing\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32744\n\nDifferential Revision: D19626090\n\nPulled By: ngimel\n\nfbshipit-source-id: ed0fc759198ee2edc23afdcb1e190a11d70ec4c8", "pr_number": "32744", "files_changed": ["aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/cuda/RangeFactories.cu", "test/test_torch.py"], "labels": ["merged", "open source"]}, "a840afbeb4": {"title": "[pytorch][embeddingbag_8bit] Add include_last_offset option to Fused 8bit EmbeddingBag and parallelize the op (#32683)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32683\n\nPull Request resolved: https://github.com/pytorch/glow/pull/4079\n\nSimilar to D17768404, we changed the EmbeddingBag operator for 8-bit fused version to add the option to include the last offset and parallelize the op.\nghstack-source-id: 97404645\n\nTest Plan:\nTo generate the AVX2 code (`embedding_lookup_fused_8bit_rowwise_idx_avx2.cc`):\n```\npython hp_emblookup_codegen.py --fused --use-offsets\n```\n\nTo test the correctness:\n\n```\nbuck test //caffe2/torch/fb/sparsenn:test -- test_embedding_bag_byte_rowwise_offsets  --print-passing-details\n```\n\nReviewed By: yinghai\n\nDifferential Revision: D19592761\n\nfbshipit-source-id: f009d675ea3f2228f62e9f86b7ccb94700a0dfe0", "pr_number": "32683", "files_changed": ["caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc", "caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup_idx.cc", "caffe2/perfkernels/hp_emblookup_codegen.py"], "labels": ["merged"]}, "2e359ef86d": {"title": "enable empty batch for all flavor of convolutions (#32709)", "body": "Summary:\nresubmitting https://github.com/pytorch/pytorch/issues/32612 after a merge gone wrong. Enables convolution with an empty batch or number of channels for all flavors of convolution (grouped convolution, convTranspose). Would make https://github.com/pytorch/pytorch/issues/31658 unnecessary. Also returns zero gradients for the parameters, that's necessary for correct DDP operation.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32709\n\nDifferential Revision: D19627968\n\nPulled By: ngimel\n\nfbshipit-source-id: 7359759bd05ff0df0eb658cac55651c607f1b59f", "pr_number": "32709", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "test/test_nn.py"], "labels": ["merged"]}, "3552be1090": {"title": "[jit] fix the NoneType param/buffer hack (#32745)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32745\n\nSome parameters (like `bias` in conv) are optional. To achieve this\npreviously, you had to add `bias` as a constant, which would invoke some\npretty weird behavior in the frontend, summarized as:\n```\nif bias is not None:\n  add it as a parameter normally\nelse: # bias is None\n  add it as a constant with the value None\n```\n\nThere are several things bad about this:\n1. Bias is not a constant. Marking it `__constants__` is confusing.\n2. It basically relies on an implementation detail (the frontend\nprocesses parameters before constants) to work.\n\nOkay, whatever. I don't even know why we did this originally, but\ngetting rid of it doesn't break anything, so I assume improved NoneType\nrefinement has made this a non-issue.\n\nNote on perf: this will make no difference; if bias was `None` it's still\nfolded out today, if bias is a Tensor it would be added as a parameter\nboth before and after this change\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19628634\n\nPulled By: suo\n\nfbshipit-source-id: d9128a09c5d096b938fcf567b8c23b09ac9ab37f", "pr_number": "32745", "files_changed": ["test/test_jit.py", "torch/jit/_recursive.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/conv.py", "torch/nn/modules/linear.py", "torch/nn/modules/loss.py", "torch/nn/modules/normalization.py"], "labels": ["jit", "merged"]}, "cccf5e7011": {"title": "Resolve rendezvous race condition", "body": "Summary:\nWhen running the ctr_mbl_feed, we've encountered hang issue related to the rendezvous handshake based on zeus. It was mitigated by this diff https://our.intern.facebook.com/intern/diff/D19167151/.\n\nThis diff resolves the race condition by adding a reference to the rendezvous handler.\n\nTest Plan: x7340282797\n\nReviewed By: yifuwang\n\nDifferential Revision: D19627293\n\nfbshipit-source-id: 560af289db8ef6cf8d6f101f95ec27d5a361fd04", "pr_number": null, "files_changed": ["torch/distributed/rpc/__init__.py"], "labels": []}, "8ead65a946": {"title": "[PyTorch][TorchScript] Add support for join on List of strings in TorchScript", "body": "Summary: Add support for join on List of strings in TorchScript.\n\nTest Plan:\n(pytorch) smummadi@smummadi-mbp pytorch % python test/test_jit_string.py\nFail to import hypothesis in common_utils, tests are not derandomized\n.\n----------------------------------------------------------------------\nRan 1 test in 1.090s\n\nOK\n\nDifferential Revision: D19611800\n\nfbshipit-source-id: cef66356abc14dfd100a806d25dd1a8bc9af0a11", "pr_number": null, "files_changed": ["test/run_test.py", "test/test_jit_string.py", "torch/csrc/jit/register_string_ops.cpp"], "labels": []}, "55c382e62b": {"title": "Fixed access to element in size tensor for scripting (#32652)", "body": "Summary:\nwhen using scripting, there was an error in attempting to access a\nspecific element from within the size tensor.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32652\n\nReviewed By: hl475\n\nDifferential Revision: D19610726\n\nPulled By: houseroad\n\nfbshipit-source-id: bca49927bbe71dbe7e7d7edf301908fe79e089b5", "pr_number": "32652", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "c2d736cefb": {"title": "Add support for Dynamic LSTM quantization on Mobile (#32757)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32757\n\nThis PR updates the main quantize_dynamic API to use QNNPACK backend for mobile\n\nTest Plan:\npython test/test_quantization.py PostTrainingDynamicQuantTest.test_quantized_rnn\n\nImported from OSS\n\nDifferential Revision: D19632220\n\nfbshipit-source-id: b4c51485c281d088524101b97c84dd806438b597", "pr_number": "32757", "files_changed": ["aten/src/ATen/native/RNN.cpp", "test/test_quantization.py"], "labels": ["merged"]}, "821b6aa769": {"title": "[pytorch] Minor: avoid acquiring GIL twice in PyRRef::localValue() (#32785)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32785\n\nAdd PythonRpcHandler::handleExceptionWithGIL() so that in PyRRef::localValue(),\nwe don't need to release the GIL and re-acquire the following line.\nghstack-source-id: 97418465\n\nTest Plan: existing test coverage\n\nDifferential Revision: D19626195\n\nfbshipit-source-id: db694d04b078811f819626789e1e86f1b35adb5b", "pr_number": "32785", "files_changed": ["torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h"], "labels": ["merged"]}, "fb159b5236": {"title": "Some work on eager op binding codegen (gen_python_functions.py) (#29986)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29986\n\nPreviously in addition to generating a python binding for each op,\nwe would generate an almost-trivial helper for each overload.\nThis PR eliminates the helpers, simplifying codegen logic a bit and\nreducing the source-level indirection by a step.\nPerf should be unchanged.\n\ncodegen diff: https://github.com/bhosmer/scratch/commit/1f2f07fb605e782cf7fdfb7d5eb33050eb65a6b4\n\nNote: in the interests of keeping the diff contained, there's only\nsome light cleanup here beyond what's necessary for the codegen changes.\nPlan is to do some more substantial refactoring in followup PRs that\nleave generated code unchanged.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18567980\n\nPulled By: bhosmer\n\nfbshipit-source-id: eb9a81babb4489abd470842757af45580d4c9906", "pr_number": "29986", "files_changed": ["caffe2/CMakeLists.txt", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_nn_functions.cpp", "tools/autograd/templates/python_nn_functions.h", "tools/autograd/templates/python_nn_functions_dispatch.h", "tools/autograd/templates/python_torch_functions.cpp", "tools/autograd/templates/python_torch_functions_dispatch.h", "tools/autograd/templates/python_variable_methods.cpp", "tools/autograd/templates/python_variable_methods_dispatch.h", "torch/csrc/Module.cpp", "torch/csrc/autograd/python_nn_functions.h"], "labels": ["merged"]}, "affd598c1f": {"title": "Fix/simplify alias annotation handling in op codegen. (#32574)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32574\n\nPreviously, we ignored alias annotations when deriving argument mutability\nand instead recognized particular signature patterns (in-place, out variant)\nand assigned mutability accordingly. Op signatures that didn't fit these\npatterns would error (e.g. see #30526, which this fixes).\n\nNo change in the generated binding code.\n\nCode changes:\n1. in function_wrapper.py, fix the mutability derivation logic used when creating an argument's c++ type property. Note that we temporarily need to trap a special case and apply the old logic, see code comment for details.\n\n2. in gen_jit_dispatch.py, update logic that assumed only one mutable Tensor argument per declaration. Happily this mostly was accomplished by bypassing some now-redundant signature regeneration machinery. Another special case here requires that we keep the old machinery around temporarily.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19564875\n\nPulled By: bhosmer\n\nfbshipit-source-id: 5637a9672923676d408c9586f3420bcc0028471a", "pr_number": "32574", "files_changed": ["aten/src/ATen/function_wrapper.py", "tools/jit/gen_jit_dispatch.py"], "labels": ["jit", "merged"]}, "b0923acb29": {"title": "Reduce RPC branches for Python/BuiltinOp/TorchScript (#32689)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32689\n\nAs described in https://github.com/pytorch/pytorch/issues/32565\nghstack-source-id: 97440343\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork -- test_script_functions_not_supported\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork\n\nbuck-out/gen/caffe2/test/distributed/rpc/rpc_fork\\#binary.par -r test_script_functions_not_supported\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:dist_autograd_fork\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:dist_autograd_fork\n\nbuck-out/gen/caffe2/test/distributed/rpc/dist_autograd_fork\\#binary.par -r test_backward_simple_script_call\n```\n\nDifferential Revision: D5721814\n\nfbshipit-source-id: 9079e81764be1e7c7b85dd72a18c76f3ecfd2547", "pr_number": "32689", "files_changed": ["tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/script_functions.cpp", "torch/csrc/distributed/rpc/script_functions.h", "torch/csrc/distributed/rpc/torchscript_functions.cpp", "torch/csrc/distributed/rpc/torchscript_functions.h", "torch/distributed/rpc/api.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "6874278985": {"title": "Revert D19611800: [PyTorch][TorchScript] Add support for join on List of strings in TorchScript", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19611800\n\nOriginal commit changeset: cef66356abc1\n\nfbshipit-source-id: 41af9e0de83b1fb808b17255ec905e137909457d", "pr_number": null, "files_changed": ["test/run_test.py", "test/test_jit_string.py", "torch/csrc/jit/register_string_ops.cpp"], "labels": []}, "85bd3e5bdb": {"title": "Removing @expectedFailureXLA from test_nll_loss_empty_tensor_reduction_mean (#32701)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32701\n\nBecause it's disabled in XLA(https://github.com/pytorch/xla/pull/1563)\nDiscussed in https://github.com/pytorch/xla/issues/1539\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19633349\n\nPulled By: pbelevich\n\nfbshipit-source-id: b9a81c976a96b325356ff210ff838dfcd5352db7", "pr_number": "32701", "files_changed": ["test/test_nn.py", "torch/testing/_internal/common_device_type.py"], "labels": ["merged"]}, "fa65859270": {"title": "Re-enable non-deterministic autograd tests", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32793\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19634632\n\nPulled By: albanD\n\nfbshipit-source-id: 9dda29536c2ed4afb81ecbea471ba615241bbac2", "pr_number": "32793", "files_changed": ["test/test_autograd.py"], "labels": ["merged"]}, "cc35c876cb": {"title": "Fix backcompat for linear_relu_dynamic_fp16 (#32803)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32803\n\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32803 Fix backcompat for linear_relu_dynamic_fp16**\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D19642281\n\nPulled By: albanD\n\nfbshipit-source-id: 3b6ae4dd81bf8a70dd81ccbb02fffd7653bbd08c", "pr_number": "32803", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "9bab617b3e": {"title": "Make python version a parameterizable option for Windows CI.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32823\n\nDifferential Revision: D19642347\n\nPulled By: ezyang\n\nfbshipit-source-id: a4d461aa29a06bb7f5e5d359a2df2c90e9a4fd41", "pr_number": "32823", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/pytorch-build-params.yml", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_miniconda3.bat"], "labels": ["merged", "open source"]}, "413c0f6c29": {"title": "Fixes moving after weight norm application (#32563)", "body": "Summary:\nThis PR updates how RNNs handle their \"flat weights.\" In particular, it allows for only some flat weights to be \"materialized\" when apply is called, and it updates the flattening behavior to only apply if all flat weights are (1) materialized, (2) share a dtype and (3) are acceptable to cuDNN.\n\nOne test is modified and another created to test these changes. One practical effect of this change is that weight norm can be successfully applied to a module BEFORE that module is moved to an accelerator. Previously doing so would throw an error.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32563\n\nDifferential Revision: D19602725\n\nPulled By: mruberry\n\nfbshipit-source-id: d8f9441d17815c8c9ba15b256d4be36f784a3cf9", "pr_number": "32563", "files_changed": ["test/test_nn.py", "torch/nn/modules/rnn.py"], "labels": ["merged"]}, "a40a19ccab": {"title": "Remove GIL from RRefContext (#32807)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32807\n\nAfter this commit, RRefContext no longer depends on pybind.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19636316\n\nPulled By: mrshenli\n\nfbshipit-source-id: 88faa101c32e9019e979ae8e5da6706e49842726", "pr_number": "32807", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h"], "labels": ["merged"]}, "3d0a470d89": {"title": "Rename DispatchKey::UndefinedTensorId to Undefined (#32728)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32728\n\nIt doesn't have much to do with tensors anymore.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19628093\n\nPulled By: ezyang\n\nfbshipit-source-id: 4d57111cdf44ba347bec8a32bb5b4b47a83c1eaf", "pr_number": "32728", "files_changed": ["aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/dispatch/DispatchTable.h", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "c10/core/Backend.h", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "c10/core/UndefinedTensorImpl.cpp", "c10/test/core/DispatchKeySet_test.cpp"], "labels": ["merged"]}, "5ddd2cd92b": {"title": "Make DispatchKeyGuards accept DispatchKey::Undefined (#32729)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32729\n\nWhen working on the vmap prototype I noticed that this was helpful\nas it lets me easily initialize a no-op guard, if I need to do it\nat constructor time (which I usually do, because the guards don't\nhave move constructors).\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19628092\n\nPulled By: ezyang\n\nfbshipit-source-id: d6259a3f70d287cdac2e4a5f3984e2880f19bdc2", "pr_number": "32729", "files_changed": ["c10/core/impl/LocalDispatchKeySet.cpp"], "labels": ["merged"]}, "690d41f24e": {"title": "Centralize addition of \"always on\" dispatch keys. (#32734)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32734\n\nVariableTensorId is the only key with this treatment today,\nbut BackendSelect and CompoundOp are coming soon.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19628091\n\nPulled By: ezyang\n\nfbshipit-source-id: 250753f90528fa282af7a18d8d2f7736382754bd", "pr_number": "32734", "files_changed": ["aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "c10/core/TensorOptions.h", "c10/core/impl/LocalDispatchKeySet.cpp"], "labels": ["merged"]}, "94ddc2c462": {"title": "Resubmit more code fakefp16 mapping unification (#32798)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32798\n\nATT\n\nTest Plan: unittests\n\nReviewed By: amylittleyang\n\nDifferential Revision: D19632251\n\nfbshipit-source-id: 670004050d67415bb24392f3520afa32b64ce740", "pr_number": "32798", "files_changed": ["caffe2/opt/custom/fakefp16_transform.h", "caffe2/quantization/server/pybind.cc"], "labels": ["fb-exported", "merged"]}, "765904f1b9": {"title": "[torch] fd error check", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32797\n\nDifferential Revision: D19642262\n\nPulled By: mrshenli\n\nfbshipit-source-id: 1720812166dd583dca6d72cb7e24b65ec013a62b", "pr_number": "32797", "files_changed": ["torch/lib/c10d/FileStore.cpp"], "labels": ["merged", "open source"]}, "5380e16db9": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/73638a87956241a954f71419ef8bc224a2642d23\nhttps://github.com/facebook/folly/commit/7a83deaa8359e347674851600c68c6a574cd79c5\nhttps://github.com/pytorch/fbgemm/commit/969d173d117670fd50e2aef85c63087190d1fcf5\n\nTest Plan: n/a\n\nReviewed By: wittgenst\n\nfbshipit-source-id: 399ed7a972876727a6bfd1409667c735c406fef5", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "bcb7c22679": {"title": "[PyTorch BC] Fix the ci (#32843)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32843\n\nfix the ci by skipping aten::join\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D19650584\n\nfbshipit-source-id: 4446eef568ded334217ff9205a795daffebe41a1", "pr_number": "32843", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["fb-exported", "merged"]}, "0f0972051a": {"title": "Cudnn bn size fix (#32763)", "body": "Summary:\nShould fix https://github.com/pytorch/pytorch/issues/29744 by falling back to native batch norm implementation, if cudnn cannot execute the provided shape.\n\nShape numbers were verified for cudnn 7.6.5.32 with tensor shapes:\n```python\n# for spatial bn\nx = torch.Size([880801, 256, 5])\nx = torch.Size([65535, 256, 5])\nx = torch.Size([880801, 64, 4, 4])\nx = torch.Size([65535, 64, 4, 4])\n\n# for per-act bn\nx = torch.Size([131070, 2048])\nx = torch.Size([262136, 2048])\n```\nfor `training()` and `eval()` mode using `torch.float32` and `torch.float16`.\n\nI've increased the shape of our current smoke test to, but I can also add all use cases of the support matrix, if wanted.\n\nCC ngimel\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32763\n\nDifferential Revision: D19644328\n\nPulled By: ngimel\n\nfbshipit-source-id: c2151bf9fe6bac79b8cbc69cff517a4b0b3867aa", "pr_number": "32763", "files_changed": ["aten/src/ATen/native/Normalization.cpp", "test/test_nn.py"], "labels": ["merged", "module: cudnn", "open source", "triaged"]}, "fcf9fcedf4": {"title": "Remove needs_dynamic_casting from TensorIterator and move it to Loops.cuh (#32755)", "body": "Summary:\nRemove `needs_dynamic_casting` from TensorIterator and move it to `Loops.cuh`.\n\nThe original design of `needs_dynamic_casting` is fundamentally flawed: it injects logics into TensorIterator and uses a bunch of boolean values to test whether the dynamic casting is needed. This makes it very fragile, as the TensorIterator is so complicated and it is easy to introduce unnecessary dynamic casts. It also makes the `gpu_kernel` very unflexible, differently cases needs to manipulate TensorIterator to make it work.\n\nFor example, currently\n```python\ntorch.zeros(10, device='cuda').mul_(0.9)\n```\nneeds dynamic cast, but it shouldn't.\n\nTesting whether dynamic casting is needed could be easy: just compare the dtypes of the lambda with the dtypes of operands. If they don't match, then dynamically cast, otherwise don't cast.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32755\n\nDifferential Revision: D19644092\n\nPulled By: ngimel\n\nfbshipit-source-id: 130bb8bd78d20c2ed1bdfc9d9fb451eb0f0c7e55", "pr_number": "32755", "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/ATen/native/cuda/Loops.cuh", "c10/core/ScalarType.h"], "labels": ["merged", "open source", "triaged"]}, "bc2e05a398": {"title": "Update Docs for building PyTorch for Android.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32578\n\nReviewed By: ljk53\n\nDifferential Revision: D19588904\n\nPulled By: dreiss\n\nfbshipit-source-id: 2934752b9c5b94f2f141417669d8385be44d703b", "pr_number": "32578", "files_changed": ["android/README.md"], "labels": ["merged", "module: android", "open source", "triaged"]}, "29fabb1fbc": {"title": "make tests for empty inputs check zero parameter grads (#32820)", "body": "Summary:\nMake batch norm with empty inputs return zero parameter gradients. Now batch norm, group norm and convolutions now return zero grads for parameters, so make tests check that. Fixes some bullet points in https://github.com/pytorch/pytorch/issues/12013 (interpolate is not fixed by this PR, is being fixed in other PRs)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32820\n\nDifferential Revision: D19651470\n\nPulled By: ngimel\n\nfbshipit-source-id: 96fdd085f9b0e98e91217dd2ac1f30f9c482b8be", "pr_number": "32820", "files_changed": ["aten/src/ATen/native/Normalization.cpp", "test/test_nn.py"], "labels": ["merged"]}, "12bcfa7c77": {"title": "Remove Python dependency (toPyTuple/fromPyTuple, jitCompilationUnit, deserialize) in rref_impl.h/cpp (#32753)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32753\n\nFunctions to be bound as an Aten operator could not have Python dependency.\n\nThis is to refactor and remove Python dependency.\nghstack-source-id: 97485800\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork -- test_script_functions_not_supported\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork\n\nbuck-out/gen/caffe2/test/distributed/rpc/rpc_fork\\#binary.par -r test_script_functions_not_supported\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:dist_autograd_fork\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:dist_autograd_fork\n\nbuck-out/gen/caffe2/test/distributed/rpc/dist_autograd_fork\\#binary.par -r test_backward_simple_script_call\n```\n\nDifferential Revision: D5741675\n\nfbshipit-source-id: 31ee60955be8d815d0773f3699e3ff2f1f9d8849", "pr_number": "32753", "files_changed": ["caffe2/CMakeLists.txt", "tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/distributed/rpc/torchscript_functions.cpp", "torch/csrc/distributed/rpc/torchscript_functions.h", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "c3b4bfcfed": {"title": "Add knobs to set the number of profiling runs and bailout depth (#32735)", "body": "Summary:\nDiagnostic API to simplify debugging and experiments.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32735\n\nDifferential Revision: D19626708\n\nPulled By: Krovatkin\n\nfbshipit-source-id: aa8c0da94d4559329fd7c8093329aea4e0271b6a", "pr_number": "32735", "files_changed": ["torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/graph_executor.h", "torch/csrc/jit/init.cpp", "torch/csrc/jit/profiling_graph_executor_impl.cpp", "torch/csrc/jit/profiling_record.cpp"], "labels": ["jit", "merged"]}, "03557a9838": {"title": "Make save_for_lite_interpreter private (#32771)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32771\n\nIt's a patch to #32621, make the api private.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19657307\n\nPulled By: iseeyuan\n\nfbshipit-source-id: e604a0cbed6a1e61413daaafc65bea92b90f1f5d", "pr_number": "32771", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "ed10408cc6": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/a3394d248c6ec0cc7e2bcf87ee6b8ec05ffc8405\nhttps://github.com/facebook/fbzmq/commit/91f92d0106c5b82b81c27c2c4a41e6324d4f7d54\nhttps://github.com/facebook/folly/commit/e50c78af577136428329c292638d529d97b6c485\nhttps://github.com/facebook/wangle/commit/d49bb54c3d25415ff3b8ada1e23a183b16bb5dad\nhttps://github.com/facebookincubator/fizz/commit/504fda5cda29bef4650e61bbfedc106d69ebda1f\nhttps://github.com/facebookincubator/katran/commit/42086f876460a1fe79cbf609308ffd2d2eaf6b71\nhttps://github.com/facebookincubator/mvfst/commit/d5b454a9c0f6d2f4b703fd35ec0e399cd2045c44\nhttps://github.com/pytorch/fbgemm/commit/0e31e0a8b00fad8816478fc83a92181567d36756\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 7ce9d3444d653c6889ffe080425aa082c33f137a", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "22466552e3": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/edc4a4f551070fee5303ba601602bb58b8108d8c\nhttps://github.com/facebook/folly/commit/72c71129647d3f12402364e818336125d911fa8d\nhttps://github.com/pytorch/fbgemm/commit/62c8286307f7abf56ff75df4d07529dc94094268\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 92dd070a28091dda81e315591d6d12cddfecf00f", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "b16dab8a41": {"title": "Coding header is better specified in lowercase letters (#32850)", "body": "Summary:\nThe Python document <https://www.python.org/dev/peps/pep-0263/> gives\nall examples using lowercase letters. Although it doesn't say\nstraightly, the following paragraph seems to indicate that uppercase\nletters aren't legitimate:\n\n> If a source file uses both the UTF-8 BOM mark signature and a magic encoding comment, the only allowed encoding for the comment is 'utf-8'.  Any other encoding will cause an error.\n\nMy Emacs also complains about the uppercase letters every time I save\nthe file.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32850\n\nDifferential Revision: D19663281\n\nPulled By: ezyang\n\nfbshipit-source-id: 48127d3c2fd6e22dd732a2766913735136ec2ebc", "pr_number": "32850", "files_changed": ["test/test_jit.py"], "labels": ["merge-this-please", "merged", "open source"]}, "fd3bd7777d": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/01fc273e29aa5323bf7e70ae25cf0ce02c89a642\nhttps://github.com/facebook/fbthrift/commit/53222db22237cc20b1d4353618ce36f1234452c2\nhttps://github.com/facebook/folly/commit/dea724242e58fc5af9c01a49690d54f29258f231\nhttps://github.com/facebook/mcrouter/commit/3dd493b1667b3ba7bf8af04409a7b8da4e8a130d\nhttps://github.com/facebook/rocksdb/commit/ec496347bc172b677eaf5f5645f0dbbea4ca8ac1\nhttps://github.com/pytorch/fbgemm/commit/03f4ec299e06c4c940b557f0577261a5499344f4\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: e362b5df2099f1c3dd2ef7702d4bbd5bb85e4b27", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "d9e99ab544": {"title": "Loops.cuh legacy code cleanup -- gpu_kernel_with_index (#32777)", "body": "Summary:\nI didn't see any use case where the functor of `gpu_kernel_with_index` needs to have argument other than the index. Merge conflict with https://github.com/pytorch/pytorch/pull/32755.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32777\n\nDifferential Revision: D19646381\n\nPulled By: ngimel\n\nfbshipit-source-id: 81d2be74170457e39943274e3689845e83758bfa", "pr_number": "32777", "files_changed": ["aten/src/ATen/native/cuda/Loops.cuh"], "labels": ["merged", "open source", "triaged"]}, "ada966b7d7": {"title": "[pytorch] avoid `thread_local std::vector<Call>` for mobile build (#32849)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32849\n\nWe learned that Android NDK's gcc + gnustl combination might produce a\nuse-after-free for thread_local variables with non-trivial destructors.\n\nThis PR removes such a thread_local use case from error_report.cpp for mobile build,\nwhich is the only case included in mobile lite-JIT build.\nghstack-source-id: 97491327\n\nTest Plan: - CI\n\nReviewed By: dreiss\n\nDifferential Revision: D19652702\n\nfbshipit-source-id: ee8d316ad5c6e6c8a8006eb25f3bba1618dd7e6d", "pr_number": "32849", "files_changed": ["torch/csrc/jit/script/error_report.cpp"], "labels": ["jit", "merged"]}, "660a93c558": {"title": "Code cleaning: Some iterating variables in builtin_functions.cpp can be const (#32852)", "body": "Summary:\nTo suppress a clang-tidy warning:\n\n    torch/csrc/jit/script/builtin_functions.cpp#L89\n\n    [performance-for-range-copy] warning: loop variable is copied but only\n    used as const reference; consider making it a const reference\n\nAlso make the const qualifier of scalar explicit.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32852\n\nDifferential Revision: D19663277\n\nPulled By: ezyang\n\nfbshipit-source-id: f4ec5688d3cbea9a5f40db6063b7d111b0bf0cce", "pr_number": "32852", "files_changed": ["torch/csrc/jit/script/builtin_functions.cpp"], "labels": ["jit", "merged", "open source"]}, "1760d5b83c": {"title": "Remove wrap_dim from codegen layer. (#32738)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32738\n\nThis is to simplify the codegen layer, with the goal of making it simple enough to just check in.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19610927\n\nPulled By: gchanan\n\nfbshipit-source-id: 760734f579b1f655775e6d270918c361985f3743", "pr_number": "32738", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/nn.yaml", "aten/src/ATen/nn_parse.py", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/THC/THCTensorIndex.cu", "aten/src/THC/THCTensorScatterGather.cu", "aten/src/THC/THCTensorTopK.cu", "aten/src/THC/generic/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorMathReduce.cu", "aten/src/THC/generic/THCTensorMathScan.cu", "aten/src/THC/generic/THCTensorScatterGather.cu", "aten/src/THC/generic/THCTensorSort.cu", "aten/src/THC/generic/THCTensorTopK.cu", "aten/src/THCUNN/GatedLinearUnit.cu", "aten/src/THCUNN/generic/GatedLinearUnit.cu", "aten/src/THNN/generic/GatedLinearUnit.c"], "labels": ["merged"]}, "8ddd5bb0e9": {"title": "Don't serialize None values in observer (#32733)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32733\n\nSimilar to https://github.com/pytorch/pytorch/pull/32318, we should stop serializing None values since they can't be broadcasted\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19611586\n\nPulled By: jerryzh168\n\nfbshipit-source-id: 369881de0567ed8eb25bdada892227f49bb5b29d", "pr_number": "32733", "files_changed": ["torch/quantization/observer.py"], "labels": ["merged"]}, "7b65acdf9e": {"title": "Solves Issue #32750 - torch.prod now works fine with FP16 Input Tensor and FP32 Output Tensor (#32831)", "body": "Summary:\nThis PR solves Issue https://github.com/pytorch/pytorch/issues/32750.\n\n- Changes function prod_kernel_impl to use `out_t` argument instead of `scalar_t` (which caused the garbage output for FP16 input and FP32 output tensor type).\n- Adds test case for `torch.prod` (for CUDA): tests both `torch.prod` and `torch.tensor.prod`. Checks all the combinations for dtypes: `torch.float16` and `torch.float32`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32831\n\nDifferential Revision: D19664666\n\nPulled By: ngimel\n\nfbshipit-source-id: c275363355c832899f10325043535949cd12b2f8", "pr_number": "32831", "files_changed": ["aten/src/ATen/native/cuda/ReduceOpsKernel.cu", "test/test_torch.py"], "labels": ["merged", "open source"]}, "fbe121e395": {"title": "Quantized sigmoid function", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31851\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19280716\n\nPulled By: z-a-f\n\nfbshipit-source-id: f47d37e32a675756fcaca293e2c14f90c43891de", "pr_number": "31851", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qsigmoid.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/test_quantized.py"], "labels": ["merged"]}, "fe01376ffe": {"title": "[JIT] namedtuple constants (#32873)", "body": "Summary:\nIf there was a namedtuple with immutable constant inputs, that was also the input / output of a function which expected a namedtuple it would fail. Fix by using namedtuple constructor on serialization. (no one has run into this bug yet).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32873\n\nDifferential Revision: D19668807\n\nPulled By: eellison\n\nfbshipit-source-id: bae33506e53b6a979b4e65a3e7c989b1408c98f4", "pr_number": "32873", "files_changed": ["test/test_jit_py3.py", "torch/csrc/jit/passes/python_print.cpp"], "labels": ["jit", "merged"]}, "d03c9aaa05": {"title": "Fix upsampling test case on ppc (#32786)", "body": "Summary:\nPower and x86 are giving slightly different results when scaling images up using `torch.nn.functional.interpolate` and when using OpenCV's `resize`. This is causing `test_upsampling_not_recompute_scale_factor` to fail on Power, but not x86. This changes the expected value to what OpenCV on Power produces if the test case is running on Power as well.\n\nSee https://github.com/pytorch/pytorch/issues/31915\n\nezyang\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32786\n\nDifferential Revision: D19672053\n\nPulled By: ezyang\n\nfbshipit-source-id: 3497f852bdc6d782646773792f9107c857c7b806", "pr_number": "32786", "files_changed": ["test/test_nn.py"], "labels": ["merged", "open source"]}, "ad78c0f4fc": {"title": "Fixed the flaky test_rref_context_debug_info (#32749)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32749\n\nThe test was flaky since the message from owner RRef confirming fork would arrive after the test checked whether the pending User RRefs map was empty - leading to an assertion error. This diff creates a utility function that should be used by any test to wait for this message to complete processing before doing any assertions related to the pending User RRefs map.\n\nGitHub Issue: https://github.com/pytorch/pytorch/issues/30988\n\nTest Plan: Stress tested `test_rref_context_debug_info` 200 times.\n\nDifferential Revision: D19612289\n\nfbshipit-source-id: 57a7c19b1cf792b94c263d3efbbbb6da60c07d07", "pr_number": "32749", "files_changed": ["torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["fb-exported", "merged"]}, "10bd21d550": {"title": "[JIT] fix nested select assign (#32877)", "body": "Summary:\nFix for https://github.com/pytorch/pytorch/issues/31902\n\n```\nself.sub.a = 1\n ```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32877\n\nDifferential Revision: D19670322\n\nPulled By: eellison\n\nfbshipit-source-id: 6d8f350b4d1169be1d2a56050fccd7c246ad9212", "pr_number": "32877", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/compiler.cpp"], "labels": ["jit", "merged"]}, "4493b10500": {"title": "[PyTorch] Gate out mobile operator logging observer.", "body": "Summary: Introduce separate gating for mobile operator logging observer.\n\nReviewed By: ljk53\n\nDifferential Revision: D19665993\n\nfbshipit-source-id: b81a228c55110a02edb8c2b6f9fd02e750b2ad69", "pr_number": null, "files_changed": ["torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/observer.h"], "labels": []}, "a8d39a7937": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/e0fd90427fd2f5dbb3c385ff6b49d691bffa15c6\nhttps://github.com/facebook/fbzmq/commit/c892e21dc629ef425acc59096aecb9e23d6635e5\nhttps://github.com/facebook/folly/commit/3cdc99f2b29f398cd7b864f49e7e09f80274a899\nhttps://github.com/facebook/rocksdb/commit/800d24ddc54a83220a55d662330840c0d2eae58f\nhttps://github.com/facebook/wangle/commit/74326cdb3c7be5a74d0049717b64246ac68a6e63\nhttps://github.com/facebookincubator/fizz/commit/e4af160c09105ef60213162f3ab3035f883a22c3\nhttps://github.com/facebookincubator/katran/commit/6c2fb05f6d3114b9124fa0894f78c359162cf75c\nhttps://github.com/facebookincubator/profilo/commit/a0555ecf377dac502f8d673323e3a6d6792333e9\nhttps://github.com/pytorch/fbgemm/commit/e4122f77fced9440c3ea54a6ceff22804ec0dc4c\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 9e3e0a7231c3e5cc0167cd935541dd7a8a4ea84d", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "144eb59756": {"title": "[rpc] don't crash callee when function does not exist on it, instead return Exception (#32726)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/27368.\nPreviously, if a function `'func` did not exist on worker A but existed in B, and the user ran `rpc.rpc_sync(A,  func)`, A would crash with a segmentation fault since it is not able to find the function. B would eventually timeout since RPCs by default time out in 60s.\n\nAt the root this comes from an unhandled exception when trying to deserialize the `PythonUDF` to run.\n\nThis PR makes it so that we can recover from this error, and A reports back a `RemoteException` to B indicating that the function was not found. Now, A will no longer crash and B can handle the exception appropriately and with more information.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32726\n\nDifferential Revision: D19648825\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 53847f4bfb68187db41c61d69ddac13613e814b4", "pr_number": "32726", "files_changed": ["torch/distributed/rpc/internal.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": []}, "4d7ab255d3": {"title": "[PyTorch][TorchScript] Add support for join on List of strings in TorchScript (#32847)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32847\n\nAdd support for join on List of strings in TorchScript.\n\nTest Plan:\n(pytorch) smummadi@smummadi-mbp pytorch % python test/test_jit_string.py\nFail to import hypothesis in common_utils, tests are not derandomized\n.\nRan 1 test in 1.090s\nOK\n\nDifferential Revision: D19650809\n\nfbshipit-source-id: 387a8f0e3cc3111fd3dadd3d54c90fc8c7774cf9", "pr_number": "32847", "files_changed": ["test/test_jit_string.py", "torch/csrc/jit/register_string_ops.cpp"], "labels": ["fb-exported", "jit"]}, "040bc1d0e1": {"title": "[JIT] make is_scripting a condvalue (#32871)", "body": "Summary:\nAdd `torch.jit.is_scripting` to the list of CondValues, or values that if they are an input to a if statement we only compile one side of the if. I'm not sure if we actually want this PR.\n\nPros:\n- Makes it easier to add features that are not yet supported in TorchScript (like has_torch_function)\n- The current idiom of writing `torch.jit.is_scripting` and factoring out the block to a function annotated with `torch.jit.ignore` is functionally equivalent and much more cumbersome\n\nCons:\n- Makes it easier to add features that are not yet supported in TorchScript\n- Perhaps is confusing as a reader what is being compiled. Potentially could give all caps name or otherwise change name to make it more visually stand out.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32871\n\nDifferential Revision: D19670383\n\nPulled By: eellison\n\nfbshipit-source-id: 5257b0bd23c66f199d59a7f2c911e948301e5588", "pr_number": "32871", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/test_jit.py", "torch/csrc/jit/script/compiler.cpp"], "labels": ["jit"]}, "c83f984906": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/folly/commit/5adba3596a37efd9e36fac324c397e0a351e2dd8\nhttps://github.com/facebook/folly/commit/d8b4f2ff667fd2607825572e5febb2c97bc183aa\nhttps://github.com/facebook/proxygen/commit/daa254211ae17999eeec4667508b160da6deb95d\nhttps://github.com/facebookincubator/mvfst/commit/9c4684ff10c9842450ebc8ad3bd38086d85a70a8\nhttps://github.com/pytorch/fbgemm/commit/fdb82b21cb0fe477ebf608cc78200fcaf4fb41a6\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 4e74f7e888cc2004ba937d3bb253645fbd2388c5", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "ce07fb26c0": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/proxygen/commit/3f4acb24bbe88b0229d26a5ea233e8e09a31d8f1\nhttps://github.com/pytorch/fbgemm/commit/930ea235485163db614b7d29577e70451fd5aa52\nhttps://github.com/pytorch/fbgemm/commit/c0c5daf3db8afd96f386746b5d475d01f6bebddd\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 878178c5412375d74e7f64d7e4142f57ddbc931f", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "4cc6e6bbbe": {"title": "Adding scalar to the c10 registration type check", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32886\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19673484\n\nPulled By: z-a-f\n\nfbshipit-source-id: ea8478a4fe6788dcb044ec1ab7d51dc50ab3fa60", "pr_number": "32886", "files_changed": ["torch/csrc/jit/register_c10_ops.cpp"], "labels": ["jit", "merged"]}, "b564eaf7a8": {"title": "Bug fixes: torch::tensor(floating-point values) -> default dtype, and torch::tensor(integer values) ->at::kLong (#32367)", "body": "Summary:\nSome of the `torch::tensor` behavior is updated to better match Python API. Fixes https://github.com/pytorch/pytorch/issues/32234.\n\nThis PR is BC-breaking in the following way:\n- `torch::tensor({1.0f, 2.0f})`: float -> default dtype\n- `torch::tensor(at::ArrayRef<int>({1, 2, 3}))`: int -> at::kLong\n- `torch::tensor(std::vector<int>({1, 2, 3}))`: int -> at::kLong\n- `torch::tensor(at::ArrayRef<float>({1.f, 2.f, 3.f}))`: float -> default dtype\n- `torch::tensor(std::vector<float>({1.f, 2.f, 3.f}))`: float -> default dtype\n- `torch::tensor(at::ArrayRef<double>({1., 2., 3.}))`: double -> default dtype\n- `torch::tensor(std::vector<double>({1., 2., 3.}))`: double -> default dtype\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32367\n\nDifferential Revision: D19498484\n\nPulled By: yf225\n\nfbshipit-source-id: 19c8dc2a56476266153cff4c404e7f84d309eb12", "pr_number": "32367", "files_changed": ["aten/src/ATen/test/pow_test.cpp", "test/cpp/api/tensor.cpp", "tools/autograd/templates/variable_factories.h", "torch/csrc/api/include/torch/detail/TensorDataContainer.h"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "71ad88199a": {"title": "Clarify the searched string is displayed in the error message", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32789\n\nDifferential Revision: D19646635\n\nPulled By: suo\n\nfbshipit-source-id: 18233fee7c75f7da2a1826fb66f78a519e6d9c77", "pr_number": "32789", "files_changed": ["torch/csrc/jit/testing/file_check.cpp"], "labels": ["jit", "merged", "open source"]}, "ff0ba563d5": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/6eb4ee98ba5975a253d20078e219921e6903734d\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 74dda0be26516756cd4d4d2df2167392fc48074a", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "f8dd65f2a1": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/e384ddc18637bd5e477afc50e1496346ed3800ba\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 18d4371821439388a6b546a1953c31856c80ec85", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "29e6f13cd1": {"title": "Enable MKL on MacOS if installed (#32905)", "body": "Summary:\nFix cmake script that missed MKL directories\n\nSigned-off-by: caozhong <zhong.z.cao@intel.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32905\n\nDifferential Revision: D19688496\n\nPulled By: ezyang\n\nfbshipit-source-id: d04a608eea5f983e153a48b0b1eb0390aebbe6c0", "pr_number": "32905", "files_changed": ["cmake/Modules/FindMKL.cmake"], "labels": ["merge-this-please", "merged", "module: binaries", "module: osx", "open source"]}, "e87887ccb4": {"title": "Update type hints for torch.optim.optimizer.Optimizer (#32900)", "body": "Summary:\nThis PR fixes type hints for `torch.optim.optimizer.Optimizer` object, issue also reported in https://github.com/pytorch/pytorch/issues/23731\n\nTo test things I used following optimiser implementation, that is fully covered with type hints:\n\n```python\nfrom typing import Optional, Callable, Union, Iterable\n\nfrom torch import Tensor\nfrom torch.optim.optimizer import Optimizer\n\nOptClosure = Optional[Callable[[], float]]\n_params_t = Union[Iterable[Tensor], Iterable[dict]]\n\nclass SGD(Optimizer):\n    def __init__(self, params: _params_t, lr: float = 0.1) -> None:\n        defaults = dict(lr=lr)\n        super(SGD, self).__init__(params, defaults)\n\n    def __setstate__(self, state: dict) -> None:\n        super(SGD, self).__setstate__(state)\n\n    def step(self, closure: OptClosure = None) -> Optional[float]:\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                p.data.add_(-group['lr'], d_p)\n        return loss\n```\n\nWithout fix `mypy` reports bunch of inconsistencies in types and missing properties:\n\n```bash\n$ mypy  torch_optimizer/sgd.py\ntorch_optimizer/sgd.py:14: error: Too many arguments for \"__init__\" of \"Optimizer\"\ntorch_optimizer/sgd.py:17: error: \"__setstate__\" undefined in superclass\ntorch_optimizer/sgd.py:19: error: Return type \"Optional[float]\" of \"step\" incompatible with return type \"None\" in supertype \"Optimizer\"\ntorch_optimizer/sgd.py:24: error: \"SGD\" has no attribute \"param_groups\"\nFound 4 errors in 1 file (checked 1 source file)\n```\n\nwith fix not issues:\n```bash\n$ mypy  torch_optimizer/sgd.py\nSuccess: no issues found in 1 source file\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32900\n\nDifferential Revision: D19697175\n\nPulled By: ezyang\n\nfbshipit-source-id: d5e2b3c421f69da3df8c32b3d53b4b6d15d61a41", "pr_number": "32900", "files_changed": ["torch/optim/optimizer.pyi"], "labels": ["merged", "open source", "triaged"]}, "7101f6b5c0": {"title": "Properly handle NaN in binary max and min (#32541)", "body": "Summary:\nThe output depends asymmetrically on whether the first or the second\nargument is NaN. See https://github.com/pytorch/pytorch/issues/25016 for detail of the issue.\n\nThis is part of a continuing effort that was dropped in https://github.com/pytorch/pytorch/issues/30851\n\nThe failure in https://github.com/pytorch/pytorch/issues/27185 is resolved by explicitly casting a half type number to float when applying `isnan`.\n\nClose https://github.com/pytorch/pytorch/issues/25016\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32541\n\nDifferential Revision: D19644643\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 8d49e6ed5a9996a817df7a9419dc5eee601430bc", "pr_number": "32541", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryCompareKernel.cu", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "e085c55e53": {"title": "Fix `\\\\` warnings/errors when building optim documentation (#32911)", "body": "Summary:\nThis PR fixes the warnings and errors attributed to the use of `\\\\` outside of a proper environment. While rendered correctly in the documentation, it produces the warning\n```\nLaTeX-incompatible input and strict mode is set to 'warn': In LaTeX, \\\\ or \\newline does nothing in display mode [newLineInDisplayMode]\n```\non the CI tools and errors with\n```\nParseError: KaTeX parse error: Expected 'EOF', got '\\\\' at position (x): ...\n```\nwhen not set to warn.\n\nThis PR also makes minor formatting adjustments. The `CosineAnnealingLR` documentation has been adjusted to remove an unnecessarily large fraction and to improve spacing. The `SGD` documentation has been adjusted so that variables are consistently typeset and so that it follows the convention of punctuating equations. I attached images of the current documentation, the new documentation and a marked version to highlight differences.\n\n* SGD:\nNew: ![new_sgd](https://user-images.githubusercontent.com/53704971/73596383-98795500-44d6-11ea-97ce-bac02a0a1638.png)\nCurrent: ![current_sgd](https://user-images.githubusercontent.com/53704971/73596384-98795500-44d6-11ea-86d3-b407cebbb513.png)\nMarked new: ![marked_sgd](https://user-images.githubusercontent.com/53704971/73596385-98795500-44d6-11ea-9e06-9ac5e5e27270.png)\n\n* CosineAnnealingLR:\nNew: ![new_calr](https://user-images.githubusercontent.com/53704971/73596382-98795500-44d6-11ea-9c90-02406d297bae.png)\nCurrent: ![current_calr](https://user-images.githubusercontent.com/53704971/73596387-9911eb80-44d6-11ea-93fb-ee72d695312a.png)\nMarked new: ![marked_calr](https://user-images.githubusercontent.com/53704971/73596386-9911eb80-44d6-11ea-91a6-ed7a62b4e255.png)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32911\n\nDifferential Revision: D19697114\n\nPulled By: ezyang\n\nfbshipit-source-id: 567304bd4adcfa4086eae497cb818cf74375fe5d", "pr_number": "32911", "files_changed": ["torch/optim/lr_scheduler.py", "torch/optim/sgd.py"], "labels": ["merged", "open source"]}, "c841ab403c": {"title": "add missing method annotations to torch.Tensor (#30576)", "body": "Summary:\nLooks like some of the tensor methods defined in https://github.com/pytorch/pytorch/blob/master/torch/tensor.py#L393 were missing.\n\nAlso add missing self object to `map_`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30576\n\nDifferential Revision: D19698355\n\nPulled By: ezyang\n\nfbshipit-source-id: 6df99f17d5de11715dbe89aecb292612405c08ac", "pr_number": "30576", "files_changed": ["tools/pyi/gen_pyi.py"], "labels": ["merged", "open source"]}, "b34e0dda24": {"title": "Emit the C++ version when compiling pytorch from source. (#32819)", "body": "Summary:\nThe need for this is felt because sometimes we change a build script and change the `std=c++XX` flag, which does not get caught until the compilation has progressed for a while.\n\nhttps://github.com/pytorch/pytorch/issues/31757\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32819\n\nDifferential Revision: D19697205\n\nPulled By: ezyang\n\nfbshipit-source-id: b045a1d15e24c4c6007b5d1464756051d32bf911", "pr_number": "32819", "files_changed": ["aten/src/ATen/Version.cpp"], "labels": ["merged", "open source"]}, "7cddc302e5": {"title": "min, max: check that operand and outputs are on the same device type (#32862)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/32001\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32862\n\nDifferential Revision: D19695935\n\nPulled By: ezyang\n\nfbshipit-source-id: bb37eb7a187214aa69259828024366f479a258d7", "pr_number": "32862", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "test/test_torch.py"], "labels": ["merged", "open source"]}, "ef50161ec9": {"title": "[JIT] Update OVERVIEW.md", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28870\n\nDifferential Revision: D19698758\n\nPulled By: ezyang\n\nfbshipit-source-id: 23167ec5bf9f7ab81012a124206bb4c2bdd6ca06", "pr_number": "28870", "files_changed": ["torch/csrc/jit/docs/OVERVIEW.md"], "labels": ["jit", "merged", "open source"]}, "10183061eb": {"title": "[ONNX] Update ONNX landing page since 1.3 (#32805)", "body": "Summary:\n* New ops supported for exporting.\n* Updates on support for tensor indexing and dynamic list of tensors.\n* lara-hdr, spandantiwari Should we also include updates on torchvision support in this page?\n\ncc houseroad, neginraoof Please review if I have missed anything.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32805\n\nReviewed By: hl475\n\nDifferential Revision: D19635699\n\nPulled By: houseroad\n\nfbshipit-source-id: b6be4fce641f852dcbceed20b4433f4037d8024a", "pr_number": "32805", "files_changed": ["docs/source/onnx.rst"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "3fa907c145": {"title": "[docs] Fix argument type of torch.masked_select (#30385)", "body": "Summary:\nThis should be `BoolTensor`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30385\n\nDifferential Revision: D19698414\n\nPulled By: ezyang\n\nfbshipit-source-id: 68f1e10eb9d4b99552bb158f6ad7e6ff0f7cc1c4", "pr_number": "30385", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "37953d92d1": {"title": "raise when jit-load.ing a folder (#27836)", "body": "Summary:\nVery similar to https://github.com/pytorch/pytorch/issues/16267 but handling directories.\n\nStoked to contribute!\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27836\n\nDifferential Revision: D19698398\n\nPulled By: ezyang\n\nfbshipit-source-id: eabc3a44d258124f860babb47ab91e22c2c3d6cc", "pr_number": "27836", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged", "open source"]}, "00c6b90327": {"title": "Fix in documentation of convolutional modules (#30079)", "body": "Summary:\nI noticed the description of the initialization of convolutional modules is inconsistent with the actual implementation. There are two such cases:\n\n1) `k` in the initialization of ConvTranspose modules is not dependent on the input channels but on the output channels (`kaiming_uniform_` uses the size of the second dimension of `weight` which is transposed in the first two dimensions).\n\n2) Both the normal convolutions and the transposed ones use `k` divided by `groups`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30079\n\nDifferential Revision: D19698511\n\nPulled By: ezyang\n\nfbshipit-source-id: 1ba938fbbd97663eaf29fd1245872179d2761fff", "pr_number": "30079", "files_changed": ["torch/nn/modules/conv.py"], "labels": ["merged", "open source", "triaged"]}, "14c15eb3b0": {"title": "Py2 -> py3 for caffe2/caffe2/contrib/tensorboard (#32882)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32882\n\nUpdate tensorboard binary and unit tests to python 3\n\nTest Plan:\n```\n> buck test //caffe2/caffe2/contrib/tensorboard:tensorboard_test\n```\n```\n> buck test //caffe2/caffe2/contrib/tensorboard:tensorboard_exporter_test\n```\n\nReviewed By: sanekmelnikov\n\nDifferential Revision: D19670873\n\nfbshipit-source-id: f5eb65ccbb4ecfdc801b9fa05a60d4c5c29dc428", "pr_number": "32882", "files_changed": ["caffe2/contrib/tensorboard/tensorboard.py"], "labels": ["fb-exported", "merged"]}, "48eff08256": {"title": "Fix the level of headers in pytorch/CONTRIBUTING.md (#28412)", "body": "Summary:\n**Running Clang-Tidy**, **Pre-commit Tidy/Linting Hook**, **Building PyTorch with ASAN** shouldn't belong to **Windows development tips**.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28412\n\nDifferential Revision: D19700228\n\nPulled By: ezyang\n\nfbshipit-source-id: 39d999c68e4bd9264f4ae1fdab517871c883a663", "pr_number": "28412", "files_changed": ["CONTRIBUTING.md"], "labels": ["merged", "open source"]}, "167a892e99": {"title": "Add missing `shuffle` attribute to DistributedSampler typing file", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28763\n\nDifferential Revision: D19698808\n\nPulled By: ezyang\n\nfbshipit-source-id: 7820acd7b0715ebf1d9ae954dca0058b6759075e", "pr_number": "28763", "files_changed": ["torch/utils/data/distributed.pyi"], "labels": ["merged", "open source"]}, "e03e4f3a2d": {"title": "[ONNX] Add einsum export (#32716)", "body": "Summary:\nAdding symbolic for onnx einsum as part of opset 12\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32716\n\nReviewed By: hl475\n\nDifferential Revision: D19626168\n\nPulled By: houseroad\n\nfbshipit-source-id: d8cc8af5f05f36aca3cd55dead602261ccdfec51", "pr_number": "32716", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset12.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "1c42b9466b": {"title": "[ONNX] Update support of exporting bool type index mask (#32445)", "body": "Summary:\ne.g. `tensor[torch.tensor([0, 1, 0], dtype=torch.bool)]`\nPreviously the mask is of type uint8. Both uint8 and bool should be supported for export.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32445\n\nReviewed By: hl475\n\nDifferential Revision: D19610713\n\nPulled By: houseroad\n\nfbshipit-source-id: 8df636e0c3cb0b82919a689242a962c79220209c", "pr_number": "32445", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "6996f8d880": {"title": "Add missing `default_collate` in dataloader.pyi", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28935\n\nDifferential Revision: D19698781\n\nPulled By: ezyang\n\nfbshipit-source-id: abdd735c98656ed16cd326529441d1fcec2ace3e", "pr_number": "28935", "files_changed": ["torch/utils/data/dataloader.pyi"], "labels": ["merged", "open source"]}, "a751ddaaa5": {"title": "Use leaky singletons for torch.distributed. (#32923)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32923\n\nAs per\nhttps://isocpp.org/wiki/faq/ctors#construct-on-first-use-v2 and\nhttps://isocpp.org/wiki/faq/ctors#static-init-order-on-first-use-members, we\nshould be using leaky singletons to avoid static initialization order problem.\n\nCloses https://github.com/pytorch/pytorch/issues/27412\nghstack-source-id: 97601384\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19688986\n\nfbshipit-source-id: 8c1935fb7da8a7116dbca55eb43dc04bc02695ac", "pr_number": "32923", "files_changed": ["torch/csrc/distributed/autograd/context/container.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp"], "labels": ["merged"]}, "5c019fede3": {"title": "[ONNX] Fix for constant folding flaky tests (#32546)", "body": "Summary:\nFix for constant folding flaky tests\nLooks like the constant folding test modules are sometimes exported with ONNX_ATEN op export type, which is causing the CI failures.\nI'm unable to repro this issue locally, but my guess is that the op export param is being overwritten on CI build at some point.\nThis PR sets the op export type and hopefully fixes the issue.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32546\n\nReviewed By: hl475\n\nDifferential Revision: D19606919\n\nPulled By: houseroad\n\nfbshipit-source-id: 31793d6857bbbf99b43b4a7c22a045a56ae19e44", "pr_number": "32546", "files_changed": ["test/onnx/test_utility_funs.py"], "labels": ["merged", "open source"]}, "4baadd54d7": {"title": "add SpatialBN lowered fake fp16", "body": "Summary:\nSpatialBNFakeLoweredFp16NNPI\n\nthis is the fake operator for SpatialBN that gets lowered into add/mul/div, etc.\n\nTest Plan: test_spatialbn\n\nReviewed By: tracelogfb, amylittleyang\n\nDifferential Revision: D19658680\n\nfbshipit-source-id: 2abddbcd9a2023ac75c494f20eaac2051b7139dc", "pr_number": null, "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc"], "labels": []}, "9c2ed2574a": {"title": "Vectorized memory access in TensorIterator GPU loop for 1d contiguous case (#32383)", "body": "Summary:\nStep 2 of https://github.com/pytorch/pytorch/issues/31975\n\nVectorized memory access is enabled. Generated code: https://github.com/zasdfgbnm/things/blob/master/2020Q1/disassembly-elementwise-vec.ipynb\n\n```\nvoid at::native::modern::elementwise_kernel<4, 64, 4, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()https://github.com/pytorch/pytorch/issues/1}::operator()() const::{lambda()https://github.com/pytorch/pytorch/issues/4}::operator()() const::{lambda(float, float)https://github.com/pytorch/pytorch/issues/1}, at::detail::Array<char*, 3> >(int, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()https://github.com/pytorch/pytorch/issues/1}::operator()() const::{lambda()https://github.com/pytorch/pytorch/issues/4}::operator()() const::{lambda(float, float)https://github.com/pytorch/pytorch/issues/1}, at::detail::Array<char*, 3>)\n\n**ASM:**\n\n\t.section\t.text._ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,\"ax\",progbits\n\t.sectioninfo\t@\"SHI_REGISTERS=20\"\n\t.align\t128\n        .global         _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_\n        .type           _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,function\n        .size           _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,(.L_40898 - _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_)\n        .other          _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n_ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_:\n.text._ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_:\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 294\n        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;\n        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;\n        /*0020*/                   S2R R9, SR_CTAID.X ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 177\n        /*0030*/                   S2R R0, SR_TID.X ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 294\n        /*0040*/                   IMAD.SHL.U32 R9, R9, 0x100, RZ ;\n        /*0050*/                   IADD3 R5, -R9, c[0x0][0x160], RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 256\n        /*0060*/                   SHF.R.S32.HI R17, RZ, 0x1f, R9 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 296\n        /*0070*/                   ISETP.GE.AND P0, PT, R5, 0x100, PT ;\n        /*0080*/              @!P0 BRA `(.L_3173) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 256\n        /*0090*/                   IMAD.SHL.U32 R12, R9.reuse, 0x4, RZ ;\n        /*00a0*/                   SHF.L.U64.HI R17, R9, 0x2, R17 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 260\n        /*00b0*/                   IADD3 R8, P0, R12.reuse, c[0x0][0x188], RZ ;\n        /*00c0*/                   IADD3 R2, P1, R12, c[0x0][0x190], RZ ;\n        /*00d0*/                   IADD3.X R9, R17.reuse, c[0x0][0x18c], RZ, P0, !PT ;\n        /*00e0*/                   IADD3.X R3, R17, c[0x0][0x194], RZ, P1, !PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 218\n        /*00f0*/                   IMAD.WIDE R8, R0, 0x10, R8 ;\n        /*0100*/                   IMAD.WIDE R2, R0, 0x10, R2 ;\n        /*0110*/                   LDG.E.128.SYS R8, [R8] ;\n        /*0120*/                   LDG.E.128.SYS R4, [R2] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 256\n        /*0130*/                   IADD3 R12, P0, R12, c[0x0][0x180], RZ ;\n        /*0140*/                   IADD3.X R13, R17, c[0x0][0x184], RZ, P0, !PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 238\n        /*0150*/                   IMAD.WIDE R12, R0, 0x10, R12 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 196\n        /*0160*/                   FFMA R7, R7, c[0x0][0x168], R11 ;\n        /*0170*/                   FFMA R6, R6, c[0x0][0x168], R10 ;\n        /*0180*/                   FFMA R5, R5, c[0x0][0x168], R9 ;\n        /*0190*/                   FFMA R4, R4, c[0x0][0x168], R8 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 238\n        /*01a0*/                   STG.E.128.SYS [R12], R4 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 301\n        /*01b0*/                   EXIT ;\n.L_3173:\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 180\n        /*01c0*/                   ISETP.GE.AND P0, PT, R0, R5, PT ;\n        /*01d0*/                   BMOV.32.CLEAR RZ, B0 ;\n        /*01e0*/                   BSSY B0, `(.L_3174) ;\n        /*01f0*/               P0 BRA `(.L_3175) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 183\n        /*0200*/                   IADD3 R3, P1, R9, R0, RZ ;\n        /*0210*/                   LEA.HI.X.SX32 R4, R0, R17, 0x1, P1 ;\n        /*0220*/                   LEA R2, P1, R3, c[0x0][0x188], 0x2 ;\n        /*0230*/                   LEA.HI.X R3, R3, c[0x0][0x18c], R4, 0x2, P1 ;\n        /*0240*/                   LDG.E.SYS R8, [R2] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 184\n        /*0250*/                   IADD3 R4, R0, 0x40, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 180\n        /*0260*/                   ISETP.GE.AND P1, PT, R4, R5, PT ;\n        /*0270*/               P1 BRA `(.L_3175) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 183\n        /*0280*/                   LDG.E.SYS R4, [R2+0x100] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 184\n        /*0290*/                   IADD3 R6, R0, 0x80, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 180\n        /*02a0*/                   ISETP.GE.AND P1, PT, R6, R5, PT ;\n        /*02b0*/               P1 BRA `(.L_3175) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 184\n        /*02c0*/                   IADD3 R10, R0, 0xc0, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 183\n        /*02d0*/                   LDG.E.SYS R7, [R2+0x200] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 180\n        /*02e0*/                   ISETP.GE.AND P1, PT, R10, R5, PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 183\n        /*02f0*/              @!P1 LDG.E.SYS R6, [R2+0x300] ;\n.L_3175:\n        /*0300*/                   BSYNC B0 ;\n.L_3174:\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 180\n        /*0310*/                   BMOV.32.CLEAR RZ, B0 ;\n        /*0320*/                   BSSY B0, `(.L_3176) ;\n        /*0330*/               P0 BRA `(.L_3177) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 183\n        /*0340*/                   IADD3 R3, P1, R9, R0, RZ ;\n        /*0350*/                   LEA.HI.X.SX32 R10, R0, R17, 0x1, P1 ;\n        /*0360*/                   LEA R2, P1, R3, c[0x0][0x190], 0x2 ;\n        /*0370*/                   LEA.HI.X R3, R3, c[0x0][0x194], R10, 0x2, P1 ;\n        /*0380*/                   LDG.E.SYS R11, [R2] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 184\n        /*0390*/                   IADD3 R10, R0, 0x40, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 180\n        /*03a0*/                   ISETP.GE.AND P1, PT, R10, R5, PT ;\n        /*03b0*/               P1 BRA `(.L_3177) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 183\n        /*03c0*/                   LDG.E.SYS R13, [R2+0x100] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 184\n        /*03d0*/                   IADD3 R10, R0, 0x80, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 180\n        /*03e0*/                   ISETP.GE.AND P1, PT, R10, R5, PT ;\n        /*03f0*/               P1 BRA `(.L_3177) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 184\n        /*0400*/                   IADD3 R10, R0, 0xc0, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 180\n        /*0410*/                   ISETP.GE.AND P1, PT, R10, R5, PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 183\n        /*0420*/                   LDG.E.SYS R10, [R2+0x200] ;\n        /*0430*/              @!P1 LDG.E.SYS R15, [R2+0x300] ;\n.L_3177:\n        /*0440*/                   BSYNC B0 ;\n.L_3176:\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 193\n        /*0450*/               P0 EXIT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 196\n        /*0460*/                   IADD3 R9, P0, R9, R0, RZ ;\n        /*0470*/                   FFMA R11, R11, c[0x0][0x168], R8 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 197\n        /*0480*/                   IADD3 R14, R0, 0x40, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 196\n        /*0490*/                   LEA.HI.X.SX32 R12, R0, R17, 0x1, P0 ;\n        /*04a0*/                   LEA R2, P0, R9.reuse, c[0x0][0x180], 0x2 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 193\n        /*04b0*/                   ISETP.GE.AND P1, PT, R14, R5, PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 196\n        /*04c0*/                   LEA.HI.X R3, R9, c[0x0][0x184], R12, 0x2, P0 ;\n        /*04d0*/                   STG.E.SYS [R2], R11 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 193\n        /*04e0*/               P1 EXIT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 197\n        /*04f0*/                   IADD3 R8, R0, 0x80, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 196\n        /*0500*/                   FFMA R13, R13, c[0x0][0x168], R4 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 193\n        /*0510*/                   ISETP.GE.AND P0, PT, R8, R5, PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 196\n        /*0520*/                   STG.E.SYS [R2+0x100], R13 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 193\n        /*0530*/               P0 EXIT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 197\n        /*0540*/                   IADD3 R0, R0, 0xc0, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh\", line 196\n        /*0550*/                   FFMA R7, R10, c[0x0][0x168], R7 ;\n        /*0560*/                   FFMA R15, R15, c[0x0][0x168], R6 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 193\n        /*0570*/                   ISETP.GE.AND P0, PT, R0, R5, PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 196\n        /*0580*/                   STG.E.SYS [R2+0x200], R7 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 193\n        /*0590*/               P0 EXIT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 196\n        /*05a0*/                   STG.E.SYS [R2+0x300], R15 ;\n        /*05b0*/                   EXIT ;\n.L_3178:\n        /*05c0*/                   BRA `(.L_3178);\n        /*05d0*/                   NOP;\n        /*05e0*/                   NOP;\n        /*05f0*/                   NOP;\n.L_40898:\n```\n\nWe can clearly see the `LDG.E.128` in it, which is a result of vectorization.\n\nBenchmark: https://github.com/zasdfgbnm/things/blob/master/2020Q1/benchmark-vec.ipynb\n\nBenchmark on P100, dtype `uint8`:\n\nbefore:\n```\n1.4.0a0+a5b4d78\ne1d97025eeeddcf083e9bee0c8f6a53168991a71\n22.2 \u00b5s \u00b1 89.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n34.7 \u00b5s \u00b1 38.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n52 \u00b5s \u00b1 312 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n86.9 \u00b5s \u00b1 135 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n154 \u00b5s \u00b1 204 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n291 \u00b5s \u00b1 668 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n566 \u00b5s \u00b1 1.16 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.18 ms \u00b1 1.54 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n2.29 ms \u00b1 1.48 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n4.4 ms \u00b1 1.15 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\nafter:\n```\n1.4.0a0+a5b4d78\n1281cdfd8188fe86241ecaf71d001809d016c3a3\n24 \u00b5s \u00b1 116 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n30.5 \u00b5s \u00b1 355 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n43.1 \u00b5s \u00b1 300 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n67.6 \u00b5s \u00b1 113 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n116 \u00b5s \u00b1 275 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n215 \u00b5s \u00b1 142 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n413 \u00b5s \u00b1 791 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n824 \u00b5s \u00b1 891 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.63 ms \u00b1 478 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n3.19 ms \u00b1 1.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\nBenchmark on P100, dtype `half`:\n\nBefore:\n```\n1.4.0a0+a5b4d78\n1c017f0c14c91bd5125ab387a90441b0c0e2f3ad\n30.8 \u00b5s \u00b1 226 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n43.4 \u00b5s \u00b1 164 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n69.1 \u00b5s \u00b1 83 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n119 \u00b5s \u00b1 103 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n224 \u00b5s \u00b1 99.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n418 \u00b5s \u00b1 206 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n865 \u00b5s \u00b1 237 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.69 ms \u00b1 695 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n3.3 ms \u00b1 527 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n6.77 ms \u00b1 741 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\nAfter\n\n```\n1.4.0a0+a5b4d78\n7e50ee27333e7047072d328d03767b4845286356\n28.9 \u00b5s \u00b1 61.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n40.2 \u00b5s \u00b1 244 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n63.8 \u00b5s \u00b1 350 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n109 \u00b5s \u00b1 196 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n199 \u00b5s \u00b1 157 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n380 \u00b5s \u00b1 446 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n743 \u00b5s \u00b1 2.17 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.47 ms \u00b1 1.34 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n2.91 ms \u00b1 9.17 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n5.8 ms \u00b1 296 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\ncc: csarofeen ptrblck\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32383\n\nDifferential Revision: D19697455\n\nPulled By: ngimel\n\nfbshipit-source-id: 0707481c2f334e6634c000b4afd275b2fee8fbe1", "pr_number": "32383", "files_changed": ["aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/MemoryAccess.cuh", "aten/src/ATen/native/cuda/ROCmLoops.cuh", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/cuda_vectorized_test.cu", "aten/tools/run_tests.sh", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "5ca7bf453d": {"title": "Tests for verifying behaviour of BatchNorm using 0-dim batch sizes. (#32384)", "body": "Summary:\nThe `BatchNorm*` part of the issue (see gh-12013) seems to have been fixed in the master branch and these tests would make it concrete.\n\nHowever I would appreciate comments on https://github.com/pytorch/pytorch/issues/12013#issuecomment-575871264 on whether the current behaviour is satisfactory.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32384\n\nDifferential Revision: D19704154\n\nPulled By: ngimel\n\nfbshipit-source-id: 1bbbbf1ae1215a460b22cf26e6b263e518ecf60b", "pr_number": "32384", "files_changed": ["test/test_nn.py", "torch/testing/_internal/common_nn.py"], "labels": ["merged", "module: nn", "open source", "triaged"]}, "612e621da0": {"title": "Improve CHECK_OP macro (#29539)", "body": "Summary:\n- Show values in question like glog.\n- Handle expressions with logical operators properly by adding\n  parentheses around expressions.\n- Allow outputting nullptr (some build failed without this)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29539\n\nReviewed By: dreiss\n\nDifferential Revision: D19698991\n\nPulled By: ljk53\n\nfbshipit-source-id: e329c01622cfc386ac009904092519a4adfe94a8", "pr_number": "29539", "files_changed": ["c10/util/logging_is_not_google_glog.h"], "labels": ["merged", "open source", "triaged"]}, "544eab37d0": {"title": "Move deprecation warning out of generated code into python_arg_parser. (#32907)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32907\n\nAll op-specific information used in this logic was available to the\nparser itself, so the check can be done in that context, no codegen\nneeded.\n\nNo change in the warning behavior itself, mod minor formatting tweak -\npasses existing tests. Saves like ~275K binary size on mac:\n```\n-rwxr-xr-x  1 bhosmer  1876110778   16502064 Feb  1 00:43 torch/lib/libtorch_python.dylib\n-rwxr-xr-x  1 bhosmer  1876110778   16247888 Feb  1 00:44 torch/lib/libtorch_python.dylib\n```\n\n[codegen diff](https://github.com/bhosmer/scratch/compare/deprecation_warning_before...deprecation_warning_after)\n\nMore important than the size savings is the minimization of codegen. Ideally the generated artifact should express distinctive per-op properties in as minimal a form as practically possible - e.g. here instead of generating check-and-warn behavior into every binding, we generate only the data that triggers the behavior in the parser. (And actually we were generating it already.)\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19679928\n\nPulled By: bhosmer\n\nfbshipit-source-id: cf0140573118430720c6b797c762fe5be98acd86", "pr_number": "32907", "files_changed": ["tools/autograd/gen_python_functions.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": ["merged"]}, "3cac9900ca": {"title": "Clarify when softplus is reverted to linear. (#32945)", "body": "Summary:\nThe default value is removed because it is explained right below.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32945\n\nReviewed By: soumith\n\nDifferential Revision: D19706567\n\nPulled By: ailzhang\n\nfbshipit-source-id: 1b7cc87991532f69b81aaae2451d944f70dda427", "pr_number": "32945", "files_changed": ["torch/nn/functional.py", "torch/nn/modules/activation.py"], "labels": ["merged", "open source"]}, "df71b3e23a": {"title": "properly update _flat_weights in RNN modules (#32939)", "body": "Summary:\nShould fix https://github.com/pytorch/pytorch/issues/32346 hopefully. Now when _flat_weights list is updated, `None` elements are appended to it if some weights are missing, subsequent `setattr` calls for the missing weights should repair _flat_weights and make it suitable to use in the backend.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32939\n\nDifferential Revision: D19710990\n\nPulled By: ngimel\n\nfbshipit-source-id: c978c7519464e94beeffa9bc33b9172854a2f298", "pr_number": "32939", "files_changed": ["test/test_nn.py", "torch/nn/modules/rnn.py"], "labels": ["merged"]}, "b4b1b100bd": {"title": "Add a loop test for onnxified net (#32935)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32935\n\nMock away the content of onnxified net with some low cost ops so that we can still mimic the input/output transfer while doing minimal work on the card.\n\nTest Plan:\n```\nbuck run glow/fb/test:sparsenn_test -- --gtest_filter='SparseNNTest.vanillaC2' --onnxifi_debug_mode --onnxifi_loop_test_mode --nocaffe2_predictor_use_memonger\n```\n\nDifferential Revision: D19631971\n\nfbshipit-source-id: f970c55ccb410702f479255eeb750e01e3f8c2ae", "pr_number": "32935", "files_changed": ["caffe2/operators/reduce_ops.cc", "caffe2/opt/backend_transformer_base.cc", "caffe2/opt/backend_transformer_base.h", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/custom/glow_net_transform.cc", "caffe2/opt/onnxifi_transformer.cc", "caffe2/opt/onnxifi_transformer.h"], "labels": ["fb-exported", "merged"]}, "b894dc06de": {"title": "[Pytorch] Propagate errors in clearAndWaitForOutstandingRpcsAsync. (#32952)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32952\n\nWhen the Async() version of clearAndWaitForOutstandingRpcs() was written,\nwe didn't yet have the generic Future<T> class, and hadn't worked out our\nerror model fully.\n\nThis change fixes that method to properly propagate the first encountered error\nto the future, using a bool+CAS.\nghstack-source-id: 97665749\n\nTest Plan: existing test coverage, buck test mode/dev-nosan caffe2/test/...\n\nDifferential Revision: D19710337\n\nfbshipit-source-id: 66ce5593a94a16ea624930dbb9409917ef5cfd5d", "pr_number": "32952", "files_changed": ["torch/csrc/distributed/autograd/context/context.cpp"], "labels": ["merged"]}, "820410b505": {"title": "Added upsample_neartest2d op for lite interpreter. (#32913)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32913\n\nThis enables mobile detection and tracking models.\n\nTest Plan: buck test caffe2/test/cpp/jit:jit -- JitTest.LiteInterpreterUpsampleNearest2d\n\nReviewed By: iseeyuan\n\nDifferential Revision: D19664502\n\nfbshipit-source-id: 1c7270dcf394aba7b510c5aa80552c58a5038f24", "pr_number": "32913", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/mobile/register_mobile_ops.cpp"], "labels": ["fb-exported", "jit", "merged"]}, "ec2c974bd5": {"title": "Simplify some TH codegen by moving code out of the switch and killing dead code. (#32888)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32888\n\nThis kills ~1500 lines of generated code by doing the following:\n1) Stop binding _th_clone, which isn't used anymore.\n\n2) Move allocation code out of the switch, because it doesn't need to be there, example:\nNow:\n```\nauto dispatch_scalar_type = infer_scalar_type(self);\nauto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(dispatch_scalar_type), 0, allocator(), true),DispatchKey::CPUTensorId).release();\nauto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));\nswitch (dispatch_scalar_type) {\n    case ScalarType::Bool: {\n        ...\n    case ScalarType::Byte: {\n\t    ...\n```\nBefore:\n```\nauto dispatch_scalar_type = infer_scalar_type(self);\nswitch(dispatch_scalar_type) {\n    case ScalarType::Bool: {\n       \tauto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),DispatchKey::CPUTensorId).release();\n        auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));\n    case ScalarType::Byte: {\n        auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(caffe2::TypeMeta::Make<byte>(), 0, allocator(), true),DispatchKey::CPUTensorId).release();\n        auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));\n```\n\nNote there's one extra lookup from ScalarType -> TypeMeta, but that can go away once we are able to put everything in a dispatch macro.\n\n3) Prepare for more moves out of the switch by using dispatch_scalar_type where we would have used an explicit ScalarType::Name\nMore moves are currently blocked by \"real\" types needing to map scalar_type -> C++ type.  Dispatch macros can solve that, but I'll need to wrap the actual TH calls in templates so the entire\nthing can be done via dispatch.\n\n4) Kill some codegen that isn't used anymore: ALLOC_WRAP, is_actual_return_long.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19672613\n\nPulled By: gchanan\n\nfbshipit-source-id: 753f480842d11757e10182e43b471bd3abaa5446", "pr_number": "32888", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/function_wrapper.py"], "labels": ["merged"]}, "9e7c47644f": {"title": "[NHWC CUDNN CONV]Update cudnn convolution memory_format behavior (#32482)", "body": "Summary:\n1. Allows both the memory_format of weight & input to dictate the output\nmemory_format.\n2. Provides utility function to recursively convert memory_format of Conv2d and\nConvTranspose2d layers. This allows easy model conversion and ensures that lost\nmemory_format through incompatible layers could be restored at Convolution-like\nlayer, where significant performance boost is expected on later generation CUDA\ndevices.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32482\n\nDifferential Revision: D19647903\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 62c96ff6208ff5e84fae1f55b63af9a010ad199a", "pr_number": "32482", "files_changed": ["aten/src/ATen/native/ConvUtils.h", "test/test_nn.py", "torch/nn/utils/__init__.py", "torch/nn/utils/memory_format.py"], "labels": ["merged", "open source", "triaged"]}, "341fb6d11d": {"title": "Make caffe2/caffe2/python/models/seq2seq python3 compatible", "body": "Test Plan: watiforsadcastle\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D19698403\n\nfbshipit-source-id: 36b73e07e598c848abbe368e522484da9ba4c78f", "pr_number": null, "files_changed": ["caffe2/python/models/seq2seq/.python3"], "labels": []}, "aa3c871739": {"title": "Adds TestViewOps, updates documentation (#32512)", "body": "Summary:\nUnderstanding which ops return views and which return tensors with new storage is a common user issue, and an issue for developers connecting accelerators to PyTorch, too. This generic test suite verifies that ops which should return views do (and a few ops that shouldn't don't).  The documentation has also been updated for .t(), permute(), unfold(), and select() to clarify they return views.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32512\n\nDifferential Revision: D19659454\n\nPulled By: mruberry\n\nfbshipit-source-id: b4334be9b698253a979e1bb8746fdb3ca24aa4e3", "pr_number": "32512", "files_changed": ["test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/functional.py"], "labels": ["merged"]}, "e922826dda": {"title": "[pytorch] simplify lazy initialization of DefaultCPUGenerator singleton (#32897)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32897\n\nMoving the default static instance into the method to achieve the same purpose.\nghstack-source-id: 97570792\n\nTest Plan: - CI\n\nReviewed By: dreiss\n\nDifferential Revision: D19674566\n\nfbshipit-source-id: 27f54da66dd7667c34905eddaac6579e64aa1118", "pr_number": "32897", "files_changed": ["aten/src/ATen/CPUGenerator.cpp"], "labels": ["merged"]}, "d3fa68eeec": {"title": "Fix for MKL detection script on Windows (#32970)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/32914.\n1. Use `DEFINED ENV{MKLProductDir}` instead of `$ENV{MKLProductDir}`\n2. Cache `INTEL_COMPILER_DIR` and `INTEL_MKL_DIR`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32970\n\nDifferential Revision: D19727677\n\nPulled By: soumith\n\nfbshipit-source-id: 065c6bee35a2295f1c478df1460cad7668b25af5", "pr_number": "32970", "files_changed": ["cmake/Modules/FindMKL.cmake"], "labels": ["merged", "open source"]}, "e999095594": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/8f3d7019bb09b9f1fa2db86242d0018c4921327a\nhttps://github.com/facebook/mcrouter/commit/a5df50cf5cd8d28120d49233c29902d7b23267a9\nhttps://github.com/facebook/proxygen/commit/b896a52075fa1f4b30d1d64fd55a219bc20a11e6\nhttps://github.com/facebook/rocksdb/commit/3a073234da663709fcb7a479ec88ce7476c48e3a\nhttps://github.com/facebook/wangle/commit/7c05bee0551a02b1a8417399bbc12b751f831b8b\nhttps://github.com/facebookincubator/mvfst/commit/90f0aa96653dc3e9cbb9b48c99857dc0addc7ba9\nhttps://github.com/pytorch/fbgemm/commit/5cdd1abbb99a6d01354c6409340ad0822775be8b\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: 70dd062814f68bda77e119bb9deaefbf71c551e6", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "b69c685c4a": {"title": "try to find cudnn header in /usr/include/cuda (#31755)", "body": "Summary:\nWith fedora negativo17 repo, the cudnn headers are installed in /usr/include/cuda directory, along side with other cuda libraries.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31755\n\nDifferential Revision: D19697262\n\nPulled By: ezyang\n\nfbshipit-source-id: be80d3467ffb90fd677d551f4403aea65a2ef5b3", "pr_number": "31755", "files_changed": ["cmake/Modules_CUDA_fix/FindCUDNN.cmake"], "labels": ["merged", "open source"]}, "67706187fb": {"title": "Fix a broken link in contribution_guide.rst", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30814\n\nDifferential Revision: D19697403\n\nPulled By: ezyang\n\nfbshipit-source-id: b01fd0e189b3bc7ccaa197c9c64e12fee70a6310", "pr_number": "30814", "files_changed": ["docs/source/community/contribution_guide.rst"], "labels": ["merged", "open source"]}, "18d1896ba0": {"title": "Fix confusing \"does not have GPU support\" warning message (#30721)", "body": "Summary:\nMany people who use caffe2 are confused about \"does not have GPU support\" warning message.\nhttps://github.com/facebookresearch/video-nonlocal-net/issues/6\nfacebookarchive/caffe2#346\nfacebookarchive/caffe2#1634\nfacebookarchive/caffe2#197\n\nMany none GPU reasons can cause this warning message. It is better to give the error info.\n![image](https://user-images.githubusercontent.com/13826327/70129721-41175e00-16ba-11ea-85df-a4b1a1690149.png)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30721\n\nDifferential Revision: D19697413\n\nPulled By: ezyang\n\nfbshipit-source-id: bd24b7c814e7e677352068b9e9f77a68de080159", "pr_number": "30721", "files_changed": ["caffe2/python/_import_c_extension.py"], "labels": ["merged", "open source"]}, "478356aeec": {"title": "Fix broken links in governance.rst", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30815\n\nDifferential Revision: D19697401\n\nPulled By: ezyang\n\nfbshipit-source-id: d7e1a1b54039624f471b6cfb568428feb73060f4", "pr_number": "30815", "files_changed": ["docs/source/community/governance.rst"], "labels": ["merged", "open source"]}, "ea968f5cc3": {"title": "fix possible pandas import error during tensorboard tests (#29650)", "body": "Summary:\nTensorBoard tests using SummaryWriter() may fail with a pandas import\ncomplaint if TensorFlow packages are installed in the same python\nenvironment as PyTorch:\n\nTraceback (most recent call last):\n  File \"test_tensorboard.py\", line 212, in test_writer\n    with self.createSummaryWriter() as writer:\n  File \"test_tensorboard.py\", line 64, in createSummaryWriter\n    return SummaryWriter(temp_dir)\n...\n  File \"[...]/site-packages/pandas/core/arrays/categorical.py\", line 52, in <module>\n    import pandas.core.algorithms as algorithms\nAttributeError: module 'pandas' has no attribute 'core'\n\nThe exact failure may depend on the pandas version. We've also seen:\n\n  File \"[...]/site-packages/pandas/core/arrays/categorical.py\", line 9, in <module>\n    import pandas.compat as compat\nAttributeError: module 'pandas' has no attribute 'compat'\n\nThe module import chain leading to the failure is tensorboard imports\ntensorflow imports tensorflow_estimator imports pandas. pandas includes\na submodule named 'bottleneck', whose name collides with the PyTorch\n'test/bottleneck/' subdirectory.\n\nSo IF tensorboard, tensorflow, tensorflow_estimator, and pandas are\ninstalled in the python environment AND IF testing is run from within\nPyTorch's 'test/' directory (or maybe just with 'test/' in PYTHONPATH,\netc.), then TensorBoard tests using SummaryWriter() will fail.\n\nRename the 'bottleneck/' directory slightly to avoid the name collision.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29650\n\nDifferential Revision: D19698638\n\nPulled By: ezyang\n\nfbshipit-source-id: cb59342ed407cb37aefc833d67f768a8809129ac", "pr_number": "29650", "files_changed": ["test/bottleneck/test.py", "test/bottleneck/test_args.py", "test/bottleneck/test_cuda.py", "test/bottleneck_test/test.py", "test/bottleneck_test/test_args.py", "test/bottleneck_test/test_cuda.py", "test/test_utils.py"], "labels": ["merged", "open source", "triaged"]}, "d3a0bdd06b": {"title": "proofreading (#29797)", "body": "Summary:\ntwo instances of if -> it in torch.nn.modules.batchnorm.py\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29797\n\nDifferential Revision: D19698613\n\nPulled By: ezyang\n\nfbshipit-source-id: 7312b2333f227113e904dfa91db90d00e525affb", "pr_number": "29797", "files_changed": ["torch/nn/modules/batchnorm.py"], "labels": ["merged", "open source"]}, "27e1fecabd": {"title": "let user specify CUDA_HOST_COMPILER", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32904\n\nDifferential Revision: D19729047\n\nPulled By: ezyang\n\nfbshipit-source-id: c233e3924f71a025c51d25a7e3a8d728dac8730a", "pr_number": "32904", "files_changed": ["tools/setup_helpers/cmake.py"], "labels": ["merged", "open source", "triaged"]}, "b0d5ce3848": {"title": "Revert D19710990: [pytorch][PR] properly update _flat_weights in RNN modules", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19710990\n\nOriginal commit changeset: c978c7519464\n\nfbshipit-source-id: 8710bc2f4f1d01d9c93d038b59caf1e6859375dd", "pr_number": null, "files_changed": ["test/test_nn.py", "torch/nn/modules/rnn.py"], "labels": []}, "6305e4a88f": {"title": "Add warning and example for seeding to DistributedSampler (#32951)", "body": "Summary:\nCloses gh-31771\n\nAlso note that the `epoch` attribute is *only* used as a manual seed in each iteration (so it could easily be changed/renamed).  Seeding consecutive iterations with `[0, 1, 2, ...]` is low-entropy, however in practice it probably doesn't matter when using the sampler in combination with a dataloader (because there won't be enough data nor epochs to run into statistical issues\ndue to low-entropy seeding). So leaving that as is.\n\nRendered docstring:\n\n<img width=\"534\" alt=\"image\" src=\"https://user-images.githubusercontent.com/98330/73701250-35134100-46e9-11ea-97b8-3baeb60fcb37.png\">\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32951\n\nDifferential Revision: D19729333\n\nPulled By: ezyang\n\nfbshipit-source-id: 3ddf90a3828b8bbae88aa2195a5d0b7d8ee1b066", "pr_number": "32951", "files_changed": ["torch/utils/data/distributed.py"], "labels": ["merged", "open source"]}, "4f5908d5d7": {"title": "Remove unneded TORCH_API (#32015)", "body": "Summary:\nIt was causing a build error when compiling on MINGW64\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32015\n\nDifferential Revision: D19697296\n\nPulled By: ezyang\n\nfbshipit-source-id: 71e58783c48f8e99755c091b2027d59740dfca47", "pr_number": "32015", "files_changed": ["torch/csrc/jit/ir.h"], "labels": ["jit", "merged", "open source"]}, "58e8d5588a": {"title": "[ONNX] Export bitwise_not for bool (logical_not) (#28439)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/25805 (for bool tensors as in the issue)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28439\n\nDifferential Revision: D19700156\n\nPulled By: ezyang\n\nfbshipit-source-id: 0706ada6a8d259dce381ba2d009f226e14c3c14f", "pr_number": "28439", "files_changed": ["torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source"]}, "7ea6559658": {"title": "Add size checks to `torch.stack` (#32931)", "body": "Summary:\nChecks the size of each tensor passed to `torch.stack` before calling `cat` to address https://github.com/pytorch/pytorch/issues/29510. This is done in the `get_stack_input` function as that is a common path. The function now compares the size of each tensor in the TensorList to the size of the first tensor and throws an exception when the sizes are not equal.\n\nTo compare:\n```\nx = torch.zeros([1, 2])\ny = torch.zeros([1, 3])\ntorch.stack([x, y]) # Errors due to size differences\n```\nCurrent error:\n```\nRuntimeError: invalid argument 0: Sizes of tensors must match\nexcept in dimension 0. Got 2 and 3 in dimension 2 at (path)\\aten\\src\\TH/generic/THTensor.cpp:612\n```\nNew error:\n```\nRuntimeError: stack expects each tensor to be equal size, but\ngot [1, 2] at entry 0 and [1, 3] at entry 1\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32931\n\nDifferential Revision: D19700110\n\nPulled By: ezyang\n\nfbshipit-source-id: 7e18bb00fa2c137e418e340d719b6b76170b83e3", "pr_number": "32931", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["merged", "open source", "triaged"]}, "a9141dd240": {"title": "Patch `Half.h` for compiling CUDA with clang (#29027)", "body": "Summary:\nFollowing discussion: https://github.com/pytorch/pytorch/issues/28417\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29027\n\nDifferential Revision: D19698745\n\nPulled By: ezyang\n\nfbshipit-source-id: fab4be3bcbac8f3b334d7e0a56e6a790e2c6b6d8", "pr_number": "29027", "files_changed": ["c10/util/Half.h"], "labels": ["merged", "open source"]}, "bda874b480": {"title": "[rpc] throw correct Exception on local client based on the RemoteException (#32936)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32936\n\nCloses https://github.com/pytorch/pytorch/issues/32732. Currently if a\nUDF run in RPC throws an exception such as ValueError or TypeError, we wrap\nthis in a RemoteException on the callee side. When raising this on the caller\nside, we currently raise a vanilla Exception. This diff changes it so that the\ncorrect exception is thrown. Tested by changing the current rpc tests to assert\non the right type of error rather than just the base `Exception`.\nghstack-source-id: 97706957\n\nTest Plan: Modified unit test.\n\nDifferential Revision: D19700434\n\nfbshipit-source-id: e451b772ea6aecc1d2e109e67e7f932eb9151f15", "pr_number": "32936", "files_changed": ["torch/distributed/rpc/internal.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "4502d8c391": {"title": "Interpolate Float [] support in ONNX (#32554)", "body": "Summary:\nThe PR https://github.com/pytorch/pytorch/pull/31791 adds support for float[] constant, which affects some cases of ONNX interpolate support.\nThis PR adds float[] constants support in ONNX, updates interpolate in ONNX, and re-enable the disabled tests.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32554\n\nReviewed By: hl475\n\nDifferential Revision: D19566596\n\nPulled By: houseroad\n\nfbshipit-source-id: 843f62c86126fdf4f9c0117b65965682a776e7e9", "pr_number": "32554", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/nn/functional.py", "torch/onnx/symbolic_helper.py", "torch/onnx/utils.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "e4f633ba0b": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/619d2503cb07d0008b177d40e179ed83bc199d88\nhttps://github.com/facebookincubator/fizz/commit/c442208177570ddcc2a023204a7630a3ed52c1c5\nhttps://github.com/facebookincubator/katran/commit/75d9b18eba8963f1e7217337d4a32756d2b2fb83\nhttps://github.com/pytorch/fbgemm/commit/ed5142083ae42d1cc8a6c8dfcf1402d5dce0fe36\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: 11a53fea064f8e40c2a89d3068421d7cad231d00", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "d141465713": {"title": "Fix torch::allclose to handle std::numeric_limits<T>::lowest() for integral types (#32978)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32978\n\nFixes #32946\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19726013\n\nPulled By: pbelevich\n\nfbshipit-source-id: ada4aeabc8e39016d24f1a40f02fb7c56f069cd3", "pr_number": "32978", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "test/cpp/api/functional.cpp"], "labels": ["merged"]}, "bc4790b3aa": {"title": "[JIT] Trace uses of torchbind classes as module attributes (#32833)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32833\nghstack-source-id: 97736046\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19645714\n\nfbshipit-source-id: 10a7271f13c3588aea666b44b916e90ba7b3c666", "pr_number": "32833", "files_changed": ["test/cpp/jit/test_custom_class.cpp", "test/test_jit.py", "torch/csrc/jit/custom_class.cpp", "torch/csrc/jit/custom_class.h", "torch/csrc/jit/register_c10_ops.cpp", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "23a4800708": {"title": "[JIT] Make IRParser use op schema (#32854)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32854\nghstack-source-id: 97736043\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19656881\n\nfbshipit-source-id: 509d09fdbd765ca5cd153bec6440aedfb4e6d23b", "pr_number": "32854", "files_changed": ["test/cpp/jit/test_alias_analysis.cpp", "torch/csrc/jit/irparser.cpp"], "labels": ["jit", "merged"]}, "f393adc0ed": {"title": "[JIT] Fix python pickle serialization for torchbind (#32878)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32878\nghstack-source-id: 97736045\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19669879\n\nfbshipit-source-id: 23ea91cffe7344d1eed014e2509983c281dd18d3", "pr_number": "32878", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/init.cpp"], "labels": ["jit", "merged"]}, "836b4c9e64": {"title": "Attempt to workaround MSVC17 static constexpr bug", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33002\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19739097\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 7ce54ddb1f56a741d88d3215b154192171c54dfa", "pr_number": "33002", "files_changed": ["torch/csrc/jit/argument_spec.cpp", "torch/csrc/jit/argument_spec.h"], "labels": ["jit", "merged"]}, "1b446aa2ee": {"title": "Expose Channel Last 3d enum", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32947\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19707716\n\nPulled By: glaringlee\n\nfbshipit-source-id: 03824769376043bc6151a4580aba27654de5077f", "pr_number": "32947", "files_changed": ["c10/core/MemoryFormat.h", "c10/core/TensorImpl.h", "torch/csrc/utils/tensor_memoryformats.cpp"], "labels": ["merged"]}, "6b0813ea5d": {"title": "Stop using dispatchTypeId to do checks for tensor list unwrap. (#32787)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32787\n\nGets rid of a longstanding TODO.  TensorList unwrap is only used for cat, which\nmeans we can assume that the inputs are dense, and do something similar to how\nwe do the dense tensor wrapping above.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19628642\n\nPulled By: ezyang\n\nfbshipit-source-id: 3264439407585fb97995a9a2302c2913efecb421", "pr_number": "32787", "files_changed": ["aten/src/ATen/Utils.h", "aten/src/ATen/function_wrapper.py"], "labels": ["merged"]}, "16c166e2ea": {"title": "Add XLAPreAutograd key for XLA use cases that need custom autograd. (#32788)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32788\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19628643\n\nPulled By: ezyang\n\nfbshipit-source-id: 7099b08eff37913144b961dda00b070bd4b939d4", "pr_number": "32788", "files_changed": ["aten/src/ATen/core/boxing/test_helpers.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "c10/core/DispatchKey.h"], "labels": ["merged"]}, "3531f99384": {"title": "Kill _th_max, _th_min overloads that aren't used.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32981\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19726831\n\nPulled By: gchanan\n\nfbshipit-source-id: 22b5b9115838360850c4ee250ed95742f3444dc8", "pr_number": "32981", "files_changed": ["aten/src/ATen/Declarations.cwrap"], "labels": ["merged"]}, "81a9046301": {"title": "Fix dispatch of argmax/argmin. (#32961)", "body": "Summary:\nThe way we currently dispatch argmax/argmin to out-of-source devices is bad and caused issues, e.g it doesn't work well when the input requires grad. https://github.com/pytorch/xla/issues/1585.\nMaking argmax/argmin dispatch at device level resolves it.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32961\n\nDifferential Revision: D19726826\n\nPulled By: ailzhang\n\nfbshipit-source-id: f7fb445fd8e7691524afcc47d24d8e6b0171d10c", "pr_number": "32961", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml"], "labels": ["merged"]}, "fbde3c05b6": {"title": "[aten] fix vector memory leak (#32478)", "body": "Summary:\nfree(y) missing.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32478\n\nDifferential Revision: D19728471\n\nPulled By: agolynski\n\nfbshipit-source-id: 73e7933c832f9c19f3fe09df76699c7b335a87bd", "pr_number": "32478", "files_changed": ["aten/src/TH/vector/VSX.cpp"], "labels": ["merged", "open source"]}, "72b9412be2": {"title": "Move some broadcasting logic away from codegen. (#32982)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32982\n\nFor masked_scatter_ and masked_fill_ (which already have manually written wrappers), move the broadcasting logic into the manually written wrappers.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19726830\n\nPulled By: gchanan\n\nfbshipit-source-id: 1f6e55e19c1314a76e43946b14d58f147c0f8204", "pr_number": "32982", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/LegacyDefinitions.cpp", "aten/src/ATen/native/cuda/LegacyDefinitions.cpp"], "labels": ["merged"]}, "e8581869f2": {"title": "Properly update _flat_weights in RNN models (#32989)", "body": "Summary:\nResubmitting https://github.com/pytorch/pytorch/issues/32939\nShould fix https://github.com/pytorch/pytorch/issues/32346 hopefully. Now when _flat_weights list is updated, None elements are appended to it if some weights are missing, subsequent setattr calls for the missing weights should repair _flat_weights and make it suitable to use in the backend.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32989\n\nDifferential Revision: D19731952\n\nPulled By: ngimel\n\nfbshipit-source-id: 2118a19840491e7ab0fef15185fad982f42795a6", "pr_number": "32989", "files_changed": ["test/test_nn.py", "torch/nn/modules/rnn.py"], "labels": ["merged"]}, "908b451efb": {"title": "Enabling the nccl/rccl test for ROCM environment (#32340)", "body": "Summary:\nEnabling the RCCL test on rocm by adding a temporary grace period to clean up.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32340\n\nDifferential Revision: D19744459\n\nPulled By: xw285cornell\n\nfbshipit-source-id: 1af3b64113a67f93e622d010ddd3020e5d6c8bc8", "pr_number": "32340", "files_changed": [".jenkins/caffe2/test.sh", "caffe2/contrib/nccl/cuda_nccl_gpu.cc"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "df1d68d52e": {"title": "[jit] fix parser for one-line functions (#32941)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32941\n\nThe Python grammar allows single-statement one-line functions. So we\nshould allow it in the string parser.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19704153\n\nPulled By: suo\n\nfbshipit-source-id: 8c06cc9c600aa2a9567b484a1ecc0360aad443e3", "pr_number": "32941", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/parser.cpp"], "labels": ["jit", "merged"]}, "ab75d64e6e": {"title": "Add ability to abort NCCL communicators from the store. (#32895)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32895\n\nWhen a particular rank calls `ncclCommAbort` on a communicator, it is\nimportant to ensure all other ranks call `ncclCommAbort` on their respective\ncommunicators. If this is not done, the other ranks could get stuck causing the\nGPU to spin with 100% utilization.\n\nTo alleviate this issue, whenever any rank calls `ncclCommAbort` we put the\nunique communicator id in the store. The NCCL watchdog thread then monitors the\nstore and aborts any communicators found in the store as \"aborted\".\n\nA few more general fixes in this PR:\n\n1) Use std::shared_ptr for the store in PrefixStore. PrefixStore was using a\nreference to the store and when that reference went out of scope the store\nobject it was holding onto was invalid. This caused a segfault in the watchdog\nthread.\n2) Enhanced logging for the watchdog thread.\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19638159\n\nfbshipit-source-id: 596cd87c9fe6d4aeaaab4cb7319cc37784d06eaa", "pr_number": "32895", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/NCCLUtils.hpp", "torch/lib/c10d/PrefixStore.cpp", "torch/lib/c10d/PrefixStore.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/test/FileStoreTest.cpp", "torch/lib/c10d/test/HashStoreTest.cpp", "torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": ["merged"]}, "74ce3a032c": {"title": "Fix some bugs with zipfile serialization (#32244)", "body": "Summary:\nStacked PRs\n * #32958 - Make zip serialization the default\n * **#32244 - Fix some bugs with zipfile serialization**\n\nIt includes the following changes:\n* Split up tests so that we can test both serialization methods\n    * Loading something within a buffer doesn't work anymore, so those tests are only on the old serialization method (it's possible but introduces a big slowdown since it requires a linear scan of the entire zipfile to find the magic number at the end)\n* Call `readinto` on a buffer if possible instead of `read` + a copy\n* Disable CRC-32 checks on read (there was some issue where miniz said the CRC was wrong but `zipinfo` and `unzip` said the zip file was fine)\n](https://our.intern.facebook.com/intern/diff/19418935/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32244\n\nPulled By: driazati\n\nReviewed By: eellison\n\nDifferential Revision: D19418935\n\nfbshipit-source-id: df140854f52ecd04236225417d625374fd99f573", "pr_number": "32244", "files_changed": ["cmake/Dependencies.cmake", "test/test_nn.py", "test/test_serialization.py", "test/test_torch.py", "third_party/miniz-2.0.8/miniz.c", "torch/csrc/generic/StorageMethods.cpp", "torch/csrc/generic/serialization.cpp", "torch/csrc/generic/serialization.h", "torch/csrc/jit/init.cpp", "torch/serialization.py", "torch/storage.py"], "labels": ["jit", "merged"]}, "1b746b95fb": {"title": "Consider hub_dir alongside TORCH_HOME env variable for storing hub models (#32844)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31944\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32844\n\nDifferential Revision: D19747566\n\nPulled By: ailzhang\n\nfbshipit-source-id: caca41a3a057d7d280d4783515aba2cc48c82012", "pr_number": "32844", "files_changed": ["test/test_utils.py", "torch/hub.py"], "labels": ["merged", "open source", "triaged"]}, "e54d954572": {"title": "[ONNX] Add flag to enable script tests (#32654)", "body": "Summary:\nThis will allow us to incrementally enable more tests for scripting as we put in fixes. houseroad spandantiwari\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32654\n\nReviewed By: hl475\n\nDifferential Revision: D19583401\n\nPulled By: houseroad\n\nfbshipit-source-id: 8dc05e4784df819c939dffdf33b00cbb80bfa364", "pr_number": "32654", "files_changed": ["test/onnx/test_pytorch_common.py", "test/onnx/test_pytorch_onnx_onnxruntime.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "6209412647": {"title": "Add option to use ninja to compile ahead-of-time cpp_extensions (#32495)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32495\n\nBackground\n------------------------------\nPreviously, ninja was used to compile+link inline cpp_extensions and\nahead-of-time cpp_extensions were compiled with distutils. This PR adds\nthe ability to compile (but not link) ahead-of-time cpp_extensions with ninja.\n\nThe main motivation for this is to speed up cpp_extension builds: distutils\ndoes not make use of parallelism. With this PR, using the new option, on my machine,\n- torchvision compilation goes from 3m43s to 49s\n- nestedtensor compilation goes from 2m0s to 28s.\n\nUser-facing changes\n------------------------------\n\nI added a `use_ninja` flag to BuildExtension. This defaults to\n`True`. When `use_ninja` is True:\n- it will attempt to use ninja.\n- If we cannot use ninja, then this throws a warning and falls back to\ndistutils.\n- Situations we cannot use ninja: Windows (NYI, I'll open a new issue\nfor this), if ninja cannot be found on the system.\n\nImplementation Details\n------------------------------\n\nThis PR makes this change in two steps. Please me know if it would be\neasier to review this if I split this up into a stacked diff.\nThose changes are:\n1) refactor _write_ninja_file to separate the policy (what compiler flags\nto pass) from the mechanism (how to write the ninja file and do compilation).\n2) call _write_ninja_file and _run_ninja_build while building\nahead-of-time cpp_extensions. These are only used to compile objects;\ndistutils still handles the linking.\n\nChange 1: refactor _write_ninja_file to seperate policy from mechanism\n- I split _write_ninja_file into: _write_ninja_file and\n_write_ninja_file_to_build_library\n- I renamed _build_extension_module to _run_ninja_build\n\nChange 2: Call _write_ninja_file while building ahead-of-time\ncpp_extensions\n- _write_ninja_file_and_compile_objects calls _write_ninja_file to only\nbuild object files.\n- We monkey-patch distutils.CCompiler.compile to call\n_write_ninja_files_and_compile_objects\n- distutils still handles the linking step. The linking step is not a\nbottleneck so it was not a concern.\n- This change only works on unix-based systems. Our code for windows\ngoes down a different codepath and I did not want to mess with that.\n- If a system does not support ninja, we raise a warning and fall back\nto the original compilation path.\n\nTest Plan\n------------------------------\n\nAdhoc testing\n- I built torchvision using pytorch master and printed out the build\ncommands. Next, I used this branch to build torchvision and looked at\nthe ninja file. I compared the ninja file with the build commands and\nasserted that they were functionally the same.\n- I repeated the above for pytorch/nestedtensor.\n\nPyTorch test suite\n- I split `test_cpp_extensions` into `test_cpp_extensions_aot` and\n`test_cpp_extensions_jit`. The AOT (ahead-of-time) version tests\nahead-of-time and the JIT version tests just-in-time (not to be confused\nwith TorchScript)\n- `test_cpp_extensions_aot` gets run TWICE by run_test.py, once with\na module that was built with ninja, and once with a module that was\nbuilt without ninja.\n- run_test.py asserts that when we are building with use_ninja=True,\nninja is actually available on the system.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19730432\n\nPulled By: zou3519\n\nfbshipit-source-id: 819590d01cf65e8da5a1e8019b8b3084792fee90", "pr_number": "32495", "files_changed": ["test/cpp_extensions/setup.py", "test/run_test.py", "test/test_cpp_extensions.py", "test/test_cpp_extensions_aot.py", "test/test_cpp_extensions_jit.py", "torch/testing/_internal/common_utils.py", "torch/utils/cpp_extension.py"], "labels": ["merged"]}, "46c3c18bcc": {"title": "Issue a warning when using zero_grad in DataParallel (#32870)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31768\n\n`DataParallel` creates replicas of the original `nn.Module` with the parameters duplicated onto the destination devices. Calling `backwards` will propagate gradients onto the original module parameters but calling `zero_grad` on the replica module doesn't clear the gradients from the parent module,\n\n~breaking any model that uses `backward`-`zero_grad` in its `forward`. I fix this by patching the replica module so that `zero_grad` clears grads on the parent as well.~\n\nHowever, any replica using backwards was broken anyway since the replica's parameters are not leaf nodes in autograd. So, we should raise a warning.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32870\n\nDifferential Revision: D19730209\n\nPulled By: ezyang\n\nfbshipit-source-id: cb9b2cb0c2e0aca688ce0ff3e56b40fbd2aa3c66", "pr_number": "32870", "files_changed": ["test/distributed/test_data_parallel.py", "torch/nn/modules/module.py"], "labels": ["merged", "open source", "triaged"]}, "b00345a6f2": {"title": "Move normal distribution to Aten(CPU) (#32031)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/24746\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32031\n\nDifferential Revision: D19729002\n\nPulled By: ezyang\n\nfbshipit-source-id: f571368a8a2ac4068c937062167a2fd89e64098c", "pr_number": "32031", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorRandom.cpp", "aten/src/TH/generic/THVectorDispatch.cpp", "aten/src/TH/vector/AVX2.cpp", "aten/src/TH/vector/AVX2.h"], "labels": ["merged", "open source", "triaged"]}, "3c17cbb6c8": {"title": "fix #30480 torch.normal shape checking is broken (#32243)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32243\n\nFollowing what gchanan proposed in #30480\n- If the (logical) shapes of mean and std are broadcastable, we broadcast them for the output\n  Done in tensor iterator already.\n- If the (logical) shapes of mean and std are not broadcastable and they have the same number of elements, we fall back to the old behavior (pick the shape of mean)\n  Done by reshape std to the same shape of mean.\n- If the (logical) shapes of mean and std are not broadcastable and don't have the same number of elements, we error out.\n  Done by tensor iterator already.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19417087\n\nPulled By: glaringlee\n\nfbshipit-source-id: 1c4bc7df923110a803620b9e2abd11a7151fc33e", "pr_number": "32243", "files_changed": ["aten/src/ATen/ExpandUtils.cpp", "aten/src/ATen/ExpandUtils.h", "aten/src/ATen/native/cuda/Distributions.cu", "test/test_torch.py"], "labels": ["merged"]}, "e76fa9822d": {"title": "[C2] Introduce extra_info force CPU tags for auto-generated iteration counter blobs (#32607)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32607\n\nAs desc.\n\nTest Plan: Unit-test.\n\nReviewed By: xw285cornell, chocjy\n\nDifferential Revision: D19551567\n\nfbshipit-source-id: 3a121351d2b4016e99a1536dec746be970698664", "pr_number": "32607", "files_changed": ["caffe2/python/utils.py", "caffe2/python/utils_test.py"], "labels": ["fb-exported", "merged"]}, "ec1e9a1ae2": {"title": "Revert D19417087: fix #30480 torch.normal shape checking is broken", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19417087\n\nOriginal commit changeset: 1c4bc7df9231\n\nfbshipit-source-id: ee579304cd79e48a6ce87daf490b53baabc655a8", "pr_number": null, "files_changed": ["aten/src/ATen/ExpandUtils.cpp", "aten/src/ATen/ExpandUtils.h", "aten/src/ATen/native/cuda/Distributions.cu", "test/test_torch.py"], "labels": []}, "8195961f20": {"title": "Revert D19730209: [pytorch][PR] Issue a warning when using zero_grad in DataParallel", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19730209\n\nOriginal commit changeset: cb9b2cb0c2e0\n\nfbshipit-source-id: 5bf53ea3c37a7ed2411a2acc34e40d07eff144c9", "pr_number": null, "files_changed": ["test/distributed/test_data_parallel.py", "torch/nn/modules/module.py"], "labels": []}, "757cea92a4": {"title": "[c10] Allow taking a std::tuple as arg (#32948)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32948\nghstack-source-id: 97736044\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19709119\n\nfbshipit-source-id: 26b069a95ae7a79a2d5cbe3845eb1a5dcd398be1", "pr_number": "32948", "files_changed": ["aten/src/ATen/core/boxing/kernel_functor.h", "aten/src/ATen/core/boxing/kernel_functor_test.cpp", "aten/src/ATen/core/jit_type.h"], "labels": ["merged"]}, "38820a7014": {"title": "[JIT] Resolve custom classes in source importer (#32977)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32977\nghstack-source-id: 97736042\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19724588\n\nfbshipit-source-id: b31b6ae14d2881d3604922e611fe4749108e674d", "pr_number": "32977", "files_changed": ["test/cpp/jit/test_class_import.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/import_source.cpp"], "labels": ["jit", "merged"]}, "b0476dc6e6": {"title": "Fix Typo", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33038\n\nDifferential Revision: D19769127\n\nPulled By: zou3519\n\nfbshipit-source-id: 53a7fa603b097d7070ca484997a587ec74e87357", "pr_number": "33038", "files_changed": ["aten/src/ATen/native/Normalization.cpp"], "labels": ["merged", "open source"]}, "ca33aeba09": {"title": "[JIT] Add Exit Transform / Convert To SSA to docs", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/24114\n\nDifferential Revision: D19780828\n\nPulled By: eellison\n\nfbshipit-source-id: d481ad886b2ad6349a1646672e507336d45759fb", "pr_number": "24114", "files_changed": ["torch/csrc/jit/docs/OVERVIEW.md"], "labels": ["jit", "merged"]}, "432858c960": {"title": "[ONNX] Fix exporting copy_ with index as tensor input (#32801)", "body": "Summary:\nSupporting the below case. Previously index for copy_ was only considered as constant integer, where as it could be a tensor input as well.\n\n```python\nclass InPlaceIndexedAssignment(torch.nn.Module):\n    def forward(self, data, index, new_data):\n        data[index] = new_data\n        return data\n\ndata = torch.zeros(3, 4)\nindex = torch.tensor(1)\nnew_data = torch.arange(4).to(torch.float32)\ntorch.onnx.export(InPlaceIndexedAssignment(), (data, index, new_data), 'inplace_assign.onnx', opset_version=11)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32801\n\nReviewed By: hl475\n\nDifferential Revision: D19731666\n\nPulled By: houseroad\n\nfbshipit-source-id: 08703fdccd817f901282e19847e259d93929e702", "pr_number": "32801", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.cpp"], "labels": ["jit", "merged", "module: onnx", "open source", "triaged"]}, "afa8cbf8c2": {"title": "Modifed randNLike for scripting (#32830)", "body": "Summary:\nthe rand N like function had required args which were not being used.\nAs such modified the method signature to give default values so when\nscripting does not provide these arguments which are not even being\nused, no error is thrown.\n\nAdditionally modified the const checker for handling prim::Constant as\nwell\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32830\n\nReviewed By: hl475\n\nDifferential Revision: D19731715\n\nPulled By: houseroad\n\nfbshipit-source-id: a3cacb3977eecb88b122e0ceb654fdbf1c8286c1", "pr_number": "32830", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source"]}, "10db323b75": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/4121390031f20c7f800aa07e833ed453ce684736\nhttps://github.com/facebook/fbthrift/commit/fdd24faa6c3461cc74e5ce73f453fb01a727c537\nhttps://github.com/facebook/fbzmq/commit/94471e632be926d7bad91a15f621e7a6322a093e\nhttps://github.com/facebook/folly/commit/0a24425afd8a2e6e0a80532b33cdbca3c0669b74\nhttps://github.com/facebook/proxygen/commit/8b79c69b6c15327e2278cd8521c969fea3bc26cc\nhttps://github.com/facebook/wangle/commit/99f391782610d9120dd56405e657822893d920a9\nhttps://github.com/facebookincubator/fizz/commit/3853cef0ba8fe1b9a02e305936aa632b5a7d55c9\nhttps://github.com/facebookincubator/katran/commit/5db0cb90fc9c0730368cb8d5403b88e41c71dfd5\nhttps://github.com/facebookincubator/mvfst/commit/714edbb20f55801ddab654cf0d1933d01071a01d\nhttps://github.com/pytorch/fbgemm/commit/880ade142005368d62de356d0910788e36de7c5c\n\nTest Plan: n/a\n\nReviewed By: yns88\n\nfbshipit-source-id: a63558a8df40c936d8959287f815835502b6cbd9", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "f0d7bd41b9": {"title": "[jit] Minor: avoid recalculating some keys for map accesses in pickler. (#33060)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33060\n\nNoticed this when tracking down a partially-related SIGSEGV.\nIf inserting a non-present key into a memoized map, don't re-calculate it twice\n(probably safer that way anyway).\nghstack-source-id: 97904485\n\nTest Plan: buck test mode/dev-nosan caffe2/test/...\n\nDifferential Revision: D19778008\n\nfbshipit-source-id: 95b1d708c034a54b96a22ccbdffb24f72d08dffd", "pr_number": "33060", "files_changed": ["torch/csrc/jit/pickler.cpp"], "labels": ["jit", "merged"]}, "05d18ffaf5": {"title": "Distributed Autograd: Allow multiple backward passes to accumulate gradients. (#32506)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32506\n\nIn this PR, we've introduced a `retain_graph` parameter to distributed\nautograd similar to `torch.autograd.backward`.\n\nIn terms of design, this parameter is sent over RPC to all nodes and is used to\ncreate the GraphTask on the local nodes. This enables us to run\n`dist_autograd.backward()` multiple times in the same context.\n\nThe use case currently for this is to benchmark only the backward pass for\ndistributed autograd. We'd like to measure the QPS for the backward pass and as\na result, running a single forward pass and multiple backward passes in a loop\nis one way to benchmark backward pass performance.\nghstack-source-id: 97868900\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19521288\n\nfbshipit-source-id: 7ad8521059fd400d7b5a6ab77ce56e1927ced90a", "pr_number": "32506", "files_changed": ["test/cpp/dist_autograd/test_dist_autograd.cpp", "torch/csrc/autograd/engine.cpp", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/context/context.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h", "torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp", "torch/csrc/distributed/autograd/init.cpp", "torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.cpp", "torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py"], "labels": ["merged"]}, "e025f393f6": {"title": "windows template specialization bug (#33076)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33076\n\nattempt at fixing https://github.com/pytorch/pytorch/issues/30886\n\nTest Plan: circleCI with `call \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=14.16` passes\n\nDifferential Revision: D19784550\n\nfbshipit-source-id: 9fb42c3854d1d00d96cd7179bef9dd1aa2972ea6", "pr_number": "33076", "files_changed": ["c10/util/C++17.h"], "labels": ["fb-exported", "merged"]}, "674dca0831": {"title": "Automatic update of fbcode/onnx to 8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e (#33075)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33075\n\nPrevious import was 65020daafa9183c769938b4512ce543fd5740f8f\n\nIncluded changes:\n- **[8b3f7e2e](https://github.com/onnx/onnx/commit/8b3f7e2e)**: Update Dropout and  BatchNorm to be Training Friendly (#2568) <Lara Haidar>\n- **[61f0bbc5](https://github.com/onnx/onnx/commit/61f0bbc5)**: Fix a bug in ScatterND shape inference (#2577) <Bowen Bao>\n- **[05bce9cf](https://github.com/onnx/onnx/commit/05bce9cf)**: add utility function to make reference attribute whose name is not the same as the attribute it refers. (#2583) <Ke Zhang>\n- **[71181c83](https://github.com/onnx/onnx/commit/71181c83)**: Clarify spec for constant of shape with dim_n = 0 (#2567) <Negin Raoof>\n- **[eadba733](https://github.com/onnx/onnx/commit/eadba733)**: Update sigs.md with link to calendar page (#2579) <Prasanth Pulavarthi>\n- **[08562f8e](https://github.com/onnx/onnx/commit/08562f8e)**: Update working-groups.md (#2580) <Prasanth Pulavarthi>\n- **[0e718913](https://github.com/onnx/onnx/commit/0e718913)**: Fix Slice op's shape inference logic (#2526) <Hariharan Seshadri>\n- **[12111410](https://github.com/onnx/onnx/commit/12111410)**: Add missing spaces to Random*Like doc (#2572) <Takeshi Watanabe>\n- **[7e6e61d6](https://github.com/onnx/onnx/commit/7e6e61d6)**: Contributing: fix typos (#2571) <Maher Jendoubi>\n- **[bbd604ef](https://github.com/onnx/onnx/commit/bbd604ef)**: Add Einsum op (#2504) <Negin Raoof>\n- **[fd3ab73a](https://github.com/onnx/onnx/commit/fd3ab73a)**: Clarify split supports zero length splits (#2544) <Negin Raoof>\n- **[6dd73774](https://github.com/onnx/onnx/commit/6dd73774)**: Fix circleci build and drop unsupported Windows builds (#2565) <Wei-Sheng Chin>\n- **[b3d201a2](https://github.com/onnx/onnx/commit/b3d201a2)**: Fix the formula of intermediate zero calculation for DynamicQuantizeLinear (#2556) <Yufeng Li>\n- **[3613eb25](https://github.com/onnx/onnx/commit/3613eb25)**: Add wording to clarify. (#2555) <Dwayne Robinson>\n- **[dfa4384c](https://github.com/onnx/onnx/commit/dfa4384c)**: Fix shape inference for Split with split attribute (#2328) <Shinichiro Hamaji>\n- **[684fc1bc](https://github.com/onnx/onnx/commit/684fc1bc)**: Keep symbolic dims in Concat with a single input (#2418) <Shinichiro Hamaji>\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D19784487\n\nfbshipit-source-id: 421cdc3394faeff0168853f4ff065fc599ca3967", "pr_number": "33075", "files_changed": ["caffe2/python/onnx/tests/onnx_backend_test.py", "third_party/onnx"], "labels": ["fb-exported", "merged"]}, "3b2f267ad8": {"title": "add to codeowner to get better inbox notification for PR", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33087\n\nDifferential Revision: D19790389\n\nPulled By: albanD\n\nfbshipit-source-id: 360ee1fc47a9b0b8d8ddbe47b77f2cbffaead9c8", "pr_number": "33087", "files_changed": ["CODEOWNERS"], "labels": ["merged"]}, "d678093907": {"title": "[ONNX] Extend op registration to next opsets (#32943)", "body": "Summary:\nCurrently, custom ops are registered for a specific opset version.\nFor example, all torchvision custom ops are registered for opset 11, and cannot be exported into higher opset versions. This PR extends op registration to higher opset versions.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32943\n\nReviewed By: hl475\n\nDifferential Revision: D19739406\n\nPulled By: houseroad\n\nfbshipit-source-id: dd8b616de3a69a529d135fdd02608a17a8e421bc", "pr_number": "32943", "files_changed": ["test/onnx/test_custom_ops.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/utils.py"], "labels": ["merged", "open source", "triaged"]}, "de27f4261d": {"title": "[jit] remove redundant variables from JIT TestCase", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29091\n\nDifferential Revision: D19746083\n\nPulled By: suo\n\nfbshipit-source-id: 76fd71740fe7a3f52da361d96a7b694ec208de24", "pr_number": "29091", "files_changed": ["test/test_jit.py"], "labels": ["merged", "open source", "triaged"]}, "e7b42209eb": {"title": "Added sparkspot model.", "body": "Summary: Lite interpereter does not have softplus and sub ops for this model.\n\nTest Plan:\nbuck run fbsource//xplat/aibench:run_bench -- -b ../xplat/aibench/specifications/models/pytorch/mobile_migration/sparkspot.json --platform android --framework pytorch --remote --devices SM-G960U-8.0.0-26\n\n https://our.intern.facebook.com/intern/aibench/details/890521439770638\n\nbuck run fbsource//xplat/aibench:run_bench -- -b ../xplat/aibench/specifications/models/pytorch/mobile_migration/sparkspot.json --platform android/arm64 --framework pytorch --remote --devices SM-G960U-8.0.0-26\n\nhttps://our.intern.facebook.com/intern/aibench/details/485779747361527\n\nFor Caffe2:\nbuck run fbsource//xplat/aibench:run_bench -- -b ../xplat/aibench/specifications/models/caffe2/mobile_migration/sparkspot.json --platform android --framework caffe2 --remote --devices SM-G950U-7.0-24\n\nhttps://our.intern.facebook.com/intern/aibench/details/177482569133423\n\nReviewed By: ljk53, iseeyuan\n\nDifferential Revision: D19757721\n\nfbshipit-source-id: cdd4b39d072925fc8de17184f2c90918de6245ba", "pr_number": null, "files_changed": ["torch/csrc/jit/mobile/register_mobile_ops.cpp"], "labels": []}, "a9583c1f75": {"title": "Vectorize softplus and its backward function on CPU (#32944)", "body": "Summary:\nThe benchmarking shows a huge performance gain (2-7x faster).\n\nAlso note that I removed Half support because it isn't generally supported on CPU.\n\nBenchmark: (Debian 10, Release build, gcc 8.3, no turbo, Intel(R) Xeon(R) E-2136 CPU @ 3.30GHz)\n\n```python\nimport timeit\nfor op in ('Softplus',):\n    print('Forward')\n    for dtype in ('torch.double', 'torch.float'):\n        for n, t in [(10_000, 10000),\n                    (100_000, 1000)]:\n            print(f'torch.nn.{op}()(a), numel() == {n} for {t} times, dtype={dtype}')\n            print(timeit.timeit('m(a)', setup=f'import torch; m = torch.nn.{op}(); a = torch.randn({n}, dtype={dtype})', number=t))\n    print('Backward')\n    for dtype in ('torch.double', 'torch.float'):\n        for n, t in [(10_000, 40000),\n                    (100_000, 4000)]:\n            print(f'torch.nn.{op}()(a), numel() == {n} for {t} times, dtype={dtype}')\n            print(timeit.timeit('y.backward(retain_graph=True)',\n                                setup=f'import torch; m = torch.nn.{op}(); a = torch.randn({n}, dtype={dtype}, requires_grad=True); x = m(a); y = x.sum()',\n                                number=t))\n```\n\nBefore:\n\n```\nForward\ntorch.nn.Softplus()(a), numel() == 10000 for 10000 times, dtype=torch.double\n3.73130346799735\ntorch.nn.Softplus()(a), numel() == 100000 for 1000 times, dtype=torch.double\n3.6790116359916283\ntorch.nn.Softplus()(a), numel() == 10000 for 10000 times, dtype=torch.float\n2.7477027159911813\ntorch.nn.Softplus()(a), numel() == 100000 for 1000 times, dtype=torch.float\n2.7382752639969112\nBackward\ntorch.nn.Softplus()(a), numel() == 10000 for 40000 times, dtype=torch.double\n7.037510035006562\ntorch.nn.Softplus()(a), numel() == 100000 for 4000 times, dtype=torch.double\n5.855093962003593\ntorch.nn.Softplus()(a), numel() == 10000 for 40000 times, dtype=torch.float\n3.413616877005552\ntorch.nn.Softplus()(a), numel() == 100000 for 4000 times, dtype=torch.float\n2.5485514330066508\n```\n\nAfter:\n\n```\nForward\ntorch.nn.Softplus()(a), numel() == 10000 for 10000 times, dtype=torch.double\n0.9465823079954134\ntorch.nn.Softplus()(a), numel() == 100000 for 1000 times, dtype=torch.double\n0.8799468770012027\ntorch.nn.Softplus()(a), numel() == 10000 for 10000 times, dtype=torch.float\n0.39715987400268205\ntorch.nn.Softplus()(a), numel() == 100000 for 1000 times, dtype=torch.float\n0.3563060039887205\nBackward\ntorch.nn.Softplus()(a), numel() == 10000 for 40000 times, dtype=torch.double\n2.400547721001203\ntorch.nn.Softplus()(a), numel() == 100000 for 4000 times, dtype=torch.double\n1.4740848699875642\ntorch.nn.Softplus()(a), numel() == 10000 for 40000 times, dtype=torch.float\n1.6684603010071442\ntorch.nn.Softplus()(a), numel() == 100000 for 4000 times, dtype=torch.float\n0.6815649690106511\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32944\n\nDifferential Revision: D19725407\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 7430de838df731bd17617eff63f10107d5ad6b8b", "pr_number": "32944", "files_changed": ["aten/src/ATen/native/cpu/Activation.cpp"], "labels": ["merge-this-please", "merged", "open source"]}, "868db903ae": {"title": "ONNX support for torch.take (#33061)", "body": "Summary:\nAdding ONNX export support for torch.take()\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33061\n\nReviewed By: hl475\n\nDifferential Revision: D19782651\n\nPulled By: houseroad\n\nfbshipit-source-id: 0168fb941e166acda4ca607165248b8e0b260ace", "pr_number": "33061", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source"]}, "c6fa6d82ae": {"title": "move Decompose before profiling to prevent clearing shape info", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33100\n\nDifferential Revision: D19793346\n\nPulled By: Krovatkin\n\nfbshipit-source-id: fdc5927f4970eabbb5a8f62a499d5b79117af2a9", "pr_number": "33100", "files_changed": ["torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/profiling_graph_executor_impl.cpp"], "labels": ["jit", "merged"]}, "7314f1c281": {"title": "[torch/multiprocessing] Update documentation indicating that start_method is ignored for mp.spawn() (#33070)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33070\n\n`start_method` parameter is intentionally ignored for `mp.spawn()`. Document this fact and point the user to `start_processes` if they want to use a different `start_method`.\n\nTest Plan:\nWarning message looks like:\n```\nmain.py:8: UserWarning: This method only supports start_method=spawn (got: fork).\nTo use a different start_method use:\n         torch.multiprocessing.start_process(...)\n  warnings.warn(msg)\n```\n\nReviewed By: ailzhang\n\nDifferential Revision: D19780235\n\nfbshipit-source-id: 4599cd18c3ba6cc401810efe4f390290ffa8023b", "pr_number": "33070", "files_changed": ["torch/multiprocessing/spawn.py"], "labels": ["fb-exported", "merged"]}, "17d4ef9e9e": {"title": "Support using scalar tensor for split (#32493)", "body": "Summary:\nsplit requires an int input, however in tracing operators such as\nsize(axis) return a tensor, which is different behavior than when not\ntracing. As such need to modify split to handle these cases.\n\nFixes https://github.com/pytorch/pytorch/issues/27551\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32493\n\nReviewed By: hl475\n\nDifferential Revision: D19538254\n\nPulled By: houseroad\n\nfbshipit-source-id: c8623009de5926aa38685e08121f4b48604bd8c0", "pr_number": "32493", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/tensor.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "0e29e9e0f6": {"title": "Re-enable internal test runs", "body": "Summary:\nFix internal error message due to old version of hypothesis\n   test_suite = self.load_tests()\n  File \"/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/buck-out/dev/gen/caffe2/test/quantization#binary,link-tree/__fb_test_main__.py\", line 678, in load_tests\n    suite = loader.load_all()\n  File \"/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/buck-out/dev/gen/caffe2/test/quantization#binary,link-tree/__fb_test_main__.py\", line 467, in load_all\n    __import__(module_name, level=0)\n  File \"/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/buck-out/dev/gen/caffe2/test/quantization#binary,link-tree/test_quantization.py\", line 45, in <module>\n    hu.assert_deadline_disabled()\n  File \"/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/buck-out/dev/gen/caffe2/test/quantization#binary,link-tree/torch/testing/_internal/hypothesis_utils.py\", line 322, in assert_deadline_disabled\n    assert settings().deadline is None\n  File \"/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/buck-out/dev/gen/caffe2/test/quantization#binary,link-tree/hypothesis/_settings.py\", line 127, in __getattr__\n    raise AttributeError('settings has no attribute %s' % (name,))\nAttributeError: settings has no attribute deadline\n\nTest Plan: buck test mode/dev //caffe2/test:quantization -- --run-disabled runs successfully\n\nDifferential Revision: D19795232\n\nfbshipit-source-id: ef1d8be20b4be30e1cfad4cd5019c4779a5f4568", "pr_number": null, "files_changed": ["torch/testing/_internal/hypothesis_utils.py"], "labels": []}, "6249d7302b": {"title": "[ONNX] Fix export for avg_pool with default stride (#33017)", "body": "Summary:\nIf using nn.functional avg_pool, stride is an optional arg. If not provided, it is set to kernel_size.\nThis PR fixes the export of avg_pool with default stride.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33017\n\nReviewed By: hl475\n\nDifferential Revision: D19759604\n\nPulled By: houseroad\n\nfbshipit-source-id: b0352db6fbaf427f4cff9ba8a942efdeb39b6f02", "pr_number": "33017", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source"]}, "e2f1288514": {"title": "Add utils to inspect fp16/int8 packed weights (#32979)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32979\n\nSince we use prepacked weights in the Fp16 FCs and future Int8 FCs in production Ads models, we provide the python utils to inspect the unpacked format of the weights for debugging purpose. The main interfaces are the following:\n\n```\nfrom deeplearning.numeric_suite.toolkit import packed_weights_inspector\n# inspect fp16 packed weights\nunpacked_fp16_weights = packed_weights_inspector.extract_fp16_fc_packed_weights(fp16_weight_blob_name)\n\n# inspect int8 packed weights\nunpacked_int8_weights, qparams = packed_weights_inspector.extract_int8_fc_packed_weights(int8_weight_blob_name)\n```\n\nTest Plan:\n```\nbuck test mode/opt deeplearning/numeric_suite/toolkit/test:packed_weights_inspector_test\n```\n\nReviewed By: amylittleyang\n\nDifferential Revision: D19724474\n\nfbshipit-source-id: e937672b3722e61bc44c2587aab2288a86aece9a", "pr_number": "32979", "files_changed": ["caffe2/quantization/server/pybind.cc"], "labels": ["fb-exported", "merged"]}, "efba630287": {"title": "Issue a warning when zero_grad is used in DataParallel (#33064)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31768, second attempt of https://github.com/pytorch/pytorch/issues/32870\n\nDataParallel creates replicas of the original `nn.Module` with the parameters duplicated onto the destination devices. Calling `backwards` will propagate gradients onto the original module parameters but calling `zero_grad` on the replica module doesn't clear the gradients from the parent module. However, any replica using backwards was broken anyway since the replica's parameters are not leaf nodes in autograd. So, we should issue a warning.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33064\n\nDifferential Revision: D19790178\n\nPulled By: albanD\n\nfbshipit-source-id: 886f36640acef4834a6fa57a26ce16b42ff0e9ad", "pr_number": "33064", "files_changed": ["test/distributed/test_data_parallel.py", "torch/nn/modules/module.py"], "labels": ["merged", "open source"]}, "e1c53a5c86": {"title": "Fix version counter bump in cpp Function (#33068)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33068\n\nThe version counter is already tracked if we use pytorch's functions but not if the user unpack the Tensor and modifies it by hand or with a third party library.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19791564\n\nPulled By: albanD\n\nfbshipit-source-id: a73c0f73d8fd0c0e5bf838f14bed54fa66937840", "pr_number": "33068", "files_changed": ["test/cpp/api/autograd.cpp", "torch/csrc/autograd/custom_function.cpp", "torch/csrc/autograd/custom_function.h"], "labels": ["merged"]}, "3e8d813263": {"title": "Add more checks to custom Function (#33069)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33069\n\nThis PR adds the following:\n- Warn when a non-input Tensor is given to `mark_dirty()` as it is not needed.\n- Raise an error if we modify inplace an input that is a view and that we have multiple output. This setting is not handled by `CopySlices` and will raise a cryptic error during the backward.\n- Raise an error if an input is modified inplace but not returned. That will prevent the graph rewrite from being done correctly.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19791563\n\nPulled By: albanD\n\nfbshipit-source-id: 4d8806c27290efe82ef2fe9c8c4dc2b26579abd1", "pr_number": "33069", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/custom_function.cpp"], "labels": ["merged"]}, "c917a247a8": {"title": "Improve error message for assertWarnsRegex (#33099)", "body": "Summary:\n`assertWarnsRegex` now prints out any warnings that it caught while failing to find a matching warning. This makes it easier to debug tests by just looking at the CI logs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33099\n\nDifferential Revision: D19800021\n\nPulled By: ezyang\n\nfbshipit-source-id: 1c31ae785c8ffc5d47619aff6597e479263be2de", "pr_number": "33099", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["merged", "open source"]}, "9d94f56ce0": {"title": "Backward operation of torch.eig for real eigenvalues (#33090)", "body": "Summary:\nAnother pull request to follow up issue https://github.com/pytorch/pytorch/issues/32531.\nHere I implemented the backward operation for `torch.eig` with a condition that all the eigenvalues are real.\n\nThis pull request is independent of my another pull request https://github.com/pytorch/pytorch/issues/32932, which means that there is no dependency between this PR and my another PR.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33090\n\nDifferential Revision: D19814347\n\nPulled By: albanD\n\nfbshipit-source-id: 2fae30964e97987abb690544df8240aedeae56e8", "pr_number": "33090", "files_changed": ["test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/templates/Functions.cpp"], "labels": ["merged", "open source", "triaged"]}, "ebed008dd4": {"title": "Correct /MP usage in MSVC (#33120)", "body": "Summary:\n## Several flags\n`/MP[M]`: It is a flag for the compiler `cl`. It leads to object-level multiprocessing. By default, it spawns M processes where M is the number of cores on the PC.\n`/maxcpucount:[M]`: It is a flag for the generator `msbuild`. It leads to project-level multiprocessing. By default, it spawns M processes where M is the number of cores on the PC.\n`/p:CL_MPCount=[M]`: It is a flag for the generator `msbuild`. It leads the generator to pass `/MP[M]` to the compiler.\n`/j[M]`: It is a flag for the generator `ninja`. It leads to object-level multiprocessing. By default, it spawns M processes where M is the number of cores on the PC.\n\n## Reason for the change\n1. Object-level multiprocessing is preferred over project-level multiprocessing.\n2. ~For ninja, we don't need to set `/MP` otherwise M * M processes will be spawned.~ Actually, it is not correct because in ninja configs, there are only one source file in the command. Therefore, the `/MP` switch should be useless.\n3. For msbuild, if it is called through Python configuration scripts, then `/p:CL_MPCount=[M]` will be added, otherwise, we add `/MP` to `CMAKE_CXX_FLAGS`.\n4. ~It may be a possible fix for https://github.com/pytorch/pytorch/issues/28271, https://github.com/pytorch/pytorch/issues/27463 and https://github.com/pytorch/pytorch/issues/25393. Because `/MP` is also passed to `nvcc`.~ It is probably not true. Because `/MP` should not be effective given there is only one source file per command.\n\n## Reference\n1. https://docs.microsoft.com/en-us/cpp/build/reference/mp-build-with-multiple-processes?view=vs-2019\n2. https://github.com/Microsoft/checkedc-clang/wiki/Parallel-builds-of-clang-on-Windows\n3. https://blog.kitware.com/cmake-building-with-all-your-cores/\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33120\n\nDifferential Revision: D19817227\n\nPulled By: ezyang\n\nfbshipit-source-id: f8d01f835016971729c7a8d8a0d1cb8a8c2c6a5f", "pr_number": "33120", "files_changed": ["CMakeLists.txt", "tools/setup_helpers/cmake.py"], "labels": ["merged", "open source"]}, "6be4ec100f": {"title": "[pytorch] Elide more Thrift Tensor send copies. (#31998)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31998\n\nThis change builds on recent torch::from_blob() changes to avoid Tensor\ncopies on send in more cases.\n\nParticularly, this change adds an enabled option to assume if the Tensor\nStorage's DataPtr has a non-trivial deleter, then the Tensor does in fact\nmanage the underlying memory. And hence we can reference the Tensor's Storage\nvia an IOBuf that is referenced while sending, saving a Tensor copy.\n\nWe add appropriate test cases, particularly re: torch::from_blob() which\nwould have been problematic would recent changes.\nghstack-source-id: 97778619\n\nTest Plan: buck test mode/dev caffe2/torch/fb/distributed/wireSerializer/test/...\n\nReviewed By: satgera\n\nDifferential Revision: D19306682\n\nfbshipit-source-id: 05f56efb2d5d6279ae4b54dfcbba0f729c2c13fa", "pr_number": "31998", "files_changed": ["torch/csrc/jit/pickler.h"], "labels": ["jit", "merged"]}, "f4fbe9549d": {"title": "Revert D19800021: [pytorch][PR] Improve error message for assertWarnsRegex", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19800021\n\nOriginal commit changeset: 1c31ae785c8f\n\nfbshipit-source-id: d7b340d678562c25a84d48be66c576075000b50d", "pr_number": null, "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": []}, "3c4cec56aa": {"title": "Enable test_distributed for ROCm but only with nccl backend [REDUX] (#32551)", "body": "Summary:\nThis is a redux of the original PR https://github.com/pytorch/pytorch/issues/28814 which was reverted in PR https://github.com/pytorch/pytorch/issues/29736 due to test_DistributedDataParallel being suspected as being flaky. Further investigation revealed it wasn't flakiness, but a bug in the PyTorch source code which has been now fixed in PR https://github.com/pytorch/pytorch/issues/32356. This PR is another attempt at enabling the test_distributed unit test suite only for the nccl backend.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32551\n\nDifferential Revision: D19729966\n\nPulled By: bddppq\n\nfbshipit-source-id: 12a0d850991a903cc7723d63693b6157071d7115", "pr_number": "32551", "files_changed": ["test/distributed/test_c10d.py", "test/distributed/test_distributed.py", "test/run_test.py", "torch/testing/_internal/common_distributed.py"], "labels": ["merged", "module: distributed", "module: rocm", "open source", "triaged"]}, "3bde97d5a5": {"title": "Move a resize from codegen to code.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33024\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19774147\n\nPulled By: gchanan\n\nfbshipit-source-id: 08cb099f1695b28117e4236e214976b548aec7a1", "pr_number": "33024", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/cuda/LegacyDefinitions.cpp"], "labels": ["merged"]}, "e8c4f5a74b": {"title": "Temporarily disable failing iOS builds", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33154\n\nDifferential Revision: D19820655\n\nPulled By: kostmo\n\nfbshipit-source-id: fc3e22b1bf4ec112085ea846c3999efd0f3e26f3", "pr_number": "33154", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows-pytorch-ios-builds.yml"], "labels": ["merged"]}, "495c1df510": {"title": "[pytorch] convert code analyzer to a binary (#33102)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33102\n\nAdd a simple main() to build code analyzer as a binary. This enables\neasier integration with FB internal build environment.\nghstack-source-id: 97958658\n\nTest Plan: - CI\n\nDifferential Revision: D19798560\n\nPulled By: ljk53\n\nfbshipit-source-id: 126230e3bf7568046a309e8a6785230f820e0222", "pr_number": "33102", "files_changed": ["tools/code_analyzer/CMakeLists.txt", "tools/code_analyzer/analyzer.cpp", "tools/code_analyzer/build.sh", "tools/code_analyzer/run_analyzer.sh"], "labels": ["merged"]}, "d672779339": {"title": "[CI][treehug] Disable xenial_py2.7 tests due to mypy min version py3.5", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33159\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19822400\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 8e7b561e6a6181ec1f9b6f56a539ddcb538b3858", "pr_number": "33159", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/config.yml"], "labels": ["merged"]}, "524fe8a96c": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/4bc5213b66d4c7d3c96c6d5c47945dae4881c921\nhttps://github.com/facebook/proxygen/commit/9ae570bb8975b2cc0c7d703f6c34e0c9075ff32f\nhttps://github.com/facebook/rocksdb/commit/b2bc1da56197aa68a71eff2344e4393042c88eb6\nhttps://github.com/pytorch/fbgemm/commit/dcde8696bdf40adf8c436cdaa805973613718ae4\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: c5ca30dab73f80cd13f5a5bf6e3867083b2512ac", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "a3e69d3405": {"title": "Use bazelisk instead of specifying bazel version manually. (#33036)", "body": "Summary:\nBazelisk automatically reads `.bazelversion` file and install the required version of Bazel. This saves us from updating CI script everytime we need a Bazel upgrade.\nUse clang-8 for consistency with pytorch/xla repo.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33036\n\nDifferential Revision: D19820819\n\nPulled By: ailzhang\n\nfbshipit-source-id: 1560ec225cd037a811769a509a704b0df77ea183", "pr_number": "33036", "files_changed": [".jenkins/pytorch/build.sh"], "labels": ["merged"]}, "61ac14a483": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/543b39c9ada4316d4d45a22b072d9b962cc07cb0\nhttps://github.com/facebook/fbzmq/commit/38c2e0ee448fa7a33b4974d7e0922738bcd8f2d3\nhttps://github.com/facebook/folly/commit/552c07c32bc6604cfc729060a4544d03a69e3f5e\nhttps://github.com/facebook/rocksdb/commit/4369f2c7bb2fe2d67b5de96688ced5196a0be3b9\nhttps://github.com/pytorch/fbgemm/commit/07dbb5d2f44d89cdfdf8337b2d08a6130ae89062\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 803108a618a5be9ea58a38644c851486bad3bfbc", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "6f46962f21": {"title": "[1/3] Bind IndexHash to PyTorch (#33015)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33015\n\nExport IndexHash to PyTorch\n\nTest Plan:\nbuck test caffe2/caffe2/python/operator_test:torch_integration_test\n\n      \u2713 caffe2/caffe2/python/operator_test:torch_integration_test-2.7 - test_index_hash_op (caffe2.caffe2.python.operator_test.torch_integration_test.TorchIntegration) 0.151 44/50 (passed)\n\nReviewed By: bddppq\n\nDifferential Revision: D19727301\n\nfbshipit-source-id: a65c954539e81a15577fe5c3c0deb3614e983534", "pr_number": "33015", "files_changed": ["caffe2/operators/index_hash_ops.cc", "caffe2/operators/index_hash_ops.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": ["fb-exported", "merged"]}, "9857d9b4cd": {"title": "fix gather regression by not materializing loop vars in the error mes\u2026 (#33108)", "body": "Summary:\n\u2026sage\n\nPer title, fixes regression reported in https://github.com/pytorch/pytorch/issues/32425. cc nikitaved\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33108\n\nDifferential Revision: D19816116\n\nPulled By: ngimel\n\nfbshipit-source-id: 9f4a84c8e4533873b71bb7bbf3a7915b05308845", "pr_number": "33108", "files_changed": ["aten/src/ATen/native/cpu/ScatterGatherKernel.cpp"], "labels": ["merged"]}, "330d051bd5": {"title": "[pytorch] Migrating index_add cuda to ATen (#30573)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30573\n\nMostly just moved code.\nIndex dim and number of indices checks are added to make checks idential to index_add_cpu_\nghstack-source-id: 98010129\n\nTest Plan: existing tests\n\nDifferential Revision: D18749922\n\nfbshipit-source-id: d243be43a3b6a9b9591caf0c35ef2fb6ec0d3ead", "pr_number": "30573", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorIndex.h"], "labels": ["merged"]}, "3655975565": {"title": "Add allow_rebase_history flag and fix codegen functions for multiple views (#32790)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32790\n\nSame as https://github.com/pytorch/pytorch/pull/31990 but without the first commit in the stack that is problematic for a lot of people.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19814116\n\nPulled By: albanD\n\nfbshipit-source-id: d104911a5b098a5807b4bc08b69803ebd4f69fa6", "pr_number": "32790", "files_changed": ["test/cpp/api/init.cpp", "test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h"], "labels": ["merged"]}, "6c0dc66cb4": {"title": "[caffe2] use JIT'ed fp32 SLS (#33123)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33123\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32413\n\nUse JIT'ed fp32 SLS in Caffe2 operators\n\nTest Plan:\n```\n./fblearner/flow/run_integration_tests --regex dper.workflows.canary.canary_workflow --wait\n```\nf167043951 was killed due to 3hr timeout instead of failed.\n\nReviewed By: jianyuh\n\nDifferential Revision: D19680711\n\nfbshipit-source-id: efaca333edcfeab0007ad88f4f5168b2229e7e66", "pr_number": "33123", "files_changed": ["caffe2/operators/lengths_reducer_ops.h"], "labels": ["fb-exported", "merged"]}, "e7f0b15473": {"title": "Remove return value for __exit__ (#32997)", "body": "Summary:\nWhen an error is raised and `__exit__` in a context manager returns `True`, the error is suppressed; otherwise the error is raised. No return value should be given to maintain the default behavior of context manager.\n\nFixes https://github.com/pytorch/pytorch/issues/32639. The `get_lr` function was overridden with a function taking an epoch parameter, which is not allowed. However, the relevant error was not being raised.\n\n```python\nIn [1]: import torch\n   ...:\n   ...: class MultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n   ...:     def __init__(self, optimizer, gamma, milestones, last_epoch = -1):\n   ...:         self.init_lr = [group['lr'] for group in optimizer.param_groups]\n   ...:         self.gamma = gamma\n   ...:         self.milestones = milestones\n   ...:         super().__init__(optimizer, last_epoch)\n   ...:\n   ...:     def get_lr(self, step):\n   ...:         global_step = self.last_epoch #iteration number in pytorch\n   ...:         gamma_power = ([0] + [i + 1 for i, m in enumerate(self.milestones) if global_step >= m])[-1]\n   ...:         return [init_lr * (self.gamma ** gamma_power) for init_lr in self.init_lr]\n   ...:\n   ...: optimizer = torch.optim.SGD([torch.rand(1)], lr = 1)\n   ...: scheduler = MultiStepLR(optimizer, gamma = 1, milestones = [10, 20])\n```\n```\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-1-7fad6ba050b0> in <module>\n     14\n     15 optimizer = torch.optim.SGD([torch.rand(1)], lr = 1)\n---> 16 scheduler = MultiStepLR(optimizer, gamma = 1, milestones = [10, 20])\n\n<ipython-input-1-7fad6ba050b0> in __init__(self, optimizer, gamma, milestones, last_epoch)\n      6         self.gamma = gamma\n      7         self.milestones = milestones\n----> 8         super().__init__(optimizer, last_epoch)\n      9\n     10     def get_lr(self, step):\n\n~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py in __init__(self, optimizer, last_epoch)\n     75         self._step_count = 0\n     76\n---> 77         self.step()\n     78\n     79     def state_dict(self):\n\n~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py in step(self, epoch)\n    141                 print(\"1a\")\n    142                 # try:\n--> 143                 values = self.get_lr()\n    144                 # except TypeError:\n    145                     # raise RuntimeError\n\nTypeError: get_lr() missing 1 required positional argument: 'step'\n```\n\nMay be related to https://github.com/pytorch/pytorch/issues/32898.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32997\n\nDifferential Revision: D19737731\n\nPulled By: vincentqb\n\nfbshipit-source-id: 5cf84beada69b91f91e36b20c3278e9920343655", "pr_number": "32997", "files_changed": ["test/test_optim.py", "torch/optim/lr_scheduler.py"], "labels": ["merged"]}, "857bae39e0": {"title": "Updated DispatchKeyExtractor to expect TensorOptions (#30981)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30981\n\nThis stack is a first step toward an effort to fix, clean up and simplify code generation logic. \ufffdPlease see the master [task](https://github.com/pytorch/pytorch/issues/30405) to see related discussions and all the known issues.\n\nMain focus of these changes is TensorOptions in code generation.\nGoals:\n- Remove TensorOptions from generated code wherever it's possible. Leave it only in python/C++ API layers.\n- Refactor TensorOptions logic to a single place.\n- Log all discovered issues.\n\nNon goals:\n- Fix Everything!\n- Remove all the hacks in code generation scripts.\n- Clean up and defector all code generation scripts.\n\n-----------\nIn this PR:\nExtended DispatchKeyExtractor logic to expect TensorOptions.\n\n-----------\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18912684\n\nPulled By: izdeby\n\nfbshipit-source-id: 25cf1c397caa14272ca65b4003f1f03ff282ea77", "pr_number": "30981", "files_changed": ["aten/src/ATen/core/BackendSelectFallbackKernel.cpp", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen.py", "aten/src/ATen/gen_backend_select_register.py", "aten/src/ATen/templates/BackendSelectRegister.cpp", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.h", "c10/core/TensorOptions.h"], "labels": ["merged"]}, "855ee6446f": {"title": "Revert D18749922: [pytorch] Migrating index_add cuda to ATen", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18749922\n\nOriginal commit changeset: d243be43a3b6\n\nfbshipit-source-id: 15dafa644d84ff8803bd9ab3cdd40e12d805924a", "pr_number": null, "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorIndex.h"], "labels": []}, "31370949be": {"title": "Add zero_mask function for vectorized functions. (#32985)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32985\n\nThis can be useful in many situations to decide whether all elements are\nzeros or non-zeros, such as elu as shown in #32986 .\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19794549\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 1be1c863d69b9a19fdcfcdd7cb52343066f740d3", "pr_number": "32985", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h"], "labels": ["merged", "open source", "triaged"]}, "367488b001": {"title": "Move where cuda implementation to TensorIterator (#32984)", "body": "Summary:\n`where` is special because the arguments do not have the same type, which does not satisfy the assumption in modern https://github.com/pytorch/pytorch/pull/32383. I migrate it to TensorIterator so that there is something to test that this case is not broken. Currently, this case fallback to using legacy (not vectorized, not unrolled) code. It should be supported in the future when I cleanup `Loops.cuh`.\n\nI also move some sharing part of `CUDALoops.cuh` and `ROCmLoops.cuh` into `Loops.cuh` so that to logic for checking whether `func_t` has the same arg types could be shared.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32984\n\nDifferential Revision: D19825127\n\nPulled By: ngimel\n\nfbshipit-source-id: bbf4682349d96b4480c4d657f3c18a3a67a9bf17", "pr_number": "32984", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cpu/TensorCompareKernel.h", "aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/ROCmLoops.cuh", "aten/src/ATen/native/cuda/TensorCompare.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/cuda_vectorized_test.cu"], "labels": ["merged", "open source", "triaged"]}, "a64d0ffe81": {"title": "Use int64 in pdist kernel to handle batches >= 46342 #30583 (#31593)", "body": "Summary:\nCurrently `torch.pdist` yields an illegal CUDA memory access for batch sizes >= 46342 as reported by SsnL in https://github.com/pytorch/pytorch/issues/30583.\nThanks for the minimal code reproduction, btw! ;)\n\nReason for this bug:\nThe calculation if `i` in the [`pdist_kerne_cuda_impl`](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/cuda/DistanceKernel.cu#L112) might overflow, if a tensor with a `batch size >= 46342` is passed to `torch.pdist`.\n\nDetailed description:\n* `result` is resizes as ` n * (n - 1) / 2 = 1073767311` ([line of code](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/Distance.cpp#L140))\n* `grid` is initialized as `result.numel()` ([line of code](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/cuda/DistanceKernel.cu#L246))\n* `k` is assigned to the `blockIdx.x` as an `int32` ([line of code](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/cuda/DistanceKernel.cu#L108))\n* `i` is calculated using `2 * k >= 2147534622` ([line of code](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/cuda/DistanceKernel.cu#L112)), which overflows, since `2147534622 > 2147483647 (int32_max)`.\n\nUsing `const int64_t k = blockIdx.x;` would solve the illegal memory access. This seems also be done for [`cdist_kernel_cuda_impl`](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/cuda/DistanceKernel.cu#L198-L201).\n\nHowever, we might expect a slowdown, so I've timed the current PyTorch master vs. this PR:\n(tested with `x = torch.randn(x.size(0), 128)` on a V100)\n\n |x.size(0) | int32 idx | int64 idx | slowdown |\n |----------|-----------|-----------|----------|\n| 50000 | -              | 4.4460 | - |\n| 25000 | 1.02522 | 1.10869 | 7.53% |\n| 12500 | 0.25182 | 0.27277 | 7.68% |\n| 6250 | 0.06291 | 0.06817 | 7.72% |\n| 3125 | 0.01573 | 0.01704 | 7.69% |\n| 1562 | 0.00393 | 0.00426 | 7.75% |\n\nWhile checking the backward kernel, it seems I'm triggering another error with a size limit of\n```python\nx = torch.randn(1449, 1, device='cuda', requires_grad=True)\nout = torch.pdist(x)\nout.mean().backward()\n> RuntimeError: CUDA error: invalid configuration argument\n```\n, while `[<=1448, 1]` works.\n\nI'll take another look at this issue. Let me know, if the potential fix should go into this PR or if I should open a new issue.\n\nCC ngimel, csarofeen\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31593\n\nDifferential Revision: D19825571\n\nPulled By: ngimel\n\nfbshipit-source-id: ace9ccab49f3cf0ce894cdb6daef0795e2e8ec03", "pr_number": "31593", "files_changed": ["aten/src/ATen/native/cuda/DistanceKernel.cu", "test/test_torch.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged", "open source", "triaged"]}, "ad90c97c0a": {"title": "Removes flaky check (#33146)", "body": "Summary:\nAddresses https://github.com/pytorch/pytorch/issues/32949.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33146\n\nDifferential Revision: D19836001\n\nPulled By: mruberry\n\nfbshipit-source-id: 773069ae0c181e1a050b65b888c87590c1dddb32", "pr_number": "33146", "files_changed": ["test/test_cuda.py"], "labels": ["merged"]}, "7b50e76255": {"title": "optimize cat performance on CPU with TensorIterator (#30806)", "body": "Summary:\nThis PR aims at improving `cat` performance on CPU.\nCurrent `cat` logic from `TH` module has no parallelization when the input tensor array are all contiguous.\nThis code also try to reuse the same `TensorIterator` as much as possible, in order to reduce overhead of creating `TensorIterator`, this is helpful when the slice of copy is not large enough.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30806\n\nDifferential Revision: D19275026\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 756e9b86891f725c256b0a6981887ff06d88b053", "pr_number": "30806", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensor.cpp", "aten/src/TH/generic/THTensor.h"], "labels": ["merged", "open source", "triaged"]}, "45818a3de4": {"title": "Remove some Half support in some binary CPU kernels (#33021)", "body": "Summary:\nThey were probably mistakenly added as we do not intend to support Half\non CPUs in general and in these situations Half type would probably be\nsignificantly slower than their float and double counterpart due to the\nlack of vectorization and the need of additional casting.\n\ncc XiaobingSuper\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33021\n\nDifferential Revision: D19795152\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: b19796db88880a46557e1b2fd06e584d46093562", "pr_number": "33021", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp"], "labels": ["merged", "open source", "topic: bc-breaking"]}, "6706c3f457": {"title": "Prepare templates (#30982)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30982\n\nThis stack is a first step toward an effort to fix, clean up and simplify code generation logic. \ufffdPlease see the master [task](https://github.com/pytorch/pytorch/issues/30405) to see related discussions and all the known issues.\n\nMain focus of these changes is TensorOptions in code generation.\nGoals:\n- Remove TensorOptions from generated code wherever it's possible. Leave it only in python/C++ API layers.\n- Refactor TensorOptions logic to a single place.\n- Log all discovered issues.\n\nNon goals:\n- Fix Everything!\n- Remove all the hacks in code generation scripts.\n- Clean up and defector all code generation scripts.\n\n-----------\nIn this PR:\nUpdating the templates.\n\n-----------\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18912680\n\nPulled By: izdeby\n\nfbshipit-source-id: 9e3828e42ee5c3aefbf3729f4a8d6db813f2e7c3", "pr_number": "30982", "files_changed": ["tools/autograd/templates/python_torch_functions_dispatch.h", "tools/autograd/templates/python_variable_methods_dispatch.h", "tools/autograd/templates/variable_factories.h"], "labels": ["merged"]}, "04829e924a": {"title": "Update CPU threading doc (#33083)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33083\n\nAdded more recommendations, some notes and warning\n\nTest Plan: cd docs ; make html\n\nDifferential Revision: D19829133\n\nPulled By: ilia-cher\n\nfbshipit-source-id: b9fbd89f5875b3ce35cc42ba75a3b44bb132c506", "pr_number": "33083", "files_changed": ["docs/source/notes/cpu_threading_runtimes.svg", "docs/source/notes/cpu_threading_torchscript_inference.rst"], "labels": ["merged"]}, "af4d6120bd": {"title": "Temporarily disable failing 'binary_macos_libtorch_2_7_cpu_build' and\u2026 (#33207)", "body": "Summary:\n\u2026 'binary_macos_wheel_3_6_cpu_build' jobs\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33207\n\nDifferential Revision: D19844787\n\nPulled By: kostmo\n\nfbshipit-source-id: d44a0e26bf76afe4a5f94d7f1ad2d558de6f5d47", "pr_number": "33207", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows-binary-builds-smoke-subset.yml"], "labels": ["merged"]}, "44723a1c24": {"title": "[ONNX] Fix ONNX CI (#33200)", "body": "Summary:\nMove the data to aws\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33200\n\nReviewed By: hl475\n\nDifferential Revision: D19843193\n\nPulled By: houseroad\n\nfbshipit-source-id: bb0451d211cfc951ddb66264b92586c43b6e8841", "pr_number": "33200", "files_changed": ["test/onnx/test_pytorch_onnx_caffe2.py"], "labels": ["merged"]}, "cb39a5400c": {"title": "Use C10_WARP_SIZE to fix functionality on HIP vs CUDA for batch_norm_backward_reduce (#33098)", "body": "Summary:\n1. Use C10_WARP_SIZE instead of hardcoded value \"32\".\n2. `getNumThreads` returns a minimum of 32 for CUDA, which is same as the warp size in CUDA. However, for HIP, it returns a minimum of 16, which is less than the warp size (64) in HIP. This creates an issue in the [reduce function](https://github.com/pytorch/pytorch/blob/14548c2d5b40d78f1b45376119eaecd297a83e6c/aten/src/ATen/native/cuda/Normalization.cuh#L115) when it zeroes out the other entries in shared memory [here](https://github.com/pytorch/pytorch/blob/14548c2d5b40d78f1b45376119eaecd297a83e6c/aten/src/ATen/native/cuda/Normalization.cuh#L137): since `blockDim.x` is at least equal to the warp size in CUDA, this never zeroes out `shared[0]`, but for HIP, since `blockDim.x` could be 16 or 32, which is less than the warp size (64), this results in `blockDim.x * blockDim.y` being potentially less than the warp size for small cases, which then zeroes out `shared[0]` as well. This results in an erroneous output of zero for the reduce function on ROCm (depending on how the block dimensions are set).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33098\n\nDifferential Revision: D19837355\n\nPulled By: bddppq\n\nfbshipit-source-id: ea526acd82ec08b1acb25be860b7e663c38ff173", "pr_number": "33098", "files_changed": ["aten/src/ATen/native/cuda/Normalization.cuh", "test/distributed/test_distributed.py"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "acd51e13f7": {"title": "TorchScript add check if quantized", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32890\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19673463\n\nPulled By: z-a-f\n\nfbshipit-source-id: 453ff662810845fcaeb8e6d5919afa8e2d395768", "pr_number": "32890", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged", "quantization"]}, "769abddfa3": {"title": "Build ahead-of-time C++ extensions with ninja on windows", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33084\n\nDifferential Revision: D19817361\n\nPulled By: ezyang\n\nfbshipit-source-id: 95a6d0ffa9beb6885c8a41688621b33da51706ae", "pr_number": "33084", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["open source"]}, "a23009f98f": {"title": "Quantized leaky relu", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33004\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19740193\n\nPulled By: z-a-f\n\nfbshipit-source-id: 32542d5465db44190366a2f8b737305a03b5fa76", "pr_number": "33004", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qrelu.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/test_quantized.py", "torch/nn/quantized/functional.py"], "labels": ["merged", "quantization"]}, "000a5e2b7f": {"title": "bad tbb lambda capture, bad chunk size (#30352)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30352\n\n1) tbb forwards us ident through parameter, we don't need to capture it.\n2) tbb is being passed steps <= 0 which is bad.\n\nTaken from TBB documentation:\n```\nThe index type must be an integral type. The loop must not wrap around. The step value must be positive. If omitted, it is implicitly 1.\n```\n\nI have a build that uses `TBB_USE_DEBUG=1` and there are currently a lot of issues with PyTorch use.\nIs TBB version not tested very much right now?\nghstack-source-id: 94459382\n\nTest Plan: CI green\n\nDifferential Revision: D18666029\n\nfbshipit-source-id: d5aa8327b03181d349e1964f9c8211298c433d6a", "pr_number": "30352", "files_changed": ["aten/src/ATen/ParallelNativeTBB.h"], "labels": ["merged"]}, "74c8a8f7bc": {"title": "Revert D19825127: [pytorch][PR] Move where cuda implementation to TensorIterator", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19825127\n\nOriginal commit changeset: bbf4682349d9\n\nfbshipit-source-id: 0c439b8c9a00a5aa46fd196396cf7cc83cddb1b4", "pr_number": null, "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cpu/TensorCompareKernel.h", "aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/ROCmLoops.cuh", "aten/src/ATen/native/cuda/TensorCompare.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/cuda_vectorized_test.cu"], "labels": []}, "139afd0ea7": {"title": "Fix link to py-spy content in contribution guide TOC (#31760)", "body": "Summary:\nThe extra dashes are breaking the link here\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31760\n\nDifferential Revision: D19697301\n\nPulled By: ezyang\n\nfbshipit-source-id: 65de026b9016dc8689c9dac9efb8aafd00b535cd", "pr_number": "31760", "files_changed": ["CONTRIBUTING.md"], "labels": ["merged", "open source"]}, "1487137c5b": {"title": "add missing default value for LRScheduler.step() (#32411)", "body": "Summary:\nsee also other type errors in https://github.com/pytorch/pytorch/pull/30576 and https://github.com/pytorch/pytorch/pull/30441\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32411\n\nDifferential Revision: D19697245\n\nPulled By: ezyang\n\nfbshipit-source-id: d0295d747541adec5d6fad646f4cf4bb2f04abf5", "pr_number": "32411", "files_changed": ["torch/optim/lr_scheduler.pyi"], "labels": ["merged", "open source"]}, "f255b7a3ac": {"title": "Drop support of the build option USE_GLOO_IBVERBS (#33163)", "body": "Summary:\nTwo releases have passed since its deprecation:\n8a026d4f74b71944ac2860c315996165a40f5626\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33163\n\nDifferential Revision: D19850713\n\nPulled By: ezyang\n\nfbshipit-source-id: 30a60df470b88e8c40e33112296e437cde29c49f", "pr_number": "33163", "files_changed": ["tools/setup_helpers/cmake.py"], "labels": ["merged", "open source", "triaged"]}, "66ee4f1c81": {"title": "[ROCm] Enable Bfloat16 type for activation and batch-norm", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32065\n\nDifferential Revision: D19728858\n\nPulled By: ezyang\n\nfbshipit-source-id: 8f828c558bfe6c5f43f476ff8a0f967341f8f351", "pr_number": "32065", "files_changed": ["aten/src/ATen/Dispatch.h", "aten/src/ATen/native/Normalization.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/cuda/Normalization.cu", "aten/src/ATen/native/cuda/Normalization.cuh", "test/test_nn.py", "torch/nn/modules/module.py", "torch/testing/_internal/common_device_type.py"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "9e7638f7c1": {"title": "\"batchSize\" was set but never used (#32294)", "body": "Summary:\nfixes a compiler warning:\n```\ntorch/aten/src/ATen/native/cuda/MaxUnpooling.cu.cc(402):\nwarning: variable \"batchSize\" was set but never used\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32294\n\nDifferential Revision: D19697277\n\nPulled By: ezyang\n\nfbshipit-source-id: b9821be325826dc4785cad7994803b54f1711a0c", "pr_number": "32294", "files_changed": ["aten/src/ATen/native/cuda/MaxUnpooling.cu"], "labels": ["merged", "open source"]}, "d609497dde": {"title": "bulk_eval_collect_histograms", "body": "Summary:\nCollect activation histograms along the model evaluation and aggregate all the histograms from multiple threads/readers into one file\nThe original functionality of bulk_eval workflow is still valid. The output predictions and extra blobs will be exported to a hive table, which will be very useful for numerical debugging.\n\nTest Plan:\nFBL\n```flow-cli canary dper.workflows.bulk_eval.export --mode dbg --parameters-file experimental/summerdeng/sparsenn/bulk_eval_input_configs.json  --run-as-secure-group team_ai_system_sw_hw_co-design --entitlement gpu_prod --name \"Histogram collection with caffe2 logging. Attach histogram observer to the predict net. Use small model 102343030. \"\n```\nf163861773\n\nWhen the flow is done, we can get all the histogram files under the specified dir. For example:\n```\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6ca65cc0\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6cde8a80\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6d144840\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6d4a9600\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6da303c0\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6dd1c800\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6e0855c0\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6e3e0380\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6e95a140\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6eafcf00\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6ed1a100\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6f094ec0\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6f561c80\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6f783a40\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb6fccb7c0\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb7003d580\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb703ae340\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb7084ae80\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb70bc1c40\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb70f43a00\n-rw-rw-r--. 1 185754 185754 3944091 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb70ff7680\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb71361300\n-rw-rw-r--. 1 185754 185754 3945012 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb716df0c0\n-rw-rw-r--. 1 185754 185754 4024538 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb7199c780\n-rw-rw-r--. 1 185754 185754 3944091 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb71b72f00\n-rw-rw-r--. 1 185754 185754 3944091 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb72330000\n-rw-rw-r--. 1 185754 185754 3944091 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb72598100\n-rw-rw-r--. 1 185754 185754 3944091 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb7290d880\n-rw-rw-r--. 1 185754 185754 3944091 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb72b03980\n-rw-rw-r--. 1 185754 185754 3944091 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb72f1f160\n-rw-rw-r--. 1 185754 185754 3944091 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fcb8bcee9e0\n-rw-rw-r--. 1 185754 185754 3944091 Jan 23 09:45 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.0x7fd51b457260\n-rw-rw-r--. 1 185754 185754 4026659 Jan 23 09:51 /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.final\n```\n\nThe aggregated histogram file is  /mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.final. It can be loaded to the following auto quant workflow for int8 static quantization.\n\n######## Code refactoring ########\n\nMoved the utility functions to process activation histograms to the deeplearning/numeric_suite/toolkit:hist_processor and add the dependency in dper.\n\nWe also had a hist_compiler in the caffe2/caffe2/fb/fbgemm/numerical_debugger/python_utils/hist_compiler.py. Also refactored the code to reuse the utility functions in deeplearning/numeric_suite/toolkit:hist_processor.\n\nThe histograms from bulk_eval and the hist_compiler are identical.\n/mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.compiled.bak\n/mnt/vol/gfsadslearner-frc3c01/fblearner_flow/users/summerdeng/sparsenn/bulk_eval.txt.final.bak\n\nReviewed By: hx89\n\nDifferential Revision: D19270090\n\nfbshipit-source-id: c7ecb4f2bbf1ea725c52e903356ad9a7b9ad73ac", "pr_number": null, "files_changed": ["caffe2/quantization/server/activation_distribution_observer.cc", "caffe2/quantization/server/activation_distribution_observer.h", "caffe2/quantization/server/pybind.cc"], "labels": []}, "7863d2413d": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/9fd0d1a3c7fb2b8adb5474825213de8f3c2e5877\nhttps://github.com/facebook/folly/commit/bcaf9cdf1f300da21fe38c1a22611de152a5701a\nhttps://github.com/facebook/rocksdb/commit/3e49249d302076b45a9cd6b3de3306f00f60bbca\nhttps://github.com/facebookincubator/fizz/commit/98307ea1ecda6b7ffe8356c4fab189b51599c1b0\nhttps://github.com/facebookincubator/katran/commit/f48ebb4d4865d7478cdd7f243fe287ccd8daa862\nhttps://github.com/facebookincubator/mvfst/commit/353f9c9f2954fac713a7b1c656717ea3cfd493e2\nhttps://github.com/facebookincubator/profilo/commit/1caef25fc03f6a0cd8a286be50e192694e0223e8\nhttps://github.com/pytorch/fbgemm/commit/805ab665f286ee96ef57a341061fc8e9cb529eea\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 609187c69ba2c6b31a05dcfdb1770054002ddb6e", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "b9a5353fee": {"title": "Move where cuda implementation to TensorIterator (#33228)", "body": "Summary:\nReopen of https://github.com/pytorch/pytorch/pull/32984\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33228\n\nDifferential Revision: D19850862\n\nPulled By: ngimel\n\nfbshipit-source-id: b92446a49b4980188fa4788220a2164650e905c2", "pr_number": "33228", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cpu/TensorCompareKernel.h", "aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/ROCmLoops.cuh", "aten/src/ATen/native/cuda/TensorCompare.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/cuda_vectorized_test.cu"], "labels": ["merged", "open source"]}, "5bc5dd58f3": {"title": "[jit] fix a typo", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29107\n\nDifferential Revision: D19698662\n\nPulled By: ezyang\n\nfbshipit-source-id: e7eea3246008e2c6d560ff5e4d84b90f65ff1afd", "pr_number": "29107", "files_changed": ["torch/csrc/jit/passes/dead_code_elimination.cpp"], "labels": ["jit", "merged", "open source"]}, "47e589eb6e": {"title": "Disable flaky tests test_DistributedDataParallel and test_backend_group for ROCm (#33211)", "body": "Summary:\nGetting intermittent error in CI runs:\n\n**TestDistBackend.test_DistributedDataParallel**\n```\n02:36:32   File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/serialization.py\", line 442, in _legacy_save\n02:36:32     pickler.dump(obj)\n02:36:32 AttributeError: Can't pickle local object 'Module._replicate_for_data_parallel.<locals>.zero_grad'\n```\nSome CI runs where it failed:\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test2/16163/console\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test2/16165/console\n\n**TestDistBackend.test_backend_group**\n```\ntest_backend_group (__main__.TestDistBackend) ... Memory access fault by GPU node-5 (Agent handle: 0x265c670) on address 0x7fded754a000. Reason: Page not present or supervisor privilege.\n```\nSome CI runs where it failed:\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test2/16288/console\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33211\n\nDifferential Revision: D19849089\n\nPulled By: bddppq\n\nfbshipit-source-id: 5e997653cc344f4c6819d46bedc6d3bd75b5d854", "pr_number": "33211", "files_changed": ["test/distributed/test_distributed.py"], "labels": ["merged", "module: rocm", "open source"]}, "9d9fa2eace": {"title": "[2/3] Bind Bucketize to PyTorch (#33014)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33014\n\nExport Bucketize to PyTorch.\n\nTest Plan: buck test caffe2/caffe2/python/operator_test:torch_integration_test\n\nReviewed By: bddppq\n\nDifferential Revision: D19737534\n\nfbshipit-source-id: be1c892bb8d01da9892f221f150f1a2788ac732e", "pr_number": "33014", "files_changed": ["caffe2/operators/bucketize_op.cc", "caffe2/operators/bucketize_op.h", "caffe2/python/operator_test/torch_integration_test.py"], "labels": ["fb-exported", "merged"]}, "1767ae8daf": {"title": "[caffe2] remove dnnlowp log code (#33184)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33184\n\ndnnlowp specific code shouldn't be in the default FC in the first place\n\nTest Plan: Just removing #ifdef #endif\n\nReviewed By: jianyuh\n\nDifferential Revision: D19835301\n\nfbshipit-source-id: 7880cf298bedb3f0bc407d140d342124663ea4a7", "pr_number": "33184", "files_changed": ["caffe2/operators/fully_connected_op.h"], "labels": ["fb-exported", "merged"]}, "c6e0360812": {"title": "Minor change of docstring example of WeightedRandomSampler (#30846)", "body": "Summary:\nPrevious example\n```python\n>>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n        [0, 0, 0, 1, 0]\n```\nmay seem misleading according to provided weights.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30846\n\nDifferential Revision: D19697367\n\nPulled By: ezyang\n\nfbshipit-source-id: 3d6e3cd0cecb5272a368707ba35bc7acdbd82c30", "pr_number": "30846", "files_changed": ["torch/utils/data/sampler.py"], "labels": ["merged", "open source"]}, "09915ad570": {"title": "[TensorBoard] Correct typo and wrap dataformats. (#31604)", "body": "Summary:\nResolves issue https://github.com/pytorch/pytorch/issues/31603\n\n- A minor spelling typo is corrected: \"suitible\" --> \"suitable\"\n- A minor quality of life improvement is added: the data format strings are better rendered as fixed width to indicate that they are string constants.  \"CHW\" --> \"`CHW`\"\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31604\n\nDifferential Revision: D19697293\n\nPulled By: ezyang\n\nfbshipit-source-id: ee38b0d4c9ca8a233ac9243c310d9a3b42ad6f32", "pr_number": "31604", "files_changed": ["torch/utils/tensorboard/writer.py"], "labels": ["merged", "open source"]}, "05281a5671": {"title": "Add nice error message if missing overrides in custom autograd.Function", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33142\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19815786\n\nPulled By: albanD\n\nfbshipit-source-id: 5513d900c7b711b625383686fcf03f822ab7ea80", "pr_number": "33142", "files_changed": ["test/test_autograd.py", "torch/autograd/function.py"], "labels": ["merged"]}, "3cfea39968": {"title": "Document how BCELoss avoids infinite results (#33160)", "body": "Summary:\nIssue https://github.com/pytorch/pytorch/issues/31453\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33160\n\nDifferential Revision: D19835527\n\nPulled By: albanD\n\nfbshipit-source-id: 82fd2dd46ffbc87e90ca8e100db411b6ff6bfe32", "pr_number": "33160", "files_changed": ["torch/nn/modules/loss.py"], "labels": ["merged", "open source", "triaged"]}, "a389f8fa18": {"title": "Revert D18912680: Prepare templates", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18912680\n\nOriginal commit changeset: 9e3828e42ee5\n\nfbshipit-source-id: 9ef81991394f4e36f0652dfe594d5122969bd9cf", "pr_number": null, "files_changed": ["tools/autograd/templates/python_torch_functions_dispatch.h", "tools/autograd/templates/python_variable_methods_dispatch.h", "tools/autograd/templates/variable_factories.h"], "labels": []}, "87640570b3": {"title": "Make CUDA OOM error a type (#33056)", "body": "Summary:\nThere are cases when we want to recover from CUDA OOM, for example, some cuDNN algorithms use huge workspace and we want to recover from OOM to pick a different algorithm, in such cases, there is no reason to catch all errors.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33056\n\nDifferential Revision: D19795359\n\nPulled By: ezyang\n\nfbshipit-source-id: a34e23bf6d172dc0257389251dafef5b38d27d2b", "pr_number": "33056", "files_changed": ["aten/src/ATen/native/cudnn/Conv.cpp", "c10/cuda/CUDACachingAllocator.cpp", "c10/cuda/CUDACachingAllocator.h", "c10/util/Exception.h"], "labels": ["merged", "open source"]}, "97bf41ca22": {"title": "Fix iOS x86_64 CI failure (#33194)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33194\n\n### Summary\n\nThe iOS x86_64 job has been failed for a few days. I haven't found the root cause, but seems like updating the torchvision to its latest version can fix the problem\n\n### Test Plan\n\n- the x86_64 job works\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19845079\n\nPulled By: xta0\n\nfbshipit-source-id: 5034e252600b6704b860d68c371a65bef4cf37fc", "pr_number": "33194", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-pytorch-ios-builds.yml"], "labels": ["merged"]}, "2e9b7c5fe1": {"title": "Migrate dist from TH to ATen(CPU, CUDA) (#29714)", "body": "Summary:\n[https://github.com/pytorch/pytorch/issues/24691](https://github.com/pytorch/pytorch/issues/24691)\n[https://github.com/pytorch/pytorch/issues/24551](https://github.com/pytorch/pytorch/issues/24551)\n\nBenchmark:\n\n**Speed**\n```python\nimport time, sys\nimport torch\nimport math\n\ninf = math.inf\n\ntorch.manual_seed(0)\ndevices = [\"cpu\", \"cuda\"]\nps = [0, 1, 2, 3, 4, inf, -inf]\n\n# Warm up\nfor device in devices:\n    for n in [1, 10, 100, 1000]:\n        x = torch.randn(100, n, requires_grad=False, device=device)\n        y = torch.randn(100, n, requires_grad=False, device=device)\n        for i in range(1000):\n            for p in ps:\n                dist_xy = torch.dist(x, y, p)\n\nfor device in devices:\n    print('On {}'.format(device))\n    for n in [1, 10, 100, 1000]:\n        total_time = 0\n        x = torch.randn(100, n, requires_grad=False, device=device)\n        y = torch.randn(100, n, requires_grad=False, device=device)\n        for i in range(10000):\n            for p in ps:\n                t1 = time.time()\n                dist_xy = torch.dist(x, y, p)\n                t2 = time.time()\n                total_time += (t2 - t1)\n        average_time = total_time / 10000 / len(ps) * 1000\n        print(\"input size(100, %d) average time is %.8f (ms).\" % (n, average_time))\n```\n\nOutput\nBefore:\n```shel\nOn cpu\ninput size(100, 1) average time is 0.0079491 (ms).\ninput size(100, 10) average time is 0.0364167 (ms).\ninput size(100, 100) average time is 0.3120752 (ms).\ninput size(100, 1000) average time is 3.0605820 (ms).\nOn cuda\ninput size(100, 1) average time is 0.04745627 (ms).\ninput size(100, 10) average time is 0.04919453 (ms).\ninput size(100, 100) average time is 0.06601572 (ms).\ninput size(100, 1000) average time is 0.07849015 (ms).\n```\n\nAfter:\n```shell\nOn cpu\ninput size(100, 1) average time is 0.0099936 (ms).\ninput size(100, 10) average time is 0.0340414 (ms).\ninput size(100, 100) average time is 0.2793379 (ms).\ninput size(100, 1000) average time is 0.7858076 (ms).\nOn cuda\ninput size(100, 1) average time is 0.04410237 (ms).\ninput size(100, 10) average time is 0.03326339 (ms).\ninput size(100, 100) average time is 0.03314828 (ms).\ninput size(100, 1000) average time is 0.03990038 (ms).\n```\n\n**Precision**\n\n```python\nfor device in devices:\n    torch.manual_seed(0)\n    print('On {}'.format(device))\n    for n in [1, 10, 100, 1000]:\n        x = torch.randn(100, n, requires_grad=False).to(device)\n        y = torch.randn(100, n, requires_grad=False).to(device)\n        for p in ps:\n            dist_xy_float = torch.dist(x, y, p)\n            dist_xy_double = torch.dist(x.double(), y.double(), p)\n            difference = torch.abs(dist_xy_double - dist_xy_float)\n            print('input size (100, {}), p: {}, float: {}, double: {}, difference: {}'.format(n, p, dist_xy_float, dist_xy_double, difference))\n```\n\nPart of [output](https://gist.github.com/rivergold/dd95014dc7f163b22f72699d1134cdd2)\nBefore:\n```shell\nOn cpu\ninput size (100, 100), p: 0, float: 10000.0, double: 10000.0, difference: 0.0\ninput size (100, 100), p: 1, float: 11474.1806640625, double: 11474.185433543797, difference: 0.00476948129653465\ninput size (100, 100), p: 2, float: 143.50729370117188, double: 143.5073391487937, difference: 4.5447621829453055e-05\ninput size (100, 100), p: 3, float: 36.045475006103516, double: 36.04550275212738, difference: 2.774602386779179e-05\ninput size (100, 100), p: 4, float: 18.796083450317383, double: 18.79609807865317, difference: 1.4628335787136848e-05\ninput size (100, 100), p: inf, float: 5.540258407592773, double: 5.5402586460113525, difference: 2.384185791015625e-07\ninput size (100, 100), p: -inf, float: 3.4868717193603516e-06, double: 3.4868717193603516e-06, difference: 0.0\nOn cuda\ninput size (100, 100), p: 0, float: 10000.0, double: 10000.0, difference: 0.0\ninput size (100, 100), p: 1, float: 11474.1865234375, double: 11474.185433543797, difference: 0.00108989370346535\ninput size (100, 100), p: 2, float: 143.50733947753906, double: 143.5073391487933, difference: 3.2874575595087663e-07\ninput size (100, 100), p: 3, float: 36.04550552368164, double: 36.045502752127405, difference: 2.7715542358919265e-06\ninput size (100, 100), p: 4, float: 18.796098709106445, double: 18.796098078653177, difference: 6.304532682577246e-07\ninput size (100, 100), p: inf, float: 5.540258407592773, double: 5.5402586460113525, difference: 2.384185791015625e-07\ninput size (100, 100), p: -inf, float: 3.4868717193603516e-06, double: 3.4868717193603516e-06, difference: 0.0\n```\n\nAfter\n```shell\nOn cpu\ninput size (100, 100), p: 0, float: 10000.0, double: 10000.0, difference: 0.0\ninput size (100, 100), p: 1, float: 11474.1806640625, double: 11474.185433543797, difference: 0.00476948129653465\ninput size (100, 100), p: 2, float: 143.50729370117188, double: 143.5073391487937, difference: 4.5447621829453055e-05\ninput size (100, 100), p: 3, float: 36.045475006103516, double: 36.04550275212738, difference: 2.774602386779179e-05\ninput size (100, 100), p: 4, float: 18.796083450317383, double: 18.79609807865317, difference: 1.4628335787136848e-05\ninput size (100, 100), p: inf, float: 5.540258407592773, double: 5.5402586460113525, difference: 2.384185791015625e-07\ninput size (100, 100), p: -inf, float: 3.4868717193603516e-06, double: 3.4868717193603516e-06, difference: 0.0\nOn cuda\ninput size (100, 100), p: 0, float: 10000.0, double: 10000.0, difference: 0.0\ninput size (100, 100), p: 1, float: 11474.185546875, double: 11474.185433543797, difference: 0.00011333120346534997\ninput size (100, 100), p: 2, float: 143.50733947753906, double: 143.5073391487933, difference: 3.2874575595087663e-07\ninput size (100, 100), p: 3, float: 36.04550552368164, double: 36.045502752127405, difference: 2.7715542358919265e-06\ninput size (100, 100), p: 4, float: 18.796096801757812, double: 18.796098078653177, difference: 1.2768953645547754e-06\ninput size (100, 100), p: inf, float: 5.540258407592773, double: 5.5402586460113525, difference: 2.384185791015625e-07\ninput size (100, 100), p: -inf, float: 3.4868717193603516e-06, double: 3.4868717193603516e-06, difference: 0.0\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29714\n\nDifferential Revision: D19769518\n\nPulled By: albanD\n\nfbshipit-source-id: 69b79b64f1f190b410efe884662b6601e903eccf", "pr_number": "29714", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/cuda/Reduce.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorMoreMath.cpp"], "labels": ["merged", "open source", "triaged"]}, "323b0e0a0f": {"title": "fix #30480 torch.normal shape checking is broken (#32243) (#33050)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33050\n\nFollowing what gchanan proposed in #30480\n- If the (logical) shapes of mean and std are broadcastable, we broadcast them for the output\n  Done in tensor iterator already.\n- If the (logical) shapes of mean and std are not broadcastable and they have the same number of elements, we fall back to the old behavior (pick the shape of mean)\n  Done by reshape std to the same shape of mean.\n- If the (logical) shapes of mean and std are not broadcastable and don't have the same number of elements, we error out.\n  Done by tensor iterator already.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19771186\n\nPulled By: glaringlee\n\nfbshipit-source-id: a0b71063c7f5fdda2d4ceb84e06384414d7b4262", "pr_number": "33050", "files_changed": ["aten/src/ATen/ExpandUtils.cpp", "aten/src/ATen/ExpandUtils.h", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/Distributions.h", "aten/src/ATen/native/cuda/Distributions.cu", "test/test_torch.py"], "labels": ["merged"]}, "40265e2d66": {"title": "prevent various warnings related to undef and redef (#33196)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33196\n\nTest Plan: Sandcastle green\n\nReviewed By: malfet\n\nDifferential Revision: D19842268\n\nfbshipit-source-id: 47bc3d7a75e803041491e11a648b4a9e7d9cc72c", "pr_number": "33196", "files_changed": ["aten/src/ATen/ParallelNativeTBB.h", "aten/src/ATen/core/DistributionsHelper.h", "aten/src/ATen/core/MT19937RNGEngine.h", "aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/TH/THAllocator.cpp", "c10/macros/Macros.h", "c10/util/llvmMathExtras.h"], "labels": ["fb-exported", "merged"]}, "ab14375b08": {"title": "Workaround for CUDA10.2.89 CUDA extension compilation error (#33230)", "body": "Summary:\nFixes: https://github.com/pytorch/pytorch/issues/33203\nPR based on https://github.com/mpark/variant/pull/73\n\nVerified locally on CUDA10.2.89 and 10.1.243\n\nThanks ngimel for the hint and gridley for the initial fix in the variant repo! :)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33230\n\nDifferential Revision: D19858083\n\nPulled By: ngimel\n\nfbshipit-source-id: b9438084f5688712c6aa6b17813c68ccde237bbb", "pr_number": "33230", "files_changed": ["c10/util/variant.h"], "labels": ["merged", "open source"]}, "72a00a8a9c": {"title": "Remove Node dependencies from operator.h (#32682)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32682\n\nThis moves code around so that operator.h/cpp no longer requires a full\ndefinition of Node* nor does it include alias analysis or the pretty printer.\n\nThis should make it possible to include in the mobile build.\n\nFunctionality for checking if operators match Node and to look up\nand operator for a Node have moved to the Node object.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19615386\n\nPulled By: zdevito\n\nfbshipit-source-id: e38bdf29971183597ef940d061c06ba56e71d9c5", "pr_number": "32682", "files_changed": ["torch/csrc/jit/autodiff.cpp", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/export.cpp", "torch/csrc/jit/interpreter.cpp", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/ir.h", "torch/csrc/jit/operator.cpp", "torch/csrc/jit/operator.h", "torch/csrc/jit/passes/alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.h", "torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/passes/decompose_ops.cpp", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/passes/requires_grad_analysis.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/passes/utils/check_alias_annotation.cpp", "torch/csrc/jit/script/schema_matching.cpp"], "labels": ["jit", "merged"]}, "99349defc1": {"title": "remove unnecessary Node* ops (#32760)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32760\n\nMinor changes to the way ops are implemented to remove incidental use of Node*\nin the operator implementation.\n\nCurrent state for operators that previously took Node:\n\n```\nTBD:\n\nUSES NODE: prim::DifferentiableGraph(...) -> (...)\nUSES NODE: prim::profile(...) -> (...)\nUSES NODE: prim::FusionGroup(...) -> (...)\nUSES NODE: prim::PythonOp(...) -> (...)\nUSES NODE: prim::ImplicitTensorToNum(Tensor a) -> Scalar # next PR\n\nShould be made interpreter primitives:\n\nUSES NODE: prim::TupleUnpack(...) -> (...)\nUSES NODE: prim::TupleSlice(...) -> (...)\nUSES NODE: prim::TupleConstruct(...) -> (...)\nUSES NODE: prim::ListUnpack(...) -> (...)\nUSES NODE: prim::ListConstruct(...) -> (...)\nUSES NODE: prim::DictConstruct(...) -> (...)\nUSES NODE: prim::Constant() -> (...)\nUSES NODE: prim::isinstance(...) -> (...)\nUSES NODE: prim::CreateObject(...) -> (...)\nUSES NODE: prim::fork(...) -> (...)\nUSES NODE: aten::warn(str message, *, int stacklevel=2) -> () # need stack level information, so ideally in interpreter so it can look at the stack\n\nShould be made into vararg operators, i.e. the operators last argument should be an IValue\nthat contains the number of arguments.\n\nUSES NODE: prim::FusedConcat(...) -> (...)\nUSES NODE: prim::MMTreeReduce(...) -> (...)\nUSES NODE: prim::MMBatchSide(...) -> (...)\nUSES NODE: prim::ConstantChunk(...) -> (...)\nUSES NODE: prim::AutogradAnyNonZero(...) -> bool\nUSES NODE: prim::BroadcastSizes(...) -> (...)\nUSES NODE: prim::ChunkSizes(...) -> (...)\nUSES NODE: aten::format(str self, ...) -> str\nUSES NODE: prim::Print(...) -> (...)\n\nfixed:\n\nUSES NODE: aten::extend(Tensor[](a!) self, Tensor [] other) -> ()\nUSES NODE: aten::copy(Tensor[](a) self) -> Tensor[]\nUSES NODE: aten::extend(int[](a!) self, int [] other) -> ()\nUSES NODE: aten::copy(int[](a) self) -> int[]\nUSES NODE: aten::extend(float[](a!) self, float [] other) -> ()\nUSES NODE: aten::copy(float[](a) self) -> float[]\nUSES NODE: aten::extend(bool[](a!) self, bool [] other) -> ()\nUSES NODE: aten::copy(bool[](a) self) -> bool[]\nUSES NODE: aten::extend(t[](a!) self, t [] other) -> ()\nUSES NODE: aten::copy(t[](a) self) -> t[]\nUSES NODE: aten::keys(Dict(str, t) self) -> str[](*)\nUSES NODE: aten::values(Dict(str, t) self) -> t[](*)\nUSES NODE: aten::dict((str, tVal)[] inputs) -> Dict(str, tVal)\nUSES NODE: aten::keys(Dict(int, t) self) -> int[](*)\nUSES NODE: aten::values(Dict(int, t) self) -> t[](*)\nUSES NODE: aten::dict((int, tVal)[] inputs) -> Dict(int, tVal)\nUSES NODE: aten::keys(Dict(float, t) self) -> float[](*)\nUSES NODE: aten::values(Dict(float, t) self) -> t[](*)\nUSES NODE: aten::dict((float, tVal)[] inputs) -> Dict(float, tVal)\nUSES NODE: aten::keys(Dict(Tensor, t) self) -> Tensor[](*)\nUSES NODE: aten::values(Dict(Tensor, t) self) -> t[](*)\nUSES NODE: aten::dict((Tensor, tVal)[] inputs) -> Dict(Tensor, tVal)\nUSES NODE: aten::test_vartype2(t a, t[] b) -> (t[])\nUSES NODE: aten::_ncf_unsqueeze(Tensor self, int ndim) -> Tensor\nUSES NODE: aten::_ncf_view(Tensor self, int[] input_shape, int normalized_ndim) -> Tensor\nUSES NODE: prim::is_none(int? a) -> bool\nUSES NODE: aten::__interpolate(Tensor input, int? size = None, float[]? scale_factor = None, str mode = 'nearest', bool? align_corners = None, bool? recompute_scale_factor = None) -> Tensor\nUSES NODE: aten::__interpolate(Tensor input, int[]? size = None, float[]? scale_factor = None, str mode = 'nearest', bool? align_corners = None, bool? recompute_scale_factor = None) -> Tensor\nUSES NODE: aten::__interpolate(Tensor input, int? size = None, float? scale_factor = None, str mode = 'nearest', bool? align_corners = None, bool? recompute_scale_factor = None) -> Tensor\nUSES NODE: aten::__interpolate(Tensor input, int[]? size = None, float? scale_factor = None, str mode = 'nearest', bool? align_corners = None, bool? recompute_scale_factor = None) -> Tensor\nUSES NODE: aten::sorted(t[](a) self) -> (t[])\nUSES NODE: aten::sort(t[](a!) self, bool reverse=False) -> ()\nUSES NODE: aten::test_vartype(t[] a, t b) -> (t)\nUSES NODE: prim::unchecked_unwrap_optional(t(a)? optional) -> t(a)\nUSES NODE: prim::unchecked_cast(...) -> (...)\nUSES NODE: aten::dict() -> Dict(str, Tensor)\nUSES NODE: prim::Load(...) -> (...)\nUSES NODE: prim::Store(...) -> (...)\nUSES NODE: prim::Drop(...) -> (...)\nUSES NODE: aten::tensor(t[] data, *, ScalarType? dtype=None, Device? device=None, bool requires_grad=False) -> Tensor\nUSES NODE: aten::as_tensor(t[] data, *, ScalarType? dtype=None, Device? device=None) -> Tensor\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19615387\n\nPulled By: zdevito\n\nfbshipit-source-id: 95298c3c4249b9f812c332d13f0fb79daeecb662", "pr_number": "32760", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/test_schema_matching.cpp", "test/jit/test_class_type.py", "test/test_jit.py", "torch/csrc/jit/passes/decompose_ops.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/register_special_ops.cpp"], "labels": ["jit", "merged"]}, "f045dab3dd": {"title": "Remove ImplicitTensorToNum (#32761)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32761\n\nThis replaces ImplicitTensorToNum with result-specific operators like\nIntImplicit, FloatImplicit, or ScalarImplicit. Note that ScalarImplicit\nwas not correctly implemented before and this PR fixes the lapse.\n\nThis does not change on-disk serialization because these operators are not\nserialized directly but written as eg. `annotated(int, foo)`.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19615385\n\nPulled By: zdevito\n\nfbshipit-source-id: 48575f408e8219d2ec5b46936fc2aa691f283976", "pr_number": "32761", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/backward_compatibility/check_backward_compatibility.py", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/ir.h", "torch/csrc/jit/passes/erase_number_types.cpp", "torch/csrc/jit/passes/erase_number_types.h", "torch/csrc/jit/passes/peephole.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/script/schema_matching.cpp", "torch/csrc/jit/script/schema_matching.h", "torch/csrc/jit/symbolic_script.cpp", "torch/csrc/jit/tracer.cpp"], "labels": ["jit", "merged"]}, "f9ad5528e0": {"title": "Fix for rand_like as well. (#33095)", "body": "Summary:\nThis is a followup PR to https://github.com/pytorch/pytorch/issues/32830 This solves the same issue for RandLike which we saw in RandNLike\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33095\n\nReviewed By: hl475\n\nDifferential Revision: D19848625\n\nPulled By: houseroad\n\nfbshipit-source-id: 147921becf79490027a93606d52c5bc41d9eaf7f", "pr_number": "33095", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source"]}, "ac8511a21e": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/927d8afa7a34edf6e99e3952904c19d16ab5f316\nhttps://github.com/facebook/rocksdb/commit/e64508917b413c7d2613c0efd115dc327e5f6a93\nhttps://github.com/pytorch/fbgemm/commit/40d690970fed31a1127ecf6d1c9ebf4877fd4272\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 9135af67550f83a598a0a0baa1f9f6b1e4311ddf", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "0e753b2818": {"title": "Fix SIGABORT caused by double exception in PyTorchStreamReader when file not found. (#33243)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33243\n\nIf a file does not exist in an archive, PyTorchStreamReader throws an exception. However, when PyTorchStreamReader is destructed another exception is thrown while processing the first exception. As a result of this double exception there is SIGABORT.\n\nThanks dreiss for catching this bug and suggesting the fix. It happened when he used _load_for_mobile to load a torch script file without bytecode session. A unittest is added to test this case.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19859205\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 8f96b6256f1a1f933fce1c256d64604c7e9269e4", "pr_number": "33243", "files_changed": ["caffe2/serialize/inline_container.cc", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h"], "labels": ["jit", "merged"]}, "bc0ab07064": {"title": "Opitmize Unfold3d to improve performance of Conv3d (#33191)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33191\n\nOpitmize Unfold3d to improve performance of Conv3d forward\n\nTest Plan: buck test mode/dev-nosan //caffe2/test:nn -- \"Conv3d\"\n\nReviewed By: houseroad\n\nDifferential Revision: D19821946\n\nfbshipit-source-id: 937adafddb9a1aef5f1d1423dd99884c59e465f9", "pr_number": "33191", "files_changed": ["aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/native/Unfold3d.cpp", "aten/src/ATen/native/Unfold3d.h"], "labels": ["fb-exported", "merged"]}, "914610d079": {"title": "[pytorch][quant] Add assert for min, max, qmin, qmax for ChooseQuantizationParams (#32739)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32739\n\nAs Title says.\nghstack-source-id: 98061467\n\nTest Plan: CI\n\nDifferential Revision: D19610810\n\nfbshipit-source-id: f9621cd7d780769941ed77974b19c5226d4b2b30", "pr_number": "32739", "files_changed": ["aten/src/ATen/native/quantized/cpu/quant_utils.h"], "labels": ["merged"]}, "91744907d4": {"title": "SGD: updated step and class design (#32592)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32592\n\nDifferential Revision: D19868154\n\nPulled By: anjali411\n\nfbshipit-source-id: ce888efc68b1531d97e8b0abf2b146198e012d2f", "pr_number": "32592", "files_changed": ["test/cpp/api/serialize.cpp", "torch/csrc/api/include/torch/optim/optimizer.h", "torch/csrc/api/include/torch/optim/serialize.h", "torch/csrc/api/include/torch/optim/sgd.h", "torch/csrc/api/src/optim/adagrad.cpp", "torch/csrc/api/src/optim/optimizer.cpp", "torch/csrc/api/src/optim/sgd.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "806e7daa1f": {"title": "Rename TorchScript compiler to IR emitter to better reflect its function. (#33127)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33127\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19806503\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: ab78bdbbac5f12dbcc6c2e2573f5862a16ffcf3d", "pr_number": "33127", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_fuser.cpp", "test/cpp/jit/test_misc.cpp", "tools/build_variables.py", "torch/csrc/api/src/jit.cpp", "torch/csrc/jit/autodiff.cpp", "torch/csrc/jit/docs/OVERVIEW.md", "torch/csrc/jit/graph_executor_impl.h", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/canonicalize.cpp", "torch/csrc/jit/passes/decompose_ops.cpp", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/inline_forked_closures.cpp", "torch/csrc/jit/passes/lift_closures.cpp", "torch/csrc/jit/script/compilation_unit.h", "torch/csrc/jit/script/compiler.cpp", "torch/csrc/jit/script/compiler.h", "torch/csrc/jit/script/convert_to_ssa.cpp", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/script/ir_emitter.cpp", "torch/csrc/jit/script/ir_emitter.h", "torch/csrc/jit/script/module.cpp", "torch/csrc/jit/symbolic_script.cpp", "torch/jit/supported_ops.py"], "labels": ["jit", "merged"]}, "f61b45fc89": {"title": "[jit] Support properties on `Device` (#32953)", "body": "Summary:\nStacked PRs\n * #32955 - [jit] Fix flipped PackedSequence outputs in script\n * **#32953 - [jit] Support properties on `Device`**\n\nPyTorch devices have a `index` and `type` property. This PR adds support for both to TorchScript\n](https://our.intern.facebook.com/intern/diff/19849320/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32953\n\nPulled By: driazati\n\nDifferential Revision: D19849320\n\nfbshipit-source-id: ce845258c6110058dd9ea1f759ef74b7ed2e786e", "pr_number": "32953", "files_changed": ["test/test_jit.py", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/script/sugared_value.cpp"], "labels": ["jit", "merged"]}, "e45343fa14": {"title": "TORCH_INTERNAL_ASSERT_DEBUG_ONLY not eating message string (#33251)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33251\n\nSomehow this was preventing `c10::Error` exceptions from ever being thrown on windows when `defined(NDEBUG) == false`. Kinda scary.\n\nTest Plan: sandcastle green, made sure `intrusive_ptr_test.cpp` (givenStackObject_whenReclaimed_thenCrashes) passed inside ovrsource using `mode/win/dev-debug`\n\nReviewed By: malfet\n\nDifferential Revision: D19865667\n\nfbshipit-source-id: c32d5752025c043e57d16c6d14a94b069bed0bc3", "pr_number": "33251", "files_changed": ["c10/util/Exception.h"], "labels": ["fb-exported", "merged"]}, "03e9b9ce18": {"title": "[PyTorch BC] Remove unnecessary items in whitelist (#33247)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33247\n\nremove stale items.\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D19861294\n\nfbshipit-source-id: 2b112e5908c19a1ff190e3850085038065d21c53", "pr_number": "33247", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["fb-exported", "merged"]}, "16685d93e9": {"title": "[TVM] Add ReplaceNaN op (#33256)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33256\n\nTest Plan: buck test caffe2/caffe2/fb/tvm:test_tvm_transform\n\nReviewed By: yinghai\n\nDifferential Revision: D19851553\n\nfbshipit-source-id: dee048c52ade16d9e531256b90e5d3391632cd8e", "pr_number": "33256", "files_changed": ["caffe2/opt/tvm_transformer.cc"], "labels": ["fb-exported", "merged"]}, "b98c7d34ed": {"title": "[TVM] Add clip op to c2_frontend (#33257)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33257\n\nTest Plan: buck test caffe2/caffe2/fb/tvm:test_tvm_transform\n\nReviewed By: yinghai\n\nDifferential Revision: D19866406\n\nfbshipit-source-id: e903e15178af323d0bd1f804e09919023c0a2989", "pr_number": "33257", "files_changed": ["caffe2/opt/tvm_transformer.cc"], "labels": ["fb-exported", "merged"]}, "d554b112e3": {"title": "Add histogram collection and weight prepacking utils (#33125)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33125\n\nProvide histogram collection and weights prepacking interface for Dper to auto quantize the Ads models.\n\nTest Plan:\nbuck test mode/opt deeplearning/numeric_suite/toolkit/test:int8_static_utils_test\n\nbuck test mode/opt deeplearning/numeric_suite/toolkit/test:histogram_utils_test\n\nReviewed By: amylittleyang\n\nDifferential Revision: D19794819\n\nfbshipit-source-id: 6a4f4a6684da0977b7df2feed8a4b961db716da8", "pr_number": "33125", "files_changed": ["caffe2/quantization/server/activation_distribution_observer.cc", "caffe2/quantization/server/activation_distribution_observer.h", "caffe2/quantization/server/pybind.cc"], "labels": ["fb-exported", "merged"]}, "ff7d147732": {"title": "Restore tests binary_macos_libtorch_2_7_cpu_build and binary_macos_li\u2026 (#33291)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/33209\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33291\n\nDifferential Revision: D19878241\n\nPulled By: zhangguanheng66\n\nfbshipit-source-id: 07bce43e466708dacd37b87ba3419435c6a7cde5", "pr_number": "33291", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows-binary-builds-smoke-subset.yml"], "labels": ["merged"]}, "0bf60e348f": {"title": "Revert D19878241: [pytorch][PR] Restore tests binary_macos_libtorch_2_7_cpu_build and binary_macos_li\u2026", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19878241\n\nOriginal commit changeset: 07bce43e4667\n\nfbshipit-source-id: 7f76717d73e264f30e8f56fb7bc38c8928dea092", "pr_number": null, "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows-binary-builds-smoke-subset.yml"], "labels": []}, "d613bd0522": {"title": "[rpc][easy] move unnecessary python call directly to pybind (#33174)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33174\n\nCloses https://github.com/pytorch/pytorch/issues/32780. It looks like\nthis is the only callsite where we do `_get_current_rpc_agent().foo()`, and we\ncan do this directly in the pybind layer to save some overhead.\nghstack-source-id: 98200664\n\nTest Plan: All UTs should pass.\n\nDifferential Revision: D19828786\n\nfbshipit-source-id: 5c34a96b5a970e57e6a1fdf7f6e54c1f6b88f3d8", "pr_number": "33174", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "40246fa63c": {"title": "Gradient scaling API (#26512)", "body": "Summary:\nThis PR implements the gradient scaling API that mruberry, jjsjann123, ngimel, zdevito, gchanan and I have been discussing.  Relevant issue/RFC: https://github.com/pytorch/pytorch/issues/25081.\n\nVolume-wise, this PR is mostly documentation and tests.  The Python API (found entirely in `torch/cuda/amp/amp_scaler.py`) is lightweight .  The exposed functions are intended to make the implementation and control flow of gradient scaling convenient, intuitive, and performant.\n\nThe API is probably easiest to digest by looking at the documentation and examples. `docs/source/amp.rst` is the homepage for the Automatic Mixed Precision package.  `docs/source/notes/amp_examples.rst` includes several examples demonstrating common but not-immediately-obvious use cases.  Examples are backed by tests in `test_cuda.py` (and thankfully the tests pass :P).\n\nTwo small utility kernels have been added in `native/cuda/AmpKernels.cu` to improve performance and avoid host-device synchronizations wherever possible.\n\nExisting optimizers, both in the wild and in Pytorch core, do not need to change to use the scaling API.\n\nHowever, the API was also designed to establish a contract between user scripts and optimizers such that writers of _new_ custom optimizers have the control points they need to implement fast, optionally sync-free updates.  User scripts that obey the scaling API can drop such custom optimizers in and reap performance benefits without having to change anything aside from the optimizer constructor itself.  [I know what the contract with custom optimizers should be](https://github.com/pytorch/pytorch/blob/35829f24ef2a71eacedcd6307b2fe9325e4a6a94/torch/cuda/amp/amp_scaler.py#L179-L184), but I'm waiting for review on the rest of the API before I go about documenting it (it will be given a dedicated section in `docs/source/notes/amp_examples.rst`.\n\nCurrently, the gradient scaling examples do not include the auto-casting API as discussed in https://github.com/pytorch/pytorch/issues/25081.  The gradient scaling API is intended to be orthogonal/modular relative to autocasting.  Without auto-casting the gradient scaling API is fully use-_able_, but not terribly use-_ful_, so it's up to you guys whether you want to wait until auto-casting is ready before merging the scaling API as well.\n\n### Todo\n- [ ] How do I get c10 registered status for my two custom kernels?  They're very simple.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26512\n\nDifferential Revision: D19859905\n\nPulled By: mruberry\n\nfbshipit-source-id: bb8ae6966214718dfee11345db824389e4286923", "pr_number": "26512", "files_changed": ["aten/src/ATen/native/cuda/AmpKernels.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/amp.rst", "docs/source/index.rst", "docs/source/notes/amp_examples.rst", "test/test_cuda.py", "torch/cuda/__init__.py", "torch/cuda/amp/__init__.py", "torch/cuda/amp/grad_scaler.py"], "labels": ["merged", "module: autograd", "module: cuda", "module: docs", "module: internals", "module: onnx", "module: operators", "module: third_party", "open source", "triaged"]}, "acea368095": {"title": "Fix compilation error when buildng with FFMPEG (#27589)", "body": "Summary:\nWhen building with FFMPEG, I encountered compilation error due to missing include/library.\nI also find the change in video_input_op.h will improve build on Windows.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27589\n\nDifferential Revision: D19700351\n\nPulled By: ezyang\n\nfbshipit-source-id: feff25daa43bd2234d5e75c66b9865b672a8fb51", "pr_number": "27589", "files_changed": ["caffe2/video/video_decoder.cc", "caffe2/video/video_input_op.h", "cmake/Modules/FindFFmpeg.cmake"], "labels": ["caffe2", "merged", "module: build", "open source"]}, "2635055229": {"title": "[ROCm] Enable 3D batch norms through MIOpen (#33262)", "body": "Summary:\nEnable test for Caffe2\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33262\n\nDifferential Revision: D19880486\n\nPulled By: bddppq\n\nfbshipit-source-id: af663a11137a53302e55198f38117ab6bdc9ec89", "pr_number": "33262", "files_changed": ["aten/src/ATen/native/Normalization.cpp", "caffe2/operators/hip/spatial_batch_norm_op_miopen.hip", "caffe2/python/operator_test/spatial_bn_op_test.py"], "labels": ["merged", "module: rocm", "open source"]}, "946f3a9ed7": {"title": "Refactor and add VS 14.16 and 2019 CI for Windows (#33117)", "body": "Summary:\nChanges according to https://github.com/pytorch/pytorch/issues/18319.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33117\n\nDifferential Revision: D19858239\n\nPulled By: ezyang\n\nfbshipit-source-id: f068d8505886b92c9388c9c636eab5bd20377ceb", "pr_number": "33117", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/pytorch-build-params.yml", ".circleci/verbatim-sources/pytorch-job-specs.yml", ".circleci/verbatim-sources/windows-build-test.yml", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", "test/cpp_extensions/setup.py", "torch/csrc/jit/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/fuser/cpu/msvc_arch.h"], "labels": ["merged", "open source", "triaged"]}, "0c474d95d9": {"title": "Remove Half support in binary cross entropy and some activation functions on CPU (#33206)", "body": "Summary:\nFor reasons similar to https://github.com/pytorch/pytorch/issues/33021. Note that the support of half type has\nnot been available in any releases yet so it should be safe to remove (All forward ones concerning this PR were added in daef363b15c8a3aaaed09892004dc655df76ff81 and 8cb05e72c69fdd837548419770f3f1ba9807c16d)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33206\n\nDifferential Revision: D19861137\n\nPulled By: ezyang\n\nfbshipit-source-id: 38a3a398a716a782c26a611c56ddeab7eb7ac79e", "pr_number": "33206", "files_changed": ["aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/cpu/Activation.cpp"], "labels": ["merge-this-please", "merged", "open source"]}, "bf16688538": {"title": "[JIT] peephole optimize values with NoneType (#33264)", "body": "Summary:\nIf a value has the type None, we can always replace it with a None constant.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33264\n\nDifferential Revision: D19878695\n\nPulled By: eellison\n\nfbshipit-source-id: 5d0e7ffb37c5747997df093fec3183039d8dff4d", "pr_number": "33264", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/peephole.cpp"], "labels": ["jit", "merged"]}, "b28a834813": {"title": "[codemod][lint][fbcode] Apply google-java-format", "body": "Test Plan: Sandcastle. Visual inspection.\n\nReviewed By: scottrice\n\nDifferential Revision: D19878711\n\nfbshipit-source-id: be56f70b35825140676be511903e5274d1808f25", "pr_number": null, "files_changed": ["android/pytorch_android/src/androidTest/java/org/pytorch/PytorchHostTests.java", "android/pytorch_android/src/androidTest/java/org/pytorch/PytorchTestBase.java", "android/pytorch_android/src/main/java/org/pytorch/DType.java", "android/pytorch_android/src/main/java/org/pytorch/NativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java", "android/pytorch_android/src/main/java/org/pytorch/Tensor.java", "android/pytorch_android_torchvision/src/androidTest/java/org/pytorch/torchvision/TorchVisionInstrumentedTests.java", "android/pytorch_android_torchvision/src/main/java/org/pytorch/torchvision/TensorImageUtils.java", "android/test_app/app/src/main/java/org/pytorch/testapp/CameraActivity.java", "android/test_app/app/src/main/java/org/pytorch/testapp/Constants.java", "android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java"], "labels": []}, "d0435604a5": {"title": "[quant] Add a quantized batch_norm operator (#33080)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33080\n\nQuantized batch norm for cases where batch norm cannot be fused with conv.\nAVX2 implementation is from Caffe2.\n\nTest Plan:\npython test/test_quantized.py TestQuantizedOps.test_batch_norm\n\nImported from OSS\n\nDifferential Revision: D19861927\n\nfbshipit-source-id: bd8cd101fc063cb6358132ab7c651a160999293c", "pr_number": "33080", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qbatch_norm.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/test_quantized.py"], "labels": ["merged"]}, "2e88d3d703": {"title": "[quant] Add Quantized BatchNorm2d module (#33109)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33109\n\nTest Plan:\npython test/test_quantized_nn_mods.py ModuleAPITest.test_batch_norm\n\nImported from OSS\n\nDifferential Revision: D19861926\n\nfbshipit-source-id: 67315e49b4b3577b965d422ca707d927d977feeb", "pr_number": "33109", "files_changed": ["test/test_quantized_nn_mods.py", "torch/nn/quantized/modules/__init__.py", "torch/nn/quantized/modules/batchnorm.py", "torch/quantization/default_mappings.py"], "labels": ["merged"]}, "6c6a814a2c": {"title": "Beef up documentation on DispatchKey.h (#33011)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33011\n\nI also reordered some of the keys in non-semantic ways to make the\norganizational grouping mroe clear.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19796584\n\nPulled By: ezyang\n\nfbshipit-source-id: 3083abadb47e9f382b9fbe981af0b34203c6ea4d", "pr_number": "33011", "files_changed": ["c10/core/DispatchKey.h"], "labels": ["merged"]}, "0c93c2b142": {"title": "Add a warning sign for anomaly detection (#33176) (#33239)", "body": "Summary:\nFixes [33176](https://github.com/pytorch/pytorch/issues/33176)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33239\n\nDifferential Revision: D19879847\n\nPulled By: albanD\n\nfbshipit-source-id: 594b936c10f98c364331e782b64f42059413a741", "pr_number": "33239", "files_changed": ["torch/autograd/anomaly_mode.py"], "labels": ["merged", "open source", "triaged"]}, "5b922918d0": {"title": "Disable flaky test TestCppExtensionAOT.test_cuda_extension in Windows CI (#33282)", "body": "Summary:\nSee https://github.com/pytorch/pytorch/issues/33270 for details.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33282\n\nDifferential Revision: D19886975\n\nPulled By: yf225\n\nfbshipit-source-id: 7e6756095b1bb8c55fc5acb8fc2cb02c1e89b032", "pr_number": "33282", "files_changed": ["test/test_cpp_extensions_aot.py"], "labels": ["merged"]}, "bbdc5b7bd0": {"title": "Optimize error checking in mvlgamma (#32665)", "body": "Summary:\n- Clean up error checking code\n- Avoid unecessary floating-point computation\n- Use float instead of double when possible to avoid massive cast in the tensor\n- Use bool instead of uint8_t for clear Boolean purpose\n- Improve error message\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32665\n\nDifferential Revision: D19601920\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 0c6c6b5ff227b1437a6c1bae79b2c4135a13cd37", "pr_number": "32665", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp", "test/test_torch.py"], "labels": ["merged", "module: operators", "open source"]}, "0808485c6a": {"title": "Workaround performance bug / memory leak in GOMP (#32875)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/32008\n\nThis is similar to CaoZhongZ's patch which runs on all OpenMP threads in the team and selectively exits early to scale the number of threads active. I have also restored the `if` clause from before https://github.com/pytorch/pytorch/issues/26963 so that running on 1 thread should still avoid additional synchronisation.\n\nOne comment is that this does slightly change the meaning of `at::get_num_threads` inside of a `parallel_for` loop since it's not guaranteed that the function was called on that many threads. I've looked at the uses within ATen and couldn't see anything that would be problematic. There are a few places in `quantized` that seem to make this assumption but they always use a grain size of 1 so should be safe:\nhttps://github.com/pytorch/pytorch/blob/d9e99ab544cceaf346605db1af4a862197a107cd/aten/src/ATen/native/quantized/cpu/qconv.cpp#L436-L437\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32875\n\nDifferential Revision: D19775823\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 4f843b78cdb9e2766339590d728923786a00af6d", "pr_number": "32875", "files_changed": ["aten/src/ATen/Parallel.h", "aten/src/ATen/ParallelOpenMP.h"], "labels": ["merged", "open source", "triaged"]}, "7ae1e023e7": {"title": "glu: port cpu forward implementation to ATen (#26410)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26410\n\nI only ported the CPU forward implementation for now to try a CPU-only benchmark.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D17454519\n\nPulled By: gchanan\n\nfbshipit-source-id: ff757cf972c5627074fea2f92a670129007a49f4", "pr_number": "26410", "files_changed": ["aten/src/ATen/native/GatedLinearUnit.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/THNN/generic/GatedLinearUnit.c"], "labels": ["merged", "module: cpu", "module: operators"]}, "eb9b4b1f29": {"title": "handle errors in ProcessGroupAgent::listenLoop(). (#32957)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32957\n\nCloses https://github.com/pytorch/pytorch/issues/29703. If there is a\ngloo timeout and `recvWork->wait()` times out in `listenLoop()`,\nprocessGroupagent crashes since there is an unhandled exception in a thread.\nThis catches the exception and exits the listen loop. In a follow up diff, we\nwill enhance these error conditions so that if users attempt to send RPCs\nagain, they are notified that the RPC agent was in a bad state and it was\nshutdown.\n\nThis PR also adds a new option, `processGroupTimeout` to PG agent's backend\noptions. This allows us to control the gloo timeout.\nghstack-source-id: 98236783\n\nTest Plan: Added a unit test.\n\nDifferential Revision: D19678979\n\nfbshipit-source-id: 3895ae754f407b84aca76c6ed3cb087d19178c40", "pr_number": "32957", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/distributed/rpc/backend_registry.py", "torch/distributed/rpc/constants.py"], "labels": ["merged"]}, "a80d0330e4": {"title": "add int4 fake fp16 mappings", "body": "Summary: update this mapping with thte int4 sls ops so we can run netrunner\n\nTest Plan: testing with net_runner\n\nReviewed By: jfix71\n\nDifferential Revision: D19879826\n\nfbshipit-source-id: eac84b10e2365c21cb8a7cfbf3123e26a9945deb", "pr_number": null, "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc"], "labels": []}, "cb4e6d025a": {"title": "Updates numpy to tensor negative stride error message (#33254)", "body": "Summary:\nSee https://discuss.pytorch.org/t/bugs-about-torch-from-numpy-array/43312.\n\nThis update incorporates albanD 's suggestion into the error message, saving future users from having to ask or look on the forums if they encounter this issue and don't mind making their arrays contiguous.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33254\n\nDifferential Revision: D19885808\n\nPulled By: mruberry\n\nfbshipit-source-id: 8f0fd994cf8c088bf3c3940ab4dfb3ddbc5b3ede", "pr_number": "33254", "files_changed": ["torch/csrc/utils/tensor_numpy.cpp"], "labels": ["merged"]}, "9ae4d38a21": {"title": "[rpc] Switch RRef to be managed by intrusive_ptr (#33189)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33189\n\nAdd RRefInterface to Aten/Core, which will later be used by IValue\n\nSwitch all the rpc code base to use intrusive_ptr instead of shared_ptr,\nso that we could add it to IValue.\n\nActual adding to IValue and JIT will be in next PR\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19871241\n\nPulled By: wanchaol\n\nfbshipit-source-id: d7e1fd04b46320e0f26c18591b49c92ad30a4032", "pr_number": "33189", "files_changed": ["aten/src/ATen/core/rref_interface.h", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/distributed/rpc/rref_interface.h", "torch/csrc/distributed/rpc/torchscript_functions.cpp", "torch/csrc/distributed/rpc/torchscript_functions.h"], "labels": ["merged"]}, "b2c5896432": {"title": "[jit] Add RRef to IValue and JIT type system (#32992)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32992\n\nThis PR add RRef to IValue and the JIT type system.\n\n- The RRefInterface abstract class inherit from intrusive_ptr_target,\n  this made the RRef class can be hold in ivalue as intrusive_ptr\n\n- Add RRefType as a JIT type, it's a container type similar to\nfuture type.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19871242\n\nPulled By: wanchaol\n\nfbshipit-source-id: cb80ca32605096f9a42ef147109fb368a7c1d4d3", "pr_number": "32992", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/unpickler.cpp"], "labels": ["jit", "merged"]}, "93179b1c1c": {"title": "[jit] Initial use RRef in TorchScript (#33190)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33190\n\nThis enable the initial RRef type to be used inside TorchScript, user\ncould pass a python RRef into a torchscript function and call to_here\ninside. Specifically, this PR:\n\n- Add RRef schema type parsing\n- Add python interop for RRef in Python and into JIT\n- register to_here op in register_distributed_ops\n\nMore support for RRef in TorchScript will be added in future PRs\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19871244\n\nPulled By: wanchaol\n\nfbshipit-source-id: 7eca6c491a84666b261c70806254b705603bd663", "pr_number": "33190", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "test/distributed/rpc/test_rpc_spawn.py", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/register_distributed_ops.cpp", "torch/csrc/jit/script/schema_type_parser.cpp", "torch/csrc/jit/script/script_type_parser.cpp", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["jit", "merged"]}, "e5c7b7b8b5": {"title": "Automatic update of fbcode/onnx to 04a29addfd5b912812addb8dea5f8763fbfaad01 (#33328)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33328\n\nPrevious import was 8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e\n\nIncluded changes:\n- **[04a29add](https://github.com/onnx/onnx/commit/04a29add)**: Use // instead of # (#2598) <Lu Fang>\n- **[f8e140a9](https://github.com/onnx/onnx/commit/f8e140a9)**: Kezhan/function update (#2596) <Ke Zhang>\n- **[6185faae](https://github.com/onnx/onnx/commit/6185faae)**: fix the attribute types section in IR.md (#2590) <Ke Zhang>\n- **[f254647a](https://github.com/onnx/onnx/commit/f254647a)**: Allow Constant operator to promote scalar and list to tensors. (#2592) <Jeremy Cochoy>\n- **[f12ec799](https://github.com/onnx/onnx/commit/f12ec799)**: Add NegativeLogLikelihood(NllLoss) op (#2551) <liqunfu>\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D19897554\n\nfbshipit-source-id: d8efb5c5ac8f9d71727de33c67af681ed8ec8123", "pr_number": "33328", "files_changed": ["caffe2/python/onnx/tests/onnx_backend_test.py", "third_party/onnx"], "labels": ["fb-exported", "merged"]}, "642bd51043": {"title": "[ONNX] Skip problematic ONNX test to unblock CI (#33323)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33323\n\nskip the tests until it is fixed\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D19894675\n\nfbshipit-source-id: 1cfc153577bf021171f4412115d84719beae7a91", "pr_number": "33323", "files_changed": ["test/onnx/test_utility_funs.py"], "labels": ["fb-exported", "merged"]}, "92fbf7cf97": {"title": "[caffe2] use JIT'ed fp16 SLS (#32432)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32432\n\nUse JIT'ed fp16 SLS in D19477209 from Caffe2 operators\n\nTest Plan: CI\n\nReviewed By: jianyuh\n\nDifferential Revision: D19477208\n\nfbshipit-source-id: ef2ccba10f5f4c475166141bf09c266dedb92d38", "pr_number": "32432", "files_changed": ["caffe2/operators/lengths_reducer_ops.h"], "labels": ["fb-exported", "merged"]}, "e5218e3e12": {"title": "Add missing error messages for container modules (#29991)", "body": "Summary:\nContainer `Module`s, including `ModuleList`, `ParameterList` and `ParameterDict`, should not be called like a regular `Module`.\nThis PR add error messages for these special modules.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29991\n\nDifferential Revision: D19698535\n\nPulled By: ezyang\n\nfbshipit-source-id: fe156a0bbb033041086734b38f8c6fde034829bf", "pr_number": "29991", "files_changed": ["torch/nn/modules/container.py"], "labels": ["merged", "open source"]}, "602aec325d": {"title": "Kill old cuda support (#33302)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33302\n\nDifferential Revision: D19899586\n\nPulled By: ezyang\n\nfbshipit-source-id: 11293475795b4bfee9a65133bb6718649e220787", "pr_number": "33302", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/native/cuda/SoftMax.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu"], "labels": ["merged", "open source"]}, "0150f40dde": {"title": "dont force msvc /Ox flag which can conflict with /RTC1 in debug config (#33164)", "body": "Summary:\nRelates to https://github.com/pytorch/pytorch/issues/33132\n\nThis fix doesn't add full multi-configuration support described in https://github.com/pytorch/pytorch/issues/33132 but at least avoid the error presented in the issue when `CMAKE_BUILD_TYPE=Debug` is used with MSVC.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33164\n\nDifferential Revision: D19899727\n\nPulled By: ezyang\n\nfbshipit-source-id: 28a364d920c4a3fb577c6b484ccd69a133fbcf5d", "pr_number": "33164", "files_changed": ["cmake/Codegen.cmake", "cmake/public/utils.cmake"], "labels": ["merge-this-please", "merged", "open source", "triaged"]}, "ecd3c252b4": {"title": "Suport all length one SLS op lowering: C2 part (#33332)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33332\n\nWe check the input shape of lengths and indices of SLS and add an attribute if they are the same.\n\nTest Plan:\n```\nbuck test glow/fb/test/numerics:test_operator_onnxifinnpi -- test_slws_fused_8bit_rowwise_length1_graph\n```\n\nReviewed By: ipiszy\n\nDifferential Revision: D19874903\n\nfbshipit-source-id: 06b643b5351d0ba19ba209b5a5b599fbb38b1dfc", "pr_number": "33332", "files_changed": ["caffe2/opt/onnxifi_transformer.cc"], "labels": ["fb-exported", "merged"]}, "b1583ceb1e": {"title": "Second try on Von Mises: Make it JIT compatible (#33177)", "body": "Summary:\nFollow up from https://github.com/pytorch/pytorch/issues/17168 .\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33177\n\nDifferential Revision: D19899550\n\nPulled By: ezyang\n\nfbshipit-source-id: fbcdd9bc91438164bcb2b1cbc314c765520754e1", "pr_number": "33177", "files_changed": ["docs/source/distributions.rst", "test/test_distributions.py", "torch/distributions/__init__.py", "torch/distributions/von_mises.py"], "labels": ["merged", "module: distributions", "open source", "triaged"]}, "ff5f38f53b": {"title": "Revert D19858239: [pytorch][PR] Refactor and add VS 14.16 and 2019 CI for Windows", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19858239\n\nOriginal commit changeset: f068d8505886\n\nfbshipit-source-id: b117e44d5552e157747920d8098ce3b86a29c6bf", "pr_number": null, "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/pytorch-build-params.yml", ".circleci/verbatim-sources/pytorch-job-specs.yml", ".circleci/verbatim-sources/windows-build-test.yml", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", "test/cpp_extensions/setup.py", "torch/csrc/jit/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/fuser/cpu/msvc_arch.h"], "labels": []}, "0c98939b7b": {"title": "Revert D19899550: [pytorch][PR] Second try on Von Mises: Make it JIT compatible", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19899550\n\nOriginal commit changeset: fbcdd9bc9143\n\nfbshipit-source-id: c8a675a8b53f884acd0e6c57bc7aa15faf83d5d6", "pr_number": null, "files_changed": ["docs/source/distributions.rst", "test/test_distributions.py", "torch/distributions/__init__.py", "torch/distributions/von_mises.py"], "labels": []}, "1b2d2ba504": {"title": "[PyTorch] Fix write-after-free (TSAN) in GraphTask::set_error() (#33156)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33156\n\nWhen dist_autograd_spawn_thrift's 'test_backward_node_failure_python_udf' test is\nrun, it was encountering a TSAN error related to holding the mutex while the\nunderlying datastructure was being dealloced.\n\nIn this change, we simply get a shared_ptr<> reference to the future, and\nset_exception() without having the lock held, to avoid deallocing underneath\nthe lock.\nghstack-source-id: 98303434\n\nTest Plan: buck test mode/opt-tsan //caffe2/test/distributed/rpc:dist_autograd_spawn_thrift -- 'test_backward_node_failure_python_udf \\(test_dist_autograd_spawn\\.DistAutogradTestWithSpawn\\)'\n\nDifferential Revision: D19821362\n\nfbshipit-source-id: 82f735e33f8e608552418ae71592400fa3621e40", "pr_number": "33156", "files_changed": ["torch/csrc/autograd/engine.cpp"], "labels": ["merged"]}, "7dde91b0ae": {"title": "Vectorize elu and its backward function on CPU (#32986)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32986\n\nBenchmark: (Debian 10, Release build, gcc 8.3, no turbo, Intel(R) Xeon(R) E-2136 CPU @ 3.30GHz)\n\n```python\nimport timeit\nfor op in ('ELU',):\n    print('Forward')\n    for dtype in ('torch.double', 'torch.float'):\n        for n, t in [(10_000, 100000),\n                    (100_000, 10000)]:\n            print(f'torch.nn.{op}()(a), numel() == {n} for {t} times, dtype={dtype}')\n            print(timeit.timeit('m(a)', setup=f'import torch; m = torch.nn.{op}(); a = torch.linspace(-1, 1, {n}, dtype={dtype})', number=t))\n    print('Backward')\n    for dtype in ('torch.double', 'torch.float'):\n        for n, t in [(20_000, 100000),\n                    (200_000, 10000)]:\n            print(f'torch.nn.{op}()(a), numel() == {n} for {t} times, dtype={dtype}')\n            print(timeit.timeit('y.backward(retain_graph=True)',\n                                setup=f'import torch; m = torch.nn.{op}(); a = torch.linspace(-1, 1, {n}, requires_grad=True, dtype={dtype}); x = m(a); y = x.sum()',\n                                number=t))\n```\n\nBefore:\n\n```\nForward\ntorch.nn.ELU()(a), numel() == 10000 for 100000 times, dtype=torch.double\n5.292799739996553\ntorch.nn.ELU()(a), numel() == 100000 for 10000 times, dtype=torch.double\n4.828570917001343\ntorch.nn.ELU()(a), numel() == 10000 for 100000 times, dtype=torch.float\n3.1359513780043926\ntorch.nn.ELU()(a), numel() == 100000 for 10000 times, dtype=torch.float\n2.7030876770004397\nBackward\ntorch.nn.ELU()(a), numel() == 20000 for 100000 times, dtype=torch.double\n4.568238995998399\ntorch.nn.ELU()(a), numel() == 200000 for 10000 times, dtype=torch.double\n1.8908141480060294\ntorch.nn.ELU()(a), numel() == 20000 for 100000 times, dtype=torch.float\n3.8652471189998323\ntorch.nn.ELU()(a), numel() == 200000 for 10000 times, dtype=torch.float\n1.13068484600808\n```\n\nAfter:\n\n```\nForward\ntorch.nn.ELU()(a), numel() == 10000 for 100000 times, dtype=torch.double\n2.1265591429983033\ntorch.nn.ELU()(a), numel() == 100000 for 10000 times, dtype=torch.double\n1.6708065870043356\ntorch.nn.ELU()(a), numel() == 10000 for 100000 times, dtype=torch.float\n1.1806934149935842\ntorch.nn.ELU()(a), numel() == 100000 for 10000 times, dtype=torch.float\n0.77735430400935\nBackward\ntorch.nn.ELU()(a), numel() == 20000 for 100000 times, dtype=torch.double\n4.494567882007686\ntorch.nn.ELU()(a), numel() == 200000 for 10000 times, dtype=torch.double\n2.007220732004498\ntorch.nn.ELU()(a), numel() == 20000 for 100000 times, dtype=torch.float\n3.615133151994087\ntorch.nn.ELU()(a), numel() == 200000 for 10000 times, dtype=torch.float\n1.105554559995653\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19794595\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: c319ec04676ced22179b8b34789ac8bf6428deab", "pr_number": "32986", "files_changed": ["aten/src/ATen/native/cpu/Activation.cpp"], "labels": ["merged", "open source", "triaged"]}, "4bef344210": {"title": "Implementation of mixture distributions (#22742)", "body": "Summary:\nAddressing issue https://github.com/pytorch/pytorch/issues/18125\nThis implements a mixture distributions, where all components are from the same distribution family. Right now the implementation supports the ```mean, variance, sample, log_prob``` methods.\n\ncc: fritzo and neerajprad\n\n- [x] add import and `__all__` string in `torch/distributions/__init__.py`\n- [x] register docs in docs/source/distributions.rst\n\n### Tests\n(all tests live in tests/distributions.py)\n- [x] add an `Example(MixtureSameFamily, [...])` to the `EXAMPLES` list,\n     populating `[...]` with three examples:\n     one with `Normal`, one with `Categorical`, and one with `MultivariateNormal`\n     (to exercise, `FloatTensor`, `LongTensor`, and nontrivial `event_dim`)\n- [x] add a `test_mixture_same_family_shape()` to `TestDistributions`. It would be good to test this with both `Normal` and `MultivariateNormal`\n- [x] add a `test_mixture_same_family_log_prob()` to `TestDistributions`.\n- [x] add a `test_mixture_same_family_sample()` to `TestDistributions`.\n- [x] add a `test_mixture_same_family_shape()` to `TestDistributionShapes`\n\n### Triaged for follup-up PR?\n- support batch shape\n- implement `.expand()`\n- implement `kl_divergence()` in torch/distributions/kl.py\nPull Request resolved: https://github.com/pytorch/pytorch/pull/22742\n\nDifferential Revision: D19899726\n\nPulled By: ezyang\n\nfbshipit-source-id: 9c816e83a2ef104fe3ea3117c95680b51c7a2fa4", "pr_number": "22742", "files_changed": ["docs/source/distributions.rst", "test/test_distributions.py", "torch/distributions/__init__.py", "torch/distributions/mixture_same_family.py"], "labels": ["merge-this-please", "merged", "module: distributions", "module: docs", "open source", "triaged"]}, "b276ddda38": {"title": "remove THC dist code which nerver be used (#33283)", "body": "Summary:\nRemove THC dist code which nerver be used.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33283\n\nDifferential Revision: D19905361\n\nPulled By: gchanan\n\nfbshipit-source-id: 367fd31e2209d36b30af31511554fdbdd67c98e4", "pr_number": "33283", "files_changed": ["aten/src/ATen/native/cuda/Reduce.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/generic/THCTensorMathReduce.cu", "aten/src/THC/generic/THCTensorMathReduce.h"], "labels": ["merged", "open source"]}, "ae53f8dd25": {"title": "Revert D19859905: [pytorch][PR] Gradient scaling API", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19859905\n\nOriginal commit changeset: bb8ae6966214\n\nfbshipit-source-id: 28f1c93e8a00e3a4bbe8cc981499b15468f0b970", "pr_number": null, "files_changed": ["aten/src/ATen/native/cuda/AmpKernels.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/amp.rst", "docs/source/index.rst", "docs/source/notes/amp_examples.rst", "test/test_cuda.py", "torch/cuda/__init__.py", "torch/cuda/amp/__init__.py", "torch/cuda/amp/grad_scaler.py"], "labels": []}, "92b67c03e4": {"title": "[RPC Reliability] Implemented retries for RPCs with exponential backoff (#32602)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32602\n\nThis adds functionality for re-trying RPC's that are sent with the function `sendWithRetries()`. It adds RPC's that will potentially need to be retried to a sorted map that contains the timeout at which to retry the RPC and associated metadata. A separate thread iteratively removes the earliest retry-able RPC from the map, sleeps until the corresponding time point, re-tries the RPC, and adds to the map again with a future timeout.\n\nGitHub Issue: https://github.com/pytorch/pytorch/issues/32124\n\nPer the first 3 milestones, the following will be addressed in future PR's:\n* enabling RPC Retries for RRef internal messages\n\nDifferential Revision: D19560159\n\nfbshipit-source-id: 40cd86f9a25dc24367624d279a3b9720b20824cf", "pr_number": "32602", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h"], "labels": ["fb-exported", "merged"]}, "8245641091": {"title": "Re-activate binary_macos_libtorch_2_7_cpu_build and binary_macos_li\u2026 (#33321)", "body": "Summary:\nRe-send the PR as Intel has restored the relevant packages.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33321\n\nDifferential Revision: D19894221\n\nPulled By: zhangguanheng66\n\nfbshipit-source-id: bc19dcfa5b17ff047f9ae09ebd8eadfb01f7ed68", "pr_number": "33321", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows-binary-builds-smoke-subset.yml"], "labels": ["merged"]}, "243cc20451": {"title": "Enable inplace relu fusion for training (#33105)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33105\n\nSupport inplace relu for Conv+BN+Relu fusion during training.\nghstack-source-id: 97944659\n\nTest Plan: buck test caffe2/test:quantization --  'test_fuse_module_train \\(test_quantization\\.FusionTest\\)' --print-passing-details\n\nDifferential Revision: D19795221\n\nfbshipit-source-id: 056dc06050d145750c4d0044c0fc1c3febcfdafc", "pr_number": "33105", "files_changed": ["torch/quantization/fuse_modules.py", "torch/testing/_internal/common_quantization.py"], "labels": ["merged", "quantization"]}, "e9e9331927": {"title": "Fractional Max Pooling: output ratios defined as double (#33304)", "body": "Summary:\nReferences https://github.com/pytorch/pytorch/issues/33240\nChanges options.output_ratio from long integer to double to allow ratios to used to calculate output size from inputs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33304\n\nDifferential Revision: D19887318\n\nPulled By: yf225\n\nfbshipit-source-id: 228c2c6bf4158307700c2a983d27d539c6b9eded", "pr_number": "33304", "files_changed": ["torch/csrc/api/include/torch/nn/functional/pooling.h", "torch/csrc/api/include/torch/nn/options/pooling.h", "torch/csrc/api/src/nn/modules/pooling.cpp"], "labels": ["merged", "open source"]}, "9823662b43": {"title": "[ONNX] Export split with list of sizes (#33161)", "body": "Summary:\nExporting Split with a dynamic list of split_sizes is not supported.\nThis PR enables export using onnx SplitToSequence + SequenceAt\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33161\n\nReviewed By: hl475\n\nDifferential Revision: D19860152\n\nPulled By: houseroad\n\nfbshipit-source-id: 300afedc22b01923efb23acd1a3627aa146bb251", "pr_number": "33161", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/onnx/symbolic_opset11.py"], "labels": ["jit", "merged", "module: onnx", "open source", "triaged"]}, "6ade7e3a15": {"title": "[ROCm] Enable 3D convolutions through ROCm (#33067)", "body": "Summary:\nFor both the Caffe2 and PyTorch backends, enable 3D convolutions through MIOpen.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33067\n\nReviewed By: BIT-silence\n\nDifferential Revision: D19880495\n\nPulled By: bddppq\n\nfbshipit-source-id: 8f6f970910654c1c5aa871b48a04c1054875691c", "pr_number": "33067", "files_changed": ["aten/src/ATen/miopen/Descriptors.h", "aten/src/ATen/native/Convolution.cpp", "caffe2/operators/hip/conv_op_miopen.hip", "caffe2/python/operator_test/conv_test.py"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "b730c5a3bd": {"title": "remove dispatch key (#33266)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33266\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19907697\n\nPulled By: anjali411\n\nfbshipit-source-id: 99fc06b7c41229e8d9ed4271de62247cda12ee6e", "pr_number": "33266", "files_changed": ["c10/core/TensorOptions.h"], "labels": ["merged"]}, "9c8b67b179": {"title": "Revert D19905015: Revert D19858239: [pytorch][PR] Refactor and add VS 14.16 and 2019 CI for Windows", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19905015\n\nOriginal commit changeset: b117e44d5552\n\nfbshipit-source-id: a10c78aed953434f69f466bdd36f914334ba82f3", "pr_number": null, "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/pytorch-build-params.yml", ".circleci/verbatim-sources/pytorch-job-specs.yml", ".circleci/verbatim-sources/windows-build-test.yml", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", "test/cpp_extensions/setup.py", "torch/csrc/jit/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/fuser/cpu/msvc_arch.h"], "labels": []}, "ecd9a5ad12": {"title": "Simplify prim::shape when we have complete tensor types. (#33336)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33336\n\nDifferential Revision: D19900566\n\nPulled By: resistor\n\nfbshipit-source-id: c8eaad70c8ea57ebbe920dcfbdaf6a9435b49506", "pr_number": "33336", "files_changed": ["torch/csrc/jit/passes/peephole.cpp"], "labels": ["jit", "merged"]}, "9c0625b004": {"title": "[iOS] Add watchOS support (#33318)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33318\n\n### Summary\n\nRecently, we have a [discussion](https://discuss.pytorch.org/t/libtorch-on-watchos/69073/14) in the forum about watchOS. This PR adds the support for building watchOS  libraries.\n\n### Test Plan\n\n- `BUILD_PYTORCH_MOBILE=1 IOS_PLATFORM=WATCHOS ./scripts/build_ios.sh`\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19896534\n\nPulled By: xta0\n\nfbshipit-source-id: 7b9286475e895d9fefd998246e7090ac92c4c9b6", "pr_number": "33318", "files_changed": ["CMakeLists.txt", "cmake/Summary.cmake", "cmake/iOS.cmake", "scripts/build_ios.sh"], "labels": ["merged"]}, "0b5b2b864a": {"title": "[BC-Breaking] Rename at::Tensor::base() to _base() (#33316)", "body": "Summary:\nThis PR renames `at::Tensor::base()` to `at::Tensor::_base()`, to achieve parity with Python `torch.Tensor._base` API.\n\n----\n\nThis PR is BC-breaking in the following way:\n\nPreviously, to get the tensor that this tensor is a view of, the user would call `tensor.base()` in C++. Now, they must call `tensor._base()`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33316\n\nDifferential Revision: D19905687\n\nPulled By: yf225\n\nfbshipit-source-id: 949d97b707b2c82becb99ac89e9ac24359d183e6", "pr_number": "33316", "files_changed": ["aten/src/ATen/core/Tensor.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/test/undefined_tensor_test.cpp", "torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/variable.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "5cab54e0db": {"title": "Revert D19560159: [RPC Reliability] Implemented retries for RPCs with exponential backoff", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19560159\n\nOriginal commit changeset: 40cd86f9a25d\n\nfbshipit-source-id: 70f5b19bc05fc34e3c912f42f9d32b9fb80aed06", "pr_number": null, "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h"], "labels": []}, "dfafe2aad1": {"title": ".cirlceci: Swap PYTORCH_BUILD_VERSION if on tag (#33326)", "body": "Summary:\nBasically just fills out PYTORCH_BUILD_VERSION to the correct version\nbaesd on the git tag.\n\nThis makes it so that we don't have to continually edit this file\nwhen doing releases.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33326\n\nDifferential Revision: D19911035\n\nPulled By: seemethere\n\nfbshipit-source-id: e27105f3e193a49dd68452d8f60232f8a132acad", "pr_number": "33326", "files_changed": [".circleci/scripts/binary_populate_env.sh"], "labels": ["merged"]}, "3359871f5d": {"title": ".circleci: Use volume mounts instead of docker cp (#33355)", "body": "Summary:\ndocker cp was erroring out, so lets just use volume mounts instead which\nshould hopefully be more consistent\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33355\n\nDifferential Revision: D19913948\n\nPulled By: seemethere\n\nfbshipit-source-id: 059ddd36a8162f946cfea451b5dcd1706f1209e9", "pr_number": "33355", "files_changed": [".circleci/scripts/binary_run_in_docker.sh"], "labels": ["merged"]}, "e1a895858f": {"title": "Allow to register custom passes both before and after fusion. (#33261)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33261\n\nIt was requested in #33114.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19910600\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 827f1744b97f386065a21d1ba5d82c1f90edbe46", "pr_number": "33261", "files_changed": ["torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/pass_manager.cpp", "torch/csrc/jit/pass_manager.h"], "labels": ["jit", "merged"]}, "c6271c63f2": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/46fd5fed104a60598268dedcd7027902f7c5ed5b\nhttps://github.com/pytorch/fbgemm/commit/87cd6087c6030fcab531965be09033b0abb679a6\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 402427af823fe31ac1f6e18c5a020ec6ec7cc1af", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "1e76649d30": {"title": "fast setup for output tensor in tensor iterator (#33165)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33165\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19825853\n\nPulled By: glaringlee\n\nfbshipit-source-id: 8f908f2e93a4e377306a77e8a771208603b20e72", "pr_number": "33165", "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "test/test_torch.py"], "labels": ["merged"]}, "c37a9b874b": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/65758fd3b159b60937e0d72d843dd9c2e4035bdc\nhttps://github.com/facebook/mcrouter/commit/fb732045845fe44b6db86c1bb86fedc3f4f54093\nhttps://github.com/pytorch/fbgemm/commit/618f71a7954a00235cd8b179b642e283a00f283b\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 814ebbcf35bcecc62ec64854a26ea645d651fbc2", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "d35a4c202e": {"title": "Add support for aten::slice to guard elimination. (#33311)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33311\n\nDifferential Revision: D19911105\n\nPulled By: resistor\n\nfbshipit-source-id: 402cfe5f2e03a62b78ed13157e1462cefd9eeafb", "pr_number": "33311", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/guard_elimination.cpp"], "labels": ["jit", "merged"]}, "6dd6b0bfae": {"title": "Revert D19900566: [pytorch][PR] Simplify prim::shape when we have complete tensor types.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19900566\n\nOriginal commit changeset: c8eaad70c8ea\n\nfbshipit-source-id: 764f2139fdf19f22a397694d011078ec525f5e8a", "pr_number": null, "files_changed": ["torch/csrc/jit/passes/peephole.cpp"], "labels": []}, "fd684cc312": {"title": "Use torch.set_default_dtype in test_data_parallel and rename dtype2prec (#32962)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32962\n\nAs per gchanan's comments on\nhttps://github.com/pytorch/pytorch/pull/30445, I've used\n`torch.set_default_dtype` in test_data_parallel instead of specifying\ndtype=torch.double everywhere. Also, renamed dtype2prec to dtype2prec_DONTUSE\nghstack-source-id: 98388429\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19714374\n\nfbshipit-source-id: eb55bbca33881625636ba9ea6dd4cb692f25668e", "pr_number": "32962", "files_changed": ["test/distributed/test_data_parallel.py", "test/test_nn.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged"]}, "cd038c0ae9": {"title": "Get rid of some template arguments in GPU loop (#33308)", "body": "Summary:\nGlobally define\n```C++\nconstexpr int num_threads = C10_WARP_SIZE * 2;\nconstexpr int thread_work_size = 4;\nconstexpr int block_work_size = thread_work_size * num_threads;\n```\nand kill all the template arguments passing these values.\n\nThese are effectively global, but we are now passing them around by template arguments, causing many inconvenience in coding.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33308\n\nDifferential Revision: D19907250\n\nPulled By: ngimel\n\nfbshipit-source-id: 4623b69baea7e6e77f460ffdfa07cf9f8cba588a", "pr_number": "33308", "files_changed": ["aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/MemoryAccess.cuh", "aten/src/ATen/native/cuda/ROCmLoops.cuh", "aten/src/ATen/test/cuda_vectorized_test.cu"], "labels": ["merged", "open source"]}, "495bd5818b": {"title": "Fix index truncation in argmin/max for large tensors (#33310)", "body": "Summary:\nFixes the `TensorIterator` parts of https://github.com/pytorch/pytorch/issues/32863 (THC is still broken)\n\n`TensorIterator::split` now keeps track of the `view_offsets` into the full tensor range. With this, I can take the base offset for the reduced dimension and translate partial results from the sub-iter into the index range of the full tensor. This happens only once for each intermediate result, so we should still benefit from the performance of 32-bit indexing in loops.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33310\n\nDifferential Revision: D19906136\n\nPulled By: ngimel\n\nfbshipit-source-id: 3372ee4b8d5b115a53be79aeafc52e80ff9c490b", "pr_number": "33310", "files_changed": ["aten/src/ATen/native/SharedReduceOps.h", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/cpu/Reduce.h", "aten/src/ATen/native/cuda/Reduce.cuh", "test/test_torch.py"], "labels": ["merged", "open source"]}, "f6808df75f": {"title": "[BC] Temporarily fix the BC check (#33387)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33387\n\nCI is broken. Skip two functions to fix the problem.\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D19926249\n\nfbshipit-source-id: a46d1465c59de8616d2af5fb0b9cc18532359f88", "pr_number": "33387", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["fb-exported", "merged"]}, "c75d06d854": {"title": "Move gating part of SparseFeatureGating to local", "body": "Summary: in dper2, local net is hard-coded by whitelisting some layers. Add SparseFeatureGating related layers to local net explicitly.\n\nTest Plan:\n* workflow: f167812211\n* QRT: fall back looks normal\n\n{F228442018}\n\nDifferential Revision: D19852280\n\nfbshipit-source-id: 6fecc3d745c3f742d029575a7b9fe320618f1863", "pr_number": null, "files_changed": ["caffe2/python/layers/tags.py"], "labels": []}, "df47a3abe0": {"title": "[distributed] pass in timeout to TCP store when initializing (#33325)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33325\n\nCloses https://github.com/pytorch/pytorch/issues/32924. There was a bug where for TCPStore, we would not respect the timeout passed into `init_process_group` while constructing the TCPStore. Instead, we'd set the timeout after the rendezvous created the store, meaning that we used the default timeout of 300s while connecting to the server. This diff passes the timeout passed into `init_process_group` to rendezvous so that it can be passed into the constructor for TCPStore, so that we can use the right timeout at construction time.\n\nQuestion: Should we make this change for FileStore as well? Currently the FileStore constructor does not take in a timeout at all.\nghstack-source-id: 98401875\n\nTest Plan: Added a UT\n\nDifferential Revision: D19871946\n\nfbshipit-source-id: dd002180c4c883216645b8a97cc472c6116ac117", "pr_number": "33325", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/constants.py", "torch/distributed/distributed_c10d.py", "torch/distributed/rendezvous.py", "torch/distributed/rpc/constants.py"], "labels": ["merged"]}, "d4e4beddc4": {"title": "Revert D19871946: [distributed] pass in timeout to TCP store when initializing", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19871946\n\nOriginal commit changeset: dd002180c4c8\n\nfbshipit-source-id: 40b0676c51e43366c0700e81d16cc7927ee8efc2", "pr_number": null, "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/constants.py", "torch/distributed/distributed_c10d.py", "torch/distributed/rendezvous.py", "torch/distributed/rpc/constants.py"], "labels": []}, "d29997373e": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/80dda4790316dcb09b183fe4f0894471576496de\nhttps://github.com/facebook/fbzmq/commit/797af57bb64558069c89835ce68672779749163f\nhttps://github.com/pytorch/fbgemm/commit/b2fceb9d05a455f614a33ccb0da8bcf8f4535895\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: dde5fb9abca185422df11dc61c658dc333ad63ca", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "c57f8984e6": {"title": "[caffe2] make order btw div and mul in adgrad consistent (#32974)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32974\n\nPull Request resolved: https://github.com/pytorch/FBGEMM/pull/286\n\nRe-attempt of D18805426 . Decided to be consistent with PyTorch Adagrad\n\nThere was an inconsistency in the order of operation between scalar and SIMD code when we compute Adagrad. This diff make them consistent by doing w += lr * grad / (sqrt(moment) + epsilon) in Adagrad and w += lr / (sqrt(moment) + epsilon) * grad in RowWiseSparseAdagrad.\n\nThe Adagrad order is consistent with PyTorch (see aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp addcmul_cpu_kernel function). The RowWiseSparseAdagrad order is to make compute more efficient. In RowWiseSparseAdagrad, lr / (sqrt(moment) + epsilon) is shared among all elements in the row\n\nAnd, we're not going to use FMA to be consistent with PyTorch (even though it provides a little accuracy benefit)\n\nTest Plan: CI\n\nReviewed By: wx1988\n\nDifferential Revision: D19342865\n\nfbshipit-source-id: e950c16f2e1c4a2f2a3ef53b1705db373c67f341", "pr_number": "32974", "files_changed": ["caffe2/perfkernels/adagrad.h", "caffe2/perfkernels/adagrad_avx.cc"], "labels": ["fb-exported", "merged"]}, "87dc2dbcce": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/19c040cb0146c8d18936e15b081280ba7e79981d\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: ddc41000622a682874ab3a11fdf4a91038f9c15f", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "1a589f50bd": {"title": "[auto quant] Add quant_scheme_generator to interface with dper", "body": "Summary:\nAdd quant_scheme_generator that will be used to interface with dper.\n\nAlso updated two related functions:\n\n- Add batch_size option to save_local_dataset() in dataset utils to be more flexible.\n\nTest Plan:\nTested in the stacked diff D19747206.\n\nbuck test deeplearning/numeric_suite/toolkit/test:int8_static_utils_test\n\nReviewed By: csummersea\n\nDifferential Revision: D19745159\n\nfbshipit-source-id: a4ac1ef0ffdddc68bdf5e209ae801b8c475d0b96", "pr_number": null, "files_changed": ["caffe2/quantization/server/activation_distribution_observer.h", "caffe2/quantization/server/pybind.cc"], "labels": []}, "c90b393c00": {"title": "Fix logging for aborted communicators in ProcessGroupNCCL. (#33147)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33147\n\nThe log mentioned that it is aborting communicators even if\n`blockingWait_` was false. This was incorrect, and I updated the logging to\nreflect the appropriate behavior.\nghstack-source-id: 98025017\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19817967\n\nfbshipit-source-id: fb3415af2cc99eb20981ceaa5203c0a1880fd6f3", "pr_number": "33147", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["merged"]}, "ebb008eb68": {"title": "Optimize Unfold3dAcc to improve performance of conv3d backward (#33317)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33317\n\nOptimize Unfold3dAcc to improve performance of conv3d backward\n\nTest Plan: buck test mode/dev-nosan //caffe2/test:nn -- \"Conv3d\"\n\nReviewed By: houseroad\n\nDifferential Revision: D19892678\n\nfbshipit-source-id: 18873dd1d1409263d9925840db302b21fb3b490d", "pr_number": "33317", "files_changed": ["aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/native/Unfold3d.cpp", "aten/src/ATen/native/Unfold3d.h"], "labels": ["fb-exported", "merged"]}, "55fa133cdc": {"title": "Remove gpu_kernel_with_index (#33370)", "body": "Summary:\nAlthough `gpu_kernel_with_index` might look like a quite general helper function at first look, it actually isn't.\n\nThe problem is not only 32bit indexing, but something more fundamental: `TensorIterator` reorder dims and shapes, so if you have non-contiguous tensor such as `torch.empty(5, 5).t()` , the index won't be correct. Since the whole point of `TensorIterator` is to manipulate shapes/strides to speedup loops, it is fundamentally impossible to get the correct linear index without tons of efforts.\n\nCurrently, the range factories are not failing on an `out=non_contiguous_tensor`  is because it is so lucky that  `has_internal_overlap` is stupid enough to return everything not contiguous as `TOO_HARD`.\n\nSince `gpu_kernel_with_index` is not general, we should move it from `Loops.cuh` to `RangeFactories.cu`. And since the kernel is so simple to implement, it makes no sense to use `TensorIterator` which goes through tons of unnecessary checks like `compute_dtypes`.\n\n`torch.range` is not tested for 64bit-indexing, and I will file a new PR to remove it (it was supposed to be removed at 0.5).\n\nBenchmark:\nThe device is GTX-1650, I don't have a good GPU at home.\n\nCode:\n```python\nimport torch\nprint(torch.__version__)\n\nfor i in range(100):\n    torch.randn(1000, device='cuda')\ntorch.cuda.synchronize()\n\nfor i in range(15, 29):\n    %timeit torch.arange(2 ** i, device='cuda'); torch.cuda.synchronize()\n```\n\nBefore:\n```\n1.5.0a0+c37a9b8\n11.9 \u00b5s \u00b1 412 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n12.7 \u00b5s \u00b1 309 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n19.6 \u00b5s \u00b1 209 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n28.9 \u00b5s \u00b1 923 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n48.4 \u00b5s \u00b1 1.64 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n85.7 \u00b5s \u00b1 1.46 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n162 \u00b5s \u00b1 1.09 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n312 \u00b5s \u00b1 9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n618 \u00b5s \u00b1 15.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.22 ms \u00b1 9.91 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n2.45 ms \u00b1 97.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n4.9 ms \u00b1 155 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n10.1 ms \u00b1 378 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\nAfter:\n```\n1.5.0a0+7960d19\n11 \u00b5s \u00b1 29.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n12.4 \u00b5s \u00b1 550 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n18.4 \u00b5s \u00b1 230 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n27.6 \u00b5s \u00b1 10.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n46.2 \u00b5s \u00b1 18.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n83.3 \u00b5s \u00b1 5.61 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n158 \u00b5s \u00b1 373 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n307 \u00b5s \u00b1 1.44 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n603 \u00b5s \u00b1 112 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.2 ms \u00b1 1.05 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n2.4 ms \u00b1 23.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n4.77 ms \u00b1 25.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n9.51 ms \u00b1 933 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33370\n\nDifferential Revision: D19925990\n\nPulled By: ngimel\n\nfbshipit-source-id: f4a732fe14a5582b35a56618941120d62e82fdce", "pr_number": "33370", "files_changed": ["aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/RangeFactories.cu", "test/test_nn.py", "test/test_torch.py", "torch/testing/_internal/common_device_type.py"], "labels": ["merged", "open source"]}, "5d7f42847c": {"title": "Add at::Tensor::retain_grad API (#33349)", "body": "Summary:\nThis PR adds `at::Tensor::retain_grad`, and its implementation mirrors the Python `torch.Tensor.retain_grad` API:\nhttps://github.com/pytorch/pytorch/blob/c6271c63f21cd886ded73787e53844588cff5e74/torch/tensor.py#L292-L315\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33349\n\nDifferential Revision: D19944524\n\nPulled By: yf225\n\nfbshipit-source-id: e61d5d761996b6d1b860c04c4b4650c1a49a6a8c", "pr_number": "33349", "files_changed": ["aten/src/ATen/native/VariableMethodStubs.cpp", "aten/src/ATen/native/native_functions.yaml", "test/cpp/api/autograd.cpp", "tools/autograd/gen_python_functions.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/variable.h"], "labels": ["merged", "module: cpp"]}, "4724964810": {"title": "[C++ API] Expose AnyValue and AnyModuleHolder classes (#33026)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33026\n\nThis PR contains necessary changes to prepare for https://github.com/pytorch/pytorch/pull/33027. It exposes the following classes to public:\n1. `torch::nn::AnyValue`, because if the user has optional arguments in their module's forward method, they must also use the `FORWARD_HAS_DEFAULT_ARGS` macro and pass in the default values for those optional arguments wrapped by `torch::nn::AnyValue`.\n2. `torch::nn::AnyModuleHolder`, because `torch::nn::Module` needs to declare it as a friend class for it to be able to access `torch::nn::Module`'s protected methods such as `_forward_has_default_args` / `_forward_num_required_args` / `_forward_populate_default_args`.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19777814\n\nPulled By: yf225\n\nfbshipit-source-id: 1c9d5aa24f0689154752c426a83ee98f64c9d02f", "pr_number": "33026", "files_changed": ["test/cpp/api/any.cpp", "torch/csrc/api/include/torch/nn/modules/container/any.h", "torch/csrc/api/include/torch/nn/modules/container/any_module_holder.h", "torch/csrc/api/include/torch/nn/modules/container/any_value.h"], "labels": ["merged"]}, "a203dc2e6d": {"title": "[C++ API] Allow skipping default arguments in module's forward method when module is used in Sequential (#33027)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33027\n\nThis PR allows default arguments in module's forward method to be skipped when module is used in `torch::nn::Sequential`, by introducing the `FORWARD_HAS_DEFAULT_ARGS` macro and requiring that all modules that have default arguments in its forward method must have a corresponding `FORWARD_HAS_DEFAULT_ARGS` macro call.\n\nFixes issue mentioned in https://github.com/pytorch/pytorch/issues/30931#issuecomment-564144468.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19777815\n\nPulled By: yf225\n\nfbshipit-source-id: 73282fcf63377530063e0092a9d84b6c139d2e32", "pr_number": "33027", "files_changed": ["test/cpp/api/any.cpp", "test/cpp/api/sequential.cpp", "torch/csrc/api/include/torch/nn/module.h", "torch/csrc/api/include/torch/nn/modules.h", "torch/csrc/api/include/torch/nn/modules/activation.h", "torch/csrc/api/include/torch/nn/modules/common.h", "torch/csrc/api/include/torch/nn/modules/container/any_module_holder.h", "torch/csrc/api/include/torch/nn/modules/conv.h", "torch/csrc/api/include/torch/nn/modules/embedding.h", "torch/csrc/api/include/torch/nn/modules/pooling.h", "torch/csrc/api/include/torch/nn/modules/rnn.h"], "labels": ["merged", "module: cpp"]}, "dde2ff4608": {"title": "[Fuser] Add a knob for disabling/enabling CUDA fuser. (#33395)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33395\n\nBy default the GPU fuser stays enabled, but this function allows to\nmanually disable it. It will be useful for working on other\nimplementations of fuser.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19926911\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 7ea9d1dd7821453d640f81c487b63e1d585123c4", "pr_number": "33395", "files_changed": ["torch/csrc/jit/fuser/interface.cpp", "torch/csrc/jit/fuser/interface.h", "torch/csrc/jit/init.cpp"], "labels": ["jit", "merged"]}, "cfb4862673": {"title": "[pytorch] correct input size check for GroupNorm (#33008)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33008\n\nCorrects D19373507 to allow valid use cases that fail now. Multiplies batch size by the number of elements in a group to get the correct number of elements over which statistics are computed.\n\n**Details**:\nThe current implementation disallows GroupNorm to be applied to tensors of shape e.g. `(1, C, 1, 1)` to prevent cases where statistics are computed over 1 element and thus result in a tensor filled with zeros.\nHowever, in GroupNorm the statistics are calculated across channels. So in case where one has an input tensor of shape `(1, 256, 1, 1)` for `GroupNorm(32, 256)`, the statistics will be computed over 8 elements and thus be meaningful.\n\nOne use case is [Atrous Spatial Pyramid Pooling (ASPPPooling)](https://github.com/pytorch/vision/blob/791c172a337d98012018f98ffde93b1020ba3ed5/torchvision/models/segmentation/deeplabv3.py#L50), where GroupNorm could be used in place of BatchNorm [here](https://github.com/pytorch/vision/blob/791c172a337d98012018f98ffde93b1020ba3ed5/torchvision/models/segmentation/deeplabv3.py#L55). However, now this is prohibited and results in failures.\n\nProposed solution consists in correcting the computation of the number of elements over which statistics are computed. The number of elements per group is taken into account in the batch size.\n\nTest Plan: check that existing tests pass\n\nReviewed By: fmassa\n\nDifferential Revision: D19723407\n\nfbshipit-source-id: c85c244c832e6592e9aedb279d0acc867eef8f0c", "pr_number": "33008", "files_changed": ["test/test_nn.py", "torch/nn/functional.py"], "labels": ["fb-exported", "merged"]}, "28c5213a97": {"title": "Add mechanism to pass a number of workers to cpp extensions (#33346)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33346\n\nFixes #33091\n\nThis PR lets users control the number of workers that cpp extensions\nuses through the environment variable `MAX_JOBS`. If the environment\nvariable is a non-negative integer we use that many threads; otherwise,\nninja falls back to the default.\n\nI chose to use the name `MAX_JOBS` because we use it in PyTorch already\nto control the number of workers PyTorch builds with. There is a risk\nthat users of cpp extensions already have `MAX_JOBS` set but we are\nhoping that that risk is small and/or it means semantically the same\nthing.\n\nTest Plan: - tested locally\n\nDifferential Revision: D19911645\n\nPulled By: zou3519\n\nfbshipit-source-id: d20ed42de4f845499ed38f1a1c73e9ccb620f780", "pr_number": "33346", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["merged"]}, "2c99ea8654": {"title": "Dirac init compatibility with group convolutions (#32825)", "body": "Summary:\nInitializing weights of group-conv with init.dirac_, and applying, previously resulted in an output that makes no sense:\n```\nx = torch.randn([1, 3, 3, 3])\nprint('input:\\n', x)\nconv_layer = torch.nn.Conv2d(3, 3, 3, padding=1, groups=3, bias=False)\ntorch.nn.init.dirac_(conv_layer.weight.data)\nprint('\\noutput (before this PR):\\n',conv_layer(x))\n\ninput:\n tensor([[[[ 0.5369, -1.1428,  0.1031],\n          [ 0.4638, -0.0854, -0.6553],\n          [ 0.8321, -2.5926, -0.3214]],\n\n         [[-0.2289, -0.0895,  0.4407],\n          [ 1.2309, -1.2096, -1.5216],\n          [-0.1798,  1.1694,  0.3469]],\n\n         [[ 0.1905,  0.8095,  0.5490],\n          [-0.4525, -0.4284, -0.1141],\n          [ 1.1857, -0.9246, -0.5119]]]])\n\noutput (before this PR):\n tensor([[[[ 0.5369, -1.1428,  0.1031],\n          [ 0.4638, -0.0854, -0.6553],\n          [ 0.8321, -2.5926, -0.3214]],\n\n         [[ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]],\n\n         [[ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]]]], grad_fn=<MkldnnConvolutionBackward>)\n````\n\nThis PR allows introducing groups to the initialization:\n```\ntorch.nn.init.dirac_(conv_layer.weight.data, groups=3)\nprint('output (after this PR):\\n', conv_layer(x))\n\noutput (after this PR):\n tensor([[[[ 0.5369, -1.1428,  0.1031],\n          [ 0.4638, -0.0854, -0.6553],\n          [ 0.8321, -2.5926, -0.3214]],\n\n         [[-0.2289, -0.0895,  0.4407],\n          [ 1.2309, -1.2096, -1.5216],\n          [-0.1798,  1.1694,  0.3469]],\n\n         [[ 0.1905,  0.8095,  0.5490],\n          [-0.4525, -0.4284, -0.1141],\n          [ 1.1857, -0.9246, -0.5119]]]], grad_fn=<MkldnnConvolutionBackward>)\n```\n\nWhen out_channels is different than input_channels, it does the natural thing which is applying identity in each group separately:\n\n```\nx = torch.randn([1, 2, 3, 3])\nprint('input:\\n', x)\nconv_layer = torch.nn.Conv2d(2, 4, 3, padding=1, groups=2, bias=False)\ntorch.nn.init.dirac_(conv_layer.weight.data, groups=2)\nprint('\\noutput:\\n', conv_layer(x))\n\ninput:\n tensor([[[[ 1.2205, -0.6608,  0.8640],\n          [-0.5464,  1.1288,  1.4726],\n          [-0.6693,  0.4000, -1.7613]],\n\n         [[-0.8760, -0.8814, -0.4705],\n          [ 0.6283, -0.5943,  0.6873],\n          [-0.6852,  1.4723,  0.3325]]]])\n\noutput:\n tensor([[[[ 1.2205, -0.6608,  0.8640],\n          [-0.5464,  1.1288,  1.4726],\n          [-0.6693,  0.4000, -1.7613]],\n\n         [[ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]],\n\n         [[-0.8760, -0.8814, -0.4705],\n          [ 0.6283, -0.5943,  0.6873],\n          [-0.6852,  1.4723,  0.3325]],\n\n         [[ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]]]], grad_fn=<MkldnnConvolutionBackward>)\n```\n\nArgument 'groups' defaults to 1 so it is backward compatible.\n\nTests are modified to include cases of with groups>1 but also contain groups=1 cases.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32825\n\nDifferential Revision: D19859926\n\nPulled By: vincentqb\n\nfbshipit-source-id: 9dfdd24471ff14d79c442dfd28c1891aff812fdf", "pr_number": "32825", "files_changed": ["test/test_nn.py", "torch/nn/init.py"], "labels": ["merged", "module: nn", "open source", "triaged"]}, "879cf0b15a": {"title": "fix typing bug of LambdaLR.__init__ (#33271)", "body": "Summary:\n## problem\n\n```python\nclass LambdaLR(_LRScheduler):\n    \"\"\"Sets the learning rate of each parameter group to the initial lr\n    times a given function. When last_epoch=-1, sets initial lr as lr.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        lr_lambda (function or list): A function which computes a multiplicative\n            factor given an integer parameter epoch, or a list of such\n            functions, one for each group in optimizer.param_groups.\n        last_epoch (int): The index of last epoch. Default: -1.\n\n    Example:\n        >>> # Assuming optimizer has two groups.\n        >>> lambda1 = lambda epoch: epoch // 30\n        >>> lambda2 = lambda epoch: 0.95 ** epoch\n        >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n        >>> for epoch in range(100):\n        >>>     train(...)\n        >>>     validate(...)\n        >>>     scheduler.step()\n    \"\"\"\n```\n\n`LambdaLR` takes a lambda that returns a float and takes a int, or a list of such lambdas.\n\n## related issue\n\nResolve https://github.com/pytorch/pytorch/issues/32645\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33271\n\nDifferential Revision: D19878665\n\nPulled By: vincentqb\n\nfbshipit-source-id: 50b16caea13de5a3cbd187e688369f33500499d0", "pr_number": "33271", "files_changed": ["torch/optim/lr_scheduler.pyi"], "labels": ["merged", "open source"]}, "f938b3b4e0": {"title": "Remove TH binding of set_(Tensor). (#33358)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33358\n\nWe just translate this code to ATen.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19911114\n\nPulled By: gchanan\n\nfbshipit-source-id: 2279e63bb7006f7253620417937e3ce9301e0cdb", "pr_number": "33358", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml"], "labels": ["merged"]}, "4468a7b7b3": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/hhvm/hsl/commit/efc34423b6b2870412f51da847571f586581e881\nhttps://github.com/facebook/fbthrift/commit/75bb4596547b2f9daf33b8d6649a434e50109f59\nhttps://github.com/facebook/litho/commit/fc1945c2e032dbdbac6ec1db02d782aa168a9b7b\nhttps://github.com/facebookresearch/pytorch-biggraph/commit/332a31a1450b87e0aea5cc0906aea6f728d0d450\nhttps://github.com/pytorch/fbgemm/commit/2b6eef4dc93c5f5cb9e41eb3779cbe21fcc89e1d\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: d105b9aa5001c53f884f007406684b73809a7680", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "abbf6e7f53": {"title": "fix clang-tidy lint (#33448)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33448\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19952962\n\nPulled By: suo\n\nfbshipit-source-id: db04bf74f6156edd1bd0716b12f6ca911c84a6bf", "pr_number": "33448", "files_changed": [".github/workflows/lint.yml"], "labels": ["merged"]}, "4c8064c9e1": {"title": "Fix avx-512 detection logic for jit fuser with MSVC 2019 (#33403)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33401.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33403\n\nDifferential Revision: D19949812\n\nPulled By: soumith\n\nfbshipit-source-id: 00dc3c99b5ba1c13394d5d38bcb148720434b0a3", "pr_number": "33403", "files_changed": ["torch/csrc/jit/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/fuser/cpu/msvc_arch.h"], "labels": ["jit", "open source"]}, "44af8ee6cd": {"title": "Add pybind11 exception translator (#30588)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/30027\n\nThe idea here is that you can bind a function with `pybind11` in a single line and without modifying the function:\n```cpp\nm.def(\"foo\", foo, py::call_guard<torch::PyWarningHandler>());\n```\nWhere warnings are handled by the [`call_guard`](https://pybind11.readthedocs.io/en/stable/advanced/functions.html#call-guard) and exceptions are handled by the `pybind11` exception translator. To do this, I have added support for handling C++ exceptions in `torch::PyWarningHandler`'s destructor without setting the python error state before hand.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30588\n\nDifferential Revision: D19905626\n\nPulled By: albanD\n\nfbshipit-source-id: 90c0a5e298b123cc0c8ab9c52c91be4e96ea47c6", "pr_number": "30588", "files_changed": ["test/test_cpp_extensions_jit.py", "torch/csrc/Exceptions.cpp", "torch/csrc/Exceptions.h", "torch/csrc/Module.cpp", "torch/csrc/distributed/autograd/init.cpp", "torch/utils/cpp_extension.py"], "labels": ["merged", "open source", "triaged"]}, "1af30451e5": {"title": "sync srcs between fbcode and ovrsource targets (#33368)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33368\n\nreorganizing files that describe sources to ensure the same list is used for both fbcode and ovrsource targets. (BUCK vs TARGETS)\n\nTest Plan: CI green\n\nReviewed By: malfet\n\nDifferential Revision: D19803036\n\nfbshipit-source-id: 69c1fa10877c3f0c0e9c1517784949c3c9939710", "pr_number": "33368", "files_changed": ["tools/build_variables.py"], "labels": ["fb-exported", "merged"]}, "1d743e3154": {"title": "Add guard elimination support for aten::unsqueeze. (#33371)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33371\n\nDifferential Revision: D19920041\n\nPulled By: resistor\n\nfbshipit-source-id: 906af47676dba014c31eef069a4753207f2efc60", "pr_number": "33371", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/guard_elimination.cpp"], "labels": ["jit", "merged"]}, "016d73bd74": {"title": "remove Complex CPU/CUDA backend enum keys (#33267)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33267\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19907696\n\nPulled By: anjali411\n\nfbshipit-source-id: 78cc55344313387c4b05bb003688915cee64e3be", "pr_number": "33267", "files_changed": ["c10/core/Backend.h", "c10/core/DispatchKey.cpp", "c10/core/TensorOptions.h", "torch/csrc/utils/tensor_layouts.cpp", "torch/csrc/utils/tensor_types.cpp"], "labels": ["merged"]}, "da015c77a1": {"title": "Cummax and Cummin doc update and performance benchmark (#32537)", "body": "Summary:\n[CPU] Benchmark results for cummax, cummin:\n\nIn [1]: import torch\n\nIn [2]: x=torch.randn(5,6,7).cuda()\n\nIn [3]: %timeit x.cummax(0)\n134 \u00b5s \u00b1 1.59 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [4]: %timeit x.max(0)\n114 \u00b5s \u00b1 560 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [5]: %timeit x.cummax(1)\n134 \u00b5s \u00b1 760 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [6]: %timeit x.max(1)\n118 \u00b5s \u00b1 514 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [7]: %timeit x.cumsum(0)\n97.1 \u00b5s \u00b1 6.93 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [8]: %timeit x.cumprod(0)\n83.6 \u00b5s \u00b1 689 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [9]: %timeit x.cumprod(1)\n86.3 \u00b5s \u00b1 528 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [10]: y=torch.randn(5,6,7)\n\nIn [11]: %timeit y.cummax(0)\n148 \u00b5s \u00b1 1.43 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [12]: %timeit y.max(0)\n111 \u00b5s \u00b1 125 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [13]: %timeit y.cumsum(0)\n54.8 \u00b5s \u00b1 311 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [14]: %timeit y.cumprod(0)\n56.2 \u00b5s \u00b1 836 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32537\n\nDifferential Revision: D19951171\n\nPulled By: anjali411\n\nfbshipit-source-id: cf972c550189473e9ce62e24ac7dd34b9373fef9", "pr_number": "32537", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/TensorDimApply.h", "aten/src/ATen/native/cuda/ReduceOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/THCTensorMathReduce.cuh", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_torch_docs.py"], "labels": ["merged"]}, "c59e35b147": {"title": "interpreter handling for varargs to remove need for looking at Node (#32791)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32791\n\nWhen a registered operator has varags (ends with ... in its schema),\nthe interpreter now appends the number of arguments to the top of\nthe stack before invoking the operator. This allows the removal of more\nuses of Node* in the interpreter.\n\nThis PR also then cleans up the constructors for Operator to make\nit more likely someone chooses the correct one. After making these ops:\n\n```\nUSES NODE: prim::TupleUnpack(...) -> (...)\nUSES NODE: prim::TupleSlice(...) -> (...)\nUSES NODE: prim::TupleConstruct(...) -> (...)\nUSES NODE: prim::ListUnpack(...) -> (...)\nUSES NODE: prim::ListConstruct(...) -> (...)\nUSES NODE: prim::DictConstruct(...) -> (...)\nUSES NODE: prim::Constant() -> (...)\nUSES NODE: prim::isinstance(...) -> (...)\nUSES NODE: prim::CreateObject(...) -> (...)\nUSES NODE: prim::fork(...) -> (...)\nUSES NODE: aten::warn(str message, *, int stacklevel=2) -> () # need stack level information, so ideally in interpreter so it can look at the stack\n```\n\nInto interpreter primitives, we can remove all but two constructors for operators:\none that is (schema_string, operation), and one that is (symbol, op_creator) for\nthe remaining weird primitives.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19673158\n\nPulled By: zdevito\n\nfbshipit-source-id: 95442a001538a6f53c1db4a210f8557ef118de66", "pr_number": "32791", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/test_constant_pooling.cpp", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/interpreter.cpp", "torch/csrc/jit/operator.h", "torch/csrc/jit/passes/batch_mm.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/register_special_ops.cpp", "torch/csrc/jit/script/schema_type_parser.cpp"], "labels": ["jit"]}, "83c347ff4a": {"title": "Remove prim::Constant op (#32804)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32804\n\nConstants are interpreter primitives so the op was not actually used.\nThis cleans up some of the logic around it.\n\nThis also fixes constant prop such that failures to look up an op\ndo not silently stop constant propagation. Instead, only errors\ninside the op implementation itself will do this.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19673156\n\nPulled By: zdevito\n\nfbshipit-source-id: 7beee59a6a67a6c2f8261d86bd505280fefa999e", "pr_number": "32804", "files_changed": ["test/test_jit.py", "test/test_jit_py3.py", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/passes/constant_propagation.cpp"], "labels": ["jit"]}, "7f2c25b6fa": {"title": "Move special ops into interpreter (#32889)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32889\n\nCommon primitive ops that have special inputs make it very hard to\nserialize the bytecode for mobile because information about how the\nop behaves is hidden in the Node*. This changes how we handle the following\nops so that they are encoded as their own interpreter bytecodes.\n\n```\n    USES NODE: prim::TupleUnpack(...) -> (...)\n    USES NODE: prim::TupleSlice(...) -> (...)\n    USES NODE: prim::TupleConstruct(...) -> (...)\n    USES NODE: prim::ListUnpack(...) -> (...)\n    USES NODE: prim::ListConstruct(...) -> (...)\n    USES NODE: prim::DictConstruct(...) -> (...)\n    USES NODE: prim::Constant() -> (...)\n    USES NODE: prim::isinstance(...) -> (...)\n    USES NODE: prim::CreateObject(...) -> (...)\n    USES NODE: prim::fork(...) -> (...)\n    USES NODE: aten::warn(str message, *, int stacklevel=2) -> () # need stack level information, so ideally in interpreter so it can look at the stack\n```\n\nThis leaves a state where the _only_ remaining Node*-consuming builtins\nare things that are only introduced during JIT optimization and will\nnot appear in mobile code.\n\nSerialization of bytecode can now be made to directly write the CodeImpl\nobject without modification.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19673157\n\nPulled By: zdevito\n\nfbshipit-source-id: 7b8c633d38a4c783b250fbdb222705e71a83ad26", "pr_number": "32889", "files_changed": ["torch/csrc/jit/export_module.cpp", "torch/csrc/jit/instruction.cpp", "torch/csrc/jit/instruction.h", "torch/csrc/jit/interpreter.cpp", "torch/csrc/jit/interpreter.h", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/function.h", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/interpreter.h", "torch/csrc/jit/mobile/type_parser.h", "torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/register_special_ops.cpp", "torch/csrc/jit/vararg_functions.cpp", "torch/csrc/jit/vararg_functions.h"], "labels": ["jit", "merged"]}, "f1b73799d5": {"title": "Clean up isinstance flags (#33265)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33265\n\nThis removes the need for isinstance to keep trace of list and tuple\nseparately by introducing AnyListType and AnyTupleType into the JIT\ntype system to be the common supertype of any lists or tuples.\n\nThis allows us to remove the weird flags from the interpreter for\nthe isinstance operator.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19883933\n\nPulled By: zdevito\n\nfbshipit-source-id: f998041b42d8b4554c5b99f4d95d1d42553c4d81", "pr_number": "33265", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "torch/csrc/jit/export_module.cpp", "torch/csrc/jit/instruction.cpp", "torch/csrc/jit/instruction.h", "torch/csrc/jit/interpreter.cpp", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/ir.h", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/function.h", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/script/ir_emitter.cpp", "torch/csrc/jit/script/string_to_type.cpp", "torch/csrc/jit/unpickler.cpp", "torch/csrc/jit/vararg_functions.cpp", "torch/csrc/jit/vararg_functions.h"], "labels": ["jit", "merged"]}, "43e015f4b1": {"title": "Bug fix in dynamic quantization kernels + better test coverage. (#33320)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33320\n\nReviewed By: supriyar\n\nDifferential Revision: D19893911\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: e79dd06af333c6629e3412315550814da28d9c24", "pr_number": "33320", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm/8x8-dq-aarch64-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/test/convolution-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/convolution.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/test/fully-connected-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/fully-connected.cc"], "labels": ["merged"]}, "feaa622fc6": {"title": "[Update transforms.py]Add `TanhTransform` (#19785)", "body": "Summary:\nResolves https://github.com/pytorch/pytorch/issues/33195\nPull Request resolved: https://github.com/pytorch/pytorch/pull/19785\n\nDifferential Revision: D19642395\n\nPulled By: ezyang\n\nfbshipit-source-id: 73c386fb89cd195201757b5fa47d6c01914a1f8f", "pr_number": "19785", "files_changed": ["test/test_distributions.py", "torch/distributions/transforms.py"], "labels": ["merge-this-please", "merged", "module: distributions", "open source"]}, "3ad59734d7": {"title": "Add type annotation for bias in _ConvNd (#32885)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32885\n\nCurrently Tensor bias is registered as parameter and None bias is registered as attribute.\nWe need the type annotation because when we try to fold ConvBn in graph mode quantization we'll\nremove the None bias attribute and add a Tensor bias attribute, without type annotation the\nbias Value in the graph  will be marked with different type in these two cases, so we have rewrite the\ngraph to change the type as well in that case. But with type annotation we don't need to modify the graph\nsince both cases the bias value will have type `Tensor?`\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19844710\n\nfbshipit-source-id: 52438bc72e481ab78560533467f9379a8b0b0cfa", "pr_number": "32885", "files_changed": ["torch/nn/modules/conv.py"], "labels": ["merged"]}, "96989a2a11": {"title": "[ONNX] Adding ONNX large model export support in exporter (#33062)", "body": "Summary:\nThere are large models such as GPT2-large which cannot be exported with the current exporter because of the 2GB protobuf limit (e.g. see https://github.com/pytorch/pytorch/issues/19277). ONNX spec specifies a special format for large (> 2GB)  models. This PR adds support for exporting large models in ONNX large model format in the PyTorch-ONNX exporter.\n\nThis is the first PR for this feature that enables the end-to-end execution. Tests for large model export have been added. We may need follow-up PRs to refine this workflow based on user feedback.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33062\n\nReviewed By: hl475\n\nDifferential Revision: D19782292\n\nPulled By: houseroad\n\nfbshipit-source-id: e972fcb066065cae6336aa91c03023d9c41c88bd", "pr_number": "33062", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export.h", "torch/csrc/jit/python_ir.cpp", "torch/onnx/__init__.py", "torch/onnx/utils.py"], "labels": ["jit", "module: onnx", "open source", "triaged"]}, "a5f01846c2": {"title": "Kill THCState_getCurrentStream (#33376)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33376\n\nDifferential Revision: D19964101\n\nPulled By: ngimel\n\nfbshipit-source-id: d6b76327191a469f3a88a54d8ffe07121139ab16", "pr_number": "33376", "files_changed": ["aten/src/THC/THCApply.cuh", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h.in", "aten/src/THC/THCReduce.cuh", "aten/src/THC/THCReduceAll.cuh", "aten/src/THC/THCSleep.cu", "aten/src/THC/THCStorage.cpp", "aten/src/THC/THCTensorMathReduce.cuh", "aten/src/THC/THCTensorSort.cu", "aten/src/THC/generic/THCStorage.cpp", "aten/src/THC/generic/THCStorage.cu", "aten/src/THC/generic/THCStorageCopy.cpp", "aten/src/THC/generic/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorMasked.cu", "aten/src/THC/generic/THCTensorMath.cu", "aten/src/THC/generic/THCTensorMathBlas.cu", "aten/src/THC/generic/THCTensorMathMagma.cu", "aten/src/THC/generic/THCTensorMathReduce.cu", "aten/src/THC/generic/THCTensorMathScan.cu", "aten/src/THC/generic/THCTensorMode.cu", "aten/src/THC/generic/THCTensorRandom.cu", "aten/src/THC/generic/THCTensorScatterGather.cu", "aten/src/THC/generic/THCTensorSort.cu", "aten/src/THC/generic/THCTensorTopK.cu", "aten/src/THCUNN/generic/ClassNLLCriterion.cu", "aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu", "aten/src/THCUNN/generic/MultiMarginCriterion.cu", "aten/src/THCUNN/generic/RReLU.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu", "aten/src/THCUNN/generic/SpatialConvolutionMM.cu", "aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu"], "labels": ["merged", "open source"]}, "d50305e2f3": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/7903fc3142625ad1eaa2bf85b80357a28accb2fd\nhttps://github.com/facebook/wangle/commit/462eaef5fcbad7544ba8ebd962e3a3e51f65dfbf\nhttps://github.com/facebookincubator/fizz/commit/e2966a7507fd8e050b94cb5ba0e42ea544d198bb\nhttps://github.com/facebookincubator/katran/commit/09013ed8c4815448be44eceba956165c74b6bf01\nhttps://github.com/facebookincubator/mvfst/commit/df7e47c39b6eaa321df033e332e5eacbf7dbc5a3\nhttps://github.com/pytorch/fbgemm/commit/f40e6d1dbf2eaadcb44ef2e6359fbeb34063f934\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 37553007eb60438d5ddd9cb16f0edc24e4637c25", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "e9ac92a242": {"title": "Make RPC message constructor actually move (#33440)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33440\n\nThe constructors make a copy without `std::move` in the initializer list.\n\nTest Plan:\nConfirmed manually that without this change, the `data()` pointer of\nthe vector changes. With this change it does not, as intended.\n\nReviewed By: mrshenli\n\nDifferential Revision: D19948685\n\nfbshipit-source-id: ee4f22e29894b858ad86068722dc2f4651987517", "pr_number": "33440", "files_changed": ["torch/csrc/distributed/rpc/message.cpp"], "labels": ["fb-exported", "merged", "module: rpc"]}, "d85c913bfd": {"title": "[jit] Delete the ErrorReport default constructor (#32879)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32879\n\nAn error report without a SourceRange context is bad, because it doesn't\ntell the user where something happened. Delete the default constructor\nto make it harder to create errors like this (you can still use a fake\nSourceRange if you absolutely need to).\n\nAlso clean up the only case where the default constructor was used.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19670924\n\nPulled By: suo\n\nfbshipit-source-id: 46888a86e5d32b84c8d6d52c0c8d70243722b14a", "pr_number": "32879", "files_changed": ["torch/csrc/jit/function.cpp", "torch/csrc/jit/function.h", "torch/csrc/jit/script/error_report.cpp", "torch/csrc/jit/script/error_report.h", "torch/csrc/jit/script/sugared_value.h"], "labels": ["jit", "merged"]}, "d13c1b8af8": {"title": "[jit] de-optionalize SourceRange context (#32880)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32880\n\nThe PR below made it impossible to construct a SourceRange without a\ncontext, so get rid of its optional-ness\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19670923\n\nPulled By: suo\n\nfbshipit-source-id: 05936fca2a3d5e613313ade9287b2210bc4a3ccd", "pr_number": "32880", "files_changed": ["torch/csrc/jit/script/error_report.cpp", "torch/csrc/jit/script/error_report.h"], "labels": ["jit", "merged"]}, "8b6a898d2b": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/d9ead2de3440f4142dd63f1fd69be066ad4c24d5\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 6c245f2a656d30b7baf8d0bff85a49090174c289", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "53ad596342": {"title": "[jit] Remove `torch.jit._dump_trace (#33453)", "body": "Summary:\nThis was old code that isn't tested and is broken, it should have been\ndeleted in #24874\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33453\n\nPulled By: driazati\n\nDifferential Revision: D19961403\n\nfbshipit-source-id: 94c52360460194d279dad5b0ea756ee366f525e1", "pr_number": "33453", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "a67691e508": {"title": "Fix isnan for integral types in MSVC (#33483)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/pull/32537#discussion_r381077989.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33483\n\nDifferential Revision: D19970623\n\nPulled By: anjali411\n\nfbshipit-source-id: 53502101822672a333ab5349d93b6e93f7ee4265", "pr_number": "33483", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/cuda/ReduceOpsKernel.cu"], "labels": ["merge-this-please", "merged", "open source"]}, "d7f00b1b45": {"title": "Remove using declaration from widely-used header file. (#33293)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33293\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19904992\n\nPulled By: gchanan\n\nfbshipit-source-id: b5ac76db2e5cdb422671c6c5424858e1d97c323e", "pr_number": "33293", "files_changed": ["tools/autograd/gen_variable_factories.py", "tools/autograd/templates/python_torch_functions.cpp", "tools/autograd/templates/variable_factories.h"], "labels": ["merged"]}, "96e5dea9f4": {"title": "Remove unused variable (#33484)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33484\n\natt\n\nTest Plan: unittests\n\nReviewed By: jfix71\n\nDifferential Revision: D19862090\n\nfbshipit-source-id: c6a33604e2fc78fb90ae2b5fcc72421ee89a02aa", "pr_number": "33484", "files_changed": ["caffe2/opt/onnxifi_transformer.cc"], "labels": ["fb-exported", "merged"]}, "165b1ad8e8": {"title": "Kill THCState_getNumDevices (#33375)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33375\n\nDifferential Revision: D19973163\n\nPulled By: ezyang\n\nfbshipit-source-id: d8edede3a3ac5012e4208bb30b6e66d8a2d1019f", "pr_number": "33375", "files_changed": ["aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h.in"], "labels": ["merged", "open source"]}, "60339a38ed": {"title": "Fixes #33001 (#33456)", "body": "Summary:\nThis fixes https://github.com/pytorch/pytorch/issues/33001.\n\nWhen subtracting 1 from a empty array, instead of being `-1` as seems to be expected in the later code (while loop), because `size()` seems to be unsigned, it becomes a very large number. This causes a segfault during the while loop later in the code where it tries to access a empty array.\n\nThis issue seemed to happen only on the pi with the following example code: `v = torch.FloatTensor(1, 135).fill_(0); v[0, [1]] += 2`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33456\n\nDifferential Revision: D19963711\n\nPulled By: ezyang\n\nfbshipit-source-id: 1dbddd59a5df544cd7e025fc540c9efe2c4e19f4", "pr_number": "33456", "files_changed": ["aten/src/ATen/TensorUtils.cpp"], "labels": ["merged", "open source"]}, "1e3664b6ef": {"title": "Remove c/pdist tests from _internal/common_utils.py (#33409)", "body": "Summary:\n* remove brute_test from `torch/testing/_internal/common_utils.py`\n* add these tests as internal tests to `test_torch.py`\n\nCC ailzhang\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33409\n\nDifferential Revision: D19951729\n\nPulled By: ailzhang\n\nfbshipit-source-id: b1126aaf26fa64a0f17cbb582dc8038b79cfe3eb", "pr_number": "33409", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged", "open source"]}, "07e5e42713": {"title": "[jit][fix] Remove slot in parameter slot (#32846)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32846\n\natt\n\nTest Plan:\nbuild/bin/test_jit\n\nImported from OSS\n\nDifferential Revision: D19844711\n\nfbshipit-source-id: 3d29e5e97e97781f5dc00069827971baed52d76e", "pr_number": "32846", "files_changed": ["test/cpp/jit/test_class_type.cpp", "torch/csrc/jit/script/class_type.cpp"], "labels": ["jit", "merged"]}, "d4e4513a64": {"title": "[JIT] Add more ops to 'removableGuard' in guard elimination pass. (#33465)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33465\n\nDifferential Revision: D19958385\n\nTest Plan: Imported from OSS\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: f89b6a2ead279b55af286072223fc9ea1b5fe3b3", "pr_number": "33465", "files_changed": ["torch/csrc/jit/passes/guard_elimination.cpp"], "labels": ["jit", "merged"]}, "a8bd1d24c9": {"title": "[Documentation] cummin doc fix (#33492)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33492\n\nDifferential Revision: D19976082\n\nPulled By: anjali411\n\nfbshipit-source-id: c9f8f541783fded98b8aba54e293f824c926496e", "pr_number": "33492", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged"]}, "62c953b348": {"title": "Fix svd tests between devices. (#33470)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33470\n\nDifferential Revision: D19974449\n\nPulled By: ailzhang\n\nfbshipit-source-id: e456608fe95d270d822e786a5955cce7c746165c", "pr_number": "33470", "files_changed": ["test/test_torch.py"], "labels": ["merged"]}, "1d9fcf8bd2": {"title": "Correct documentation for torch.unsqueeze (#33478)", "body": "Summary:\n\"out\" argument in torch.unsqueeze is not actually implemented, fixed documentation https://github.com/pytorch/pytorch/issues/29800\nAfter: ![image](https://user-images.githubusercontent.com/33493903/74796371-6289ee00-5296-11ea-8493-e8c18ac63bdf.png)\n\nBefore: ![image](https://user-images.githubusercontent.com/33493903/74796444-96651380-5296-11ea-816c-2adacfa79e35.png)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33478\n\nDifferential Revision: D19978477\n\nPulled By: yf225\n\nfbshipit-source-id: 42337326c1ec04975307366c94591ee32a11b091", "pr_number": "33478", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "20c1e25832": {"title": "Re-sync with internal repository (#33519)", "body": "", "pr_number": null, "files_changed": ["tools/build_variables.bzl"], "labels": []}, "8908b62fb2": {"title": "Clean views created inside no_grad that are modified inplace (#32839)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32839\n\nAs mentioned in the updated comment in `variable.h`, this disambiguate code like:\n```python\nbase = torch.rand(10, requires_grad=True)\nwith torch.no_grad():\n    view = base[1]\nview.copy_(var)\ntorch.autograd.grad(base.sum(), var)  # <- what should it return?\n```\nGiven that there is no consensus of what should happen here (does the gradient flow through the view in the no_grad or not). This special case is detected and forbidden.\nAs mentionned in the error message:\n- If you want it to be tracked: move both out of the no_grad\n- If do not want them to be tracked, move both inside the no_grad\n\nThis implies that any custom Function that returns views does not allow inplace modification on its output. I'll add a PR to the stack to relax this to be a DeprecationWarning for now. And we will make it into an actual error for 1.6\n\nThis replaces https://github.com/pytorch/pytorch/pull/26607\ncc sublee\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19814114\n\nPulled By: albanD\n\nfbshipit-source-id: ff2c9d97c8f876d9c31773a2170e37b06d88bed7", "pr_number": "32839", "files_changed": ["test/test_autograd.py", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/custom_function.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h"], "labels": ["merged", "topic: bc-breaking"]}, "cbf8657945": {"title": "[jit] Fix ModuleDict type sharing (#33515)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33515\n\nPreviously, if we had a `ModuleDict` with the same value types but\ndifferent names for keys, they would share types under certain\nconditions. This only happens for `ModuleDict`, because in other cases\na simple Python class check invalidates the class.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19978552\n\nPulled By: suo\n\nfbshipit-source-id: f31b2af490064f89b70aa35f83ba740ddaf2a77a", "pr_number": "33515", "files_changed": ["test/jit/test_type_sharing.py", "torch/csrc/jit/script/concrete_module_type.cpp"], "labels": ["jit", "merged"]}, "5e80ca12bb": {"title": "[pt][fbgemm] Turn on USE_FBGEMM on Windows env (#297)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/FBGEMM/pull/297\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33250\n\nAs Title says. FBGEMM has recently added the support for Windows.\n\nghstack-source-id: 97932881\n\nTest Plan: CI\n\nReviewed By: jspark1105\n\nDifferential Revision: D19738268\n\nfbshipit-source-id: e7f3c91f033018f6355edeaf6003bd2803119df4", "pr_number": "33250", "files_changed": [".circleci/scripts/vs_install.ps1", "aten/src/ATen/native/QuantizedLinear.cpp", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.h", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "caffe2/quantization/server/CMakeLists.txt", "caffe2/quantization/server/activation_distribution_observer.h", "caffe2/quantization/server/batch_matmul_dnnlowp_op.cc", "caffe2/quantization/server/conv_dnnlowp_op.cc", "caffe2/quantization/server/dnnlowp.h", "caffe2/quantization/server/elementwise_sum_dnnlowp_op.cc", "caffe2/quantization/server/fbgemm_pack_op.cc", "caffe2/quantization/server/fully_connected_fake_lowp_op_avx2.cc", "caffe2/quantization/server/norm_minimization.cc", "caffe2/quantization/server/transpose.cc", "cmake/Dependencies.cmake"], "labels": ["fb-exported", "merged"]}, "416413dec4": {"title": "[jit] add `inlined_graph` method to ScriptFunctions (#33508)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33508\n\nEver since we switched to not inlining by default, some users have\ncomplained since they relied on inlining occuring to, e.g. process the\ngraph with some other tool. Add an inlined_graph for convenience in\nthose cases.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19977638\n\nPulled By: suo\n\nfbshipit-source-id: fe1fa92ff888959203d5d1995930d488b5f9e24c", "pr_number": "33508", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "507f963aa6": {"title": "[RPC Reliability] Enabled retries for RPCs with exponential backoff (#33365)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33365\n\nThis adds functionality for re-trying RPC's that are sent with the function sendWithRetries(). It adds RPC's that will potentially need to be retried to a sorted map that contains the timeout at which to retry the RPC and associated metadata. A separate thread iteratively removes the earliest retry-able RPC from the map, sleeps until the corresponding time point, re-tries the RPC, and adds to the map again with a future timeout.\n\nGitHub Issue: https://github.com/pytorch/pytorch/issues/32124\n\nPer the first 4 milestones, the following will be addressed in future PR's:\n* enabling RPC Retries for RRef internal messages\n\nDifferential Revision: D19915694\n\nfbshipit-source-id: 4a520e32d5084ebcf90e97fd9f26867115a35c0c", "pr_number": "33365", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h"], "labels": ["fb-exported", "merged"]}, "8527ba8b70": {"title": "[jit] Add None parameter as parameter instead of attributes (#32964)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32964\n\natt\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19913188\n\nfbshipit-source-id: 9cdd93cbaf9892f4311656c786637765a675a68c", "pr_number": "32964", "files_changed": ["test/cpp/jit/test_module_api.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/script/class_type.cpp", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/script/module.h", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "bd3c6e8e91": {"title": "avoid large vector copy when query per_channel q_params (#31040)", "body": "Summary:\nThe quantizer use std::vector to save per_channel scales and zero_points, but when query scales(zero_points), it requires to return tensor. These lead to use std::vector to initialize tensors and it dose cost lots of time. So I change quantizer to save per_channel scales and zero_points by using tensor directly.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31040\n\nDifferential Revision: D19701070\n\nPulled By: jerryzh168\n\nfbshipit-source-id: 9043f16c44b74dd8289b8474e540171765a7f92a", "pr_number": "31040", "files_changed": ["aten/src/ATen/native/quantized/QTensor.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "aten/src/ATen/quantized/Quantizer.h", "torch/_utils.py", "torch/csrc/jit/pickler.cpp", "torch/csrc/jit/unpickler.cpp", "torch/tensor.py"], "labels": ["merged", "open source", "quantization", "triaged"]}, "e5a02aa2fe": {"title": "[caffe2] simplify relative error expr (#32999)", "body": "Summary:\nsimplify relative error expr\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32999\n\nDifferential Revision: D19739382\n\nPulled By: jerryzh168\n\nfbshipit-source-id: 95e0c68f6d9cb6708f400cc1cdb311af83b0621e", "pr_number": "32999", "files_changed": ["caffe2/share/contrib/depthwise/depthwise3x3_conv_op_test.cc"], "labels": ["merged", "open source"]}, "ea514c819a": {"title": "Make slow_conv_transpose2d_backward tensors contiguous (#33462)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33462\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19956516\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 4fa9dcba0dd02b891ab36e6ecee8fc59e049c15c", "pr_number": "33462", "files_changed": ["test/test_torch.py", "tools/autograd/derivatives.yaml"], "labels": ["merged"]}, "ecb05f12c3": {"title": "Support broadcast for quantized mul kernel (#30442)", "body": "Summary:\nSince the tensor iterator supports the broadcast, we will just remove the assertion on input shapes.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30442\n\nDifferential Revision: D19976562\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: 91b27fc8b2570f29d110c6df26eacdd16f587b9f", "pr_number": "30442", "files_changed": ["aten/src/ATen/native/quantized/cpu/qmul.cpp", "test/test_quantized.py", "torch/nn/quantized/modules/functional_modules.py"], "labels": ["merged"]}, "6cb9e6b015": {"title": "Back out \"Revert D19871946: [distributed] pass in timeout to TCP store when initializing\" (#33434)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33434\n\nReland of https://github.com/pytorch/pytorch/pull/33325, since the\nunit test was flaky and failed on land.\nTo ensure that the test is not flaky, I bumped the timeout so the rendezvous\ndoes not timeout (timing out the rendezvous in 1s led to the flakiness). I also\ngeneralized our mechanism for retrying on errors to include retrying on errors\ndue to timeout in rendezvous.\nghstack-source-id: 98558377\n\nTest Plan: Added UT test_tcp_store_timeout_set\n\nDifferential Revision: D19935390\n\nfbshipit-source-id: 56ccf8c333dd2f954a33614d35cd1642d4e9473a", "pr_number": "33434", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/constants.py", "torch/distributed/distributed_c10d.py", "torch/distributed/rendezvous.py", "torch/distributed/rpc/constants.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged"]}, "602ef0d9d0": {"title": "[WIP] migrate scatter_ to ATen CPU (+multithreading, nondeterministic) (#33139)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/24757, partially https://github.com/pytorch/pytorch/issues/33094. Uses fix introduces in https://github.com/pytorch/pytorch/issues/33108 to avoid regressions for some compilers.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33139\n\nDifferential Revision: D19882462\n\nPulled By: ngimel\n\nfbshipit-source-id: 5016f186a4aadc3cc32edcfd9abdea11786f27e9", "pr_number": "33139", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/ScatterGatherShapeChecks.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/cpu/ScatterGatherKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.h"], "labels": ["merged", "open source", "topic: porting", "triaged"]}, "81394581a3": {"title": "[Caffe2][ThreadPool] Make sure numThreads does not exceed the number of big cores (#33523)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33523\n\nWhen using `ThreadPool::setNumThreads` to set the number of threads, it should not exceed the number of big cores. Otherwise, the performance could degrade significantly.\n\nTest Plan:\n```\ncd ~/fbsource/xplat\nbuck test caffe2:caffe2_testAndroid\n```\n\nReviewed By: dreiss\n\nDifferential Revision: D19779267\n\nfbshipit-source-id: 4e980e8a0ccc2f37e1c8ed16e2f4651d72924dbd", "pr_number": "33523", "files_changed": ["caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/ThreadPool.h"], "labels": ["fb-exported", "merged"]}, "1fe635be3c": {"title": "Allow vectorized gpu loop to have different argument types (#33222)", "body": "Summary:\nAlthough currently the only user of GPU loops that has args with different dtypes is `where`, it sounds strange to restrict the args to have the same dtype. Allowing args to have different dtypes also makes it possible for me to clean up legacy code by reusing current code to implement unrolled GPU loop for non-contiguous tensors.\n\nThe stack storage of `elementwise_kernel_helper` is changed from `arg_t args[nt][arity]` to `traits:: ArgsTuple args[nt]`. Due to this change, we can no longer get element by `operator[]`, but instead we should use `std::get`. As a result, we can no longer unroll the loop wrt arity using pragma, but we have to\ncreate a `static_unroll` to make use of template meta-programming to do the same job.\n\nA good side effect of this change is, `invoke_with_array` is no longer needed and can be replaced with already existing `c10::guts::apply`. And we don't need the `namespace arg_type` workaround either. This makes the code less ugly.\n\nThe same approach might also work for ROCm loops, but I didn't change anything on ROCm in this PR, because I don't want potential compilation error or perf regression to delay this PR. But after this gets merged, I will try on ROCm and send a separate PR to make the code less diverge if the same approach trivially applies (trivially apply means a mindless copy-paste doesn't introduce unexpected compilation error or perf regression).\n\nAssembly (https://github.com/zasdfgbnm/things/blob/master/2020Q1/disassembly-elementwise-vec.ipynb#33222):\n```\n**Symbol:**\nvoid at::native::modern::elementwise_kernel<4, 64, 4, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()https://github.com/pytorch/pytorch/issues/1}::operator()() const::{lambda()https://github.com/pytorch/pytorch/issues/4}::operator()() const::{lambda(float, float)https://github.com/pytorch/pytorch/issues/1}, at::detail::Array<char*, 3> >(int, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()https://github.com/pytorch/pytorch/issues/1}::operator()() const::{lambda()https://github.com/pytorch/pytorch/issues/4}::operator()() const::{lambda(float, float)https://github.com/pytorch/pytorch/issues/1}, at::detail::Array<char*, 3>)\n\n**ASM:**\n\n\t.section\t.text._ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,\"ax\",progbits\n\t.sectioninfo\t@\"SHI_REGISTERS=20\"\n\t.align\t128\n        .global         _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_\n        .type           _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,function\n        .size           _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,(.L_40520 - _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_)\n        .other          _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n_ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_:\n.text._ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_:\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh\", line 253\n        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;\n        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;\n        /*0020*/                   S2R R9, SR_CTAID.X ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 39\n        /*0030*/                   S2R R0, SR_TID.X ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh\", line 253\n        /*0040*/                   IMAD.SHL.U32 R9, R9, 0x100, RZ ;\n        /*0050*/                   IADD3 R5, -R9, c[0x0][0x160], RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh\", line 227\n        /*0060*/                   SHF.R.S32.HI R17, RZ, 0x1f, R9 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh\", line 255\n        /*0070*/                   ISETP.GE.AND P0, PT, R5, 0x100, PT ;\n        /*0080*/              @!P0 BRA `(.L_2919) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh\", line 227\n        /*0090*/                   IMAD.SHL.U32 R12, R9.reuse, 0x4, RZ ;\n        /*00a0*/                   SHF.L.U64.HI R17, R9, 0x2, R17 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh\", line 229\n        /*00b0*/                   IADD3 R8, P0, R12.reuse, c[0x0][0x188], RZ ;\n        /*00c0*/                   IADD3 R2, P1, R12, c[0x0][0x190], RZ ;\n        /*00d0*/                   IADD3.X R9, R17.reuse, c[0x0][0x18c], RZ, P0, !PT ;\n        /*00e0*/                   IADD3.X R3, R17, c[0x0][0x194], RZ, P1, !PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 82\n        /*00f0*/                   IMAD.WIDE R8, R0, 0x10, R8 ;\n        /*0100*/                   IMAD.WIDE R2, R0, 0x10, R2 ;\n        /*0110*/                   LDG.E.128.SYS R8, [R8] ;\n        /*0120*/                   LDG.E.128.SYS R4, [R2] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh\", line 227\n        /*0130*/                   IADD3 R12, P0, R12, c[0x0][0x180], RZ ;\n        /*0140*/                   IADD3.X R13, R17, c[0x0][0x184], RZ, P0, !PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 102\n        /*0150*/                   IMAD.WIDE R12, R0, 0x10, R12 ;\n\t//## File \"/usr/include/c++/8/tuple\", line 1315\n        /*0160*/                   FFMA R7, R7, c[0x0][0x168], R11 ;\n        /*0170*/                   FFMA R6, R6, c[0x0][0x168], R10 ;\n        /*0180*/                   FFMA R5, R5, c[0x0][0x168], R9 ;\n        /*0190*/                   FFMA R4, R4, c[0x0][0x168], R8 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 102\n        /*01a0*/                   STG.E.128.SYS [R12], R4 ;\n        /*01b0*/                   EXIT ;\n.L_2919:\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 42\n        /*01c0*/                   ISETP.GE.AND P0, PT, R0, R5, PT ;\n        /*01d0*/                   BMOV.32.CLEAR RZ, B0 ;\n        /*01e0*/                   BSSY B0, `(.L_2920) ;\n        /*01f0*/                   IMAD.MOV.U32 R4, RZ, RZ, RZ ;\n        /*0200*/                   CS2R R6, SRZ ;\n        /*0210*/                   IMAD.MOV.U32 R8, RZ, RZ, RZ ;\n        /*0220*/                   IMAD.MOV.U32 R10, RZ, RZ, RZ ;\n        /*0230*/               P0 BRA `(.L_2921) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 45\n        /*0240*/                   IADD3 R3, P1, R9, R0, RZ ;\n        /*0250*/                   LEA.HI.X.SX32 R6, R0, R17, 0x1, P1 ;\n        /*0260*/                   LEA R2, P1, R3, c[0x0][0x188], 0x2 ;\n        /*0270*/                   LEA.HI.X R3, R3, c[0x0][0x18c], R6, 0x2, P1 ;\n        /*0280*/                   LDG.E.SYS R10, [R2] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 46\n        /*0290*/                   IADD3 R6, R0, 0x40, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 42\n        /*02a0*/                   ISETP.GE.AND P1, PT, R6, R5, PT ;\n        /*02b0*/               P1 BRA `(.L_2922) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 45\n        /*02c0*/                   LDG.E.SYS R6, [R2+0x100] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 46\n        /*02d0*/                   IADD3 R8, R0, 0x80, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 42\n        /*02e0*/                   ISETP.GE.AND P1, PT, R8, R5, PT ;\n        /*02f0*/               P1 BRA `(.L_2923) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 46\n        /*0300*/                   IADD3 R8, R0, 0xc0, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 42\n        /*0310*/                   ISETP.GE.AND P1, PT, R8, R5, PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 45\n        /*0320*/                   LDG.E.SYS R8, [R2+0x200] ;\n        /*0330*/              @!P1 LDG.E.SYS R7, [R2+0x300] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 102\n        /*0340*/               P1 IMAD.MOV.U32 R7, RZ, RZ, RZ ;\n        /*0350*/                   BRA `(.L_2921) ;\n.L_2923:\n        /*0360*/                   IMAD.MOV.U32 R7, RZ, RZ, RZ ;\n        /*0370*/                   IMAD.MOV.U32 R8, RZ, RZ, RZ ;\n        /*0380*/                   BRA `(.L_2921) ;\n.L_2922:\n        /*0390*/                   CS2R R6, SRZ ;\n        /*03a0*/                   IMAD.MOV.U32 R8, RZ, RZ, RZ ;\n.L_2921:\n        /*03b0*/                   BSYNC B0 ;\n.L_2920:\n        /*03c0*/                   BMOV.32.CLEAR RZ, B0 ;\n        /*03d0*/                   BSSY B0, `(.L_2924) ;\n        /*03e0*/               P0 BRA `(.L_2925) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 45\n        /*03f0*/                   IADD3 R3, P1, R9, R0, RZ ;\n        /*0400*/                   LEA.HI.X.SX32 R12, R0, R17, 0x1, P1 ;\n        /*0410*/                   LEA R2, P1, R3, c[0x0][0x190], 0x2 ;\n        /*0420*/                   LEA.HI.X R3, R3, c[0x0][0x194], R12, 0x2, P1 ;\n        /*0430*/                   LDG.E.SYS R11, [R2] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 46\n        /*0440*/                   IADD3 R12, R0, 0x40, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 42\n        /*0450*/                   ISETP.GE.AND P1, PT, R12, R5, PT ;\n        /*0460*/               P1 BRA `(.L_2926) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 45\n        /*0470*/                   LDG.E.SYS R13, [R2+0x100] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 46\n        /*0480*/                   IADD3 R12, R0, 0x80, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 42\n        /*0490*/                   ISETP.GE.AND P1, PT, R12, R5, PT ;\n        /*04a0*/               P1 BRA `(.L_2927) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 45\n        /*04b0*/                   LDG.E.SYS R15, [R2+0x200] ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 46\n        /*04c0*/                   IADD3 R12, R0, 0xc0, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 42\n        /*04d0*/                   ISETP.GE.AND P1, PT, R12, R5, PT ;\n        /*04e0*/               P1 BRA `(.L_2928) ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 45\n        /*04f0*/                   LDG.E.SYS R4, [R2+0x300] ;\n        /*0500*/                   BRA `(.L_2928) ;\n.L_2927:\n        /*0510*/                   IMAD.MOV.U32 R15, RZ, RZ, RZ ;\n        /*0520*/                   BRA `(.L_2928) ;\n.L_2926:\n        /*0530*/                   IMAD.MOV.U32 R15, RZ, RZ, RZ ;\n        /*0540*/                   IMAD.MOV.U32 R13, RZ, RZ, RZ ;\n        /*0550*/                   BRA `(.L_2928) ;\n.L_2925:\n        /*0560*/                   IMAD.MOV.U32 R15, RZ, RZ, RZ ;\n        /*0570*/                   IMAD.MOV.U32 R13, RZ, RZ, RZ ;\n        /*0580*/                   IMAD.MOV.U32 R11, RZ, RZ, RZ ;\n.L_2928:\n        /*0590*/                   BSYNC B0 ;\n.L_2924:\n\t//## File \"/usr/include/c++/8/tuple\", line 1315\n        /*05a0*/               P0 EXIT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 58\n        /*05b0*/                   IADD3 R9, P0, R9, R0, RZ ;\n\t//## File \"/usr/include/c++/8/tuple\", line 1315\n        /*05c0*/                   FFMA R11, R11, c[0x0][0x168], R10 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 59\n        /*05d0*/                   IADD3 R14, R0, 0x40, RZ ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 58\n        /*05e0*/                   LEA.HI.X.SX32 R12, R0, R17, 0x1, P0 ;\n        /*05f0*/                   LEA R2, P0, R9.reuse, c[0x0][0x180], 0x2 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 55\n        /*0600*/                   ISETP.GE.AND P1, PT, R14, R5, PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 58\n        /*0610*/                   LEA.HI.X R3, R9, c[0x0][0x184], R12, 0x2, P0 ;\n        /*0620*/                   STG.E.SYS [R2], R11 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 55\n        /*0630*/               P1 EXIT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 59\n        /*0640*/                   IADD3 R10, R0, 0x80, RZ ;\n\t//## File \"/usr/include/c++/8/tuple\", line 1315\n        /*0650*/                   FFMA R13, R13, c[0x0][0x168], R6 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 55\n        /*0660*/                   ISETP.GE.AND P0, PT, R10, R5, PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 58\n        /*0670*/                   STG.E.SYS [R2+0x100], R13 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 55\n        /*0680*/               P0 EXIT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 59\n        /*0690*/                   IADD3 R0, R0, 0xc0, RZ ;\n\t//## File \"/usr/include/c++/8/tuple\", line 1315\n        /*06a0*/                   FFMA R15, R15, c[0x0][0x168], R8 ;\n        /*06b0*/                   FFMA R7, R4, c[0x0][0x168], R7 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 55\n        /*06c0*/                   ISETP.GE.AND P0, PT, R0, R5, PT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 58\n        /*06d0*/                   STG.E.SYS [R2+0x200], R15 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 55\n        /*06e0*/               P0 EXIT ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh\", line 58\n        /*06f0*/                   STG.E.SYS [R2+0x300], R7 ;\n\t//## File \"/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh\", line 260\n        /*0700*/                   EXIT ;\n.L_2929:\n        /*0710*/                   BRA `(.L_2929);\n        /*0720*/                   NOP;\n        /*0730*/                   NOP;\n        /*0740*/                   NOP;\n        /*0750*/                   NOP;\n        /*0760*/                   NOP;\n        /*0770*/                   NOP;\n.L_40520:\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33222\n\nDifferential Revision: D19964089\n\nPulled By: ngimel\n\nfbshipit-source-id: a1e8e62d1ebcc67fb49f00d87c02bcdd13194024", "pr_number": "33222", "files_changed": ["aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/MemoryAccess.cuh", "aten/src/ATen/native/cuda/ROCmLoops.cuh", "aten/src/ATen/test/cuda_vectorized_test.cu", "c10/util/C++17.h"], "labels": ["merged", "open source", "triaged"]}, "bf0951d937": {"title": "Updating ONNX checker logic. (#33522)", "body": "Summary:\nWe want to run ONNX checker only when selected operator type is ONNX, and nowhere else. This PR updates the logic in the exporter.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33522\n\nReviewed By: hl475\n\nDifferential Revision: D19983954\n\nPulled By: houseroad\n\nfbshipit-source-id: 15db726321637a96fa110051cc54e9833e201133", "pr_number": "33522", "files_changed": ["torch/onnx/utils.py"], "labels": ["merged", "open source"]}, "e95282ab28": {"title": "[caffe2] make fused rowwise quant/dequant op work for N-dim tensors (#33426)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33426\n\nMake 2/4/8-bit fused rowwise conversion operators more general to work for N-dim tensors\n\nTest Plan: CI\n\nReviewed By: ellie-wen\n\nDifferential Revision: D19943136\n\nfbshipit-source-id: 47008544dd7e1d11a346d34f35449e0fcc0e7ee0", "pr_number": "33426", "files_changed": ["caffe2/operators/fused_rowwise_8bit_conversion_ops.cc", "caffe2/operators/fused_rowwise_8bit_conversion_ops.h", "caffe2/python/fused_8bit_rowwise_conversion_ops_test.py"], "labels": ["fb-exported", "merged"]}, "883b18ea70": {"title": "Delete build_variables.bzl following configerator change.", "body": "Signed-off-by: Edward Z. Yang <ezyang@fb.com>", "pr_number": null, "files_changed": ["tools/build_variables.bzl"], "labels": []}, "05fb160048": {"title": "Revert D19964089: [pytorch][PR] Allow vectorized gpu loop to have different argument types", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19964089\n\nOriginal commit changeset: a1e8e62d1ebc\n\nfbshipit-source-id: fee9423d5924714f0e92eea712cde2d2163b3cf0", "pr_number": null, "files_changed": ["aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/MemoryAccess.cuh", "aten/src/ATen/native/cuda/ROCmLoops.cuh", "aten/src/ATen/test/cuda_vectorized_test.cu", "c10/util/C++17.h"], "labels": []}, "ffe327f7d9": {"title": "Revert \"Disable flaky test TestCppExtensionAOT.test_cuda_extension in\u2026 (#33404)", "body": "Summary:\n\u2026 Windows CI (https://github.com/pytorch/pytorch/issues/33282)\"\n\nThis reverts commit 5b922918d023126ad1f468c68577c9b599ad202d.\n\nFixes https://github.com/pytorch/pytorch/issues/33270.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33404\n\nDifferential Revision: D19972594\n\nPulled By: ezyang\n\nfbshipit-source-id: c8f67536fd6e4b7135171d621ad671b1b2a21fd4", "pr_number": "33404", "files_changed": ["test/test_cpp_extensions_aot.py", "torch/utils/cpp_extension.py"], "labels": ["merge-this-please", "merged", "open source"]}, "196fda5a79": {"title": "Remove special case codegen for tril_indices/triu_indices. (#33305)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33305\n\nThe current TensorOptions code is written to exactly extract out\nTensorOptions based on exact struct match, including default arguments.\nThat meant that tril_indices/triu_indices which had a different\ndefault argument didn't match, and thus needed a special case.\n\nI resolve this special case by instead replacing the explicit long\ndefault argument with a None default argument, and then adjusting\nthe actual implementations to select the correct dtype when none\nwas specified.  I think the general rule I'm following here is that\nit is always acceptable to replace an explicit default argument,\nwith a None argument (assuming the backend will compute it appropriately);\nthe documentation gets modestly worse, but everything that was\npreviously expressible continues to be expressible.  Maybe later\nwe should switch the default argument back to long, but for now\nthe simplification in code is worth it.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19975411\n\nPulled By: ezyang\n\nfbshipit-source-id: 996598759bed9e8d54fe61e19354ad038ed0e852", "pr_number": "33305", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "tools/autograd/gen_python_functions.py"], "labels": ["merged"]}, "a9e4448dff": {"title": "Update documentation on why _cudnn_init_dropout_state looks the way it is. (#33347)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33347\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19975410\n\nPulled By: ezyang\n\nfbshipit-source-id: eb729870c2d279d7d9ca43c92e514fe38dedb06d", "pr_number": "33347", "files_changed": ["aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/native/cudnn/RNN.cpp"], "labels": ["merged"]}, "01e1de8220": {"title": "allow remote torchscript call to itself (#32990)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32990\n\nright now remote torchscript call can not call to itself, this diff is to support this in the same way as how is supported when calling remote python call to itself\nghstack-source-id: 98599082\n\nTest Plan: unit test\n\nDifferential Revision: D19731910\n\nfbshipit-source-id: 6495db68c3eaa58812aa0c5c1e72e8b6057dc5c4", "pr_number": "32990", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/torchscript_functions.cpp", "torch/csrc/distributed/rpc/torchscript_functions.h", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "718c538ff9": {"title": "Add ability to enable/disable MIOpen at runtime (#33118)", "body": "Summary:\n1. Set `torch._C.has_cudnn` to `True` for ROCm\n2. Make MIOpen invocations respect value of `cudnn_enabled` or `at::globalContext().userEnabledCuDNN()`\n3. `torch/backends/cudnn/__init__.py`: Add hip-specific changes (use \"hide whitespace changes\" option to view simpler diff)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33118\n\nDifferential Revision: D19977719\n\nPulled By: bddppq\n\nfbshipit-source-id: 64d4dd1d78afcf96201360d85b8be5950f96dfad", "pr_number": "33118", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/Normalization.cpp", "aten/src/ATen/native/RNN.cpp", "torch/backends/cudnn/__init__.py", "torch/csrc/Module.cpp"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "3233033a17": {"title": "Revert D19975410: Update documentation on why _cudnn_init_dropout_state looks the way it is.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19975410\n\nOriginal commit changeset: eb729870c2d2\n\nfbshipit-source-id: 4d4cc8ae78ad18751c126b93d82932ac2732f1b5", "pr_number": null, "files_changed": ["aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/native/cudnn/RNN.cpp"], "labels": []}, "cdf381c967": {"title": "Fix LambdaLR scheduler side effects (#32848)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/32756\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32848\n\nDifferential Revision: D19859736\n\nPulled By: vincentqb\n\nfbshipit-source-id: 43b3cbb2b6bed208c75aad37aebc2a8a9565fe0d", "pr_number": "32848", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["merged", "module: optimizer", "open source", "triaged"]}, "d19a50bf27": {"title": "Add missing weight_decay parameter validation for Adam and AdamW (#33126)", "body": "Summary:\nAdam and AdamW are missing parameter validation for weight_decay. Other optimisers have this check present.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33126\n\nDifferential Revision: D19860366\n\nPulled By: vincentqb\n\nfbshipit-source-id: 286d7dc90e2f4ccf6540638286d2fe17939648fc", "pr_number": "33126", "files_changed": ["test/test_optim.py", "torch/optim/adam.py", "torch/optim/adamw.py"], "labels": ["merged", "module: optimizer", "open source", "triaged"]}, "687a7e4a25": {"title": "Revert D19975411: Remove special case codegen for tril_indices/triu_indices.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19975411\n\nOriginal commit changeset: 996598759bed\n\nfbshipit-source-id: 6bdb4b8f903e13815fc146e6f3260e5bb04c1045", "pr_number": null, "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "tools/autograd/gen_python_functions.py"], "labels": []}, "71225ecc8c": {"title": "Revert D20006312: Revert D19975410: Update documentation on why _cudnn_init_dropout_state looks the way it is.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20006312\n\nOriginal commit changeset: 4d4cc8ae78ad\n\nfbshipit-source-id: 4bd4b9d1331dc97f5b83e0df491be5fd0a11214a", "pr_number": null, "files_changed": ["aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/native/cudnn/RNN.cpp"], "labels": []}, "1a25747342": {"title": "Check for consistent devices in at::where (#33432)", "body": "Summary:\nChangelog:\n- Add a check to ensure that all inputs to `where` lie on the same device\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33432\n\nTest Plan:\n- Added test_where_invalid_device\n\nFixes https://github.com/pytorch/pytorch/issues/33422\n\nDifferential Revision: D19981115\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 745896927edb53f61f3dd48ba9e1e6cd10d35434", "pr_number": "33432", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "test/test_torch.py"], "labels": ["merge-this-please", "merged", "module: operators", "open source", "triaged"]}, "36d724c963": {"title": "run peephole to do profile-based optimizations (#33337)", "body": "Summary:\nWe need to run a peephole before constant propagation in the profiling pipeline, so we fold `prim::shape` for inputs with complete tensor types.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33337\n\nDifferential Revision: D19905624\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 80fff067941556053847ddc7afe0fd1c7a89a3ba", "pr_number": "33337", "files_changed": ["torch/csrc/jit/profiling_graph_executor_impl.cpp"], "labels": ["jit", "merged"]}, "13e4ee7883": {"title": "Added tensor.is_complex(), is_complex and dtype.is_complex py binding, tensor printing, and dixed the scalar type returned for complex float (#33268)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33268\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19907698\n\nPulled By: anjali411\n\nfbshipit-source-id: c3ce2e99fc09da91a90a8fb94e5525a00bb23703", "pr_number": "33268", "files_changed": ["docs/source/tensors.rst", "docs/source/torch.rst", "test/run_test.py", "test/test_complex.py", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_tensor_str.py", "torch/_torch_docs.py", "torch/csrc/Dtype.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/utils/python_scalars.h"], "labels": ["merged"]}, "e5cf7afd0a": {"title": "torch.tensor can infer complex dtype now (#33361)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33361\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19943477\n\nPulled By: anjali411\n\nfbshipit-source-id: ff6d7d2a6fdb6c58390f33bdd8be2f3fa182518b", "pr_number": "33361", "files_changed": ["test/test_torch.py", "torch/csrc/utils/tensor_new.cpp"], "labels": ["merged"]}, "108fc78395": {"title": "[caffe2] fix invalid % escape in inline assembly strings (#33554)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33554\n\nNVCC/GCC accepts the existing syntax, but not Clang which requires a proper escape. Here `%laneid` is one of the many registers that CUDA's pseudo-asm provides [1]. And using the extra `%` doesn't change the semantics, as PTX expects `%laneid` value after it's processed by the asm tool.\n\n1. https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\n\nTest Plan:\n```lang=bash\nbuck build mode/opt -c fbcode.cuda_use_clang=true //fblearner/flow/projects/dper:workflow\nbuck build mode/opt //fblearner/flow/projects/dper:workflow\n\nReviewed By: bddppq\n\nDifferential Revision: D20003621\n\nfbshipit-source-id: 8e550e55a3455925e7bd92c6df3e504b5d38c2dc", "pr_number": "33554", "files_changed": ["aten/src/THC/THCAsmUtils.cuh", "caffe2/utils/GpuDefs.cuh"], "labels": ["fb-exported", "merged"]}, "5782758b54": {"title": "Add instructions and operators for new bytecode format of PyText model (#33555)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33555\n\nA quick fix for the PyText model (in internal production) on the new bytecode format.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20008266\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 1916bd0bf41093898713c567c7f6fa546b9ea440", "pr_number": "33555", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "torch/csrc/jit/instruction.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/register_mobile_ops.cpp"], "labels": ["jit", "merged"]}, "23846d5a38": {"title": "[caffe2] use Clang identification macro in various places (#33574)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33574\n\nSprinkle with Clang identification macro places that otherwise would cause build errors when Clang is used to drive the CUDA compilation.\n\nNote: `__clang__` is defined when either Clang is used as host compiler by NVCC or when Clang drives the compilation. `__CUDA__` is defined only for the latter case.\n\nTest Plan:\n```lang=bash\nbuck build mode/opt -c fbcode.cuda_use_clang=true //fblearner/flow/projects/dper:workflow\nbuck build mode/opt //fblearner/flow/projects/dper:workflow\n```\n\nReviewed By: BIT-silence\n\nDifferential Revision: D20007440\n\nfbshipit-source-id: 53caa70695b99461a3910d41dc71a9f6d0728a75", "pr_number": "33574", "files_changed": ["caffe2/utils/conversions.h", "caffe2/utils/fixed_divisor.h", "caffe2/utils/math/utils.h"], "labels": ["fb-exported", "merged"]}, "c882425c24": {"title": "Add 64-bit indexing support to THC index reductions (#33405)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/32863, (together with https://github.com/pytorch/pytorch/issues/33310 for the `TensorIterator` reductions)\n\nThis adds 64-bit indexed kernels for `THC_reduceDimIndex` and uses `THCTensor_canUse32BitIndexMath` to switch between the two at runtime.\n\nI have a test for this locally but haven't included it here because `max` is much slower than `argmax`. To the point where the test takes several minutes to call max on just one `2**32` element tensor. That seems excessive, even for a slow test but I can push it if preferred.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33405\n\nDifferential Revision: D20010769\n\nPulled By: ezyang\n\nfbshipit-source-id: a8a86f662598d5fade4d90448436418422c699a3", "pr_number": "33405", "files_changed": ["aten/src/THC/THCTensorMathReduce.cuh", "test/test_cuda.py"], "labels": ["merged", "open source"]}, "faa800eb5b": {"title": "[JIT] remove inline everything jitter skip (#33468)", "body": "Summary:\nThe `not inline_everything` check was causing the jitter check to be skipped whenever we emitted a function. thanks SplitInfinity for pointing this out.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33468\n\nDifferential Revision: D19975934\n\nPulled By: eellison\n\nfbshipit-source-id: 03faf8d2fd93f148100d8cf49cb67b8e15cf1f04", "pr_number": "33468", "files_changed": ["test/test_jit.py", "torch/testing/_internal/jit_utils.py"], "labels": ["merged"]}, "3498c000e2": {"title": "[TVM] Remove dynamic batch size dispatching (#33584)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33584\n\n- Remove dynamic batch size dispatching\n- Set caffe2_tvm_min_ops to 8\n- Set caffe2_tvm_profiling_based_jit to false\n- Rename some variable names\n\nTest Plan: buck test caffe2/caffe2/fb/tvm:test_tvm_transform\n\nReviewed By: yinghai\n\nDifferential Revision: D19850620\n\nfbshipit-source-id: 2ec9bbd9fa72f953e79f3e27609ad00d4e135710", "pr_number": "33584", "files_changed": ["caffe2/opt/tvm_transformer.cc"], "labels": ["fb-exported", "merged"]}, "0bde610c14": {"title": "Re-sync with internal repository (#33591)", "body": "", "pr_number": null, "files_changed": ["tools/build_variables.bzl"], "labels": []}, "9266bde970": {"title": "[pytorch] Minor: add GIL assert to PythonRpcHandler::handleExceptionGILHeld (#33557)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33557\n\nWe should add GIL asserts in some places to keep assumptions documented.\nThis just adds one in an exception codepath as a placeholder for more.\n\nThis change also moves a #define from a .h to the .cpp to reduce scope.\nghstack-source-id: 98673532\n\nTest Plan: buck test mode/dev-nosan caffe2/test/...\n\nDifferential Revision: D20005387\n\nfbshipit-source-id: b7eff54a6f1dd69d199f8ca05cdb3001c50b37c4", "pr_number": "33557", "files_changed": ["torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h"], "labels": ["merged"]}, "d3d975cbf6": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/a16cb11a77ff018c1a98ff780b33fdc7493156e8\nhttps://github.com/facebook/fbthrift/commit/d92f4e3e1e610cac714916e210c96c9b053a84ca\nhttps://github.com/facebook/fbzmq/commit/d021412065f88551395d4e155df369d6b3264676\nhttps://github.com/facebook/mcrouter/commit/a7c056b5b4da404fabc2aa326da7a01c8fa4e2f1\nhttps://github.com/facebook/proxygen/commit/ac6d53d1c920934a473ac74b34d6363533591051\nhttps://github.com/facebook/rocksdb/commit/d75ce0a8ae6690aa64ca5fade3d9543348c9d5ef\nhttps://github.com/facebook/wangle/commit/622abbcbb3d4a600dacc6bd4f7c4705209534b82\nhttps://github.com/facebookincubator/fizz/commit/e1f7368d51db4f6a1dbec611408af51c0ca1c754\nhttps://github.com/facebookincubator/katran/commit/dc2e654b756d0bfc18fd8b2414d32882e41fe4fa\nhttps://github.com/pytorch/fbgemm/commit/50c9e44631c3cf7714efd2b21bface74307a6f6f\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 452151a75a70f744cba309b2700f274275d476bd", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "ac9b40164d": {"title": "Use cheaper check in isTensorList (#33528)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33528\n\nTest Plan: Imported from OSS\n\nReviewed By: ajyu\n\nDifferential Revision: D19989166\n\nPulled By: zdevito\n\nfbshipit-source-id: b0c484e037ca48226ed4d9204a06982e0c627ff0", "pr_number": "33528", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["merged"]}, "009293ec5c": {"title": "[pytorch][size] remove unused SparseCPUType from mobile build (#33517)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33517\n\nI don't think any mobile model uses SparseCPU backend yet so we can skip\ngenerating dispatch code for this backend type.\n\nThis will help reduce mobile code size with dynamic dispatch turned on,\nroughly ~100K for uncompressed iOS: D19616007 +413K v.s. D19616016 +319K.\n\nIt probably doesn't affect much static dispatch build size as the unused\nstatic dispatch methods will be stripped by linker in the end.\nghstack-source-id: 98615810\n\nTest Plan: - CI & BuildSizeBot\n\nReviewed By: linbinyu\n\nDifferential Revision: D19978633\n\nfbshipit-source-id: 27bf6ada2ba98482084cf23724cf400b538b0a03", "pr_number": "33517", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/templates/TensorMethods.h"], "labels": ["merged"]}, "e8a03438cc": {"title": "Make TestCuda.test_memory_stats more robust (#33575)", "body": "Summary:\nIIUC Python does not guarantee when an object is garbage collected. So it is possible that, some other test running before `TestCuda.test_memory_stats` creates object which is only garbage collected during  `TestCuda.test_memory_stats`, causing mem stats to change and causing this test to fail. This kind of failure is very hard to debug (it took me and mcarilli and ptrblck quite a while to figure out what is happening), and it is the root cause of mcarilli's gradient scaling PR https://github.com/pytorch/pytorch/pull/26512 failing on Windows.\n\ncc: csarofeen\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33575\n\nDifferential Revision: D20009260\n\nPulled By: ngimel\n\nfbshipit-source-id: 62f2716aefac3aa6c7d1898aa8a78e6b8aa3075a", "pr_number": "33575", "files_changed": ["test/test_cuda.py"], "labels": ["merged", "open source"]}, "a943b0518b": {"title": "strict check for a device type in Fuser (#33025)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33025\n\nDifferential Revision: D19975873\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 57f160bec9e4285dda63611f12665264754aac32", "pr_number": "33025", "files_changed": ["torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/graph_executor_impl.h", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/graph_fuser.h", "torch/csrc/jit/passes/guard_elimination.cpp", "torch/csrc/jit/profiling_graph_executor_impl.cpp"], "labels": ["jit", "merged"]}, "4588f49f68": {"title": "Kill cudaDeviceAllocator in THCState (#33380)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33380\n\nDifferential Revision: D19973151\n\nPulled By: ezyang\n\nfbshipit-source-id: 41634c43b28ca723e39e761afd32e5015e122368", "pr_number": "33380", "files_changed": ["aten/src/ATen/cuda/CUDAContext.cpp", "aten/src/ATen/native/cuda/Reduce.cuh", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.hpp", "aten/src/THC/THCStorage.cpp", "aten/src/THC/generic/THCStorage.cpp"], "labels": ["merged", "module: cuda", "open source", "triaged"]}, "a6a72ac68f": {"title": "Fix all occurrences of C416. (#33429)", "body": "Summary:\nC416: Unnecessary (list/set) comprehension - rewrite using list/set().\n\nSee https://pypi.org/project/flake8-comprehensions/\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33429\n\nDifferential Revision: D19972858\n\nPulled By: ezyang\n\nfbshipit-source-id: faac042a94c59d737bd5ae983121a0a029346e23", "pr_number": "33429", "files_changed": [".circleci/validate-docker-version.py", ".flake8", "aten/src/ATen/preprocess_declarations.py", "test/distributed/test_distributed.py", "test/jit/test_list_dict.py", "test/test_dataloader.py", "test/test_jit.py", "test/test_sparse.py", "torch/autograd/gradcheck.py", "torch/onnx/symbolic_opset9.py", "torch/optim/lr_scheduler.py", "torch/storage.py", "torch/testing/_internal/distributed/rpc/rpc_test.py", "torch/utils/hipify/hipify_python.py", "torch/utils/tensorboard/_caffe2_graph.py"], "labels": ["merged", "module: lint", "open source"]}, "e2a9ea0f72": {"title": "Ensure that lambda is no less than zero in softshrink (#33201)", "body": "Summary:\nSoftshrink is ill-defined when `lambda < 0`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33201\n\nDifferential Revision: D19899571\n\nPulled By: ezyang\n\nfbshipit-source-id: ac0dd8edea3435810a76a3a88152f83a024c7859", "pr_number": "33201", "files_changed": ["aten/src/ATen/native/Activation.cpp", "test/test_nn.py", "torch/nn/modules/activation.py"], "labels": ["merge-this-please", "merged", "open source"]}, "90f4c5695e": {"title": "Revert \"Revert D19975411: Remove special case codegen for tril_indices/triu_indices.\" (#33572)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33572\n\nThis reverts commit 687a7e4a2566861c53c8fb53a80b198465168b38.\n\nOriginal PR #33305\n\nReland with BC tests whitelisted. See https://github.com/pytorch/pytorch/issues/33580 for reasoning why this change is not actually BC breaking.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20011011\n\nPulled By: ezyang\n\nfbshipit-source-id: 116374efc93af12b8ad738a0989d6f0daa9569e2", "pr_number": "33572", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "test/backward_compatibility/check_backward_compatibility.py", "tools/autograd/gen_python_functions.py"], "labels": ["merged"]}, "6cec555926": {"title": "Replace AT_CHECK with TORCH_CHECK in torch/csrc/jit/pybind_utils.h (#33524)", "body": "Summary:\nThis is generating a considerable amount of warning, due to the fact\nthat the header file is included in multiple places.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33524\n\nDifferential Revision: D20006604\n\nPulled By: ezyang\n\nfbshipit-source-id: 0885cd2a708679ba5eeabb172366eb4c5a3bbef4", "pr_number": "33524", "files_changed": ["torch/csrc/jit/pybind_utils.h"], "labels": ["jit", "merge-this-please", "merged", "open source"]}, "fa80299bdf": {"title": "__torch_function__ overrides for torch.functional and torch.nn.functional (#32799)", "body": "Summary:\nThis adds `__torch_function__` support for all functions in `torch.functional` and `torch.nn.functional`.\n\nThe changes to C++ code and codegen scripts are to facilitate adding `__torch_function__` support for the native functions in `torch._C._nn`. Note that I moved the `handle_torch_function` C++ function to a header that both `python_torch_functions.cpp` and `python_nn_functions.cpp` include. The changes to `python_nn_functions.cpp` mirror the changes I made to `python_torch_functions.cpp` when `__torch_function__` support was first added in https://github.com/pytorch/pytorch/issues/27064. Due to the somewhat different way the `torch._C` and `torch._C._nn` namespaces are initialized I needed to create a new static reference to the `torch._C._nn` namespace (`THPNNVariableFunctions`). I'm not sure if that is the best way to do this. In principle I could import these namespaces in each kernel and avoid the global variable but that would have a runtime cost.\n\nI added `__torch_function__` support to the Python functions in `torch.nn.functional` following the approach in https://github.com/pytorch/pytorch/issues/32194.\n\nI re-enabled the test that checks if all functions in the `torch` namespace are explicitly tested for `__torch_function__` support. I also generalized the check to work for `torch.functional` and `torch.nn.functional` as well. This test was explicitly disabled in https://github.com/pytorch/pytorch/issues/30730 and I'm happy to disable it again if you think that's appropriate. I figured now was as good a time as any to try to re-enable it.\n\nFinally I adjusted the existing torch API tests to suppress deprecation warnings and add keyword arguments used by some of the code in `torch.nn.functional` that were missed when I originally added the tests in https://github.com/pytorch/pytorch/issues/27064.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32799\n\nDifferential Revision: D19956809\n\nPulled By: ezyang\n\nfbshipit-source-id: 40d34e0109cc4b9f3ef62f409d2d35a1d84e3d22", "pr_number": "32799", "files_changed": ["test/test_overrides.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_nn_functions.cpp", "tools/autograd/templates/python_torch_functions.cpp", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/functional.py", "torch/nn/functional.py"], "labels": ["merged", "open source"]}, "ee28831341": {"title": "[jit] Fix aug assign for non-tensor attributes (#32993)", "body": "Summary:\nInstead of erroring out this de-sugars augmented assignments to class\nmembers from `self.a += 1` to `self.a = self.a + 1`.\n\nFixes #32973\n](https://our.intern.facebook.com/intern/diff/19737636/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32993\n\nPulled By: driazati\n\nDifferential Revision: D19737636\n\nfbshipit-source-id: 07307cde88d8c348a7affdafe26db21c74e28ec0", "pr_number": "32993", "files_changed": ["test/test_jit.py", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/script/ir_emitter.cpp", "torch/csrc/jit/script/sugared_value.cpp"], "labels": ["jit", "merged"]}, "e77abb9a5b": {"title": "Normalize reward-to-go in C++ actor-critic (#33550)", "body": "Summary:\nComparing to the [Python implementation](https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py), it seems like the tensor of normalized reward-to-go is computed but never used. Even if it's just an integration test, this PR switches to the normalized version for better convergence.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33550\n\nDifferential Revision: D20024393\n\nPulled By: yf225\n\nfbshipit-source-id: ebcf0fee14ff39f65f6744278fb0cbf1fc92b919", "pr_number": "33550", "files_changed": ["test/cpp/api/integration.cpp"], "labels": ["merged", "module: cpp", "open source", "triaged"]}, "293fa5fc44": {"title": "[Documentation] Fix minor typo in torch.serialization (#33549)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33549\n\nDifferential Revision: D20002545\n\nPulled By: albanD\n\nfbshipit-source-id: 46fe2002329e5250c009eb066432909b71ecd74d", "pr_number": "33549", "files_changed": ["torch/serialization.py"], "labels": ["merged", "open source", "triaged"]}, "ca8e025cdf": {"title": "improve the doc of enforce_sorted in pack_padded_sequence (#33617)", "body": "Summary:\nthis is a follow up PR to https://github.com/pytorch/pytorch/issues/33602:\n\ntorch/nn/utils/rnn.html:\n\n`pack_padded_sequence` has a confusing and incomplete description of the `enforce_sorted` param. Currently it goes:\n\n```\n        enforce_sorted (bool, optional): if ``True``, the input is expected to\n            contain sequences sorted by length in a decreasing order. If\n            ``False``, this condition is not checked. Default: ``True``.\n```\n\nThe second part \"this condition is not checked\" (1) makes no sense since the alluded to condition is not described and (2) it's incomplete as it doesn't reflect the important part, that it actually does the sorting. I think it should say something like:\n\n```\n        enforce_sorted (bool, optional): if ``True``, the input is expected to\n            contain sequences sorted by length in a decreasing order. If\n            ``False``, the input will get sorted unconditionally. Default: ``True``.\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33617\n\nDifferential Revision: D20035131\n\nPulled By: albanD\n\nfbshipit-source-id: 654382eb0cb62b5abc78497faa5b4bca42db5fda", "pr_number": "33617", "files_changed": ["torch/nn/utils/rnn.py"], "labels": ["merged", "open source", "triaged"]}, "616beb1412": {"title": "[ROCm] Added support for pytorch extensions to use HIP (#32669)", "body": "Summary:\nThis pull request has changes for:\n1. Enabling a torch module with HIP code to be compiled by cpp_extensions.py\n2. Fixes for hipify module to be able to be used by a torch extension\n\ncc: ezyang iotamudelta jeffdaily\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32669\n\nDifferential Revision: D20033893\n\nPulled By: zou3519\n\nfbshipit-source-id: fd6ddc8cdcd3930f41008636bb2bc9dd26cdb008", "pr_number": "32669", "files_changed": ["test/cpp_extensions/setup.py", "test/run_test.py", "test/test_cpp_extensions_aot.py", "torch/utils/cpp_extension.py", "torch/utils/hipify/hipify_python.py"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "089d658153": {"title": "[TensorExpr] Add classes for memory management in tensor expressions. (#33216)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33216\n\nAll tensor expressions belong to a kernel arena and are freed when the\narena is destroyed. Until it is destroyed, all expressions stay valid.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19848382\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: a581ea2b635b9ba2cc53949616a13d8d3a47caae", "pr_number": "33216", "files_changed": ["caffe2/CMakeLists.txt", "tools/build_variables.bzl", "torch/csrc/jit/tensorexpr/mem_arena.cpp", "torch/csrc/jit/tensorexpr/mem_arena.h"], "labels": ["jit", "merged"]}, "1a4f997178": {"title": "[TensorExpr] Add a class for representing data type. (#33217)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33217\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19848380\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: d8683f8fc4555d2456cd2a7c827d8e8231915b49", "pr_number": "33217", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/tensorexpr/CMakeLists.txt", "test/cpp/tensorexpr/README.md", "test/cpp/tensorexpr/__init__.py", "test/cpp/tensorexpr/gtest.cpp", "test/cpp/tensorexpr/test_base.h", "test/cpp/tensorexpr/test_type.cpp", "test/cpp/tensorexpr/test_utils.h", "test/cpp/tensorexpr/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/tensorexpr/types.cpp", "torch/csrc/jit/tensorexpr/types.h"], "labels": ["jit", "merged"]}, "49af9425a7": {"title": "[TensorExpr] Add core classes for representing expressions and statements. (#33218)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33218\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19848378\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 48399f8651324d5ad0607e08573d5d7b2026bb23", "pr_number": "33218", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/test_type.cpp", "test/cpp/tensorexpr/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/tensorexpr/buffer.cpp", "torch/csrc/jit/tensorexpr/buffer.h", "torch/csrc/jit/tensorexpr/expr.cpp", "torch/csrc/jit/tensorexpr/expr.h", "torch/csrc/jit/tensorexpr/ir.cpp", "torch/csrc/jit/tensorexpr/ir.h", "torch/csrc/jit/tensorexpr/unique_name_manager.cpp", "torch/csrc/jit/tensorexpr/unique_name_manager.h"], "labels": ["jit", "merged"]}, "fc70fc3610": {"title": "[TensorExpr] Add IR visitor, IR mutator, and IR evaluator. (#33219)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33219\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19848381\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 44ca7cd99c25e290a8ffd8146785c19f9c785dfd", "pr_number": "33219", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/tensorexpr/codegen.cpp", "torch/csrc/jit/tensorexpr/codegen.h", "torch/csrc/jit/tensorexpr/eval.cpp", "torch/csrc/jit/tensorexpr/eval.h", "torch/csrc/jit/tensorexpr/expr.h", "torch/csrc/jit/tensorexpr/ir_mutator.cpp", "torch/csrc/jit/tensorexpr/ir_mutator.h", "torch/csrc/jit/tensorexpr/ir_visitor.cpp", "torch/csrc/jit/tensorexpr/ir_visitor.h"], "labels": ["jit", "merged"]}, "bb5181b716": {"title": "[TensorExpr] Add IR Printer. (#33220)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33220\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19848379\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 1c6ab4f63080d4506dedc3c47938de92fb4bfba2", "pr_number": "33220", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/test_ir_printer.cpp", "test/cpp/tensorexpr/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/tensorexpr/ir_printer.cpp", "torch/csrc/jit/tensorexpr/ir_printer.h"], "labels": ["jit", "merged"]}, "47e90d774e": {"title": "C++/Python API Parity: add pad_sequence (#32387)", "body": "Summary:\n- add `pad_sequence` and tests\n- related issue https://github.com/pytorch/pytorch/issues/25883\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32387\n\nDifferential Revision: D20025421\n\nPulled By: yf225\n\nfbshipit-source-id: caa9ae2114bece8db387a3a1610f24a3e06b1324", "pr_number": "32387", "files_changed": ["test/cpp/api/nn_utils.cpp", "torch/csrc/api/include/torch/nn/utils.h", "torch/csrc/api/include/torch/nn/utils/rnn.h"], "labels": ["merged", "module: cpp", "open source", "triaged"]}, "d5b768dffd": {"title": "refactor strongTypePtr (#33590)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33590\n\nghstack-source-id: 98713798\n\nTest Plan: unit test\n\nDifferential Revision: D20015521\n\nfbshipit-source-id: 8c744a6f30f12671bef89c3555110ce26609d9a3", "pr_number": "33590", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "torch/csrc/jit/import.cpp", "torch/csrc/jit/import.h", "torch/csrc/jit/pickle.cpp", "torch/csrc/jit/pickle.h", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/csrc/jit/unpickler.cpp", "torch/csrc/jit/unpickler.h"], "labels": ["jit", "merged"]}, "22963f42ec": {"title": "Delete unnecessary aliasAnalysis specification from operator registrations. (#33093)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33093\n\nIn #30187 the aliasAnalysis field on operator registration was updated\nso that alias analysis could be specified in only some registration call\nsites, rather than requiring it be consistently specified in all call\nsites.  With this change, we can eliminate the requirement that all\nregistrations specify aliasAnalysis; as long as we know *one* site\nspecifies the correct aliasAnalysis, we don't have to specify it\nany of the other sites.\n\nIn this patch, the \"one site\" is TypeDefault.cpp (previously we only\ngenerated these stub declarations for manually registered functions,\nbut now we generate the stubs for everything).  Then I delete aliasAnalysis\nanywhere we register an op for an existing function (which is a lot\nof places).\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19837897\n\nPulled By: ezyang\n\nfbshipit-source-id: 26a7fbc809ec1553da89ea5c0361f3e81526d4c2", "pr_number": "33093", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/TensorProperties.cpp", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp", "aten/src/ATen/test/extension_backend_test.cpp", "test/cpp_extensions/complex_registration_extension.cpp", "test/cpp_extensions/msnpu_extension.cpp", "test/mobile/op_deps/simple_ops.cpp", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["merged"]}, "a72946dbab": {"title": "Stop generating out full function type for registration, use decltype or infer it (#33097)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33097\n\nPreviously, we had to specify full types because the functions we registering\nmight be overloaded, and the type was necessary to resolve the ambiguity.  I\ndisambiguate all of these names by mangling the names of the methods we\nplace on CPUType/CUDAType/TypeDefault with the overload name (these are\n*internal* wrappers which are not user visible), and then can strip\nthe generation of full function types from the registration.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19837898\n\nPulled By: ezyang\n\nfbshipit-source-id: 5f557184f6ec84cb0613d4eb2e33b83fd1712090", "pr_number": "33097", "files_changed": ["aten/src/ATen/common_with_cwrap.py", "aten/src/ATen/function_wrapper.py", "tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["merged"]}, "f62f1b2ef0": {"title": "Revert \"Revert D19964089: [pytorch][PR] Allow vectorized gpu loop to \u2026 (#33553)", "body": "Summary:\n\u2026have different argument types\"\n\nThis reverts commit 05fb160048b71c1b8b00d2083a08618318158c1a.\n\nPlease go to https://github.com/pytorch/pytorch/pull/33558 and check the CUDA9 on CI\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33553\n\nDifferential Revision: D20017575\n\nPulled By: ngimel\n\nfbshipit-source-id: a5fd78eea00c7b0925ab21fd90a7daeb66725f1a", "pr_number": "33553", "files_changed": ["aten/src/ATen/native/cuda/CUDA9Workarounds.cuh", "aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/MemoryAccess.cuh", "aten/src/ATen/native/cuda/ROCmLoops.cuh", "aten/src/ATen/test/cuda_vectorized_test.cu", "c10/util/C++17.h"], "labels": ["merged", "open source"]}, "16d6c17845": {"title": "improve roll performance (#33623)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33544\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33623\n\nDifferential Revision: D20037643\n\nPulled By: ngimel\n\nfbshipit-source-id: 9fd293eca5242daf414c116344b2e1fde9f9ebc5", "pr_number": "33623", "files_changed": ["aten/src/ATen/native/TensorTransformations.cpp"], "labels": ["merged", "module: numpy"]}, "15ba902c08": {"title": "Turn ONNX_ML into a proper build option. (#33424)", "body": "Summary:\nThe detection of the env variable ONNX_ML has been properly handled in tools/setup_helpers/cmake.py,\nline 242.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33424\n\nDifferential Revision: D20043991\n\nPulled By: ezyang\n\nfbshipit-source-id: 91d1d49a5a12f719e67d9507cc203c8a40992f03", "pr_number": "33424", "files_changed": ["CMakeLists.txt", "cmake/Dependencies.cmake"], "labels": ["merged", "module: build", "module: onnx", "open source", "triaged"]}, "6474ea404d": {"title": "[C2] Native GPU implementation for bucketize (#33529)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33529\n\nCurrent version goes through GPU -> CPU -> GPU copy and is pretty slow: ~19 ms\nfor 1M elements with 20 possible buckets based on benchmark.\n\nThis new version is ~0.2 on the same\n\nTest Plan: benchmark + unit-test\n\nReviewed By: chocjy\n\nDifferential Revision: D19969518\n\nfbshipit-source-id: 51889bc9a232b6d45d9533e53b7b7f4531da481f", "pr_number": "33529", "files_changed": ["caffe2/operators/bucketize_op.cc", "caffe2/operators/bucketize_op.cu", "caffe2/operators/bucketize_op.h"], "labels": ["fb-exported", "merged"]}, "e10aa6b72f": {"title": "Fix flaky DagNetTest unittest", "body": "Summary: The first run of the net is noisy sometimes - just run it twice.\n\nReviewed By: cheshen1\n\nDifferential Revision: D20039274\n\nfbshipit-source-id: 639e65646bf52f3efe1ecd4bbcd0e413d9389b29", "pr_number": null, "files_changed": ["caffe2/core/parallel_net_test.cc"], "labels": []}, "d971007c29": {"title": "Migrate `random_` from the TH to Aten (CPU) (#32534)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32534\n\nFixes #24752\nFixes #32510\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19776613\n\nPulled By: pbelevich\n\nfbshipit-source-id: a8d262bccf5f2807f6125c83080aa16d77491b19", "pr_number": "32534", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/Dispatch.h", "aten/src/ATen/native/DistributionTemplates.h", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/DistributionTemplates.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/DistributionTemplates.h", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/cpu_rng_test.cpp", "aten/src/ATen/test/rng_test.cpp", "aten/src/ATen/test/rng_test.h", "aten/src/TH/generic/THTensorRandom.cpp", "aten/src/TH/generic/THTensorRandom.h", "test/test_nn.py", "test/test_torch.py", "tools/autograd/derivatives.yaml"], "labels": ["merged"]}, "9b2b15f4fc": {"title": "misc windows warning fixes (#33632)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33632\n\n* `inline_container.h` was unnecessarily exposing all includers to caffe2 headers via `caffe2/core/logging.h`\n* Add msvc version of hiding unused warnings.\n* Make sure clang on windows does not use msvc pragmas.\n* Don't redefine math macro.\n\nTest Plan: CI green\n\nDifferential Revision: D20017046\n\nfbshipit-source-id: 230a9743eb88aee08d0a4833680ec2f01b7ab1e9", "pr_number": "33632", "files_changed": ["aten/src/ATen/native/cpu/Activation.cpp", "c10/macros/Macros.h", "caffe2/serialize/inline_container.h"], "labels": ["fb-exported", "merged"]}, "a7e22b4c6a": {"title": "add bailout checks to checkScript (#32802)", "body": "Summary:\nthis adds enough infrastructure to run bailout checks in `checkScript`. I'll need to figure out the best way to enable it for nightly builds now.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32802\n\nDifferential Revision: D19974718\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 40485503f6d3ae14edcce98e1eec1f0559f3ad08", "pr_number": "32802", "files_changed": ["test/test_jit.py", "torch/testing/_internal/common_utils.py", "torch/testing/_internal/jit_utils.py"], "labels": ["merged"]}, "1c08fa7051": {"title": "[Caffe2] Skip caffe2/caffe2:caffe2_test_cpu - DBSeekTest.LMDB", "body": "Summary: skip broken tests in https://fburl.com/svc/zsbsrc7a to unblock dper fbpkg push.\n\nTest Plan: buck test //caffe2/caffe2:caffe2_test_cpu -- 'DBSeekTest\\.LMDB' --run-disabled\n\nReviewed By: cheshen1\n\nDifferential Revision: D20042330\n\nfbshipit-source-id: 5b86e66da2a219c915c471b8e87f33239bdc5ba9", "pr_number": null, "files_changed": ["caffe2/db/db_test.cc"], "labels": []}, "59daf1611b": {"title": "[Caffe2] Skip //caffe2/caffe2:caffe2_test_cpu -- 'DBSeekTest\\.RocksDB'", "body": "Summary: Skip the test to unblock dper fbpkg push\n\nTest Plan: buck test //caffe2/caffe2:caffe2_test_cpu -- 'DBSeekTest\\.RocksDB' --run-disabled\n\nReviewed By: cheshen1\n\nDifferential Revision: D20043418\n\nfbshipit-source-id: 05ceb2cea08722a671fa211d73680fd4b78f354c", "pr_number": null, "files_changed": ["caffe2/db/db_test.cc"], "labels": []}, "8291e06f8f": {"title": "Fixes cuda->numpy and non-strided->numpy segfaults (#33612)", "body": "Summary:\nAddresses https://github.com/pytorch/pytorch/issues/33300.\n\nCalling .numpy() on a CUDA or non-strided (e.g. sparse) tensor segfaults in current PyTorch. This fixes the segfaults and throws the appropriate TypeError, as was intended.\n\nTwo tests, one in test_cuda.py and the other in test_sparse.py, are added to verify the behavior.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33612\n\nDifferential Revision: D20038210\n\nPulled By: mruberry\n\nfbshipit-source-id: 265531dacd37c392232fd3ec763489a62ef54795", "pr_number": "33612", "files_changed": ["test/test_cuda.py", "test/test_sparse.py", "torch/csrc/utils/tensor_numpy.cpp"], "labels": ["merged", "module: numpy"]}, "a2f3c6c26f": {"title": "Call RandomNumberSeed() on-demand (#33539)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33539\n\nWe rarely use the `random_seed_` in context but we always initialize it with `RandomNumberSeed()` which isn't trivial. This diff makes it that we only call `RandomNumberSeed()` once when we want to use `random_seed_`.\n\nTest Plan:\nunittests.\n\nCanaries:\nAF: https://our.intern.facebook.com/intern/ads/canary/424753437441438410\nAI: https://our.intern.facebook.com/intern/ads/canary/424753467414318838\nProspector: https://our.intern.facebook.com/intern/ads/canary/424753976999968569\n\nReviewed By: ipiszy\n\nDifferential Revision: D19993190\n\nfbshipit-source-id: 1d2606bd65476ff3b519c69f9cbfa3b80f75cdff", "pr_number": "33539", "files_changed": ["caffe2/core/context.h"], "labels": ["fb-exported", "merged"]}, "312627a7c3": {"title": "Revert D19776613: Migrate `random_` from the TH to Aten (CPU)", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19776613\n\nOriginal commit changeset: a8d262bccf5f\n\nfbshipit-source-id: 36389ffa3d8377743f55f97221d7a7ee25a409f6", "pr_number": null, "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/Dispatch.h", "aten/src/ATen/native/DistributionTemplates.h", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/DistributionTemplates.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/DistributionTemplates.h", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/cpu_rng_test.cpp", "aten/src/ATen/test/rng_test.cpp", "aten/src/ATen/test/rng_test.h", "aten/src/TH/generic/THTensorRandom.cpp", "aten/src/TH/generic/THTensorRandom.h", "test/test_nn.py", "test/test_torch.py", "tools/autograd/derivatives.yaml"], "labels": []}, "9e384f9ce4": {"title": "Remove duplicate header include. (#33656)", "body": "Summary:\nThe same header `<torch/nn/functional/conv.h>` is included twice.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33656\n\nDifferential Revision: D20056913\n\nPulled By: yf225\n\nfbshipit-source-id: b1563035c9821731b99c26eec130ff0b9cc627a7", "pr_number": "33656", "files_changed": ["torch/csrc/api/src/nn/modules/conv.cpp"], "labels": ["merge-this-please", "merged", "open source"]}, "6d448acb34": {"title": "[PyTorch BC] Skip aten::random_ to fix BC CI (#33666)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33666\n\nit's caused by a revert. So let's skip it.\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D20057382\n\nfbshipit-source-id: d71af8efe68b31befcef5dddc372540e8a8ae2ac", "pr_number": "33666", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["fb-exported", "merged"]}, "7aa605ed92": {"title": "Remove uses of `.data` in test_torch (#33638)", "body": "Summary:\nRemoves almost every usage of `.data` in test_torch to address part of https://github.com/pytorch/pytorch/issues/33629.\n\nLines 4706-4710 had to be refactored to allow this. The changed test is fundamentally the same, as it appears to be meant to confirm that using an input of a different type than the weight causes an appropriate error.\n\nThere is one remaining usage of `.data`, and it is on line 5132. This was left as the `set_` and `resize_` methods still mention `.data` explicitly. I figure the right time to remove this is when those methods have their runtime errors updated.\n\nNote: ~~some tests are skipped locally, and so I am still verifying that nothing has been obviously broken.~~ Appears to be passing early tests.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33638\n\nDifferential Revision: D20062288\n\nPulled By: albanD\n\nfbshipit-source-id: 672a6d7a20007baedb114a20bf1ddcf6c4c0a16a", "pr_number": "33638", "files_changed": ["test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "941b42428a": {"title": "Mobile Backend: NHWC memory layout + XNNPACK integration. (#32509)", "body": "Summary:\nIn order to improve CPU performance on floating-point models on mobile, this PR introduces a new CPU backend for mobile that implements the most common mobile operators with NHWC memory layout support through integration with XNNPACK.\n\nXNNPACK itself, and this codepath, are currently only included in the build, but the actual integration is gated with USE_XNNPACK preprocessor guards.  This preprocessor symbol is intentionally not passed on to the compiler, so as to enable this rollout in multiple stages in follow up PRs.  This changeset will build XNNPACK as part of the build if the identically named USE_XNNPACK CMAKE variable, defaulted to ON, is enabled, but will not actually expose or enable this code path in any other way.\n\nFurthermore, it is worth pointing out that in order to efficiently map models to these operators, some front-end method of exposing this backend to the user is needed.  The less efficient implementation would be to hook these operators into their corresponding **native** implementations, granted that a series of XNNPACK-specific conditions are met, much like how NNPACK is integrated with PyTorch today for instance.\n\nHaving said that, while the above implementation is still expected to outperform NNPACK based on the benchmarks I ran, the above integration would be leave a considerable gap between the performance achieved and the maximum performance potential XNNPACK enables, as it does not provide a way to compute and factor out one-time operations out of the inner most forward() loop.\n\nThe more optimal solution, and one we will  decide on soon, would involve either providing a JIT pass that maps nn operators onto these newly introduced operators, while allowing one-time calculations to be factored out, much like quantized mobile models.  Alternatively, new eager-mode modules can also be introduced that would directly call into these implementations either through c10 or some other mechanism, also allowing for decoupling of op creation from op execution.\n\nThis PR does not include any of the front end changes  mentioned above.  Neither does it include the mobile threadpool unification present in the original https://github.com/pytorch/pytorch/issues/30644.  Furthermore, this codepath seems to be faster than NNPACK in a good number of use cases, which can potentially allow us to remove NNPACK from aten to make the codebase a little simpler, granted that there is widespread support for such a move.\n\nRegardless, these changes will be introduced gradually and in a more controlled way in subsequent PRs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32509\n\nReviewed By: dreiss\n\nDifferential Revision: D19521853\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 99a1fab31d0ece64961df074003bb852c36acaaa", "pr_number": "32509", "files_changed": [".circleci/scripts/binary_ios_upload.sh", ".gitmodules", "CMakeLists.txt", "android/pytorch_android/CMakeLists.txt", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/utils/Allocator.h", "aten/src/ATen/native/xnnpack/Common.h", "aten/src/ATen/native/xnnpack/Convolution.cpp", "aten/src/ATen/native/xnnpack/Factory.cpp", "aten/src/ATen/native/xnnpack/Factory.h", "aten/src/ATen/native/xnnpack/Init.cpp", "aten/src/ATen/native/xnnpack/Linear.cpp", "aten/src/ATen/native/xnnpack/Shim.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "cmake/Dependencies.cmake", "cmake/TorchConfig.cmake.in", "scripts/xcode_build.rb", "third_party/XNNPACK", "third_party/cpuinfo", "third_party/psimd", "third_party/pthreadpool"], "labels": ["merged"]}, "e1bddbbaf6": {"title": "Bounds checking for functor execution in vectorized/unrolled kernels (#33642)", "body": "Summary:\nThe current logic for vectorized/unrolled operations in CUDALoops.cuh applies bounds checking to loads and stores, [but not to the actual functor's execution](https://github.com/pytorch/pytorch/blob/16d6c17845426294274850f9161e292345f2afa5/aten/src/ATen/native/cuda/CUDALoops.cuh#L264).  In other words, for a block acting on the tail of a tensor that doesn't require the whole block to participate in memory transactions, many threads execute their functor on uninitialized data.  For functors that only communicate with the outside world via the bounds-checked loads and stores, that's ok.  The threads acting on garbage data never actually write their results.  But [my proposed inf/nan checking kernel](https://github.com/pytorch/pytorch/pull/33366/files#diff-9701a2b34900195d160bdc234e001b79R70-R79) has the additional side effect of writing to a `found_inf` flag in global memory.  For irregularly-shaped tensors where tail threads execute the functor on garbage data, these threads would sometimes see and report spurious infs/nans.\n\nIn general, we can't guarantee functors won't have side effects.  For safety (and efficiency) we should apply bounds checking to the functor execution as well as the loads and stores.\n\nIs it possible that other elementwise kernels (in addition to the strided/vectorized implementation) are also executing functors unconditionally?  That would cause similar failures.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33642\n\nDifferential Revision: D20062985\n\nPulled By: ngimel\n\nfbshipit-source-id: 65b8d75a001ce57865ed1c0cf89105d33f3f4dd4", "pr_number": "33642", "files_changed": ["aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/MemoryAccess.cuh"], "labels": ["merged", "open source"]}, "5fa03d4dbb": {"title": "Fix bug where we were trying to get a schema for prim::Constant, which is not registered as an operator. (#33645)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33645\n\nFix bug where we were trying to get a schema for prim::Constant, which is not registered as an operator.\nghstack-source-id: 98785729\n\nTest Plan: buck test mode/dev //pytext/models/test:scripted_seq2seq_generator_test -- 'test_generator \\(pytext\\.models\\.test\\.scripted_seq2seq_generator_test\\.ScriptedSeq2SeqGeneratorTest\\)'\n\nDifferential Revision: D20050833\n\nfbshipit-source-id: cc38510b0135b750fdf57fb9c1e66ce1d91ee128", "pr_number": "33645", "files_changed": ["torch/csrc/jit/autodiff.cpp"], "labels": ["jit", "merged"]}, "9d834cc889": {"title": "[JIT] Fix FunctionType::python_str() (#33680)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33680\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20062777\n\nPulled By: jamesr66a\n\nfbshipit-source-id: fcdb0527ca6776ff161cd535794e9c12bb32bdde", "pr_number": "33680", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["merged"]}, "039dc90854": {"title": "Revert D19521853: [pytorch][PR] Mobile Backend: NHWC memory layout + XNNPACK integration.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19521853\n\nOriginal commit changeset: 99a1fab31d0e\n\nfbshipit-source-id: 76dfc1f481797ba2386997533cf19957637687d6", "pr_number": null, "files_changed": [".circleci/scripts/binary_ios_upload.sh", ".gitmodules", "CMakeLists.txt", "android/pytorch_android/CMakeLists.txt", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/utils/Allocator.h", "aten/src/ATen/native/xnnpack/Common.h", "aten/src/ATen/native/xnnpack/Convolution.cpp", "aten/src/ATen/native/xnnpack/Factory.cpp", "aten/src/ATen/native/xnnpack/Factory.h", "aten/src/ATen/native/xnnpack/Init.cpp", "aten/src/ATen/native/xnnpack/Linear.cpp", "aten/src/ATen/native/xnnpack/Shim.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "cmake/Dependencies.cmake", "cmake/TorchConfig.cmake.in", "scripts/xcode_build.rb", "third_party/XNNPACK", "third_party/cpuinfo", "third_party/psimd", "third_party/pthreadpool"], "labels": []}, "641750e33c": {"title": "Fix NaN handling in torch.mv. (#31666)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31666\n\nList of changes:\n1) Fix a case where torch.mv was not handling NaNs correctly.  In particular, with a transposed tensor and expanded vector, NaNs in the output are kept, even if beta = 0.\nThis is handled in the `out=` case by zero-ing out the passed-in Tensor, but this can happen just the same with the non-out variant if the allocated tensor happens to have a NaN.\nAlso adds tests for this case.\nNOTE: we zero out the output tensor in all cases for mv and mm, even though this is probably overkill.  I didn't find another case where this would be a problem, but the old code at least\nattempted to do this for all mv and mm calls and I didn't add comprehensive testing to be sure that it's not a problem.\n\n2) on CPU: move mv, mv_out, mm, mm_out to be direct wrappers on _th_addmv, _th_addmm, rather than having their own wrappers in Declarations.cwrap.\nThs is to remove the magic around cpu_zero from the codegen, which simplifies the codegen and makes testing this easier.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19239953\n\nPulled By: gchanan\n\nfbshipit-source-id: 27d0748d215ad46d17a8684696d88f4cfd8a917e", "pr_number": "31666", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/BlasWrappersCPU.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THBlas.cpp", "test/test_torch.py"], "labels": ["merged"]}, "e3ba533c8b": {"title": "Minimize the cases where we have to cpu_zero. (#33570)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33570\n\nIn this PR, we are a bit more careful about avoiding zero-ing the output.  Analysis as follows:\n1) `mm` doesn't need zero_ because it never calls scal, which is the underlying problem.\n2) for `mv`, which does call scal (in certain cases), we can just move the zeroing to where it would actually be a problem, namely when the scalar value is 0.\nIn this case we just run the non-BLAS version of the code.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20007665\n\nPulled By: gchanan\n\nfbshipit-source-id: 1f3a56954501aa9b2940d2f4b35095b2f60089a8", "pr_number": "33570", "files_changed": ["aten/src/ATen/native/BlasWrappersCPU.cpp", "aten/src/TH/generic/THBlas.cpp"], "labels": ["merged"]}, "6a275b696e": {"title": "adding IterableDataset to utils.data.__init__ (#33543)", "body": "Summary:\nthis shall fix issue https://github.com/pytorch/pytorch/issues/27820 again\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33543\n\nDifferential Revision: D20002446\n\nPulled By: vincentqb\n\nfbshipit-source-id: 7563a56fd6238efe8ea5626b02ba5e8fcda0780e", "pr_number": "33543", "files_changed": ["torch/utils/data/__init__.pyi"], "labels": ["merged", "open source"]}, "6a76433b9d": {"title": "[Update independent.py]add explicit string representation (#33676)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33676\n\nDifferential Revision: D20069202\n\nPulled By: ngimel\n\nfbshipit-source-id: 48b609d4fb7a098e9e3383553103a9441673d63f", "pr_number": "33676", "files_changed": ["torch/distributions/independent.py"], "labels": ["merged", "open source", "triaged"]}, "481e7f2e78": {"title": "catch and propagate warnings for JIT ScriptMethods (#33010)", "body": "Summary:\nWe align it with ScriptFunctions by using the HANDLE_TH_ERRORS/END_HANDLE_TH_ERRORS_PYBIND macros.\n\nFixes https://github.com/pytorch/pytorch/issues/24155  or https://github.com/pytorch/pytorch/issues/24828 ?\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33010\n\nDifferential Revision: D20053585\n\nPulled By: suo\n\nfbshipit-source-id: c8876b54069285ba9638bb2328fd8738b59c396d", "pr_number": "33010", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/init.cpp"], "labels": ["jit", "merged", "open source", "triaged"]}, "4d9b649261": {"title": "jit pickling rref (#32959)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32959\n\nin rpc torch script call path, we need to pickle/unpickle rref, this diff is added to make jit pickler/unpickler be able to pickle/unpickle rref. It is similar to what is implemented for PyRef::pickle() and PyRef::unpickle().\nThe pickling/unpickling design assumes it is always coupled with RPC calls. It is not needed to checkpoint a model with rref, before checkpointing the model, user should call ref.to_here() to get value inside rref.\n\nThe pickling process is:\n1. push torch.distributed.rpc.rref global string\n1. call rref.fork() and create rrefForkData, which is a few IDs and type str of the value held inside the rref, the IDs includes rref id, fork id, caller work id, callee work id, owner work id\n2. push the rrefForkData\n\nThe unpickling process is:\n1. read torch.distributed.rpc.rref global string, and retrieve the cached global lamda function\n2. the globa lamda function will get rrefForkData\n3. if callee is also owner work id, then get owner rref based on Ids inside rrefFork data and return the ownerRRef\n4. if callee is not owner work id, then create user rref using the rrefForkData and return the userRRef\n5. meanwhile owner rref will be notified and do reference counting correctly\n\nDuring unpickling, a type_resolver is needed to parse type str. This type_resolver has python dependency, so we get it from rpc_agent, and pass it to unpickler during construction. So we added a type_resolver argumenmt to jit unpickler constructor in this diff.\nghstack-source-id: 98814793\n\nTest Plan: unit test\n\nDifferential Revision: D19713293\n\nfbshipit-source-id: 4fd776cdd4ce8f457c4034d79acdfb4cd095c52e", "pr_number": "32959", "files_changed": ["caffe2/CMakeLists.txt", "torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_req.cpp", "torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/message.h", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/python_remote_call.cpp", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/distributed/rpc/rref_proto.cpp", "torch/csrc/distributed/rpc/script_call.cpp", "torch/csrc/distributed/rpc/script_remote_call.cpp", "torch/csrc/distributed/rpc/script_resp.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/jit/pickle.h", "torch/csrc/jit/pickler.cpp", "torch/csrc/jit/pickler.h", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/unpickler.cpp", "torch/csrc/jit/unpickler.h", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["jit", "merged"]}, "32c93099c4": {"title": "Add typing info for data members of utils.data.sampler classes (#33679)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33490\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33679\n\nDifferential Revision: D20063099\n\nPulled By: ngimel\n\nfbshipit-source-id: 1bbf71a65408d117019ab38d7d095cfd337f5d1e", "pr_number": "33679", "files_changed": ["torch/utils/data/sampler.pyi"], "labels": ["merged", "open source"]}, "ced8865d91": {"title": "Add sigmoid to mobile ops", "body": "Summary: Used by segmentation model.\n\nTest Plan: Ran segmentation model on mobile.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D19881378\n\nfbshipit-source-id: 87f00058050fd173fbff1e88987ce09007622b83", "pr_number": null, "files_changed": ["torch/csrc/jit/mobile/register_mobile_ops.cpp"], "labels": []}, "062ac6b472": {"title": "Bring up new-style registration API as wrapper around old-style (#33205)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33205\n\nA number of important use-cases are implemented:\n\n- def(schema): defines a schema, with no implementation (alias\n  inferred from schema, by default)\n- def(schema, fn_ptr): registers fn_ptr as a catch-all kernel\n  for the operation\n- def(schema, lambda): registers lambda as a catch-all kernel\n  for the operation\n- def(schema, torch::dispatch(dispatch_key, fn)), and\n  def(schema, torch::dispatch(device_type, fn)): registers\n  the function to only be executed when dispatch_key/device_type\n  is selected for use\n- def(schema, TORCH_OPTIMIZED_FN(fn)): registers the function\n  as unboxed only, using the inline syntax\n\nAll of our code generated registrations in ATen are switched to\nthe new API.\n\nSome aspects of the API which are not fully implemented:\n\n- It's still not valid to omit the schema when registering a function\n  pointer, due to #32549\n- Although it's possible to take advantage of top-level namespaces\n  ala torch::import(\"aten\"), we don't use it because this results\n  in worse code (as we have to cat everything back together).  This\n  is not an essential problem, we just need the internals to be less\n  stupid.\n\nThere are some aspects of the API which don't semantically make sense,\nbut I chose not to fix them in this PR:\n\n- For some reason, TORCH_OPTIMIZED_FN uses the *runtime* wrapper to\n  do wrapping, rather than the compile time one which inlines the\n  function in.  This means that there isn't any reason we should be\n  passing in the function pointer as a template argument; a regular\n  old argument ought to have worked fine.  This is seemingly\n  consistent with the current API though; needs further investigation.\n- There's no reason to optional<DispatchKey>, DispatchKey would\n  work just fine (use DispatchKey::Undefined for the nullopt case)\n\nIn the long term, we should swap the wrapper around: the new-style\nAPI has the real implementation, and the old-style API is backwards\ncompatibility.  However, this implies a lot of internal refactoring,\nso I decided to short circuit around it to get this in faster\n\nAncillary changes:\n- I stopped moving optional<DispatchKey>, it's literally just two\n  words, pass it by value please.\n- Needed to add a & qualified version of RegisterOps::op, since\n  I'm storing RegisterOps as a member inside the new style\n  Namespace and I cannot conveniently get a rvalue reference\n  to it in that situation.  (BTW, register_ = std::move(register_)\n  really doesn't work, don't try it!)\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19856626\n\nPulled By: ezyang\n\nfbshipit-source-id: 104de24b33fdfdde9447c104853479b305cbca9a", "pr_number": "33205", "files_changed": ["aten/src/ATen/core/op_registration/op_registration.cpp", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/templates/SparseTypeDerived.cpp", "aten/src/ATen/templates/TypeDefault.cpp", "aten/src/ATen/templates/TypeDerived.cpp"], "labels": ["merged"]}, "533b973fd0": {"title": "Fix visibility of torch::nn::RNNImpl::options (#33718)", "body": "Summary:\nIn PR https://github.com/pytorch/pytorch/issues/33027, `options` in RNNImpl was mistakenly changed to `protected` (it was `public` before)\n\n```\n protected:\n  FORWARD_HAS_DEFAULT_ARGS({1, AnyValue(Tensor())})\n\n  RNNOptions options;\n```\n\nThis PR changes it back to `public` again.\n\nFixes https://github.com/pytorch/pytorch/issues/33694.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33718\n\nDifferential Revision: D20075149\n\nPulled By: yf225\n\nfbshipit-source-id: 82901369eeaacd82df849e17df64dc1aaf98f9fe", "pr_number": "33718", "files_changed": ["torch/csrc/api/include/torch/nn/modules/rnn.h"], "labels": ["merged", "module: cpp"]}, "dc3d47110a": {"title": "[docs] add experimental warning to TorchScript classes in language reference (#33697)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33697\n\nreference\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20070220\n\nPulled By: suo\n\nfbshipit-source-id: 9828d876afed59203cc472eaf0134d52d399069e", "pr_number": "33697", "files_changed": ["docs/source/jit_language_reference.rst"], "labels": ["merged"]}, "996c0adb53": {"title": "[quant] Regsiter fake_quant and observer attributes as buffers (#33626)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33626\n\nFor DDP we require the attributes to be registered as buffers. By doing this the value is broadcast from one device to the rest.\n\nTest Plan:\nTested on actual model on GPU\n\nImported from OSS\n\nDifferential Revision: D20038839\n\nfbshipit-source-id: 82e829fc3baca0b3262c3894a283c375eb08a4a4", "pr_number": "33626", "files_changed": ["torch/quantization/fake_quantize.py", "torch/quantization/observer.py"], "labels": ["merged"]}, "330b69fef8": {"title": "Kill dead scalar_check. (#33695)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33695\n\nI'm not sure how this stuck around, but it has no effect.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20068867\n\nPulled By: gchanan\n\nfbshipit-source-id: 79191338a8bc7a195e2b7265005ca6f00aab3818", "pr_number": "33695", "files_changed": ["aten/src/ATen/Declarations.cwrap"], "labels": ["merged"]}, "5090d7082b": {"title": "add propagate flag USE_DISTRIBUTED for libtorch_python_source", "body": "Reviewed By: pritamdamania87\n\nDifferential Revision: D20070789\n\nfbshipit-source-id: fdb8a2eefb5bfc1ae1d80e29bd15eb1d70920c87", "pr_number": null, "files_changed": ["tools/build_variables.bzl"], "labels": []}, "696527e659": {"title": "[caffe2] Add embedding empty ratio checker (disabled by default) (#33145)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33145\n\nReviewed By: xianjiec\n\nDifferential Revision: D19716574\n\nfbshipit-source-id: 42a636600ac3977910d35093916865790bbe5b10", "pr_number": "33145", "files_changed": ["caffe2/operators/gather_ranges_to_dense_op.cc", "caffe2/operators/gather_ranges_to_dense_op.h", "caffe2/python/operator_test/gather_ranges_op_test.py"], "labels": ["fb-exported", "merged"]}, "5b031d961d": {"title": "[pt][quant] RNN debug test (#33621)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33621\n\nghstack-source-id: 98746093\n\nTest Plan: buck test mode/dev caffe2/test:quantization -- 'test_quantized_rnn \\(test_quantization\\.PostTrainingDynamicQuantTest\\)'  --print-passing-details\n\nDifferential Revision: D20036968\n\nfbshipit-source-id: 7cbb027a6afbe28bc250fc663089c6a9406e880b", "pr_number": "33621", "files_changed": ["test/test_quantization.py"], "labels": ["merged"]}, "54e41a87eb": {"title": "Make ELU great again (#33244)", "body": "Summary:\nDue to compiler bug, we have to make some workaround on ELU for CUDA. A necessary condition for this bug to happen is `invoke_with_array` in `Loops.cuh`. Now, https://github.com/pytorch/pytorch/issues/33222 will kill that function, and we need to remove that workaround once https://github.com/pytorch/pytorch/issues/33222 is landed.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33244\n\nDifferential Revision: D20076197\n\nPulled By: ngimel\n\nfbshipit-source-id: 39f99783014c78cecad1c39cb46092278ff220b9", "pr_number": "33244", "files_changed": ["aten/src/ATen/native/cuda/Activation.cu"], "labels": ["merged", "open source", "triaged"]}, "479e474a37": {"title": "[quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes (#32379)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32379\n\nFolding Conv2d - BatchNorm2d modules means recalculate the weight and bias of Conv2d module by incorproating the parameters\nof BatchNorm2d, and also change the method calls to calling only forward of Conv2d module, this involves change of both module\ntypes and graph because the bias of Conv2d is a parameter when it has value and is an attribute when it is\nNone(since JIT code has assumption of prameter being Tensor in multiple places), therefore\nwe'll need to remove the bias attribute when it is None and add a bias attribute later. Since ClassType might be shared, we separate\nremove and add in separate steps and also keep track of the processed graph to avoid modifying the graph and type multiple times.\nHowever we'll have to record the slot index of bias as well so we can replay the slot removal on other instances of Conv2d module.\n\nTest Plan:\ntbd\n\nImported from OSS\n\nDifferential Revision: D20078719\n\nfbshipit-source-id: cee5cf3764f3e0c0a4a2a167b78dbada2e3835cc", "pr_number": "32379", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "97da60d511": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/ea8bae1f0f2a57618e8316bcdb2ecc4d34d9f538\nhttps://github.com/facebook/folly/commit/134472ee45780ca2afa6f64cb7baac318c60a7c3\nhttps://github.com/facebook/proxygen/commit/37e6cf9d62637be4936bfcebab188ccdf374e0fc\nhttps://github.com/facebook/rocksdb/commit/eb367d45c0d96969f66aff0a16bee201f52beb1a\nhttps://github.com/facebookincubator/mvfst/commit/76de6e15c0b6c0bcad664d59e40d113f86ccc0ea\nhttps://github.com/pytorch/fbgemm/commit/e1b1a55309701f0f6d70afd7ad659d6f22c3e1ab\n\nTest Plan: n/a\n\nReviewed By: wittgenst\n\nfbshipit-source-id: 9d0d688d81be822900475223a787c5649e143e85", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "b10a39bb32": {"title": "Migrate _cat from TH to ATen (CUDA) (#33237)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/24520\n\nBenchmarks:\n\nUpstream:\n\n```\n$ python -m pt.cat_test --tag_filter all --device cuda  --omp_num_threads 1 --mkl_num_threads 1\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : all\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1,1,1)_N2_dim0_cuda\n# Input: sizes: (1, 1, 1), N: 2, dim: 0, device: cuda\nForward Execution Time (us) : 17.355\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(512,512,2)_N2_dim1_cuda\n# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 30.718\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(128,1024,2)_N2_dim1_cuda\n# Input: sizes: (128, 1024, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 17.329\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(512,512,2)_N2_dim1_cuda\n# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 30.176\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1024,1024,2)_N2_dim0_cuda\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 0, device: cuda\nForward Execution Time (us) : 74.417\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1025,1023,2)_N2_dim1_cuda\n# Input: sizes: (1025, 1023, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 75.728\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1024,1024,2)_N2_dim2_cuda\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 2, device: cuda\nForward Execution Time (us) : 190.165\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7fa8876fcf28>,111,65]_N5_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7fa8876fcf28>, 111, 65], N: 5, dim: 0, device: cuda\nForward Execution Time (us) : 57.711\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[96,<function<lambda>at0x7fa886237048>,64]_N5_dim1_cuda\n# Input: sizes: [96, <function <lambda> at 0x7fa886237048>, 64], N: 5, dim: 1, device: cuda\nForward Execution Time (us) : 49.903\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[128,64,<function<lambda>at0x7fa7b57bb840>]_N5_dim2_cuda\n# Input: sizes: [128, 64, <function <lambda> at 0x7fa7b57bb840>], N: 5, dim: 2, device: cuda\nForward Execution Time (us) : 84.181\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7fa7b57bba60>,32,64]_N50_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7fa7b57bba60>, 32, 64], N: 50, dim: 0, device: cuda\nForward Execution Time (us) : 82.339\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[32,<function<lambda>at0x7fa7b57bbae8>,64]_N50_dim1_cuda\n# Input: sizes: [32, <function <lambda> at 0x7fa7b57bbae8>, 64], N: 50, dim: 1, device: cuda\nForward Execution Time (us) : 82.312\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[33,65,<function<lambda>at0x7fa7b57bbb70>]_N50_dim2_cuda\n# Input: sizes: [33, 65, <function <lambda> at 0x7fa7b57bbb70>], N: 50, dim: 2, device: cuda\nForward Execution Time (us) : 90.715\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(64,32,4,16,32)_N2_dim2_cuda\n# Input: sizes: (64, 32, 4, 16, 32), N: 2, dim: 2, device: cuda\nForward Execution Time (us) : 129.021\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(16,32,4,16,32)_N8_dim2_cuda\n# Input: sizes: (16, 32, 4, 16, 32), N: 8, dim: 2, device: cuda\nForward Execution Time (us) : 142.966\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(9,31,5,15,33)_N17_dim4_cuda\n# Input: sizes: (9, 31, 5, 15, 33), N: 17, dim: 4, device: cuda\nForward Execution Time (us) : 387.023\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7fa7b57bbbf8>]_N100_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7fa7b57bbbf8>], N: 100, dim: 0, device: cuda\nForward Execution Time (us) : 36.647\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7fa7b57bbc80>]_N1000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7fa7b57bbc80>], N: 1000, dim: 0, device: cuda\nForward Execution Time (us) : 278.890\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7fa7b57bbd08>]_N2000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7fa7b57bbd08>], N: 2000, dim: 0, device: cuda\nForward Execution Time (us) : 557.752\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7fa7b57bbd90>]_N3000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7fa7b57bbd90>], N: 3000, dim: 0, device: cuda\nForward Execution Time (us) : 842.512\n\n```\n\nNew version:\n\n```\n$ python -m pt.cat_test --tag_filter all --device cuda  --omp_num_threads 1 --mkl_num_threads 1\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : all\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1,1,1)_N2_dim0_cuda\n# Input: sizes: (1, 1, 1), N: 2, dim: 0, device: cuda\nForward Execution Time (us) : 24.419\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(512,512,2)_N2_dim1_cuda\n# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 25.025\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(128,1024,2)_N2_dim1_cuda\n# Input: sizes: (128, 1024, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 24.247\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(512,512,2)_N2_dim1_cuda\n# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 25.098\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1024,1024,2)_N2_dim0_cuda\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 0, device: cuda\nForward Execution Time (us) : 74.441\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1025,1023,2)_N2_dim1_cuda\n# Input: sizes: (1025, 1023, 2), N: 2, dim: 1, device: cuda\nForward Execution Time (us) : 74.866\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(1024,1024,2)_N2_dim2_cuda\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 2, device: cuda\nForward Execution Time (us) : 189.280\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f1c9b056048>,111,65]_N5_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f1c9b056048>, 111, 65], N: 5, dim: 0, device: cuda\nForward Execution Time (us) : 57.629\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[96,<function<lambda>at0x7f1c9b0560d0>,64]_N5_dim1_cuda\n# Input: sizes: [96, <function <lambda> at 0x7f1c9b0560d0>, 64], N: 5, dim: 1, device: cuda\nForward Execution Time (us) : 49.975\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[128,64,<function<lambda>at0x7f1bce8f38c8>]_N5_dim2_cuda\n# Input: sizes: [128, 64, <function <lambda> at 0x7f1bce8f38c8>], N: 5, dim: 2, device: cuda\nForward Execution Time (us) : 83.643\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f1bce8f3ae8>,32,64]_N50_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f1bce8f3ae8>, 32, 64], N: 50, dim: 0, device: cuda\nForward Execution Time (us) : 82.307\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[32,<function<lambda>at0x7f1bce8f3b70>,64]_N50_dim1_cuda\n# Input: sizes: [32, <function <lambda> at 0x7f1bce8f3b70>, 64], N: 50, dim: 1, device: cuda\nForward Execution Time (us) : 82.323\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[33,65,<function<lambda>at0x7f1bce8f3bf8>]_N50_dim2_cuda\n# Input: sizes: [33, 65, <function <lambda> at 0x7f1bce8f3bf8>], N: 50, dim: 2, device: cuda\nForward Execution Time (us) : 90.549\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(64,32,4,16,32)_N2_dim2_cuda\n# Input: sizes: (64, 32, 4, 16, 32), N: 2, dim: 2, device: cuda\nForward Execution Time (us) : 129.022\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(16,32,4,16,32)_N8_dim2_cuda\n# Input: sizes: (16, 32, 4, 16, 32), N: 8, dim: 2, device: cuda\nForward Execution Time (us) : 142.969\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes(9,31,5,15,33)_N17_dim4_cuda\n# Input: sizes: (9, 31, 5, 15, 33), N: 17, dim: 4, device: cuda\nForward Execution Time (us) : 386.973\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f1bce8f3c80>]_N100_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f1bce8f3c80>], N: 100, dim: 0, device: cuda\nForward Execution Time (us) : 43.800\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f1bce8f3d08>]_N1000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f1bce8f3d08>], N: 1000, dim: 0, device: cuda\nForward Execution Time (us) : 279.023\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f1bce8f3d90>]_N2000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f1bce8f3d90>], N: 2000, dim: 0, device: cuda\nForward Execution Time (us) : 565.790\n\n# Benchmarking PyTorch: cat\n# Mode: Eager\n# Name: cat_sizes[<function<lambda>at0x7f1bce8f3e18>]_N3000_dim0_cuda\n# Input: sizes: [<function <lambda> at 0x7f1bce8f3e18>], N: 3000, dim: 0, device: cuda\nForward Execution Time (us) : 845.153\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33237\n\nDifferential Revision: D20069181\n\nPulled By: ngimel\n\nfbshipit-source-id: b392e1ffd72c0d8df0c5a2d3ac96f59b37c84e32", "pr_number": "33237", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/cuda/detail/IndexUtils.cu", "aten/src/ATen/native/cuda/Shape.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/THCTensorMath.cuh", "aten/src/THC/generic/THCTensorMath.cu", "aten/src/THC/generic/THCTensorMath.h", "benchmarks/operator_benchmark/pt/cat_test.py"], "labels": ["merged", "open source", "topic: porting", "triaged"]}, "98af01ee7c": {"title": "[quant] Make FakeQuant use REGISTER_DISPATCH (#33682)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33682\n\nPreviously, there were two API's for CPU and CUDA. This change keeps one top level API, i.e `fake_quantize_per_tensor_affine` and `fake_quantize_per_channel_affine` and uses the device type to dispatch to different backends (CPU and CUDA).\nCPU kernel implementation is in QuantizedOpKernels.cpp\nCUDA kernel implementation is in fake_quantize_core.cu\n\nTest Plan:\npython test/test_fake_quant.py\n\nBenchmark Results for CPU\nFakeQuantize tensor of size (2, 256, 128, 128)\n\nBefore:\nper tensor quant ms 9.905877113342285\nper channel quant ms 74.93825674057007\n\nAfter:\nper tensor quant ms 6.028120517730713\nper channel quant ms 44.91588592529297\n\nImported from OSS\n\nDifferential Revision: D20072656\n\nfbshipit-source-id: 0424f763775f88b93380a452e3d6dd0c90cb814b", "pr_number": "33682", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/fake_quantize_core.cpp", "aten/src/ATen/native/quantized/cpu/fake_quantize_core.h", "aten/src/ATen/native/quantized/cpu/fake_quantize_per_channel_affine.cpp", "aten/src/ATen/native/quantized/cpu/fake_quantize_per_tensor_affine.cpp", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu", "aten/src/ATen/native/quantized/cuda/fake_quantize_core.h", "aten/src/ATen/native/quantized/cuda/fake_quantize_per_channel_affine.cu", "aten/src/ATen/native/quantized/cuda/fake_quantize_per_tensor_affine.cu", "aten/src/ATen/native/quantized/fake_quant_affine.h", "aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp", "aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp"], "labels": ["merged"]}, "9278196d89": {"title": "scatter_add uses src, not other (#32307)", "body": "Summary:\nusing `other` kwarg gives `TypeError: scatter_add_() missing 1 required positional arguments: \"src\"`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32307\n\nDifferential Revision: D20076859\n\nPulled By: zou3519\n\nfbshipit-source-id: dfb417c087d5be41fad02dc0b2cf0506c89b1b02", "pr_number": "32307", "files_changed": ["torch/_tensor_docs.py"], "labels": ["merged", "open source", "triaged"]}, "bf00b4d305": {"title": "[TensorExpr] Add a boilerplate pass for future TensorExpr fusion pass. (#33464)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33464\n\nI added a python-exposed knob to register this pass in custom passes pipeline. If the knob is not used, the pass is not registered and thus not run at all.\n\nDifferential Revision: D19958217\n\nTest Plan: Imported from OSS\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: fecdd98567fcda069fbdf8995c796899a3dbfa5c", "pr_number": "33464", "files_changed": ["caffe2/CMakeLists.txt", "tools/build_variables.bzl", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.h"], "labels": ["jit", "merged"]}, "bc5e9e0d55": {"title": "[quant][graphmode][refactor] Move the check for qconfig inside insertObserver call (#32809)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32809\n\nThis is a refactor to help further changes to quantization.cpp\nWe want some operations on the graph happen before we call insertObserver for invoked methods,\nespecially `addIntermediateValuesToSkipObserver` since we want to skip the input of the ReLU\nmodule in `Conv - ReLU` pattern.\n\nTest Plan:\ntest_jit.py\ntest_quantization.py\n\nImported from OSS\n\nDifferential Revision: D20087844\n\nfbshipit-source-id: 28b7fa0c7ce9e254ab9208eb344893fb705e14d9", "pr_number": "32809", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "cba8af9b24": {"title": "[pytorch] Set alias analysis kind to FROM_SCHEMA for qadd, qmul, qclamp, qconcat (#33359)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33359\n\nUpdated alias analysis kind to FROM_SCHEMA so input tensors can be marked as nonmutable\nwhen appropriate, allowing for constant folding of these tensors.\n\nNeeded to update the schemas of the _out variants with annotations to mark the output input\ntensor as aliased and mutable.\n\nTest Plan:\n```\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n\n    def forward(self, x):\n        w = torch.tensor([3], dtype=torch.float)\n        w = torch.quantize_per_tensor(w, 1.0, 0, torch.qint8)\n        y = torch.tensor([3], dtype=torch.float)\n        y = torch.quantize_per_tensor(w, 1.0, 0, torch.qint8)\n        return torch.ops.quantized.add_out(x, w, y)\n\nm = torch.jit.script(M())\ntorch._C._jit_pass_constant_propagation(m.graph)\nprint(m.graph)\n```\n```\ngraph(%self : __torch__.___torch_mangle_9.M,\n      %x.1 : Tensor):\n  %11 : int = prim::Constant[value=12]() # <ipython-input-11-1dd94c30cb58>:9:49\n  %9 : float = prim::Constant[value=1.]() # <ipython-input-11-1dd94c30cb58>:9:41\n  %10 : int = prim::Constant[value=0]() # <ipython-input-11-1dd94c30cb58>:9:46\n  %36 : QInt8(1) = prim::Constant[value={3}]()\n  %y.2 : Tensor = aten::quantize_per_tensor(%36, %9, %10, %11) # <ipython-input-11-1dd94c30cb58>:11:12\n  %24 : Tensor = quantized::add_out(%x.1, %36, %y.2) # <ipython-input-11-1dd94c30cb58>:12:15\n  return (%24)\n```\nAs expected, the aten::quantize_per_tensor() for w is now folded. The aten::quantize_per_tensor()\nfor y is not folded, since that tensor is aliased/modified.\n\nDifferential Revision: D19910667\n\nfbshipit-source-id: 127071909573151dc664500d363399e3643441b7", "pr_number": "33359", "files_changed": ["aten/src/ATen/native/quantized/cpu/qadd.cpp", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "aten/src/ATen/native/quantized/cpu/qconcat.cpp", "aten/src/ATen/native/quantized/cpu/qmul.cpp", "test/backward_compatibility/check_backward_compatibility.py"], "labels": ["fb-exported", "merged"]}, "54aac4af1f": {"title": "Update hypothesis_utils.py (#33739)", "body": "Summary:\nA typo..\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33739\n\nDifferential Revision: D20088096\n\nPulled By: jerryzh168\n\nfbshipit-source-id: d8b5d263c25f8c779698607be87bf76aca1811ab", "pr_number": "33739", "files_changed": ["torch/testing/_internal/hypothesis_utils.py"], "labels": ["merged"]}, "d6ea4be153": {"title": "Fix minor problems in index_put_ docs (#33689)", "body": "Summary:\nFix for https://github.com/pytorch/pytorch/issues/33641\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33689\n\nDifferential Revision: D20086967\n\nPulled By: ngimel\n\nfbshipit-source-id: d9dde8edb904de1cf56b9337920cb29e008b72fb", "pr_number": "33689", "files_changed": ["torch/_tensor_docs.py"], "labels": ["merged", "open source"]}, "3cf97bc23c": {"title": "Fix typing error of torch/nn/modules/container.pyi.in (#33686)", "body": "Summary:\n* `Sequential` has `__iter__` method, but type stub doesn't\n* `ModuleList.__getitem__` returns `Module`, but type stub doesn't\n* Type stub says `ParameterList` has `insert` method, but actual `ParameterList` doesn't\n* `ParameterDict.__getitem__` should returns `Parameter`\n* `ParameterList` and `ParameterDict` have `extra_repr` methods\n\n ---\n\ntorch/nn/modules/container.py: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/container.py\ntorch/nn/modules/container.pyi.in: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/container.pyi.in\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33686\n\nDifferential Revision: D20086730\n\nPulled By: ngimel\n\nfbshipit-source-id: a8271489417461c67ff84a239c4cd96c3aa17b5c", "pr_number": "33686", "files_changed": ["torch/nn/modules/container.pyi.in"], "labels": ["merged", "open source"]}, "a9cef05f5d": {"title": "improve EmbeddingBag performance on cuda (#33589)", "body": "Summary:\nThis PR improves performance of EmbeddingBag on cuda by removing 5 kernel launches (2 of those are synchronizing memcopies).\n- 2 memcopies are checking values of offsets[0] and offsets[-1] to be in expected range (0 for the former, less than number of indices for the latter). It seems strange to check only those 2 values, if users are providing invalid offsets, invalid values can be anywhere in the array, not only the first and last element. After this PR, the checks are skipped on cuda, the first value is forced to 0, if the last value is larger than expected, cuda kernel will assert. It is less nice than ValueError, but then again, the kernel could have asserted if other offset values were invalid. On the cpu, the checks are moved inside the cpu implementation from functional.py, and will throw RuntimeError instead of ValueError.\n- 3 or 4 initializations (depending on the mode) of the output tensors with .zeros() are unnecessary, because every element of those tensors is written to, so their data can be uninitialized on the start.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33589\n\nReviewed By: jianyuh\n\nDifferential Revision: D20078011\n\nPulled By: ngimel\n\nfbshipit-source-id: 2fb2e2080313af64adc5cf1b9fc6ffbdc6efaf16", "pr_number": "33589", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "test/test_nn.py", "torch/nn/functional.py"], "labels": ["merged"]}, "a1862468d0": {"title": "Add missing test launchers for JitRpcTest and JitDistAutogradTest (#32891)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32891\n\n- Add JitDistAutoGradTest into fork/spawn test launcher\n- Add JitRpcTest into fork/spawn test launcher\n\nghstack-source-id: 98900090\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork\n\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_spawn\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:dist_autograd_fork\n\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:dist_autograd_spawn\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork_thrift\n\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_spawn\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_spawn_thrift\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:dist_autograd_fork\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:dist_autograd_fork_thrift\n\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:dist_autograd_spawn\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:dist_autograd_spawn_thrift\n```\n\nDifferential Revision: D5785394\n\nfbshipit-source-id: 335a85424d22f1a83874be81a8139499c9a68ce2", "pr_number": "32891", "files_changed": ["test/distributed/rpc/jit/test_dist_autograd_spawn.py", "test/distributed/rpc/jit/test_rpc_spawn.py", "test/distributed/rpc/test_dist_autograd_spawn.py", "test/distributed/rpc/test_rpc_spawn.py", "test/run_test.py", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/jit/__init__.py", "torch/testing/_internal/distributed/rpc/jit/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "7caf3c396b": {"title": "[quant][graphmode][refactor] Change signature of getModuleAccessPath (#32812)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32812\n\nWe'll error out for the case we can't handle inside the function,\ninstead of checking each time in the callsite\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D20087846\n\nfbshipit-source-id: ae6d33a94adf29c4df86d67783e7ef8753c91f90", "pr_number": "32812", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "2a4aad7466": {"title": "Don't activate vc env again for cuda with ninja on Windows (#33700)", "body": "Summary:\nPossibly get rid of https://github.com/pytorch/pytorch/issues/28271, https://github.com/pytorch/pytorch/issues/27463 and https://github.com/pytorch/pytorch/issues/25393.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33700\n\nDifferential Revision: D20089251\n\nPulled By: ezyang\n\nfbshipit-source-id: 0cfe62b869fb874e25f06894aa76fadc44cf6817", "pr_number": "33700", "files_changed": ["cmake/public/cuda.cmake"], "labels": ["merged", "open source", "triaged"]}, "6aecfd1e80": {"title": "Mobile Backend: NHWC memory layout + XNNPACK integration. (#33722)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33722\n\nIn order to improve CPU performance on floating-point models on mobile, this PR introduces a new CPU backend for mobile that implements the most common mobile operators with NHWC memory layout support through integration with XNNPACK.\n\nXNNPACK itself, and this codepath, are currently only included in the build, but the actual integration is gated with USE_XNNPACK preprocessor guards.  This preprocessor symbol is intentionally not passed on to the compiler, so as to enable this rollout in multiple stages in follow up PRs.  This changeset will build XNNPACK as part of the build if the identically named USE_XNNPACK CMAKE variable, defaulted to ON, is enabled, but will not actually expose or enable this code path in any other way.\n\nFurthermore, it is worth pointing out that in order to efficiently map models to these operators, some front-end method of exposing this backend to the user is needed.  The less efficient implementation would be to hook these operators into their corresponding native implementations, granted that a series of XNNPACK-specific conditions are met, much like how NNPACK is integrated with PyTorch today for instance.\n\nHaving said that, while the above implementation is still expected to outperform NNPACK based on the benchmarks I ran, the above integration would be leave a considerable gap between the performance achieved and the maximum performance potential XNNPACK enables, as it does not provide a way to compute and factor out one-time operations out of the inner most forward() loop.\n\nThe more optimal solution, and one we will  decide on soon, would involve either providing a JIT pass that maps nn operators onto these newly introduced operators, while allowing one-time calculations to be factored out, much like quantized mobile models.  Alternatively, new eager-mode modules can also be introduced that would directly call into these implementations either through c10 or some other mechanism, also allowing for decoupling of op creation from op execution.\n\nThis PR does not include any of the front end changes  mentioned above.  Neither does it include the mobile threadpool unification present in the original https://github.com/pytorch/pytorch/issues/30644.  Furthermore, this codepath seems to be faster than NNPACK in a good number of use cases, which can potentially allow us to remove NNPACK from aten to make the codebase a little simpler, granted that there is widespread support for such a move.\n\nRegardless, these changes will be introduced gradually and in a more controlled way in subsequent PRs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32509\n\nTest Plan:\nBuild: CI\nFunctionality: Not exposed\n\nReviewed By: dreiss\n\nDifferential Revision: D20069796\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: d46c1c91d4bea91979ea5bd46971ced5417d309c", "pr_number": "33722", "files_changed": [".circleci/scripts/binary_ios_upload.sh", ".gitmodules", "CMakeLists.txt", "android/pytorch_android/CMakeLists.txt", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/utils/Allocator.h", "aten/src/ATen/native/xnnpack/Common.h", "aten/src/ATen/native/xnnpack/Convolution.cpp", "aten/src/ATen/native/xnnpack/Factory.cpp", "aten/src/ATen/native/xnnpack/Factory.h", "aten/src/ATen/native/xnnpack/Init.cpp", "aten/src/ATen/native/xnnpack/Linear.cpp", "aten/src/ATen/native/xnnpack/Shim.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "cmake/Dependencies.cmake", "cmake/TorchConfig.cmake.in", "ios/TestApp/benchmark/setup.rb", "scripts/xcode_build.rb", "third_party/XNNPACK", "third_party/cpuinfo", "third_party/psimd", "third_party/pthreadpool"], "labels": ["fb-exported", "merged"]}, "36919278cc": {"title": "C++ tensor multi-dim indexing: add index() and index_put_() overloads, simple indexing tests, merge with Python indexing path (#32841)", "body": "Summary:\nThis PR adds the following items:\n- **1st item**: `ArrayRef<TensorIndex>` and `std::initializer_list<TensorIndex>` overloads for `Tensor::index` and `Tensor::index_put_`, to be used specifically for multi-dim indexing purpose.\n\nDesign rationale:\n* C++ `Tensor::index` and `Tensor::index_put_` are both existing tensor APIs, and they currently (before this PR) only accept a list of tensors (i.e. `ArrayRef<Tensor>`) as indices. If we change their signatures to also accept non-tensors as indices (i.e. `ArrayRef<TensorIndex>`, and `TensorIndex` is convertible from `Tensor` / `Slice` / `None` / `Ellipsis`), it would slow down the original code path (since now it has to go through more steps), which is undesirable.\n\n    To get around this problem, the proposed solution is to keep the original `ArrayRef<Tensor>` overload, and add `ArrayRef<TensorIndex>` and `std::initializer_list<TensorIndex>` overloads to `Tensor::index` and `Tensor::index_put_`. This way, the original code path won\u2019t be affected, and the tensor multi-dim indexing API is only used when the user explicitly pass an `ArrayRef<TensorIndex>` or a braced-init-list of `TensorIndex`-convertible types to `Tensor::index` and `Tensor::index_put_` .\n\n    Note that the above proposed solution would still affect perf for the user\u2019s original `Tensor::index` or `Tensor::index_put_` call sites that use a braced-init-list of tensors as input, e.g. `tensor.index({...})` or `tensor.index_put_({...}, value)`, since now such function calls would take the multi-dim indexing path instead of the original advanced indexing path. However, there are only two instances of this in our codebase (one in ATen cpp test, one in a C++ API nn init function), and they can be easily changed to explicitly use `ArrayRef<Tensor>` as input (I changed them in this PR). For external user\u2019s code, since this is part of the C++ frontend which is still considered experimental, we will only talk about this change in the release note, and ask users to switch to using `ArrayRef<Tensor>` explicitly if they want to keep using the original advanced indexing code path.\n\n- **2nd item**: Mechanisms for parsing `ArrayRef<TensorIndex>` indices and performing indexing operations (mirroring the functions in `torch/csrc/autograd/python_variable_indexing.cpp`).\n- **3rd item**: Simple tests to demonstrate that the `Tensor::index()` and `Tensor::index_put_()` APIs work. I will add more tests after the first few PRs are reviewed.\n- **4th item**: Merge Python/C++ indexing code paths, for code simplicity. I tested locally and found that there is no perf regression resulting from the merge. I will get more concrete numbers for common use cases when we settle on the overall design.\n\nThis PR supersedes https://github.com/pytorch/pytorch/pull/30425.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32841\n\nDifferential Revision: D19919692\n\nPulled By: yf225\n\nfbshipit-source-id: 7467e64f97fc0e407624809dd183c95ea16b1482", "pr_number": "32841", "files_changed": ["aten/src/ATen/native/TensorIndexing.cpp", "aten/src/ATen/native/TensorIndexing.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/TensorBody.h", "c10/util/Exception.h", "test/cpp/api/tensor_indexing.cpp", "torch/csrc/Exceptions.h", "torch/csrc/autograd/python_variable_indexing.cpp"], "labels": ["module: cpp"]}, "adbe289870": {"title": "Update MKL to 2020.0.166 for Windows (#33690)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33690\n\nDifferential Revision: D20089300\n\nPulled By: ezyang\n\nfbshipit-source-id: 887c006fbdb2c837f0a1c607a196811f44f1fb35", "pr_number": "33690", "files_changed": [".jenkins/pytorch/win-test-helpers/installation-helpers/install_mkl.bat", "docs/source/notes/windows.rst"], "labels": ["merged", "open source", "triaged"]}, "65864d3634": {"title": "[C2] Small improvement for elementwise_mul operator. (#33537)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33537\n\nCases of embeddings smaller than 128, we can get a bit more compute by\nallocating less threads per block.\n\nTest Plan: Unit-test, benchmark.\n\nReviewed By: xianjiec\n\nDifferential Revision: D19969594\n\nfbshipit-source-id: 6cc6b14fc61302804bed9093ea3591f21e3827d8", "pr_number": "33537", "files_changed": ["caffe2/operators/elementwise_mul_op.cu"], "labels": ["fb-exported", "merged"]}, "4460c8b034": {"title": "[C2] Tiny changes to adagrad to make it slightly better. (#33727)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33727\n\nSome small changes to adagrad (tiny bit faster, though there is more interesting diff in the stack on this).\n\nTest Plan: Part of the stack\n\nReviewed By: chocjy\n\nDifferential Revision: D20029499\n\nfbshipit-source-id: 7f4fddb9288d7881ef54673b17a0e19ef10d64c0", "pr_number": "33727", "files_changed": ["caffe2/sgd/adagrad_op_gpu.cu"], "labels": ["fb-exported", "merged"]}, "7a8b6c2c6b": {"title": "[pytorch] blas gemm fix for k=0 (#33419)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33419\n\nThese conditions are for the specific implementation, the fallback implementation works without these checks. So use that if any of these checks isn't true.\nghstack-source-id: 98836075\n\nTest Plan: Previously got error for special case where k=0 which has gone. The error was in some complicated autograd, and I'm not sure how and where an simple regression test should be added.\n\nDifferential Revision: D19941103\n\nfbshipit-source-id: e1c85d1e75744b1c51ad9b71c7b3211af3c5bcc6", "pr_number": "33419", "files_changed": ["aten/src/TH/generic/THBlas.cpp", "test/test_torch.py"], "labels": ["merged"]}, "4ef854b4b4": {"title": "Fix potential hang when exiting main process (#33721)", "body": "Summary:\nThe following script reproduces the hang\n```py\nimport multiprocessing, logging\nlogger = multiprocessing.log_to_stderr()\nlogger.setLevel(multiprocessing.SUBDEBUG)\n\nimport torch\n\nclass Dataset:\n    def __len__(self):\n        return 23425\n\n    def __getitem__(self, idx):\n        return torch.randn(3, 128, 128), idx % 100\n\nds = Dataset()\ntrdl = torch.utils.data.DataLoader(ds, batch_size=64, num_workers=300, pin_memory=True, shuffle=True)\n\nfor e in range(1000):\n    for ii, (x, y) in enumerate(trdl):\n        print(f'tr {e: 5d} {ii: 5d} avg y={y.mean(dtype=torch.double).item()}')\n        if ii % 2 == 0:\n            print(\"=\"*200 + \"BEFORE ERROR\" + \"=\"*200)\n            1/0\n```\n\nThe process will hang at joining the putting thread of `data_queue` in **main process**. The root cause is that too many things are put in the queue from the **worker processes**, and the `put` at https://github.com/pytorch/pytorch/blob/062ac6b472af43c9cf83d285e661e24244551f85/torch/utils/data/dataloader.py#L928 is blocked at background thread. The `pin_memory_thread` exits from the set `pin_memory_thread_done_event`, without getting the `(None, None)`. Hence, the main process needs the same treatment as the workers did at\nhttps://github.com/pytorch/pytorch/blob/062ac6b472af43c9cf83d285e661e24244551f85/torch/utils/data/_utils/worker.py#L198 .\n\nAfter the patch, the script finishes correctly.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33721\n\nDifferential Revision: D20089209\n\nPulled By: ezyang\n\nfbshipit-source-id: e73fbfdd7631afe1ce5e1edd05dbdeb7b85ba961", "pr_number": "33721", "files_changed": ["torch/utils/data/dataloader.py"], "labels": ["merged", "open source", "triaged"]}, "6bdb59539f": {"title": "follow-up test_torch .data removal (#33696)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33696\n\nThis changes two tests:\n- The batchnorm inference cannot change the memory format of the weights as they are 1D. So this is removed.\n- The batchnorm test now run both in affine and not affine mode.\n- I added back the test for type errors using .data. In particular, `.data` allows to change the type of a Tensor inplace (very bad, never do it!) but since it is possible, we should test it until .data is removed.\n\ncc Enealor who did the first version of the PR.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20069241\n\nPulled By: albanD\n\nfbshipit-source-id: a0348f40c44df38d654fb2a2b2b526d9d42f598a", "pr_number": "33696", "files_changed": ["test/test_torch.py"], "labels": ["merged"]}, "fd175fa8a2": {"title": "fix bugs in gen_pyi.py (#33748)", "body": "Summary:\nThis loop should generate type hints for inplace binary operator methods (`binop` variable) but had been using `name` variable. That's why that wrong type hints had been generated.\n\nResolve https://github.com/pytorch/pytorch/issues/33698\n\n ---\n\nCurrent `__init__.pyi` has these type hints.\n\n```python\nclass Tensor:\n\n    # some codes here...\n\n    overload\n    def zeros_like_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like_(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like_(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like_(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like__(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like__(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like__(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like__(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like___(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like___(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like___(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like___(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like____(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like____(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like____(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like____(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n\n    # some codes here...\n```\n\nBut `__init__.pyi` should generate these type hints.\n\n```python\nclass Tensor:\n\n    # some codes here...\n\n    overload\n    def add_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def add_(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def add_(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def add_(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n\n    # some codes here...\n\n    overload\n    def div_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def div_(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def div_(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def div_(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n\n    # some codes here...\n\n    overload\n    def mul_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def mul_(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def mul_(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def mul_(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n\n    # some codes here...\n\n    overload\n    def sub_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def sub_(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def sub_(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def sub_(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n\n    # some codes here...\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33748\n\nDifferential Revision: D20090444\n\nPulled By: ngimel\n\nfbshipit-source-id: e4a5dd08126629ec4c54b630a87ee540e669ec9a", "pr_number": "33748", "files_changed": ["tools/pyi/gen_pyi.py"], "labels": ["merged", "open source"]}, "819ca2c285": {"title": "add bfloat16 conversion method in type stub (__init__.pyi) (#33747)", "body": "Summary:\nResolve https://github.com/pytorch/pytorch/issues/33699\n\n`torch/__init__.pyi` will be generated like\n\n```python\n# TODO: One downside of doing it this way, is direct use of\n# torch.tensor.Tensor doesn't get type annotations.  Nobody\n# should really do that, so maybe this is not so bad.\nclass Tensor:\n    requires_grad: _bool = ...\n    grad: Optional[Tensor] = ...\n\n    # some methods here...\n\n    overload\n    def bernoulli_(self, p: _float=0.5, *, generator: Generator=None) -> Tensor: ...\n    def bfloat16(self) -> Tensor: ...\n    def bincount(self, weights: Optional[Tensor]=None, minlength: _int=0) -> Tensor: ...\n\n    # some methods here...\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33747\n\nDifferential Revision: D20090316\n\nPulled By: ngimel\n\nfbshipit-source-id: b9ce4c0d4ef720c94ccac0a0342a012e8cf3af0c", "pr_number": "33747", "files_changed": ["tools/pyi/gen_pyi.py"], "labels": ["merged", "open source"]}, "ee23944f46": {"title": "[Caffe2] Fix shape inference for element-wise operators (#33431)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33431\n\nSome elementwise operators don't have shape and type inference specified for the output tensor: `BitwiseOr`, `BitwiseAnd`, `BitwiseXor`, `Not`, `Sign`.\n\nThis change fixes this issue:\n- For `Not` and `Sign` operators, the output has the same type and shape as the input, so `IdenticalTypeAndShapeOfInput` function is used to specify that.\n- For bitwise operators created by `CAFFE2_SCHEMA_FOR_BINARY_BITWISE_OP` macro, the type and shape inference rules should be the same as for other binary element-wise operators, so `TensorInferenceFunction(ElementwiseOpShapeInference)` is used to specify that.\n\nAlso some tests were modified to ensure that the shape and type are inferred (`ensure_outputs_are_inferred` parameter)\n\nTest Plan:\n```\nCAFFE2_ASSERT_SHAPEINFERENCE=1 buck test caffe2/caffe2/python/operator_test:elementwise_ops_test\nCAFFE2_ASSERT_SHAPEINFERENCE=1 buck test caffe2/caffe2/python/operator_test:math_ops_test\n```\n\nNote that the tests have to be executed with `CAFFE2_ASSERT_SHAPEINFERENCE=1` in order to fail upon shape inference failure.\n\nReviewed By: idning\n\nDifferential Revision: D19880164\n\nfbshipit-source-id: 5d7902e045d79e5669e5e98dfb13a39711294939", "pr_number": "33431", "files_changed": ["caffe2/operators/elementwise_ops_schema.cc", "caffe2/python/operator_test/elementwise_ops_test.py", "caffe2/python/operator_test/math_ops_test.py", "caffe2/python/serialized_test/serialized_test_util.py"], "labels": ["fb-exported", "merged"]}, "5ef1c2c5d2": {"title": "Back out \"[pt][quant] RNN debug test\" (#33750)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33750\n\nOriginal commit changeset: 8c38d8f067e5\nghstack-source-id: 98911215\n\nTest Plan: CI\n\nDifferential Revision: D20090521\n\nfbshipit-source-id: 73df43ad60574e44e80b36ebf6392030c3efb66e", "pr_number": "33750", "files_changed": ["test/test_quantization.py"], "labels": []}, "8196ec0115": {"title": "Remove some dead THStorage related code. (#33734)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33734\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20084030\n\nPulled By: gchanan\n\nfbshipit-source-id: 29aa5459e8ecc8af8af31157797f44057d6a786e", "pr_number": "33734", "files_changed": ["aten/src/TH/generic/THTensor.cpp", "aten/src/TH/generic/THTensor.h", "aten/src/THC/generic/THCTensor.cpp", "aten/src/THC/generic/THCTensor.h"], "labels": ["merged"]}, "98526c7444": {"title": "Migrate fake_quant_slice to TensorIterator (#33744)", "body": "Summary:\nThis is a quick improvement for per tensor quantization.\n\nper-channel should remove the loop in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp\n\n# Benchmark:\ndevice = GTX-1650\n```python\nimport torch\nprint(torch.__version__)\n\nfor i in range(1000):\n    torch.randn(1024 * 128, device='cuda')\n\ndef f(e):\n    a = torch.randn(2 ** e, device='cuda')\n    torch.cuda.synchronize()\n    %timeit torch.fake_quantize_per_tensor_affine(a, 0.5, 0, 0, 1); torch.cuda.synchronize()\n\nfor i in range(15, 27):\n    f(i)\n```\nBefore\n```\n1.5.0a0+bf00b4d\n14.5 \u00b5s \u00b1 981 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n18.2 \u00b5s \u00b1 1.09 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n25.6 \u00b5s \u00b1 2.72 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n38.6 \u00b5s \u00b1 135 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n70.2 \u00b5s \u00b1 5.21 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n125 \u00b5s \u00b1 4.98 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n231 \u00b5s \u00b1 1.36 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n461 \u00b5s \u00b1 22.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n891 \u00b5s \u00b1 88.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.77 ms \u00b1 8.13 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n3.77 ms \u00b1 80.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n7.16 ms \u00b1 216 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\nAfter\n```\n1.5.0a0+3f18ac3\n12.5 \u00b5s \u00b1 738 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n13.7 \u00b5s \u00b1 195 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n17.9 \u00b5s \u00b1 850 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n29.7 \u00b5s \u00b1 285 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n50.4 \u00b5s \u00b1 1.94 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n95 \u00b5s \u00b1 8.23 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n173 \u00b5s \u00b1 7.37 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n348 \u00b5s \u00b1 29.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n657 \u00b5s \u00b1 22.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.33 ms \u00b1 77.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n2.71 ms \u00b1 211 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n5.33 ms \u00b1 439 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33744\n\nDifferential Revision: D20090129\n\nPulled By: ngimel\n\nfbshipit-source-id: 5dd48a0c5455a2b6c5c638d747c1767cb259255d", "pr_number": "33744", "files_changed": ["aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu"], "labels": ["merged", "open source"]}, "2b404de347": {"title": "[scripts] Add script to fetch clang-format binary from AWS S3 (#33644)", "body": "Summary:\n**Summary**\nThis commit adds a script that fetches a platform-appropriate `clang-format` binary\nfrom S3 for use during PyTorch development. The goal is for everyone to use the exact\nsame `clang-format` binary so that there are no formatting conflicts.\n\n**Testing**\nRan the script.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33644\n\nDifferential Revision: D20076598\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: cd837076fd30e9c7a8280665c0d652a33b559047", "pr_number": "33644", "files_changed": ["scripts/get_clang_format.py"], "labels": ["merged"]}, "d82093e665": {"title": "[profiler] remove redundant assert in record_function_ops (#33225)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33225\n\nThis removes a redundant assert statement in `record_function_ops`. In\nthe else branch in question, we are guaranteed to have `current == &rec`, so\nthis assert will never fire.\n\nAlthough, maybe we should add an assert failure when `current == &rec` since it\nseems that `current` should always be profiler::record_function_exit.\nghstack-source-id: 98852219\n\nTest Plan: Existing autograd profiler UTs past\n\nDifferential Revision: D19849145\n\nfbshipit-source-id: 2014a0d3b9d11e5b64942a54e0fb45e21f46cfa2", "pr_number": "33225", "files_changed": ["torch/csrc/autograd/record_function_ops.cpp"], "labels": ["merged"]}, "7eba36b1f6": {"title": "[quant][graphmode][refactor] Separate preprocess step for insertObserver (#32813)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32813\n\nWe need to separate the step to make the logic more clear\nand also to find all the values we want to skip in advance\nwithout the interference of inserted observers\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D20087841\n\nfbshipit-source-id: ec3654ca561c0d4e2c05011988bb9ecc8671c5c2", "pr_number": "32813", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "9bc922d518": {"title": "Extend cuda install timeout for Windows jobs (#33755)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33755\n\nDifferential Revision: D20100372\n\nPulled By: soumith\n\nfbshipit-source-id: 8b39177d3e87d248857f0582de6c9e203d09d4a7", "pr_number": "33755", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/pytorch-job-specs.yml"], "labels": ["merged", "open source"]}, "0e74cbcc54": {"title": "Revert \"Revert \"Revert D19975411: Remove special case codegen for tril_indices/triu_indices.\" (#33572)\" (#33742)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33742\n\nThis reverts commit 90f4c5695e1785883d9ae7c86ad3fabd1963a4cb.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20095103\n\nPulled By: ezyang\n\nfbshipit-source-id: ff47dae21c278570b4ca497d76deedb75823d6d7", "pr_number": "33742", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "tools/autograd/gen_python_functions.py"], "labels": ["merged"]}, "72288e82e2": {"title": "Use shim executable sccache-cl as the compiler instead of sccache cl (#33745)", "body": "Summary:\nCMake only views the first item of `CC` and `CXX` as executable. So calling `sccache.exe` directly won't work. Using a shim executable resolves this problem.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33745\n\nDifferential Revision: D20100397\n\nPulled By: soumith\n\nfbshipit-source-id: 3a130d30dd548b7c2e726c064e66ae4fccb30c44", "pr_number": "33745", "files_changed": [".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_sccache.bat"], "labels": ["merged", "open source"]}, "c20628c5f6": {"title": "Remove `clean_tag` from tensorboard (#33133)", "body": "Summary:\nThe function originally comes from https://github.com/tensorflow/tensorflow/blob/4279f99847e9fcce9410bda61d3b71065e0df65f/tensorflow/python/ops/summary_op_util.py#L45-L68\n\nAs its comment says:\n```\n    # In the past, the first argument to summary ops was a tag, which allowed\n    # arbitrary characters. Now we are changing the first argument to be the node\n    # name. This has a number of advantages (users of summary ops now can\n    # take advantage of the tf name scope system) but risks breaking existing\n    # usage, because a much smaller set of characters are allowed in node names.\n    # This function replaces all illegal characters with _s, and logs a warning.\n    # It also strips leading slashes from the name.\n```\n\nThis function is only for compatibility with TF's operator name restrictions, and is therefore no longer valid in pytorch. By removing it, tensorboard summaries can use more characters in the names.\n\nBefore:\n![0209-12:10:14](https://user-images.githubusercontent.com/1381301/74109072-37382e00-4b35-11ea-8c9f-ab37a8bd5808.png)\n\nAfter:\n![0209-12:10:57](https://user-images.githubusercontent.com/1381301/74109081-4323f000-4b35-11ea-9dab-447f8466a41e.png)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33133\n\nDifferential Revision: D20089307\n\nPulled By: ezyang\n\nfbshipit-source-id: 3552646dce1d5fa0bde7470f32d5376e67ec31c6", "pr_number": "33133", "files_changed": ["torch/utils/tensorboard/summary.py"], "labels": ["merged"]}, "0dded4026e": {"title": "[C++ API] Add PackedSequence / pack_padded_sequence / pad_packed_sequence / pack_sequence (#33652)", "body": "Summary:\nMost of the function implementation and test code are translated from the Python version.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33652\n\nDifferential Revision: D20052211\n\nPulled By: yf225\n\nfbshipit-source-id: ce6767db54364f91ef4f06674239a12278c2752a", "pr_number": "33652", "files_changed": ["test/cpp/api/nn_utils.cpp", "test/cpp_api_parity/parity-tracker.md", "torch/csrc/api/include/torch/nn/utils/rnn.h"], "labels": ["merged", "module: cpp"]}, "4d203c6fc8": {"title": "Move cumprod and cumsum to Aten(CPU) (#33280)", "body": "Summary:\nThis PR is about move cumprod and cumsum to Aten.\nTest script:\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\n\ndef _time():\n    return time.time()\n\ndevice = \"cpu\"\n\n#torch.set_num_threads(1)\n\n#warm up\nfor n in [10, 300]:\n    input = torch.randn(n, n, n, requires_grad=False, device=device)\n    input = input * 0.01 + 1\n    for dim in range(input.dim()):\n        for i in range(100):\n            #output = input.cumsum(dim)\n            output = input.cumprod(dim)\n\nfor n in [10, 300]:\n    input = torch.randn(n, n, n, requires_grad=False, device=device)\n    input = input * 0.01 + 1\n    for dim in range(input.dim()):\n        fwd_t = 0\n        for i in range(1000):\n            t1 = _time()\n            #output = input.cumsum(dim)\n            output = input.cumprod(dim)\n            t2 = _time()\n            fwd_t = fwd_t + (t2 -t1)\n        fwd_avg = fwd_t / 1000 * 1000\n        print(\"size = (%d, %d, %d); reduce dim=%d; compute time is %.4f(ms)\" % (n, n, n, dim, fwd_avg))\n```\nTest device: **skx-8180**.\nPerformance:\n```\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0098(ms)\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0089(ms)\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0089(ms)\nsize = (300, 300, 300); reduce dim=0; compute time is 208.9403(ms)\nsize = (300, 300, 300); reduce dim=1; compute time is 241.5989(ms)\nsize = (300, 300, 300); reduce dim=2; compute time is 66.2587(ms)\nAfter:\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0065(ms)\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0063(ms)\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0053(ms)\nsize = (300, 300, 300); reduce dim=0; compute time is 36.0139(ms)\nsize = (300, 300, 300); reduce dim=1; compute time is 36.0776(ms)\nsize = (300, 300, 300); reduce dim=2; compute time is 21.0111(ms)\nnumber_threads = 1:\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0053(ms)\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0052(ms)\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0051(ms)\nsize = (300, 300, 300); reduce dim=0; compute time is 81.8831(ms)\nsize = (300, 300, 300); reduce dim=1; compute time is 88.5687(ms)\nsize = (300, 300, 300); reduce dim=2; compute time is 54.9922(ms)\n\ncumprod:\nBefore:\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0096(ms)\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0088(ms)\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0088(ms)\nsize = (300, 300, 300); reduce dim=0; compute time is 221.2601(ms)\nsize = (300, 300, 300); reduce dim=1; compute time is 249.7894(ms)\nsize = (300, 300, 300); reduce dim=2; compute time is 71.5182(ms)\nnumber_threads = 1:\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0100(ms)\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0093(ms)\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0093(ms)\nsize = (300, 300, 300); reduce dim=0; compute time is 207.6287(ms)\nsize = (300, 300, 300); reduce dim=1; compute time is 241.6693(ms)\nsize = (300, 300, 300); reduce dim=2; compute time is 66.2977(ms)\nAfter:\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0063(ms)\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0062(ms)\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0053(ms)\nsize = (300, 300, 300); reduce dim=0; compute time is 36.4283(ms)\nsize = (300, 300, 300); reduce dim=1; compute time is 38.1139(ms)\nsize = (300, 300, 300); reduce dim=2; compute time is 20.9140(ms)\nnumber_threads =1:\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0052(ms)\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0052(ms)\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0050(ms)\nsize = (300, 300, 300); reduce dim=0; compute time is 82.6926(ms)\nsize = (300, 300, 300); reduce dim=1; compute time is 90.1265(ms)\nsize = (300, 300, 300); reduce dim=2; compute time is 55.0196(ms)\n```\nFix https://github.com/pytorch/pytorch/issues/24668, https://github.com/pytorch/pytorch/issues/24669.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33280\n\nDifferential Revision: D20076997\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 12225767da8cfdc5e44257462a432bffa04cd469", "pr_number": "33280", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReduceOps.h", "aten/src/ATen/native/ReduceOpsUtils.h", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cpu/ScatterGatherKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "test/test_torch.py"], "labels": ["merged", "open source", "topic: bc-breaking", "topic: porting", "triaged"]}, "8159316714": {"title": "Revert D19941103: [pytorch] blas gemm fix for k=0", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19941103\n\nOriginal commit changeset: e1c85d1e7574\n\nfbshipit-source-id: da12747130c60b61452aa46e269c66546a1075f9", "pr_number": null, "files_changed": ["aten/src/TH/generic/THBlas.cpp", "test/test_torch.py"], "labels": []}, "a13ee18982": {"title": "[quant][graphmode] refactor nodeQuantizable (#33171)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33171\n\nFor better code reuse\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D20087845\n\nfbshipit-source-id: f88cffb410bd54a1b3f937786104f46bcd1190d3", "pr_number": "33171", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "8667379133": {"title": "[quant][graphmode][refactor] Factor out insertDequantCall (#33172)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33172\n\nFor code reuse\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D20087842\n\nfbshipit-source-id: 797868d31b96c4ff8640121ea4bee1396deb6b57", "pr_number": "33172", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "038ee01393": {"title": "Disable printing of the histogram when dump (#33749)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33749\n\nDisable printing of the histogram when dump to make the log cleaner.\n\nTest Plan: CI\n\nReviewed By: amylittleyang\n\nDifferential Revision: D20087735\n\nfbshipit-source-id: 5421cd9d25c340d92f29ce63fed2a58aefef567d", "pr_number": "33749", "files_changed": ["caffe2/quantization/server/activation_distribution_observer.h"], "labels": ["fb-exported", "merged"]}, "5bac7febad": {"title": "removed padding and dilation from LPPool2d Doc (#33714)", "body": "Summary:\nremoved padding and dilation from LPPool2d Doc as the function dose not support padding and dilation\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33714\n\nDifferential Revision: D20097021\n\nPulled By: ngimel\n\nfbshipit-source-id: fc1c2d918b32f4b45c7e6e6bd93f018e867a628f", "pr_number": "33714", "files_changed": ["torch/nn/modules/pooling.py"], "labels": ["merged", "open source", "triaged"]}, "9a5ea71380": {"title": "pad_packed_sequence: doc improvement (#33768)", "body": "Summary:\npad_packed_sequence:\n1. clarify that batch's order is restored to the original one\n2. add example\n\nThis is a follow up to https://github.com/pytorch/pytorch/issues/33746\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33768\n\nDifferential Revision: D20102792\n\nPulled By: ngimel\n\nfbshipit-source-id: 5ef511e5e3833edcb85cc01af0e92568b6d7a3cf", "pr_number": "33768", "files_changed": ["torch/nn/utils/rnn.py"], "labels": ["merged", "open source"]}, "a836c4ca78": {"title": "Skip manual backward for `cdist` with case `p=2` (#31167)", "body": "Summary:\nFixes an issue with `cdist` backward calculation for large inputs for the euclidean case.\n\nThe grid size when launching the kernel exceeded the 2^16 limit for the second dimension, resulting in `RuntimeError: CUDA error: invalid configuration argument`\n\nCode to reproduce:\n\n```\nh, w, d = 800, 1216, 12\nn = 133\nA = torch.randn(n, d).cuda()\nB = torch.randn(h, w, d).cuda()\nA.requires_grad = True\nB.requires_grad = True\n\nB = B.reshape(-1, d).contiguous()\ndist = torch.cdist(A, B)\nloss = dist.sum()\nloss.backward()\n```\n\nThanks to tkerola for the bug report, reproduction and suggesting a solution.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31167\n\nDifferential Revision: D20035605\n\nPulled By: ngimel\n\nfbshipit-source-id: ae28ba4b549ee07a8bd937bb1de2438dc24eaa17", "pr_number": "31167", "files_changed": ["aten/src/ATen/native/Distance.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_autograd.py", "tools/autograd/derivatives.yaml"], "labels": ["merged", "open source", "triaged"]}, "fc6a153688": {"title": "[WIP] Reanimate gradient scaling API with original scale update heuristic (#33366)", "body": "Summary:\nAlso, windows memory failures responsible for the earlier reversion have been fixed.\n\nThis PR (initially) contains 2 commits:\n* a revert of the revert\n* all changes to implement the original Apex scale update heuristic, squashed into a single commit for easier diff review\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33366\n\nDifferential Revision: D20099026\n\nPulled By: ngimel\n\nfbshipit-source-id: 339b9b6bd5134bf055057492cd1eedb7e4461529", "pr_number": "33366", "files_changed": ["aten/src/ATen/native/cuda/AmpKernels.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/amp.rst", "docs/source/index.rst", "docs/source/notes/amp_examples.rst", "test/test_cuda.py", "torch/cuda/__init__.py", "torch/cuda/amp/__init__.py", "torch/cuda/amp/grad_scaler.py"], "labels": ["merged", "open source"]}, "758ad516f3": {"title": "[Lite interpreter] Pass shared_ptr properly (#33667)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33667\n\nPass shared_ptr properly according to C++ guidances. Thank kimishpatel for pointing it out.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20111001\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 213a0f950a7f3b9199d789dc0155911f6102d77a", "pr_number": "33667", "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/module.h"], "labels": ["jit", "merged"]}, "24659d28a1": {"title": "Feature/vonmises upstream (#33418)", "body": "Summary:\nThird try of https://github.com/pytorch/pytorch/issues/33177 \ud83d\ude04\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33418\n\nDifferential Revision: D20069683\n\nPulled By: ezyang\n\nfbshipit-source-id: f58e45e91b672bfde2e41a4480215ba4c613f9de", "pr_number": "33418", "files_changed": ["docs/source/distributions.rst", "test/test_distributions.py", "torch/distributions/__init__.py", "torch/distributions/von_mises.py"], "labels": ["merged", "module: distributions", "open source", "triaged"]}, "c1dd70688a": {"title": "Fix deprecated python \"add\" calls (#33428)", "body": "Summary:\nThis PR fixed those python \"add\" calls using deprecated signature `add(Scalar, Tensor)`. The alternative signature `add(Tensor, alpha = Scalar)` is used.\n\ncc csarofeen zasdfgbnm ptrblck ngimel\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33428\n\nDifferential Revision: D20002534\n\nPulled By: vincentqb\n\nfbshipit-source-id: 81f2dd6170a47a9b53a17e5817c26e70d8afa130", "pr_number": "33428", "files_changed": ["torch/_tensor_docs.py", "torch/nn/modules/_functions.py", "torch/optim/adadelta.py", "torch/optim/adagrad.py", "torch/optim/adam.py", "torch/optim/adamax.py", "torch/optim/adamw.py", "torch/optim/asgd.py", "torch/optim/lbfgs.py", "torch/optim/rmsprop.py", "torch/optim/rprop.py", "torch/optim/sgd.py"], "labels": ["merged", "module: optimizer", "open source"]}, "f87b0b2515": {"title": "Remove the use of macros in defining binary ops for base Vec256 (#33733)", "body": "Summary:\nThis greatly improves readability and maintainability (e.g., debugging)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33733\n\nDifferential Revision: D20103187\n\nPulled By: ezyang\n\nfbshipit-source-id: e539e46f5d378a2b01da7ecaa6b850655e0fa866", "pr_number": "33733", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h"], "labels": ["merged", "open source", "triaged"]}, "2eb95d8f4a": {"title": "Migrate `fmod` and `fmod_` from TH to ATen (CPU) (#33592)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/24701\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33592\n\nDifferential Revision: D20043875\n\nPulled By: ezyang\n\nfbshipit-source-id: b8c0a4e73a3cef6e55e91bbd35f8aadca8114c56", "pr_number": "33592", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h"], "labels": ["merged", "open source", "triaged"]}, "a8e7ed48f4": {"title": "[pt][quant] Parallelize quantize and dequantize (#33765)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33765\n\nquantize and dequantize methods now make use of multiple threads. This makes use of shz0116's recent parallelization of quantize/dequantize routines in FBGEMM.\n\nFixes:\nhttps://github.com/pytorch/pytorch/issues/32006\nhttps://github.com/pytorch/FBGEMM/issues/142\n\nAlternative to https://github.com/pytorch/pytorch/pull/30153\n\n```\n#!/usr/bin/env python\n\nimport time\nimport torch\nimport torch.nn as nn\ntorch.set_num_threads(4)\n# print(torch.__config__.parallel_info())\n\nW = torch.rand(1, 54, 54, 256)\n\nNITER = 1000\ns = time.time()\nfor i in range(NITER):\n    W_q = torch.quantize_per_tensor(W, scale=1.0, zero_point = 0, dtype=torch.quint8)\ntime_per_iter = (time.time() - s) / NITER\n\nprint('quantize time per iter ms', time_per_iter * 1000)\n\ns = time.time()\nfor i in range(NITER):\n    W_deq = W_q.dequantize()\ntime_per_iter = (time.time() - s) / NITER\n\nprint('dequantize time per iter ms', time_per_iter * 1000)\n```\n\n### With 1 thread\nquantize time per iter ms 0.22633790969848633\ndequantize time per iter ms 0.6573665142059326\n\n### With 4 threads\nquantize time per iter ms 0.0905618667602539\ndequantize time per iter ms 0.19511842727661133\nghstack-source-id: 98935895\n\nTest Plan: python test/test_quantized.py\n\nReviewed By: jspark1105\n\nDifferential Revision: D20098521\n\nfbshipit-source-id: bd8c45761b4651fcd5b20b95759e3868a136c048", "pr_number": "33765", "files_changed": ["aten/src/ATen/quantized/Quantizer.cpp"], "labels": ["merged"]}, "b8f0acf50f": {"title": "Fix examples with updated pruning naming convention (#33144)", "body": "Summary:\nFix in docs requested by vainaijr.\nCloses issue https://github.com/pytorch/pytorch/issues/32991\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33144\n\nDifferential Revision: D20104640\n\nPulled By: albanD\n\nfbshipit-source-id: 9b1be2c1cbde1964967967a9581bb6932a305d81", "pr_number": "33144", "files_changed": ["torch/nn/utils/prune.py"], "labels": ["merged"]}, "f597ac6efc": {"title": "Fix grid_sample gradients at image borders (#32829)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/23925\n\nThis fixes the incorrect gradients returned by `F.grid_sample` at image borders under `\"border\"` and `\"reflection\"` padding modes.\n\nAt nondifferentiable points, the choice of which gradient to return among its super- or subgradients is rather arbitrary and generally does not affect training. Before this change, however, a bug in the code meant that the gradient returned at the exact borders was not selected from among the super- or subgradients.\n\nThe gradient is now set to zero at the borders, which is a defensible choice for both the `\"border\"` and `\"reflection\"` padding modes:\n* For `\"border\"` padding, this effectively means that the exact borders of the image are now considered out of bounds, and therefore receive zero gradient.\n* For `\"reflection\"` padding, this effectively treats the exact borders as extrema.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32829\n\nDifferential Revision: D20118564\n\nPulled By: soumith\n\nfbshipit-source-id: ef8571ff585be35ab1b90a922af299f53ab9c095", "pr_number": "32829", "files_changed": ["aten/src/ATen/native/GridSampler.h", "aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "aten/src/ATen/native/cuda/GridSampler.cuh", "test/test_nn.py"], "labels": ["merged", "open source", "triaged"]}, "45e4b614d1": {"title": "Per channel quantization performance improvement (#33772)", "body": "Summary:\nBenchmark:\nNVIDIA GTX 1650 + AMD Ryzen Threadripper 3970X\n```python\nimport torch\nprint(torch.__version__)\n\nfor i in range(1000):\n    torch.randn(1024 * 128, device='cuda')\n\ndef cuda(e):\n    a = torch.randn(2 ** e, 32, device='cuda')\n    s = torch.randn(32, device='cuda')\n    z = torch.randn(32, device='cuda')\n    torch.cuda.synchronize()\n    %timeit torch.fake_quantize_per_channel_affine(a, s, z, 1, -999, 999); torch.cuda.synchronize()\n\ndef cpu(e):\n    a = torch.randn(2 ** e, 32, device='cpu')\n    s = torch.randn(32, device='cpu')\n    z = torch.randn(32, device='cpu')\n    %timeit torch.fake_quantize_per_channel_affine(a, s, z, 1, -999, 999);\n\nfor i in range(10, 24):\n    cuda(i)\nprint()\nfor i in range(10, 32):\n    cpu(i)\n```\nBefore\n```\n1.5.0a0+9bc922d\n849 \u00b5s \u00b1 44.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n817 \u00b5s \u00b1 30.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n814 \u00b5s \u00b1 2.93 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.11 ms \u00b1 1.32 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.19 ms \u00b1 4.19 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.6 ms \u00b1 5.58 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n2.44 ms \u00b1 14.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n4.14 ms \u00b1 2.55 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n7.41 ms \u00b1 2.46 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n13.9 ms \u00b1 2.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n26.9 ms \u00b1 254 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n52.6 ms \u00b1 260 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n104 ms \u00b1 176 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n207 ms \u00b1 1.24 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n249 \u00b5s \u00b1 158 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n420 \u00b5s \u00b1 230 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n766 \u00b5s \u00b1 391 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.45 ms \u00b1 574 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n2.84 ms \u00b1 34.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n5.69 ms \u00b1 83 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n7.29 ms \u00b1 2.58 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n7.32 ms \u00b1 13.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n17.4 ms \u00b1 38.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n47.5 ms \u00b1 264 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n187 ms \u00b1 1.19 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n379 ms \u00b1 5.05 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n652 ms \u00b1 11.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n1.22 s \u00b1 4.58 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n2.34 s \u00b1 8.77 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n4.56 s \u00b1 7.15 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n8.97 s \u00b1 33.6 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n17.8 s \u00b1 32.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n35.2 s \u00b1 167 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n```\nAfter\n```\n1.5.0a0+a7ec8cc\n92.5 \u00b5s \u00b1 2.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n97.7 \u00b5s \u00b1 469 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n109 \u00b5s \u00b1 4.73 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n119 \u00b5s \u00b1 6.17 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n146 \u00b5s \u00b1 1.84 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n211 \u00b5s \u00b1 2.45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n347 \u00b5s \u00b1 4.18 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n624 \u00b5s \u00b1 14.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.17 ms \u00b1 16.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n2.25 ms \u00b1 48.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n4.43 ms \u00b1 220 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n8.51 ms \u00b1 44.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n16.9 ms \u00b1 30.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n33.7 ms \u00b1 7.64 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n201 \u00b5s \u00b1 234 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n285 \u00b5s \u00b1 465 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n287 \u00b5s \u00b1 214 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n287 \u00b5s \u00b1 221 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n287 \u00b5s \u00b1 761 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n347 \u00b5s \u00b1 399 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n675 \u00b5s \u00b1 213 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.34 ms \u00b1 643 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n4.82 ms \u00b1 34.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n10.7 ms \u00b1 88.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n20.3 ms \u00b1 25.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n39.4 ms \u00b1 242 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n78.8 ms \u00b1 2 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n153 ms \u00b1 786 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n285 ms \u00b1 911 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n541 ms \u00b1 1.09 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n1.03 s \u00b1 1.67 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n1.97 s \u00b1 8.59 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n3.81 s \u00b1 10.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n```\n\nFixes https://github.com/pytorch/pytorch/issues/33647\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33772\n\nDifferential Revision: D20112531\n\nPulled By: ngimel\n\nfbshipit-source-id: f90e3ef1b5be8276851637f3e1251cb8f1af411f", "pr_number": "33772", "files_changed": ["aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu", "aten/src/ATen/native/quantized/fake_quant_affine.h", "aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp", "aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp", "test/test_fake_quant.py", "torch/quantization/fake_quantize.py"], "labels": ["merged", "open source"]}, "93e30c16cb": {"title": ".circleci: Switch to using robot token for conda uploads (#33786)", "body": "Summary:\nThanks to pjh5 for continued use of his account to upload binaries but I\nthink we can start using a bot account now for this.\n\nJust a draft until we can ensure the env variables get injected correctly and the token can actually upload\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33786\n\nDifferential Revision: D20122423\n\nPulled By: seemethere\n\nfbshipit-source-id: 0444584831a40ae730325d258935f6d1b873961b", "pr_number": "33786", "files_changed": [".circleci/scripts/binary_linux_upload.sh", ".circleci/scripts/binary_macos_upload.sh"], "labels": ["merged"]}, "8aa09de19e": {"title": "build: set -DNDEBUG in Release (#32719)", "body": "Summary:\nThis might lead to silent undefined behaviour (e.g. with out-of-bound indices). This affects `test_multinomial_invalid_probs_cuda` which is now removed.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32719\n\nTest Plan:\n* Build with VERBOSE=1 and manually inspect `less ndebug.build.log | grep 'c++' | grep -v -- -DNDEBUG` (only with nina on Linux)\n* CI\n\nFixes https://github.com/pytorch/pytorch/issues/22745\n\nDifferential Revision: D20104340\n\nPulled By: yf225\n\nfbshipit-source-id: 2ebfd7ddae632258a36316999eeb5c968fb7642c", "pr_number": "32719", "files_changed": ["aten/src/ATen/native/cuda/MultinomialKernel.cu", "c10/macros/Macros.h", "cmake/Dependencies.cmake"], "labels": ["merged", "open source", "triaged"]}, "5c33d98b0d": {"title": "Add assert_tensor_equal and assert_tensor_not_equal to test/cpp/api/support.h (#30426)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30426\n\nThis PR adds `assert_tensor_equal` and `assert_tensor_not_equal` to `test/cpp/api/support.h`, as better functions for testing whether two tensors are equal / not equal.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18695900\n\nPulled By: yf225\n\nfbshipit-source-id: c19b9bc4c4e84d9f444015023649d27618fcbdf5", "pr_number": "30426", "files_changed": ["test/cpp/api/support.h"], "labels": ["merged"]}, "c32fa465a5": {"title": "Preserve Backward compatibility of models serialized before #31040 (#33796)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33796\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20109662\n\nPulled By: jerryzh168\n\nfbshipit-source-id: 9bc936a59fd6dd1031fbf05eb90f98ae9677b936", "pr_number": "33796", "files_changed": ["test/test_quantized_tensor.py", "torch/_utils.py"], "labels": ["merged"]}, "867990dc17": {"title": "[jit] Unify augmented assign handling (#33578)", "body": "Summary:\nStacked PRs\n * **#33578 - [jit] Unify augmented assign handling**\n * #32993 - [jit] Fix aug assign for non-tensor attributes\n\nWe handle augmented assignments to `Select` and `Var` statements differently, but the actual in place update is the same for both, so this PR factors it out into a method so we don't have 2 code paths doing the same thing.\n](https://our.intern.facebook.com/intern/diff/20010383/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33578\n\nPulled By: driazati\n\nDifferential Revision: D20010383\n\nfbshipit-source-id: 52e559ce907e95e5c169ab9d9690d0d235db36f3", "pr_number": "33578", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/ir_emitter.cpp"], "labels": ["jit", "merged"]}, "51e405743f": {"title": "Revert D20010383: [jit] Unify augmented assign handling", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20010383\n\nOriginal commit changeset: 52e559ce907e\n\nfbshipit-source-id: 7ca938070d5e98c91e7a7b8485a3c1e790c3ceb2", "pr_number": null, "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/ir_emitter.cpp"], "labels": []}, "04f88a3a7b": {"title": "Add partition info message to NetDef (#33616)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33616\n\nAtt. We start by assign `node_name` of DeviceOption in each of the op in the net. The for each unique node_name, we will have a PartitionInfo describing the partition, including logic devices that it can be assigned and we establish the link by partition names.\n\nTest Plan:\nunittests\n\nCanaries:\nAF: https://our.intern.facebook.com/intern/ads/canary/424817103900710410\nAI: https://our.intern.facebook.com/intern/ads/canary/424737510862189908\n\nReviewed By: ipiszy, bangshengtang, jfix71\n\nDifferential Revision: D20015493\n\nfbshipit-source-id: 0bb0f30cfc3892f7b8709d87b8bc1fbab2f2c46d", "pr_number": "33616", "files_changed": ["caffe2/proto/caffe2.proto"], "labels": ["fb-exported", "merged"]}, "02908dfa67": {"title": "remove setStorage with null StorageImpl support. (#33735)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33735\n\nThis apparently used to create a new storage, but I couldn't find anywhere in the code where this actually happens.\n\nChanging it to an assert to see what happens.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20084029\n\nPulled By: gchanan\n\nfbshipit-source-id: e9c4db115a25fc2e17a3b166c1ff5a0e6b56d690", "pr_number": "33735", "files_changed": ["aten/src/TH/THTensor.cpp", "aten/src/TH/generic/THTensor.cpp", "aten/src/THC/THCTensor.cpp", "aten/src/THC/generic/THCTensor.cpp"], "labels": ["merged"]}, "a0e90e1b45": {"title": "ONNX Error Message on Missing Op (#33593)", "body": "Summary:\nPrint a complete and comprehensive error message with a description of the issue when an op is missing during ONNX export (previously an ambiguous \"key not in registry\" error was thrown which was not helpful for the user to understand the failure).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33593\n\nReviewed By: hl475\n\nDifferential Revision: D20052213\n\nPulled By: houseroad\n\nfbshipit-source-id: ae3010a97efdab26effad5e4a418e9cc41f5b04e", "pr_number": "33593", "files_changed": ["torch/onnx/symbolic_registry.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "cd0acf4374": {"title": "port masked_fill from TH to ATen (#33330)", "body": "Summary:\nport `masked_fill` from TH to ATen with TensorIterator.\n\nsingle core performance roughly stays the same, single socket performance has **3~16x** boost.\n\n`masked_fill` is missing from https://github.com/pytorch/pytorch/issues/24507\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33330\n\nDifferential Revision: D20098812\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: ff20712ffc00cc665550997abcfdfb8916c18e40", "pr_number": "33330", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/LegacyDefinitions.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.h"], "labels": ["merged", "open source", "topic: porting", "triaged"]}, "6eef66e1f4": {"title": ".circleci: Divert packages to test channel on tag (#33842)", "body": "Summary:\nThis sets up PIP_UPLOAD_FOLDER to point to the correct channel for\nrelease candidates as opposed to nightlies.\n\nRemoves an old safety check that's not needed anymore for devtoolset3\n\nAnd provides a nice default for PIP_UPLOAD_FOLDER, which should clear up\nconfusion on where it's initially set\n\nThis is a stepping stone towards the promotable pipeline.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33842\n\nDifferential Revision: D20130791\n\nPulled By: seemethere\n\nfbshipit-source-id: dac94ef46299574c36c08c968dd36faddeae6363", "pr_number": "33842", "files_changed": [".circleci/scripts/binary_linux_upload.sh", ".circleci/scripts/binary_macos_upload.sh", ".circleci/scripts/binary_populate_env.sh"], "labels": ["merged", "releng"]}, "057fd5e10d": {"title": "add support for _modules, reducing special casing of nn.Sequential (#29495)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29495\n\nThis PR adds support for `_modules`, making it so we no longer need to special case support for `nn.Sequential`. I was getting internal errors around the previous approach using `self.define()`, so i am adding this PR as part of the stack.\n\nFix for https://github.com/pytorch/pytorch/issues/28998\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18412561\n\nPulled By: eellison\n\nfbshipit-source-id: a8b24ebee39638fccf63b2701f65f8bb0de84faa", "pr_number": "29495", "files_changed": ["test/test_jit.py", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/csrc/jit/script/python_sugared_value.h"], "labels": ["jit", "merged"]}, "fddf73250d": {"title": "[JIT] fix resolving of functions in torch/functional. fix compilation of torch.stft (#33504)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33504\n\nFix resolution fo functions that are bound onto torch in torch/functional.py. This does not fix compilation of all of those functions, those will be done in follow ups. Does torch.stft as a start.\n\nFixes #21478\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20014591\n\nPulled By: eellison\n\nfbshipit-source-id: bb362f1b5479adbb890e72a54111ef716679d127", "pr_number": "33504", "files_changed": ["test/test_jit.py", "torch/_VF.py", "torch/functional.py", "torch/jit/_builtins.py", "torch/jit/quantized.py", "torch/nn/_VF.py", "torch/nn/functional.py", "torch/nn/modules/rnn.py", "torch/nn/quantized/dynamic/modules/rnn.py", "torch/nn/utils/rnn.py"], "labels": ["jit", "merged"]}, "4543cf4eb1": {"title": "[JIT] add support for torch.lu to torchscript (#33724)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33724\n\nFix for https://github.com/pytorch/pytorch/issues/33381, partial fix of https://github.com/pytorch/pytorch/issues/30786\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20077321\n\nPulled By: eellison\n\nfbshipit-source-id: a1e6a0370712b36c9f66979098ac2f9d500ca5f6", "pr_number": "33724", "files_changed": ["docs/source/jit_unsupported.rst", "test/jit/test_unsupported_ops.py", "test/test_jit.py", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/functional.py", "torch/jit/_builtins.py"], "labels": ["jit", "merged"]}, "f31b1d3453": {"title": "[JIT] add support for lu_unpack (#33736)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33736\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20121914\n\nPulled By: eellison\n\nfbshipit-source-id: 1136f4d7678a2233129aefe3e30234af385b8353", "pr_number": "33736", "files_changed": ["docs/source/jit_unsupported.rst", "test/jit/test_unsupported_ops.py", "test/test_jit.py", "torch/functional.py", "torch/jit/_builtins.py"], "labels": ["jit", "merged"]}, "857eb4145e": {"title": "[JIT] add support for torch.cdist (#33737)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33737\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20121916\n\nPulled By: eellison\n\nfbshipit-source-id: b0427bbfd3ade1f3129c4a95a542fbc32c3abd76", "pr_number": "33737", "files_changed": ["docs/source/jit_unsupported.rst", "test/jit/test_unsupported_ops.py", "test/test_jit.py", "torch/functional.py", "torch/jit/_builtins.py"], "labels": ["jit", "merged"]}, "2448c97a53": {"title": "[jit] infer RRef type as container type (#33369)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33369\n\nThis PR add RRef type infer rule when we try to infer a type from a\npyobject, this allow script module attributes could contain a rref,\n(i.e. List[RRefs] as a module attribute)\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19918320\n\nPulled By: wanchaol\n\nfbshipit-source-id: e5fd99c0ba5693b22ed48f0c0550b5e1dac89990", "pr_number": "33369", "files_changed": ["torch/csrc/jit/pybind_utils.h", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["jit", "merged"]}, "d494986171": {"title": "[jit] make RRef type annotation available in Python (#33526)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33526\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19988848\n\nPulled By: wanchaol\n\nfbshipit-source-id: aeebc946d08b38dac0b656617bf395e86bcea558", "pr_number": "33526", "files_changed": ["torch/_jit_internal.py", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/python_ir.cpp", "torch/jit/annotations.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["jit", "merged"]}, "4dad00b64b": {"title": "[rpc] special case tensor type check when getting RRef (#33582)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33582\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20009837\n\nPulled By: wanchaol\n\nfbshipit-source-id: 7e9ab87d4dddb822c7575891a2b620eff83bfa00", "pr_number": "33582", "files_changed": ["torch/csrc/distributed/rpc/rref_context.cpp"], "labels": ["merged"]}, "150e025be8": {"title": "[jit] stop printing crap in test_jit (#33779)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33779\n\nThis should eliminate random warnings and print spew from test_jit.\n\nIt also fixes a bug where we weren't properly comparing captured outputs\n(!)\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20124224\n\nPulled By: suo\n\nfbshipit-source-id: 9241d21fdf9470531b0437427b28e325cdf08d3a", "pr_number": "33779", "files_changed": ["test/jit/test_class_type.py", "test/jit/test_export_modes.py", "test/jit/test_models.py", "test/jit/test_recursive_script.py", "test/jit/test_unsupported_ops.py", "test/test_jit.py", "torch/onnx/utils.py", "torch/testing/_internal/jit_utils.py"], "labels": ["jit", "merged"]}, "2b9fa4a756": {"title": "[jit] Fix flipped PackedSequence outputs in script (#32955)", "body": "Summary:\nStacked PRs\n * **#32955 - [jit] Fix flipped PackedSequence outputs in script**\n * #32953 - [jit] Support properties on `Device`\n\nFixes #32605\n](https://our.intern.facebook.com/intern/diff/20103905/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32955\n\nPulled By: driazati\n\nDifferential Revision: D20103905\n\nfbshipit-source-id: 84081213ed214846e563b9f05bcab0210bb1a71b", "pr_number": "32955", "files_changed": ["test/test_jit.py", "torch/nn/utils/rnn.py"], "labels": ["merged"]}, "4c33222c51": {"title": "[quant][graphmode] Replicate dequantize nodes (#33531)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33531\n\nWe already insert dequantize for each use of the value, but there might still be cases where we only\nsee the value is used multiple times after inline. This pass adds the support to replicate dequantize\nafter inline to ensure output of dequantize is only used by one node, which is necessary to preserve all\nquantization patterns like `dequant - conv - quant`\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D20123591\n\nfbshipit-source-id: 6edb10a4566538bcf9379d332233f870372b7a63", "pr_number": "33531", "files_changed": ["test/test_jit.py", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization.h"], "labels": ["jit", "merged"]}, "24dd800e6a": {"title": "[Dist Autograd] Functional API for Dist Autograd and Dist Optimizer (#33711)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33711\n\nFixed #33480\n\nThis makes `dist_autograd.backward` and `dist_optimizer.step` functional by making the user explicitly pass in the `context_id` as opposed to relying on the confusing thread_local context_id.\n\nThis diff incorporates these API changes and all places where these functions are called.\n\nMore concretely, this code:\n\n```\nwith dist_autograd.context():\n    # Forward pass.\n    dist_autograd.backward([loss.sum()])\n    dist_optim.step()\n```\n\nshould now be written as follows:\n\n```\nwith dist_autograd.context() as context_id:\n    # Forward pass.\n    dist_autograd.backward(context_id, [loss.sum()])\n    dist_optim.step(context_id)\n```\n\nTest Plan: Ensuring all existing dist_autograd and dist_optimizer tests pass with the new API. Also added a new test case for input checking.\n\nDifferential Revision: D20011710\n\nfbshipit-source-id: 216e12207934a2a79c7223332b97c558d89d4d65", "pr_number": "33711", "files_changed": ["docs/source/notes/distributed_autograd.rst", "test/cpp/dist_autograd/test_dist_autograd.cpp", "torch/csrc/distributed/autograd/context/container.cpp", "torch/csrc/distributed/autograd/context/container.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h", "torch/csrc/distributed/autograd/init.cpp", "torch/distributed/autograd/__init__.py", "torch/distributed/optim/optimizer.py", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/dist_optimizer_test.py", "torch/testing/_internal/distributed/rpc/jit/dist_autograd_test.py"], "labels": ["fb-exported", "merged"]}, "cea0cc8ca8": {"title": "[jit] Unify augmented assign handling (#33578)", "body": "Summary:\nStacked PRs\n * **#33578 - [jit] Unify augmented assign handling**\n * #32993 - [jit] Fix aug assign for non-tensor attributes\n\nWe handle augmented assignments to `Select` and `Var` statements differently, but the actual in place update is the same for both, so this PR factors it out into a method so we don't have 2 code paths doing the same thing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33578\n\nPulled By: driazati\n\nDifferential Revision: D20127647\n\nfbshipit-source-id: 94f37acbd2551498de9d2ca09a514508266f7d31", "pr_number": "33578", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/ir_emitter.cpp"], "labels": ["jit", "merged"]}, "421e3e9a54": {"title": "Release GIL for RPC pybind functions. (#33610)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33610\n\nOur pybind definitions for several RPC functions didn't release GIL\nonce we were processing stuff in C++.\n\nThis PR adds asserts that we release GIL appropriately and adds\npy::gil_scoped_release and py::gil_scoped_acquire in the appropriate places.\nghstack-source-id: 99066749\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D20025847\n\nfbshipit-source-id: 57a778cba0336cf87352b07c89bbfb9254c4bdd7", "pr_number": "33610", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp"], "labels": ["merged"]}, "84101f353e": {"title": "Avoid problematic pickle usages on Python 3.8.0 and 3.8.1 (#33824)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/32289\n\nThis has been fixed upstream as of Python 3.8.2. I think the easiest and least invasive way to ameliorate this is to catch the error condition and print a more informative error asking the user to update their Python version. It might be possible to buffer the data into memory and then read from memory, but that would be an invasive change and might cause memory exhaustion for very large models.\n\nSuggestions for alternate fixes or ways to improve the error message wording are very welcome.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33824\n\nDifferential Revision: D20131722\n\nPulled By: ezyang\n\nfbshipit-source-id: a6e3fbf4bf7f9dcce5772b36f7a622cbf14b5ae4", "pr_number": "33824", "files_changed": ["test/test_serialization.py", "torch/serialization.py"], "labels": ["merged", "open source"]}, "ca002a0f6b": {"title": "Switch empty_like to use merge_in to process TensorOptions. (#33505)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33505\n\nThis shouldn't change semantics, but it has the benefit of making\ntorch::empty_like(x, dtype(kFloat)) actually work (previously, this\nwould just ignore all of the properties from x).\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20001776\n\nPulled By: ezyang\n\nfbshipit-source-id: ba81186d3293abc65da6130b2684d42e9e675208", "pr_number": "33505", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/test/basic.cpp"], "labels": ["merged"]}, "d41c8d0461": {"title": "Correctly preserve \"not set anywhere\" TensorOptions when merging. (#33510)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33510\n\nPreviously, we would fill in TensorOption with defaults whenever an\nitem was missing from both the left and right side of the merge.  This\nis morally incorrect: if we don't have an item on the left or right,\nwe should keep the entry empty (so the downstream user can apply\nthe appropriate defaulting rule).\n\nI don't think this caused any bugs, but I noticed this error when\nworking on a later patch in my diff stack.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20001775\n\nPulled By: ezyang\n\nfbshipit-source-id: 88139fc268b488cd1834043584a0d73f46c8ecaa", "pr_number": "33510", "files_changed": ["c10/core/TensorOptions.h"], "labels": ["merged"]}, "00f685d2d8": {"title": "Add Scalar::type() (#33603)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33603\n\nThis function returns ScalarType based on its value. This is helpful\nto avoid code generated in aten_op.h has returned Scalars depending on\narg self to determine its type.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20100218\n\nPulled By: ezyang\n\nfbshipit-source-id: 337729a7559e6abb3a16b2a563a2b92aa96c7016", "pr_number": "33603", "files_changed": ["c10/core/Scalar.h", "caffe2/contrib/aten/gen_op.py"], "labels": ["merge-this-please", "merged", "open source", "triaged"]}, "9733711394": {"title": "[JIT] Support calling Tensor.element_size() in TorchScript (#33808)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33808\n\n# Problem\n\nhttps://github.com/pytorch/pytorch/issues/33620\nghstack-source-id: 99073701\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test:jit -- test_numel\n\nbuck test mode/dev-nosan //caffe2/test:jit -- test_element_size\n\nbuck build mode/dev-nosan //caffe2/test:jit \\\n&& buck-out/gen/caffe2/test/jit\\#binary.par -r test_numel\n\nbuck build mode/dev-nosan //caffe2/test:jit \\\n&& buck-out/gen/caffe2/test/jit\\#binary.par -r test_element_size\n```\n\nCompile error\n\nP126667043\n\nGenerated code,\n```\nbuck-out/dev/gen/caffe2/generate-code=register_aten_ops_0.cpp/register_aten_ops_0.cpp\n\nbuck-out/dev/gen/caffe2/generate-code=register_aten_ops_2.cpp/register_aten_ops_2.cpp\n```\nP126667064\n\nDifferential Revision: D7050644\n\nfbshipit-source-id: 20dbdb9c500b6d7683c23e3049d43ed0ca06d831", "pr_number": "33808", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/templates/TensorBody.h", "test/test_jit.py", "tools/jit/gen_jit_dispatch.py"], "labels": ["jit", "merged"]}, "f5952cf7cb": {"title": "fix lint (#33861)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33861\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20136865\n\nPulled By: suo\n\nfbshipit-source-id: 4bf7ac324a0abce9b45121ac5ab438448a6f3149", "pr_number": "33861", "files_changed": ["test/jit/test_export_modes.py", "torch/nn/utils/rnn.py"], "labels": ["jit", "merged"]}, "095de1e872": {"title": "Migrate `random_` from the TH to Aten (CPU and CUDA) (#33663)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33663\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20056350\n\nPulled By: pbelevich\n\nfbshipit-source-id: f9859b79ffdec70c48d6ee3ec70fd6fad593a9f5", "pr_number": "33663", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/Dispatch.h", "aten/src/ATen/native/DistributionTemplates.h", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/DistributionTemplates.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/DistributionTemplates.h", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/cpu_rng_test.cpp", "aten/src/ATen/test/rng_test.cpp", "aten/src/ATen/test/rng_test.h", "aten/src/TH/generic/THTensorRandom.cpp", "aten/src/TH/generic/THTensorRandom.h", "test/test_nn.py", "test/test_torch.py", "tools/autograd/derivatives.yaml"], "labels": ["merged"]}, "b10761d890": {"title": "fix type stub errors (#33762)", "body": "Summary:\nI've been using pytorch with type hintings, and I found errors that can be easily fixed. So I'm creating this PR to fix type bugs.\n\nI expected below code should be type-checked without any errors.\n\n```python\nimport torch\nfrom torch.nn import Linear\nfrom torch.autograd import Variable\nfrom torch.optim import AdamW\nfrom torch.utils import hooks\n\n# nn.Module should have training attribute\nmodule = Linear(10, 20)\nmodule.training\n\n# torch should have dtype bfloat16\ntensor2 = torch.tensor([1,2,3], dtype=torch.bfloat16)\n\n# torch.Tensor.cuda should accept int or str value\ntorch.randn(5).cuda(1)\ntorch.tensor(5).cuda('cuda:0')\n\n# optimizer should have default attribute\nmodule = Linear(10, 20)\nprint(AdamW(module.weight).default)\n\n# torch.Tensor should have these boolean attributes\ntorch.tensor([1]).is_sparse\ntorch.tensor([1]).is_quantized\ntorch.tensor([1]).is_mkldnn\n\n# Size class should tuple of int\na, b = torch.tensor([[1,2,3]]).size()\n\n# check modules can be accessed\ntorch.nn.parallel\ntorch.autograd.profiler\ntorch.multiprocessing\ntorch.sparse\ntorch.onnx\ntorch.jit\ntorch.hub\ntorch.random\ntorch.distributions\ntorch.quantization\ntorch.__config__\ntorch.__future__\n\ntorch.ops\ntorch.classes\n\n# Variable class's constructor should return Tensor\ndef fn_to_test_variable(t: torch.Tensor):\n    return None\n\nv = Variable(torch.tensor(1))\nfn_to_test_variable(v)\n\n# check RemovableHandle attributes can be accessed\nhandle = hooks.RemovableHandle({})\nhandle.id\nhandle.next_id\n\n# check torch function hints\ntorch.is_grad_enabled()\n```\n\nBut current master branch raises errors. (I checked with pyright)\n\n```\n$ pyright test.py\nSearching for source files\nFound 1 source file\ntest.py\n  12:45 - error: 'bfloat16' is not a known member of module\n  15:21 - error: Argument of type 'Literal[1]' cannot be assigned to parameter 'device' of type 'Optional[device]'\n  'int' is incompatible with 'device'\n  Cannot assign to 'None'\n  16:22 - error: Argument of type 'Literal['cuda:0']' cannot be assigned to parameter 'device' of type 'Optional[device]'\n  'str' is incompatible with 'device'\n  Cannot assign to 'None'\n  23:19 - error: Cannot access member 'is_sparse' for type 'Tensor'\n  Member 'is_sparse' is unknown\n  24:19 - error: Cannot access member 'is_quantized' for type 'Tensor'\n  Member 'is_quantized' is unknown\n  25:19 - error: Cannot access member 'is_mkldnn' for type 'Tensor'\n  Member 'is_mkldnn' is unknown\n  32:7 - error: 'autograd' is not a known member of module\n  33:7 - error: 'multiprocessing' is not a known member of module\n  34:7 - error: 'sparse' is not a known member of module\n  35:7 - error: 'onnx' is not a known member of module\n  36:7 - error: 'jit' is not a known member of module\n  37:7 - error: 'hub' is not a known member of module\n  38:7 - error: 'random' is not a known member of module\n  39:7 - error: 'distributions' is not a known member of module\n  40:7 - error: 'quantization' is not a known member of module\n  41:7 - error: '__config__' is not a known member of module\n  42:7 - error: '__future__' is not a known member of module\n  44:7 - error: 'ops' is not a known member of module\n  45:7 - error: 'classes' is not a known member of module\n  60:7 - error: 'is_grad_enabled' is not a known member of module\n20 errors, 0 warnings\nCompleted in 1.436sec\n```\n\nand below list is not checked as errors, but I think these are errors too.\n\n* `nn.Module.training` is not boolean\n* return type of `torch.Tensor.size()` is `Tuple[Unknown]`.\n\n ---\n\nrelated issues.\n\nhttps://github.com/pytorch/pytorch/issues/23731, https://github.com/pytorch/pytorch/issues/32824, https://github.com/pytorch/pytorch/issues/31753\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33762\n\nDifferential Revision: D20118884\n\nPulled By: albanD\n\nfbshipit-source-id: 41557d66674a11b8e7503a48476d4cdd0f278eab", "pr_number": "33762", "files_changed": ["tools/pyi/gen_pyi.py", "torch/__init__.pyi.in", "torch/autograd/__init__.pyi", "torch/nn/__init__.pyi", "torch/nn/modules/module.pyi.in", "torch/nn/quantized/functional.py", "torch/optim/optimizer.pyi", "torch/utils/hooks.pyi"], "labels": ["merged", "open source"]}, "bd77abffe3": {"title": "Kill some unused (TH)Storage-based APIs. (#33815)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33815\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20119333\n\nPulled By: gchanan\n\nfbshipit-source-id: 15042ca0fabdc88b53d662b6dd964968f64997f4", "pr_number": "33815", "files_changed": ["aten/src/TH/generic/THTensor.cpp", "aten/src/TH/generic/THTensor.h", "aten/src/THC/generic/THCTensor.cpp", "aten/src/THC/generic/THCTensor.h"], "labels": ["merged"]}, "6647a44e8c": {"title": "Automatic update of fbcode/onnx to 9fdae4c68960a2d44cd1cc871c74a6a9d469fa1f (#33858)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33858\n\nPrevious import was 04a29addfd5b912812addb8dea5f8763fbfaad01\n\nIncluded changes:\n- **[9fdae4c6](https://github.com/onnx/onnx/commit/9fdae4c6)**: Copy sizes in some optimizers to remain shape information (#2574) <daquexian>\n- **[c978d102](https://github.com/onnx/onnx/commit/c978d102)**: Implement CELU node as a Function (#2575) <Jeremy Cochoy>\n- **[c677aef4](https://github.com/onnx/onnx/commit/c677aef4)**: Fix CI build break (#2603) <Changming Sun>\n- **[d343755d](https://github.com/onnx/onnx/commit/d343755d)**: Allow function body to rely on other operator sets (#2597) <Ke Zhang>\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D20135343\n\nfbshipit-source-id: d719c4ba2ae26892a5fa921691c84eba64b59291", "pr_number": "33858", "files_changed": ["third_party/onnx"], "labels": ["fb-exported", "merged"]}, "afbd04449e": {"title": "[quant][graphmode] Swap dequantize after inline for ops that doesn't require observation (#33173)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33173\n\nHow to deal with ops that\u2019s defined for both floating point and quantized Tensor?\n\nCategory of ops: the ones that doesn\u2019t require observers, which means the quantization parameters(scale/zero_point) of the output of this op can be inferred from the quantization parameters of inputs.\nFor example:\navg_pool, max_pool, flatten, transpose, upsample\n\nAnother related topic to previous one is how do we deal with things like adaptive_avg_pool2d that does not require to be observed and it works with quantized tensor as well? If we insert quant/dequant for them, even the quant fusion becomes a numerically changing operation because the scale/zero_point for input and output are different.\n\nProposal\n\nWe can swap the operator with dequantize whenever we see it. For example, for pattern\nLet\u2019s say aten::general_op is defined for both floating point and quantized\n\n%r = aten::conv(...)\n%q = quantize(%r)\n%dq = dequantize(%q)\n%f = aten::general_op(%dq)\n...\n\nWe detect that all inputs of aten::general_op is produced by dequantize, we\u2019ll first delete all the dequantize for the inputs and then insert dequantize for each use of the output of the aten::general_op, note that this should work generally for all the case we might encounter.\n\nAfter transformation we\u2019ll have:\n\n%r = aten::conv(...)\n%q = quantize(%r)\n%x = aten::general_op(%q)\n%f = dequantize(%x)\n...\n\n1. Multiple inputs\n    1. We need to make sure all inputs of the aten::general_op are produced by dequantize before we do this transformation\n2. Input used by multiple operators\n    1. We already did this by inserting dequantize for each use of the value\n3. Output used by multiple operators\n    1. We\u2019ll reuse the code that inserts dequantize(might need some refactor)\n\nNote that current concat does not belong to this category right now since it does not inherit quantization parameters from inputs.\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D20123590\n\nfbshipit-source-id: de2febe1f37e4079457a23acaeccbc6d9c9e1f8a", "pr_number": "33173", "files_changed": ["test/test_jit.py", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization.h"], "labels": ["jit", "merged"]}, "dbe850af5b": {"title": "[jit] do the code reorg (#33851)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33851\n\nRationale and context described in #33828.\n\nScript to reproduce the move:\nhttps://gist.github.com/suo/16cbefaaeb67ca5a7c6caffd49b7f6e9\nghstack-source-id: 99079645\n\nTest Plan: Make sure CI passes\n\nReviewed By: jamesr66a\n\nDifferential Revision: D20133869\n\nfbshipit-source-id: 390e9241a9c85366d9005c492ac31f10aa96488e", "pr_number": "33851", "files_changed": ["android/pytorch_android/generate_test_asset.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "aten/src/ATen/core/CMakeLists.txt", "aten/src/ATen/core/boxing/kernel_function_legacy_test.cpp", "aten/src/ATen/core/boxing/kernel_function_test.cpp", "aten/src/ATen/core/boxing/kernel_functor_test.cpp", "aten/src/ATen/core/boxing/kernel_lambda_legacy_test.cpp", "aten/src/ATen/core/boxing/kernel_lambda_test.cpp", "aten/src/ATen/core/boxing/kernel_stackbased_test.cpp", "aten/src/ATen/core/custom_class.cpp", "aten/src/ATen/core/op_registration/op_registration.cpp", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/quantized/Quantizer.h", "aten/src/ATen/test/backend_fallback_test.cpp", "aten/src/ATen/test/extension_backend_test.cpp", "binaries/dump_operator_names.cc", "binaries/speed_benchmark_torch.cc", "caffe2/CMakeLists.txt", "caffe2/core/export_caffe2_op_to_c10.h", "caffe2/python/pybind_state.cc", "docs/cpp/source/Doxyfile", "docs/source/onnx.rst", "ios/TestApp/TestApp/Benchmark.mm", "setup.py", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_argument_spec.cpp", "test/cpp/jit/test_autodiff.cpp", "test/cpp/jit/test_base.h", "test/cpp/jit/test_class_import.cpp", "test/cpp/jit/test_class_parser.cpp", "test/cpp/jit/test_code_template.cpp", "test/cpp/jit/test_constant_pooling.cpp", "test/cpp/jit/test_custom_operators.cpp", "test/cpp/jit/test_fuser.cpp", "test/cpp/jit/test_graph_executor.cpp", "test/cpp/jit/test_inliner.cpp", "test/cpp/jit/test_interface.cpp", "test/cpp/jit/test_ir.cpp", "test/cpp/jit/test_irparser.cpp", "test/cpp/jit/test_jit_type.cpp", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/test_peephole_optimize.cpp", "test/cpp/jit/test_save_load.cpp", "test/cpp/jit/test_schema_matching.cpp", "test/cpp/jit/test_subgraph_matcher.cpp", "test/cpp/jit/test_subgraph_rewriter.cpp", "test/cpp/jit/test_utils.h", "tools/autograd/templates/python_torch_functions.cpp", "tools/autograd/templates/python_variable_methods.cpp", "tools/autograd/templates/variable_factories.h", "tools/build_variables.bzl", "tools/git-pre-commit", "tools/jit/templates/register_aten_ops.cpp", "torch/CMakeLists.txt", "torch/csrc/Exceptions.h", "torch/csrc/Module.cpp", "torch/csrc/Size.cpp", "torch/csrc/api/include/torch/jit.h", "torch/csrc/api/include/torch/serialize/input-archive.h", "torch/csrc/api/include/torch/serialize/output-archive.h", "torch/csrc/api/src/jit.cpp", "torch/csrc/api/src/serialize.cpp", "torch/csrc/api/src/serialize/input-archive.cpp", "torch/csrc/api/src/serialize/output-archive.cpp", "torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/functions/init.cpp", "torch/csrc/autograd/profiler.cpp", "torch/csrc/autograd/python_function.cpp", "torch/csrc/autograd/python_legacy_variable.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/autograd/symbolic.h", "torch/csrc/distributed/autograd/init.cpp", "torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_req.cpp", "torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_remote_call.cpp", "torch/csrc/distributed/rpc/python_remote_call.h", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_proto.cpp", "torch/csrc/distributed/rpc/rref_proto.h", "torch/csrc/distributed/rpc/script_call.cpp", "torch/csrc/distributed/rpc/script_call.h", "torch/csrc/distributed/rpc/script_remote_call.cpp", "torch/csrc/distributed/rpc/script_remote_call.h", "torch/csrc/distributed/rpc/script_resp.cpp", "torch/csrc/distributed/rpc/script_resp.h", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/jit/api/compilation_unit.h", "torch/csrc/jit/api/custom_class.cpp", "torch/csrc/jit/api/custom_class.h", "torch/csrc/jit/api/function.cpp", "torch/csrc/jit/api/function.h", "torch/csrc/jit/api/method.h", "torch/csrc/jit/api/module.cpp", "torch/csrc/jit/api/module.h", "torch/csrc/jit/api/module_save.cpp", "torch/csrc/jit/api/object.cpp", "torch/csrc/jit/api/object.h", "torch/csrc/jit/argument_spec.cpp", "torch/csrc/jit/argument_spec.h", "torch/csrc/jit/attributes.cpp", "torch/csrc/jit/attributes.h", "torch/csrc/jit/autodiff.cpp", "torch/csrc/jit/autodiff.h", "torch/csrc/jit/catch_utils.hpp", "torch/csrc/jit/code_template.h", "torch/csrc/jit/codegen/fuser/README.md", "torch/csrc/jit/codegen/fuser/arg_spec.h", "torch/csrc/jit/codegen/fuser/codegen.cpp", "torch/csrc/jit/codegen/fuser/codegen.h", "torch/csrc/jit/codegen/fuser/compiler.cpp", "torch/csrc/jit/codegen/fuser/compiler.h", "torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/codegen/fuser/cpu/fused_kernel.h", "torch/csrc/jit/codegen/fuser/cpu/resource_strings.h", "torch/csrc/jit/codegen/fuser/cpu/temp_file.h", "torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp", "torch/csrc/jit/codegen/fuser/cuda/fused_kernel.h", "torch/csrc/jit/codegen/fuser/cuda/resource_strings.h", "torch/csrc/jit/codegen/fuser/executor.cpp", "torch/csrc/jit/codegen/fuser/executor.h", "torch/csrc/jit/codegen/fuser/fallback.cpp", "torch/csrc/jit/codegen/fuser/fallback.h", "torch/csrc/jit/codegen/fuser/fused_kernel.h", "torch/csrc/jit/codegen/fuser/interface.cpp", "torch/csrc/jit/codegen/fuser/interface.h", "torch/csrc/jit/codegen/fuser/kernel_cache.cpp", "torch/csrc/jit/codegen/fuser/kernel_cache.h", "torch/csrc/jit/codegen/fuser/kernel_spec.h", "torch/csrc/jit/codegen/fuser/partition_desc.h", "torch/csrc/jit/codegen/fuser/tensor_desc.h", "torch/csrc/jit/codegen/fuser/tensor_info.h", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/constants.h", "torch/csrc/jit/custom_class.cpp", "torch/csrc/jit/custom_class.h", "torch/csrc/jit/custom_operator.h", "torch/csrc/jit/exception_message.h", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export.h", "torch/csrc/jit/export_module.cpp", "torch/csrc/jit/frontend/builtin_functions.cpp", "torch/csrc/jit/frontend/builtin_functions.h", "torch/csrc/jit/frontend/canonicalize_modified_loop.cpp", "torch/csrc/jit/frontend/canonicalize_modified_loop.h", "torch/csrc/jit/frontend/code_template.h", "torch/csrc/jit/frontend/concrete_module_type.cpp", "torch/csrc/jit/frontend/concrete_module_type.h", "torch/csrc/jit/frontend/convert_to_ssa.cpp", "torch/csrc/jit/frontend/convert_to_ssa.h", "torch/csrc/jit/frontend/edit_distance.cpp", "torch/csrc/jit/frontend/edit_distance.h", "torch/csrc/jit/frontend/error_report.cpp", "torch/csrc/jit/frontend/error_report.h", "torch/csrc/jit/frontend/exit_transforms.cpp", "torch/csrc/jit/frontend/exit_transforms.h", "torch/csrc/jit/frontend/function_schema_parser.cpp", "torch/csrc/jit/frontend/function_schema_parser.h", "torch/csrc/jit/frontend/inline_loop_condition.cpp", "torch/csrc/jit/frontend/inline_loop_condition.h", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/frontend/ir_emitter.h", "torch/csrc/jit/frontend/lexer.cpp", "torch/csrc/jit/frontend/lexer.h", "torch/csrc/jit/frontend/mini_environment.h", "torch/csrc/jit/frontend/parse_string_literal.h", "torch/csrc/jit/frontend/parser.cpp", "torch/csrc/jit/frontend/parser.h", "torch/csrc/jit/frontend/parser_constants.h", "torch/csrc/jit/frontend/resolver.h", "torch/csrc/jit/frontend/schema_matching.cpp", "torch/csrc/jit/frontend/schema_matching.h", "torch/csrc/jit/frontend/schema_type_parser.cpp", "torch/csrc/jit/frontend/schema_type_parser.h", "torch/csrc/jit/frontend/script_type_parser.cpp", "torch/csrc/jit/frontend/script_type_parser.h", "torch/csrc/jit/frontend/source_range.cpp", "torch/csrc/jit/frontend/source_range.h", "torch/csrc/jit/frontend/string_to_type.cpp", "torch/csrc/jit/frontend/strtod.cpp", "torch/csrc/jit/frontend/strtod.h", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/frontend/sugared_value.h", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/frontend/tracer.h", "torch/csrc/jit/frontend/tree.h", "torch/csrc/jit/frontend/tree_views.h", "torch/csrc/jit/function.cpp", "torch/csrc/jit/function.h", "torch/csrc/jit/fuser/README.md", "torch/csrc/jit/fuser/arg_spec.h", "torch/csrc/jit/fuser/codegen.cpp", "torch/csrc/jit/fuser/codegen.h", "torch/csrc/jit/fuser/compiler.cpp", "torch/csrc/jit/fuser/compiler.h", "torch/csrc/jit/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/fuser/cpu/fused_kernel.h", "torch/csrc/jit/fuser/cpu/resource_strings.h", "torch/csrc/jit/fuser/cpu/temp_file.h", "torch/csrc/jit/fuser/cuda/fused_kernel.cpp", "torch/csrc/jit/fuser/cuda/fused_kernel.h", "torch/csrc/jit/fuser/cuda/resource_strings.h", "torch/csrc/jit/fuser/executor.cpp", "torch/csrc/jit/fuser/executor.h", "torch/csrc/jit/fuser/fallback.cpp", "torch/csrc/jit/fuser/fallback.h", "torch/csrc/jit/fuser/fused_kernel.h", "torch/csrc/jit/fuser/interface.cpp", "torch/csrc/jit/fuser/interface.h", "torch/csrc/jit/fuser/kernel_cache.cpp", "torch/csrc/jit/fuser/kernel_cache.h", "torch/csrc/jit/fuser/kernel_spec.h", "torch/csrc/jit/fuser/partition_desc.h", "torch/csrc/jit/fuser/tensor_desc.h", "torch/csrc/jit/fuser/tensor_info.h", "torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/graph_executor.h", "torch/csrc/jit/graph_executor_impl.h", "torch/csrc/jit/graph_node_list.h", "torch/csrc/jit/hooks_for_testing.cpp", "torch/csrc/jit/hooks_for_testing.h", "torch/csrc/jit/import.cpp", "torch/csrc/jit/import.h", "torch/csrc/jit/import_export_helpers.cpp", "torch/csrc/jit/import_export_helpers.h", "torch/csrc/jit/import_legacy.cpp", "torch/csrc/jit/import_legacy.h", "torch/csrc/jit/import_source.cpp", "torch/csrc/jit/import_source.h", "torch/csrc/jit/init.cpp", "torch/csrc/jit/init.h", "torch/csrc/jit/instruction.cpp", "torch/csrc/jit/instruction.h", "torch/csrc/jit/interpreter.cpp", "torch/csrc/jit/interpreter.h", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/ir.h", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/alias_analysis.h", "torch/csrc/jit/ir/attributes.cpp", "torch/csrc/jit/ir/attributes.h", "torch/csrc/jit/ir/class_type.cpp", "torch/csrc/jit/ir/constants.cpp", "torch/csrc/jit/ir/constants.h", "torch/csrc/jit/ir/graph_node_list.h", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/ir/ir.h", "torch/csrc/jit/ir/ir_views.h", "torch/csrc/jit/ir/irparser.cpp", "torch/csrc/jit/ir/irparser.h", "torch/csrc/jit/ir/named_value.h", "torch/csrc/jit/ir/node_hashing.cpp", "torch/csrc/jit/ir/node_hashing.h", "torch/csrc/jit/ir/scope.cpp", "torch/csrc/jit/ir/scope.h", "torch/csrc/jit/ir/subgraph_matcher.cpp", "torch/csrc/jit/ir/subgraph_matcher.h", "torch/csrc/jit/ir/type_hashing.cpp", "torch/csrc/jit/ir/type_hashing.h", "torch/csrc/jit/ir_views.h", "torch/csrc/jit/irparser.cpp", "torch/csrc/jit/irparser.h", "torch/csrc/jit/jit_log.cpp", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/interpreter.h", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/type_parser.cpp", "torch/csrc/jit/named_value.h", "torch/csrc/jit/netdef_converter.h", "torch/csrc/jit/node_hashing.cpp", "torch/csrc/jit/node_hashing.h", "torch/csrc/jit/operator.cpp", "torch/csrc/jit/operator.h", "torch/csrc/jit/operator_options.cpp", "torch/csrc/jit/operator_options.h", "torch/csrc/jit/pass_manager.cpp", "torch/csrc/jit/pass_manager.h", "torch/csrc/jit/passes/alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.h", "torch/csrc/jit/passes/bailout_graph.cpp", "torch/csrc/jit/passes/bailout_graph.h", "torch/csrc/jit/passes/batch_mm.cpp", "torch/csrc/jit/passes/batch_mm.h", "torch/csrc/jit/passes/canonicalize.cpp", "torch/csrc/jit/passes/canonicalize.h", "torch/csrc/jit/passes/canonicalize_ops.h", "torch/csrc/jit/passes/clear_undefinedness.h", "torch/csrc/jit/passes/common_subexpression_elimination.cpp", "torch/csrc/jit/passes/common_subexpression_elimination.h", "torch/csrc/jit/passes/constant_pooling.cpp", "torch/csrc/jit/passes/constant_pooling.h", "torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/passes/constant_propagation.h", "torch/csrc/jit/passes/create_autodiff_subgraphs.cpp", "torch/csrc/jit/passes/create_autodiff_subgraphs.h", "torch/csrc/jit/passes/dead_code_elimination.cpp", "torch/csrc/jit/passes/dead_code_elimination.h", "torch/csrc/jit/passes/decompose_ops.cpp", "torch/csrc/jit/passes/decompose_ops.h", "torch/csrc/jit/passes/erase_number_types.cpp", "torch/csrc/jit/passes/erase_number_types.h", "torch/csrc/jit/passes/fixup_trace_scope_blocks.cpp", "torch/csrc/jit/passes/fixup_trace_scope_blocks.h", "torch/csrc/jit/passes/fuse_linear.h", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/graph_fuser.h", "torch/csrc/jit/passes/guard_elimination.cpp", "torch/csrc/jit/passes/guard_elimination.h", "torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp", "torch/csrc/jit/passes/inline_autodiff_subgraphs.h", "torch/csrc/jit/passes/inline_fork_wait.h", "torch/csrc/jit/passes/inline_forked_closures.cpp", "torch/csrc/jit/passes/inline_forked_closures.h", "torch/csrc/jit/passes/inliner.cpp", "torch/csrc/jit/passes/inliner.h", "torch/csrc/jit/passes/inplace_check.h", "torch/csrc/jit/passes/insert_guards.h", "torch/csrc/jit/passes/lift_closures.cpp", "torch/csrc/jit/passes/lift_closures.h", "torch/csrc/jit/passes/liveness.cpp", "torch/csrc/jit/passes/liveness.h", "torch/csrc/jit/passes/loop_unrolling.cpp", "torch/csrc/jit/passes/loop_unrolling.h", "torch/csrc/jit/passes/lower_grad_of.h", "torch/csrc/jit/passes/lower_graph.cpp", "torch/csrc/jit/passes/lower_graph.h", "torch/csrc/jit/passes/lower_tuples.cpp", "torch/csrc/jit/passes/lower_tuples.h", "torch/csrc/jit/passes/onnx.cpp", "torch/csrc/jit/passes/onnx.h", "torch/csrc/jit/passes/onnx/cast_all_constant_to_floating.h", "torch/csrc/jit/passes/onnx/constant_fold.h", "torch/csrc/jit/passes/onnx/fixup_onnx_conditionals.h", "torch/csrc/jit/passes/onnx/fixup_onnx_loop.h", "torch/csrc/jit/passes/onnx/helper.h", "torch/csrc/jit/passes/onnx/peephole.h", "torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp", "torch/csrc/jit/passes/onnx/prepare_division_for_onnx.h", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.h", "torch/csrc/jit/passes/onnx/scalar_type_analysis.h", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.h", "torch/csrc/jit/passes/pass_manager.cpp", "torch/csrc/jit/passes/pass_manager.h", "torch/csrc/jit/passes/peephole.cpp", "torch/csrc/jit/passes/peephole.h", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/passes/python_print.h", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization.h", "torch/csrc/jit/passes/remove_expands.h", "torch/csrc/jit/passes/remove_inplace_ops.h", "torch/csrc/jit/passes/requires_grad_analysis.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/passes/specialize_autogradzero.cpp", "torch/csrc/jit/passes/specialize_autogradzero.h", "torch/csrc/jit/passes/subgraph_rewrite.cpp", "torch/csrc/jit/passes/subgraph_rewrite.h", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/utils/check_alias_annotation.cpp", "torch/csrc/jit/passes/utils/check_alias_annotation.h", "torch/csrc/jit/passes/utils/subgraph_utils.h", "torch/csrc/jit/pickle.cpp", "torch/csrc/jit/pickle.h", "torch/csrc/jit/pickler.cpp", "torch/csrc/jit/pickler.h", "torch/csrc/jit/print_handler.cpp", "torch/csrc/jit/print_handler.h", "torch/csrc/jit/profiling_graph_executor_impl.cpp", "torch/csrc/jit/profiling_graph_executor_impl.h", "torch/csrc/jit/profiling_record.cpp", "torch/csrc/jit/profiling_record.h", "torch/csrc/jit/pybind.h", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/python/init.h", "torch/csrc/jit/python/module_python.h", "torch/csrc/jit/python/pybind.h", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/python/python_arg_flatten.cpp", "torch/csrc/jit/python/python_arg_flatten.h", "torch/csrc/jit/python/python_custom_class.cpp", "torch/csrc/jit/python/python_custom_class.h", "torch/csrc/jit/python/python_interpreter.cpp", "torch/csrc/jit/python/python_ir.cpp", "torch/csrc/jit/python/python_ir.h", "torch/csrc/jit/python/python_ivalue.h", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_sugared_value.h", "torch/csrc/jit/python/python_tracer.cpp", "torch/csrc/jit/python/python_tracer.h", "torch/csrc/jit/python/python_tree_views.cpp", "torch/csrc/jit/python/python_tree_views.h", "torch/csrc/jit/python/script_init.cpp", "torch/csrc/jit/python/script_init.h", "torch/csrc/jit/python/update_graph_executor_opt.cpp", "torch/csrc/jit/python/update_graph_executor_opt.h", "torch/csrc/jit/python_arg_flatten.cpp", "torch/csrc/jit/python_arg_flatten.h", "torch/csrc/jit/python_custom_class.cpp", "torch/csrc/jit/python_custom_class.h", "torch/csrc/jit/python_interpreter.cpp", "torch/csrc/jit/python_ir.cpp", "torch/csrc/jit/python_ir.h", "torch/csrc/jit/python_ivalue.h", "torch/csrc/jit/python_tracer.cpp", "torch/csrc/jit/python_tracer.h", "torch/csrc/jit/register_c10_ops.cpp", "torch/csrc/jit/register_distributed_ops.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/register_prim_ops_c10.cpp", "torch/csrc/jit/register_special_ops.cpp", "torch/csrc/jit/register_string_ops.cpp", "torch/csrc/jit/runtime/argument_spec.cpp", "torch/csrc/jit/runtime/argument_spec.h", "torch/csrc/jit/runtime/autodiff.cpp", "torch/csrc/jit/runtime/autodiff.h", "torch/csrc/jit/runtime/custom_operator.h", "torch/csrc/jit/runtime/exception_message.h", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/graph_executor.h", "torch/csrc/jit/runtime/graph_executor_impl.h", "torch/csrc/jit/runtime/instruction.cpp", "torch/csrc/jit/runtime/instruction.h", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/interpreter.h", "torch/csrc/jit/runtime/jit_exception.cpp", "torch/csrc/jit/runtime/jit_exception.h", "torch/csrc/jit/runtime/logging.cpp", "torch/csrc/jit/runtime/logging.h", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/operator.h", "torch/csrc/jit/runtime/operator_options.cpp", "torch/csrc/jit/runtime/operator_options.h", "torch/csrc/jit/runtime/print_handler.cpp", "torch/csrc/jit/runtime/print_handler.h", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.h", "torch/csrc/jit/runtime/profiling_record.cpp", "torch/csrc/jit/runtime/profiling_record.h", "torch/csrc/jit/runtime/register_c10_ops.cpp", "torch/csrc/jit/runtime/register_distributed_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_c10.cpp", "torch/csrc/jit/runtime/register_special_ops.cpp", "torch/csrc/jit/runtime/register_string_ops.cpp", "torch/csrc/jit/runtime/symbolic_script.cpp", "torch/csrc/jit/runtime/symbolic_script.h", "torch/csrc/jit/runtime/vararg_functions.cpp", "torch/csrc/jit/runtime/vararg_functions.h", "torch/csrc/jit/runtime/variable_tensor_list.h", "torch/csrc/jit/scope.cpp", "torch/csrc/jit/scope.h", "torch/csrc/jit/script/builtin_functions.cpp", "torch/csrc/jit/script/builtin_functions.h", "torch/csrc/jit/script/canonicalize_modified_loop.cpp", "torch/csrc/jit/script/canonicalize_modified_loop.h", "torch/csrc/jit/script/class_type.cpp", "torch/csrc/jit/script/compilation_unit.h", "torch/csrc/jit/script/concrete_module_type.cpp", "torch/csrc/jit/script/concrete_module_type.h", "torch/csrc/jit/script/convert_to_ssa.cpp", "torch/csrc/jit/script/convert_to_ssa.h", "torch/csrc/jit/script/edit_distance.cpp", "torch/csrc/jit/script/edit_distance.h", "torch/csrc/jit/script/error_report.cpp", "torch/csrc/jit/script/error_report.h", "torch/csrc/jit/script/exit_transforms.cpp", "torch/csrc/jit/script/exit_transforms.h", "torch/csrc/jit/script/function_schema_parser.cpp", "torch/csrc/jit/script/function_schema_parser.h", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/script/init.h", "torch/csrc/jit/script/inline_loop_condition.cpp", "torch/csrc/jit/script/inline_loop_condition.h", "torch/csrc/jit/script/ir_emitter.cpp", "torch/csrc/jit/script/ir_emitter.h", "torch/csrc/jit/script/jit_exception.cpp", "torch/csrc/jit/script/jit_exception.h", "torch/csrc/jit/script/lexer.cpp", "torch/csrc/jit/script/lexer.h", "torch/csrc/jit/script/logging.cpp", "torch/csrc/jit/script/logging.h", "torch/csrc/jit/script/method.h", "torch/csrc/jit/script/mini_environment.h", "torch/csrc/jit/script/module.cpp", "torch/csrc/jit/script/module.h", "torch/csrc/jit/script/module_python.h", "torch/csrc/jit/script/module_save.cpp", "torch/csrc/jit/script/object.cpp", "torch/csrc/jit/script/object.h", "torch/csrc/jit/script/parse_string_literal.h", "torch/csrc/jit/script/parser.cpp", "torch/csrc/jit/script/parser.h", "torch/csrc/jit/script/parser_constants.h", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/csrc/jit/script/python_sugared_value.h", "torch/csrc/jit/script/python_tree_views.cpp", "torch/csrc/jit/script/python_tree_views.h", "torch/csrc/jit/script/resolver.h", "torch/csrc/jit/script/schema_matching.cpp", "torch/csrc/jit/script/schema_matching.h", "torch/csrc/jit/script/schema_type_parser.cpp", "torch/csrc/jit/script/schema_type_parser.h", "torch/csrc/jit/script/script_type_parser.cpp", "torch/csrc/jit/script/script_type_parser.h", "torch/csrc/jit/script/string_to_type.cpp", "torch/csrc/jit/script/strtod.cpp", "torch/csrc/jit/script/strtod.h", "torch/csrc/jit/script/sugared_value.cpp", "torch/csrc/jit/script/sugared_value.h", "torch/csrc/jit/script/tree.h", "torch/csrc/jit/script/tree_views.h", "torch/csrc/jit/serialization/export.cpp", "torch/csrc/jit/serialization/export.h", "torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/import.cpp", "torch/csrc/jit/serialization/import.h", "torch/csrc/jit/serialization/import_export_helpers.cpp", "torch/csrc/jit/serialization/import_export_helpers.h", "torch/csrc/jit/serialization/import_legacy.cpp", "torch/csrc/jit/serialization/import_legacy.h", "torch/csrc/jit/serialization/import_source.cpp", "torch/csrc/jit/serialization/import_source.h", "torch/csrc/jit/serialization/pickle.cpp", "torch/csrc/jit/serialization/pickle.h", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/pickler.h", "torch/csrc/jit/serialization/python_print.cpp", "torch/csrc/jit/serialization/python_print.h", "torch/csrc/jit/serialization/source_range_serialization.cpp", "torch/csrc/jit/serialization/source_range_serialization.h", "torch/csrc/jit/serialization/source_range_serialization_impl.h", "torch/csrc/jit/serialization/unpickler.cpp", "torch/csrc/jit/serialization/unpickler.h", "torch/csrc/jit/source_range.cpp", "torch/csrc/jit/source_range.h", "torch/csrc/jit/source_range_serialization.cpp", "torch/csrc/jit/source_range_serialization.h", "torch/csrc/jit/source_range_serialization_impl.h", "torch/csrc/jit/subgraph_matcher.cpp", "torch/csrc/jit/subgraph_matcher.h", "torch/csrc/jit/symbolic_script.cpp", "torch/csrc/jit/symbolic_script.h", "torch/csrc/jit/testing/catch_utils.hpp", "torch/csrc/jit/testing/file_check.cpp", "torch/csrc/jit/testing/hooks_for_testing.cpp", "torch/csrc/jit/testing/hooks_for_testing.h", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h", "torch/csrc/jit/type_hashing.cpp", "torch/csrc/jit/type_hashing.h", "torch/csrc/jit/unpickler.cpp", "torch/csrc/jit/unpickler.h", "torch/csrc/jit/update_graph_executor_opt.cpp", "torch/csrc/jit/update_graph_executor_opt.h", "torch/csrc/jit/vararg_functions.cpp", "torch/csrc/jit/vararg_functions.h", "torch/csrc/jit/variable_tensor_list.h", "torch/csrc/utils/python_arg_parser.h", "torch/csrc/utils/python_numbers.h", "torch/csrc/utils/throughput_benchmark-inl.h", "torch/csrc/utils/throughput_benchmark.cpp", "torch/csrc/utils/throughput_benchmark.h", "torch/custom_class.h", "torch/jit/supported_ops.py", "torch/script.h"], "labels": ["jit", "merged"]}, "390d4d6df3": {"title": "[JIT] Introduce a fake Tensor creation node for IR unit tests (#33595)", "body": "Summary:\n**Summary**\nThere is often a need to create a Tensor when writing IR by hand for JIT\noptimisation pass unit tests. The only options for this today are real\nTensor creation functions like `aten::ones`. Any test that uses these functions\nmust also use the same default arguments as the Python/C++ API, which means\nthat all of the tests have to be updated when the API is updated. This commit\nintroduces a new primitive, `prim::MakeTestTensor` with schema `() -> Tensor` that\nshould be used in unit tests instead of real Tensor creation functions. This new\nprimitive has no public-facing API, so the maintenance burden is much lower.\n\n**Testing**\nThis commit updates the alias analysis and DCE tests to use `prim::MakeTestTensor` instead of\n`aten::rand`, `aten::ones`, and `aten::zeros`.\n\n```\n$ ./bin/test_jit\nCUDA not available. Disabling CUDA and MultiCUDA tests\nNote: Google Test filter = *-*_CUDA:*_MultiCUDA\n[==========] Running 75 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 75 tests from JitTest\n[ RUN      ] JitTest.ADFormulas\n[       OK ] JitTest.ADFormulas (82 ms)\n[ RUN      ] JitTest.Attributes\n[       OK ] JitTest.Attributes (0 ms)\n...\n...\n...\n[ RUN      ] JitTest.LiteInterpreterPrim\n[       OK ] JitTest.LiteInterpreterPrim (0 ms)\n[ RUN      ] JitTest.LiteInterpreterLoadOrigJit\n[       OK ] JitTest.LiteInterpreterLoadOrigJit (2 ms)\n[----------] 75 tests from JitTest (150 ms total)\n\n[----------] Global test environment tear-down\n[==========] 75 tests from 1 test case ran. (150 ms total)\n[  PASSED  ] 75 tests.\n```\n\n**Fixes**\nThis pull request fixes https://github.com/pytorch/pytorch/issues/33500.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33595\n\nDifferential Revision: D20127441\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 56da4f23ac46335227254f606c6481718108f378", "pr_number": "33595", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_base.cpp", "test/cpp/jit/test_dce.cpp"], "labels": ["jit", "merged"]}, "908eee5583": {"title": "remove .data from test/distributed/ (#33874)", "body": "Summary:\n`.data` calls are unsafe and should not be used.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33874\n\nDifferential Revision: D20141059\n\nPulled By: izdeby\n\nfbshipit-source-id: 8e11afc74f0cb04f5b18b458068fb813a6d51708", "pr_number": "33874", "files_changed": ["test/distributed/test_c10d.py", "test/distributed/test_data_parallel.py", "test/distributed/test_distributed.py"], "labels": ["merged"]}, "a7cf5c859f": {"title": "Revert D20136865: fix lint", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20136865\n\nOriginal commit changeset: 4bf7ac324a0a\n\nfbshipit-source-id: 94cc83cda180f744cec174d269f1b82babff0e5c", "pr_number": null, "files_changed": ["test/jit/test_export_modes.py", "torch/nn/utils/rnn.py"], "labels": []}, "243af17d65": {"title": "Revert D20103905: [jit] Fix flipped PackedSequence outputs in script", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20103905\n\nOriginal commit changeset: 84081213ed21\n\nfbshipit-source-id: 2b260654fac87e52fbaf8035018e4ea484928af1", "pr_number": null, "files_changed": ["test/test_jit.py", "torch/nn/utils/rnn.py"], "labels": []}, "53630f7681": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/ae68f84fcd865d07bec6bbb90b50cadd4ab9eb20\nhttps://github.com/facebook/fbthrift/commit/6cb0beaf0e65f3317e0d16b6c989f79341c34376\nhttps://github.com/facebook/fbzmq/commit/401fb540299d51be8b69a202e33f1408fa0dce39\nhttps://github.com/facebook/folly/commit/fe8777e593300e3249994bffb243c367dfbef050\nhttps://github.com/facebook/litho/commit/44fcf005eba39227f80896bcde8428faf412c8f7\nhttps://github.com/facebook/rocksdb/commit/72ee067b90796a54d5190089773e141943003074\nhttps://github.com/facebook/wangle/commit/01a3c124d454c5b768e4b1900abc9e1b21cd39ef\nhttps://github.com/facebookincubator/fizz/commit/c94f8f43b956fc8f5240dbf53e0de4076b0f83aa\nhttps://github.com/facebookincubator/katran/commit/a09b292a28e96ce1246fbc5bf4c16a5e638e03a3\nhttps://github.com/facebookincubator/mvfst/commit/472e40a90245e115a150e4d1639ce518bc7802cd\nhttps://github.com/pytorch/fbgemm/commit/967d4bc05191ab67c9ae4e34a508e36260a30238\n\nTest Plan: n/a\n\nReviewed By: wittgenst\n\nfbshipit-source-id: e8e43b1cbc365fd7f5b068d625c4020240358690", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "910acafc79": {"title": "Revert D20124224: [jit] stop printing crap in test_jit", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20124224\n\nOriginal commit changeset: 9241d21fdf94\n\nfbshipit-source-id: 0680f9db922f9a33a4e859eedd142b87a51bbede", "pr_number": null, "files_changed": ["test/jit/test_class_type.py", "test/jit/test_export_modes.py", "test/jit/test_models.py", "test/jit/test_recursive_script.py", "test/jit/test_unsupported_ops.py", "test/test_jit.py", "torch/onnx/utils.py", "torch/testing/_internal/jit_utils.py"], "labels": []}, "c4d611a0f5": {"title": "Split BinaryMiscOpsKernels into more files for faster build times. (#33873)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33873\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20140974\n\nPulled By: gchanan\n\nfbshipit-source-id: 88b982881e8034f3b03cdb6911ae4239d2bb1596", "pr_number": "33873", "files_changed": ["aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu"], "labels": ["merged"]}, "5eacdfb21f": {"title": "Revert D20127441: [pytorch][PR] [JIT] Introduce a fake Tensor creation node for IR unit tests", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20127441\n\nOriginal commit changeset: 56da4f23ac46\n\nfbshipit-source-id: 7d4602e5011bec6f6871eab16af05a3198694e5d", "pr_number": null, "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_base.cpp", "test/cpp/jit/test_dce.cpp"], "labels": []}, "d97560999b": {"title": "Split BinaryCompareKernel.cu into a file-per-kernel to speed up compilation. (#33871)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33871\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20140862\n\nPulled By: gchanan\n\nfbshipit-source-id: a4fde38c1c7c5905e3855fa490ea2e87bb24c703", "pr_number": "33871", "files_changed": ["aten/src/ATen/native/cuda/BinaryCompareKernel.cu", "aten/src/ATen/native/cuda/CompareEQKernel.cu", "aten/src/ATen/native/cuda/CompareGEKernel.cu", "aten/src/ATen/native/cuda/CompareGTKernel.cu", "aten/src/ATen/native/cuda/CompareLEKernel.cu", "aten/src/ATen/native/cuda/CompareLTKernel.cu", "aten/src/ATen/native/cuda/CompareNEKernel.cu", "aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu"], "labels": ["merged"]}, "edd5c009f7": {"title": "fix docs mistakes in lr_scheduler.MultiplicativeLR (#33805)", "body": "Summary:\nThis PR is referenced to an issue: [The docs of `MultiplicativeLR` use `LambdaLR` as example](https://github.com/pytorch/pytorch/issues/33752#issue-570374087)\n\nhttps://github.com/pytorch/pytorch/issues/33752\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33805\n\nDifferential Revision: D20121314\n\nPulled By: mruberry\n\nfbshipit-source-id: 5afa63bbe83d35ce4e55705b8cbd96326a907651", "pr_number": "33805", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["merged", "open source", "triaged"]}, "524dad13a8": {"title": "Add device to the test tensor. Default device type is CPU, in pytorch\u2026 (#33635)", "body": "Summary:\n\u2026/xla this will result in a failure since it is comparing a XLA tensor with a CPU tensor.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33635\n\nDifferential Revision: D20043517\n\nPulled By: ailzhang\n\nfbshipit-source-id: d84038ea675e4d4a9c02e7a8b0924bdb12f40501", "pr_number": "33635", "files_changed": ["test/test_type_promotion.py"], "labels": ["merged", "open source", "triaged"]}, "a7fe200f5f": {"title": "[caffe2] simplify caffe2 code with fbgemm handling block size 1 emb (#33774)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33774\n\nSimplify caffe2 code using D19246900\n\nTest Plan: CI\n\nReviewed By: jianyuh\n\nDifferential Revision: D20102410\n\nfbshipit-source-id: 8de4d9cfac66898db0718ac6477339fd5e5428e3", "pr_number": "33774", "files_changed": ["caffe2/perfkernels/adagrad.cc", "caffe2/perfkernels/adagrad.h", "caffe2/perfkernels/adagrad_avx.cc"], "labels": ["fb-exported", "merged"]}, "aff1da5aac": {"title": ".circleci: Remove trailing slash, fix conda upload (#33903)", "body": "Summary:\nConda registers a suffixed slash as a new user so it was failing to\nupload the anaconda packages.\n\nIn the future this should be handled through a single variable that can\nbe used for both but until then this will have to do.\n\nBug was introduced in https://github.com/pytorch/pytorch/issues/33842\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33903\n\nDifferential Revision: D20148679\n\nPulled By: seemethere\n\nfbshipit-source-id: 27c95f5d906ce84aa34bf5d76fd6f1ef5df08fb9", "pr_number": "33903", "files_changed": [".circleci/scripts/binary_linux_upload.sh", ".circleci/scripts/binary_macos_upload.sh"], "labels": ["merged", "releng"]}, "87e97ced20": {"title": "Split UnaryOpsKernel into smaller files for faster compilation. (#33888)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33888\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20143653\n\nPulled By: gchanan\n\nfbshipit-source-id: de708030e93e96091e0c01a89b4342872d0657dd", "pr_number": "33888", "files_changed": ["aten/src/ATen/native/cuda/UnaryComplexKernels.cu", "aten/src/ATen/native/cuda/UnaryFractionKernels.cu", "aten/src/ATen/native/cuda/UnaryGammaKernels.cu", "aten/src/ATen/native/cuda/UnaryGeometricKernels.cu", "aten/src/ATen/native/cuda/UnaryLogKernels.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/cuda/UnarySignKernels.cu"], "labels": ["merged"]}, "48fd410e44": {"title": "Try fix XLAPreAutograd with *_like functions. (#33848)", "body": "Summary:\nIn *_like functions we call\n`globalLegacyTypeDispatch().initForDispatchKeySet(c10::detail::multi_dispatch_key_set(self, options));` -> `dispatchKeyToBackend` and thus this change.\n`self` has both `XLAPreAutograd` and `XLATensorId` in key set.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33848\n\nDifferential Revision: D20135898\n\nPulled By: ailzhang\n\nfbshipit-source-id: a8585f39f3fa77b53718f20d3144f4f2f3cb8e53", "pr_number": "33848", "files_changed": ["c10/core/Backend.h"], "labels": ["merged"]}, "746e5218e7": {"title": "Mistake in MSELoss documentation (#33836)", "body": "Summary:\nReplaced `sum` with `mean` in [line 392](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/loss.py#L392)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33836\n\nDifferential Revision: D20142053\n\nPulled By: ailzhang\n\nfbshipit-source-id: 2bfe19944ffc5534902dd9087023e70ddf5746c3", "pr_number": "33836", "files_changed": ["torch/nn/modules/loss.py"], "labels": ["merged", "open source"]}, "877ab3afe3": {"title": "Better handing of Autograd+Fork errors. (#33885)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33885\n\nFixes: #32835\nFixes: #5834\n\nCan not combine with CUDA's implementation as each of them requires individual `std::once_flag` as well as different `forked_autograd_child` functions. CUDA version relays to python module, autograd uses TORCH_CHECK to report error to python and cpp.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20144024\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: e7cf30568fff5110e9df7fe5b23f18ed992fa17f", "pr_number": "33885", "files_changed": ["test/test_multiprocessing.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h"], "labels": ["merged"]}, "db4a24e008": {"title": "[jit] remove some unused/redundant files (#33806)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33806\n\nas title\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20122117\n\nPulled By: suo\n\nfbshipit-source-id: 209d29ed2c873181140c9fb5cdc305c200ce4008", "pr_number": "33806", "files_changed": ["torch/csrc/jit/alias_info.h", "torch/csrc/jit/backends/bleh", "torch/csrc/jit/interned_strings_class.h", "torch/csrc/jit/ir/alias_analysis.h", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/operator_options.cpp"], "labels": ["jit", "merged"]}, "997b5b5797": {"title": "[quant][graphmode][refactor] Simplify signature for insertObserverFor (#33274)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33274\n\natt\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D20123588\n\nfbshipit-source-id: e656d96e0b6004bfcca5df2ab222184d4e1dd6ad", "pr_number": "33274", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "d66c320b10": {"title": "disable leaky_relu_ backward calculation with negative slope (#33639)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33639\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20045735\n\nPulled By: glaringlee\n\nfbshipit-source-id: b3becf30a8fe9ee178792bd88f6ee10102504ed5", "pr_number": "33639", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_autograd.py", "tools/autograd/derivatives.yaml"], "labels": ["merged"]}, "bd7e9c490a": {"title": "[jit] stop printing crap in test_jit (#33917)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33917\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20150750\n\nPulled By: suo\n\nfbshipit-source-id: 9a35298a8856d423fb6b9043174853cccf968706", "pr_number": "33917", "files_changed": ["test/jit/test_class_type.py", "test/jit/test_export_modes.py", "test/jit/test_models.py", "test/jit/test_recursive_script.py", "test/jit/test_unsupported_ops.py", "test/test_jit.py", "torch/onnx/utils.py", "torch/testing/_internal/jit_utils.py"], "labels": ["jit", "merged"]}, "d6485b411b": {"title": "[jit] add top-level readme to csrc/jit (#33916)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33916\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20150771\n\nPulled By: suo\n\nfbshipit-source-id: c7550954ddd6a294ce833348bf9fa058503e9bd7", "pr_number": "33916", "files_changed": ["torch/csrc/jit/OVERVIEW.md", "torch/csrc/jit/README.md", "torch/csrc/jit/docs/OVERVIEW.md"], "labels": ["jit", "merged"]}, "8f84deddd1": {"title": "[jit] fix up refs in overview.md (#33919)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33919\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20154953\n\nPulled By: suo\n\nfbshipit-source-id: 2ef83cce8da88212bed7edc813c9b233267ea81b", "pr_number": "33919", "files_changed": ["torch/csrc/jit/OVERVIEW.md"], "labels": ["jit", "merged"]}, "5029ff001b": {"title": "[Revert] manual revert of D19918320 (#33920)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33920\n\nrevert D19918320\n\nTest Plan: revert diff\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D20151299\n\nfbshipit-source-id: c346554ae9074991331479e434e54b0cc513f1a4", "pr_number": "33920", "files_changed": ["torch/csrc/jit/python/pybind_utils.h", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["fb-exported", "jit", "merged"]}, "cb8d9f99aa": {"title": "[JIT] Implement Tensor.tolist() (#33472)", "body": "Summary:\n**Summary**\nThis commit adds an implementation of `Tensor.tolist()` to the JIT interpreter.\n\n**Testing**\nThis commit adds several unit tests that test that this function works correctly for\n0D, 1D, 2D and 3D tensors of type `float`, `int` and `bool`.\n\n```\n(base) meghanl-mbp:pytorch meghanl$ python test/test_jit.py TestList.test_to_list -v\nFail to import hypothesis in common_utils, tests are not derandomized\ntest_to_list (jit.test_list_dict.TestList)\nUnit tests for Tensor.tolist() function. ... ok\n\n----------------------------------------------------------------------\nRan 1 test in 0.329s\n\nOK\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33472\n\nDifferential Revision: D20109738\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: a6e3fee5e3201d5e1f0c4ca45048488ae2bf5e33", "pr_number": "33472", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_list_dict.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/ir/ir.h", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/serialization/python_print.cpp"], "labels": ["jit", "merged"]}, "c18cb1eb52": {"title": "Improve dll loading logic on Windows (#33856)", "body": "Summary:\nThe way it works on the Anaconda distribution of Python 3.8 is a bit different. Loading DLLs explicitly  (e.g. `ctype.CDLL`) relies on paths appended by `os.add_dll_directory`. But if you try to load DLLs implicitly (e.g. `from torch._C import *`), it will rely on `PATH`.\n\nFixes https://github.com/pytorch/vision/issues/1916.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33856\n\nDifferential Revision: D20150080\n\nPulled By: soumith\n\nfbshipit-source-id: cdbe76c138ea259ef7414c6634d4f7e0b1871af3", "pr_number": "33856", "files_changed": ["torch/__init__.py"], "labels": ["merged", "open source", "triaged"]}, "1507573a52": {"title": "[caffe2] fix no return statement in constexpr function Clang error in TypeIndex.h (#33576)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33576\n\n`throw` statement at the end of `constexpr` is ill-formed according to Clang. It happens when Clang is driving CUDA compilation and compiles for device the effected code. Due to its compilation model it requires host code to be well-formed even when compiling for device.\n\nFix the error by guarding the entire definition of `type_index_impl` with `__CUDA_ARCH__` check.\n\nTest Plan:\n```lang=bash\nbuck build mode/opt -c fbcode.cuda_use_clang=true //fblearner/flow/projects/dper:workflow\nbuck build mode/opt //fblearner/flow/projects/dper:workflow\n```\nExecute tests on devgpu:\n```\nbuck test mode/dev-nosan -j 8 //caffe2/caffe2/python/operator_test/... //caffe2/test:cuda\n```\n\nReviewed By: smessmer\n\nDifferential Revision: D20008881\n\nfbshipit-source-id: b0dc9abf0dc308b8b8637b54646a0411baf7fef3", "pr_number": "33576", "files_changed": ["c10/util/TypeIndex.h"], "labels": ["fb-exported", "merged"]}, "64aab3260a": {"title": "[jit] allow RRef local creation with IValue objects (#33263)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33263\n\nThis PR allow PyRRef local creation to inspect the pyobject, if it\nfounds that we could turn it to an IValue, turn to an IValue first,\notherwise hold it as a PyObjectType\n\nTest Plan:\nImported from OSS\n\nhttps://fb.quip.com/aGxRAh2lCg05\n\nDifferential Revision: D19871243\n\nPulled By: wanchaol\n\nfbshipit-source-id: ae5be3c52fb1e6db33c64e64ef64bc8b9ea63a9a", "pr_number": "33263", "files_changed": ["torch/csrc/distributed/rpc/py_rref.cpp", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["jit", "merged"]}, "97541a5106": {"title": "[quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized (#33275)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33275\n\natt\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D20123592\n\nfbshipit-source-id: 2b56ea8bab27eb9ea2bf792c83e48a7af8917e1a", "pr_number": "33275", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "7c13f576ea": {"title": "[quant][graphmode][refactor] Checks for bias and weight (#33273)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33273\n\n- Move the check for bias to valueNeedsToBeQuantized\n- Move TORCH_CHECK inside the functions for checking if a value is bias or weight\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D20123595\n\nfbshipit-source-id: 4b805d57dcaf41a6436506d021dd5f6518bc88fd", "pr_number": "33273", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "7f1112820a": {"title": "[quant][graphmode][refactor] Move check for weight outside of insertObserverFor (#33276)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33276\n\natt\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D20123593\n\nfbshipit-source-id: 45dc8488ddf02225ba2c20374c9385edd77a4912", "pr_number": "33276", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "f5f1e5e7f6": {"title": "[quant][graphmode][refactor] Factor out getInvokedMethod (#33649)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33649\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D20123589\n\nfbshipit-source-id: 0853d757434fb85c6d86666ff9fc991f8c4cb4bc", "pr_number": "33649", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "09046713cc": {"title": "removed .data from test_autograd.py (#33886)", "body": "Summary:\nissue: https://github.com/pytorch/pytorch/issues/33630\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33886\n\nDifferential Revision: D20160292\n\nPulled By: anjali411\n\nfbshipit-source-id: 14a42d8148bd60db2dd8ec39f83f99c061ae19c1", "pr_number": "33886", "files_changed": ["test/test_autograd.py"], "labels": ["merged"]}, "dece155335": {"title": "Modified assertEqual to handle complex tensors (#33773)", "body": "Summary:\n- Modified assertEqual to handle complex tensors\n- added a test in test_torch.py to test torch.zeros\n- added dispatch for complex for index_kernel, index_put_kernel\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33773\n\nDifferential Revision: D20135553\n\nPulled By: anjali411\n\nfbshipit-source-id: f716604535c0447ecffa335b0fc843431397c988", "pr_number": "33773", "files_changed": ["aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "c10/core/ScalarType.h", "test/test_torch.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged", "module: complex"]}, "04dc0e6973": {"title": "Split Distribution.cu into smaller files to reduce compilation time. (#33892)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33892\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20148925\n\nPulled By: gchanan\n\nfbshipit-source-id: 955e6ff22ee5fb24000b9f2ee58a243e76edf993", "pr_number": "33892", "files_changed": ["aten/src/ATen/native/cuda/DistributionBernoulli.cu", "aten/src/ATen/native/cuda/DistributionCauchyKernel.cu", "aten/src/ATen/native/cuda/DistributionExponentialKernel.cu", "aten/src/ATen/native/cuda/DistributionGeometricKernel.cu", "aten/src/ATen/native/cuda/DistributionLogNormalKernel.cu", "aten/src/ATen/native/cuda/DistributionNormal.cu", "aten/src/ATen/native/cuda/DistributionRandomKernel.cu", "aten/src/ATen/native/cuda/DistributionUniform.cu", "aten/src/ATen/native/cuda/Distributions.cu"], "labels": ["merged"]}, "890242254b": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/6f4df6e0cd05b4ec51a71d42b4056b3bce088d69\nhttps://github.com/facebook/litho/commit/6b7df86da15557001d16aaa7d71b4e34e3db85eb\nhttps://github.com/facebook/proxygen/commit/f873713ad6e2eb401688ece78ae67189d82dd8a0\nhttps://github.com/facebookincubator/mvfst/commit/2b3b76cc4d18ccedf87d6c50acfc134893a0a69b\nhttps://github.com/pytorch/fbgemm/commit/b990727d337637740389980a1f811f516d2adde3\n\nTest Plan: n/a\n\nReviewed By: wittgenst\n\nfbshipit-source-id: bf7b1639ee23e1e823bc2217f56c87dc7befaf7f", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "c6d301220a": {"title": "Fix torch.cat() performance regression on single core CPU (#33534)", "body": "Summary:\nThis PR addresses the performance regression on `torch.cat()` on CPU with single thread.\nPrevious optimization https://github.com/pytorch/pytorch/issues/30806 introduced regression for several cases on pytorch operator benchmark.\nSee https://github.com/pytorch/pytorch/issues/33334 for detail.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33534\n\nDifferential Revision: D20129963\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 3fa6cd266978e5b54fa37105555502b77352df3e", "pr_number": "33534", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/cpu/CatKernel.cpp", "aten/src/ATen/native/cpu/CatKernel.h", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "5dde8cd483": {"title": "[caffe2] fix no matching function min/max Clang errors (#33563)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33563\n\nWhen NVCC or Clang are driving CUDA compilation many math functions are declared by default, with a small difference: Clang marks them as `__device__` only, while NVCC uses both `__host__` and `__device__`. This makes every un-elaborated `min` or `max` function call from a `__host__` function generate a syntax error when Clang is used.\n\nFix the errors by using `std::min` and `std::max` from `<algorithm>`, since C++14 they are `constexpr` and can be used in the `__device__` code [1].\n\n1. https://llvm.org/docs/CompileCudaWithLLVM.html#algorithm\n\nTest Plan:\n```lang=bash\nbuck build mode/opt -c fbcode.cuda_use_clang=true //fblearner/flow/projects/dper:workflow\nbuck build mode/opt //fblearner/flow/projects/dper:workflow\n```\nExecute tests on devgpu:\n```\nbuck test mode/dev-nosan -j 8 //caffe2/caffe2/python/operator_test/... //caffe2/test:cuda\n```\n\nReviewed By: ngimel\n\nDifferential Revision: D20005795\n\nfbshipit-source-id: 98a3f35e8a96c15d3ad3d2066396591f5cca1696", "pr_number": "33563", "files_changed": ["aten/src/THC/generic/THCTensorMath.cu", "aten/src/THC/generic/THCTensorMathScan.cu", "aten/src/THCUNN/RReLU.cu", "caffe2/core/context_gpu.cu", "caffe2/operators/boolean_mask_ops.cu", "caffe2/operators/boolean_unmask_ops.cu", "caffe2/operators/normalize_ops.cu", "caffe2/operators/scale_blobs_op.cu", "caffe2/operators/segment_reduction_op_gpu.cu", "caffe2/operators/sequence_ops.cu", "caffe2/sgd/adagrad_op_gpu.cu"], "labels": ["fb-exported", "merged"]}, "a726827ec8": {"title": "Formatting changes for gradient scaling (#33832)", "body": "Summary:\nhard to get right locally...I can build the docs but never quite match what it looks like live.  the bullet point indentation was just an oversight.\n\nRemoving `Returns:` formatting tabs because they take up a lot of space when rendered and add no clarity.  Some functions in Pytorch [do use them](https://pytorch.org/docs/master/torch.html#torch.eye), but [many don't bother](https://pytorch.org/docs/master/torch.html#torch.is_tensor), so apparently some people shared my feelings (Not using them is in line with existing practice).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33832\n\nDifferential Revision: D20135581\n\nPulled By: ngimel\n\nfbshipit-source-id: bc788a7e57b142f95c4fa5baf3fe01f94c45abd8", "pr_number": "33832", "files_changed": ["torch/cuda/amp/grad_scaler.py"], "labels": ["merged", "open source"]}, "9fd1a7697f": {"title": "Create CODE_OF_CONDUCT.md", "body": "", "pr_number": null, "files_changed": ["CODE_OF_CONDUCT.md"], "labels": []}, "ad44394f15": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fbthrift/commit/e5b1164ad7389190beb5444345ad941586e17efd\nhttps://github.com/facebook/proxygen/commit/6df461c14e9464e91e63de1735838e9471625628\nhttps://github.com/facebook/rocksdb/commit/41535d0218d8928d60b6561eccc236f3a6675ed3\nhttps://github.com/facebookincubator/fizz/commit/30c57a1a0e0c2d8ce24f797ad29e64c02bdfef02\nhttps://github.com/pytorch/fbgemm/commit/3b9aeb2ebe70b5c240a007e0c3209b8f2b95850d\n\nTest Plan: n/a\n\nReviewed By: wittgenst\n\nfbshipit-source-id: 8361b5814c531edc99f96f11db97d6b2adcc5280", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "56d9906083": {"title": "update mapping of fake operators (#33946)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33946\n\nupdate mapping of fake operators to model nnpi\nupdate SpatialBN to non-lowered\n\nTest Plan:\ncompilation\n\nhttps://github.com/pytorch/pytorch/pull/33946\n\nReviewed By: amylittleyang\n\nDifferential Revision: D20156136\n\nfbshipit-source-id: e6ed87c3c5eba692a49376f0d9dae37ae185f185", "pr_number": "33946", "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc"], "labels": ["fb-exported", "merged"]}, "f5d92fbc25": {"title": "Get rid of newWithStorage2d calls. (#33823)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33823\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20122448\n\nPulled By: gchanan\n\nfbshipit-source-id: b249372c93ee71b84a293dfb5c298a8fb664da16", "pr_number": "33823", "files_changed": ["aten/src/TH/generic/THTensor.cpp", "aten/src/TH/generic/THTensor.h", "aten/src/TH/generic/THTensorLapack.cpp", "aten/src/THC/generic/THCTensor.cpp", "aten/src/THC/generic/THCTensor.h", "aten/src/THCUNN/generic/SpatialConvolutionMM.cu"], "labels": ["merged"]}, "917e56e950": {"title": "Throw an error if nbytes is called on a sparse tensor. (#33897)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33897\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20146388\n\nPulled By: gchanan\n\nfbshipit-source-id: b5853096e290fa7fb50be41446b138ebdf71009f", "pr_number": "33897", "files_changed": ["aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/test/basic.cpp"], "labels": ["merged", "topic: bc-breaking"]}, "2fa51dde28": {"title": "Remove unnecessary tensor copies (#33732)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33732\n\nmove and forward instead of copy\n\nBenchmarks:\nA microbenchmark calling the add operation on two tensors in a tight loop shows a 5% improvement in performance.\nNo visible change for a model like resnet that does more work in its kernels.\nghstack-source-id: 99161486\n\nTest Plan: benchmarks\n\nDifferential Revision: D20082642\n\nfbshipit-source-id: eeac59686f8621dd5eaa85d61e6d219bba48c847", "pr_number": "33732", "files_changed": ["aten/src/ATen/core/boxing/kernel_functor.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/stack.h", "c10/util/C++17.h", "torch/custom_class.h"], "labels": ["merged"]}, "3c5677a676": {"title": "Use codegen'ed unboxing wrappers (#32521)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32521\n\nNot all ops support the templated unboxing wrappers yet. For the ones that don't,\nlet's use the codegen'ed unboxing wrappers from register_aten_ops.cpp, but register\nthem with c10 directly instead of JIT.\n\nThe `use_c10_dispatcher` setting in `native_functions.yaml` now has a new option 'with_codegenerated_unboxing_wrapper' which means we take the codegened unboxing wrapper from register_aten_ops.cpp and stuff it into c10. This new argument is the default, 'unboxed_only' is not the default anymore. For the (very few) ops that don't support boxed dispatch yet (i.e. ops taking TensorOptions arguments), we set them to 'unboxed_only' and they follow the old behavior of having register_aten_ops.cpp register the jit op.\n\nNext steps here are (1) to make TensorOptions work with boxed dispatch and remove the `unboxed_only` option from `use_c10_dispatcher`, so that all ops go through the new path and (2) make the new path template-only and remove codegen from it (see https://github.com/pytorch/pytorch/issues/32366).\n\nFirst experiments show that\n- For a small JITted model that calls add (i.e. a op with just two arguments that are both tensors) on two tensors in a loop, we see a 2-4% performance improvement (~35-50ns) when compared to the old path. This is a simple op that takes two tensor arguments and no non-tensor arguments, so iterating over it in boxed dispatch is cheap.\n- For a small JITted model that calls avgpool1d (i.e. an op that has one tensor arg and 5 non-tensor args) on a tensor in a loop, we see a 3-4% performance regression (~60ns) when compared to the old path. This is an op that takes only one tensor argument and then 6 non-tensor arguments. Unboxed dispatch doesn\u2019t have to look at those but boxed dispatch still needs to iterate over them.\n\nThis performance difference is likely due to boxed dispatch iterating over all arguments in a loop and unboxed dispatch not having to look at non-tensor arguments.\n\nghstack-source-id: 99161484\n\nTest Plan: unit tests that call existing ops through JIT\n\nDifferential Revision: D18672405\n\nfbshipit-source-id: bf2a7056082dfad61e7e83e9eeff337060eb6944", "pr_number": "32521", "files_changed": ["aten/src/ATen/core/OpsAlreadyMovedToC10.h", "aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/dispatch/DispatchTable.h", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen.py", "aten/src/ATen/native/README.md", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "aten/src/ATen/templates/OpsAlreadyMovedToC10.cpp", "test/backward_compatibility/check_backward_compatibility.py", "test/jit/test_custom_operators.py", "tools/autograd/gen_variable_type.py", "tools/jit/gen_jit_dispatch.py", "tools/jit/templates/register_aten_ops.cpp", "torch/csrc/jit/runtime/operator.h", "torch/csrc/jit/runtime/register_c10_ops.cpp"], "labels": ["jit", "merged"]}, "b678256bfb": {"title": "Move glu to Aten(CPU) (#33179)", "body": "Summary:\nThis PR move glu to Aten(CPU).\nTest script:\n```\nimport torch\nimport torch.nn.functional as F\nimport time\n\ntorch.manual_seed(0)\n\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\n\n#warm up\nfor n in [10, 100, 1000, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n // 2, device=device)\n    for i in range(1000):\n        output = F.glu(input)\n        output.backward(grad_output)\n\nfor n in [10, 100, 1000, 10000]:\n    fwd_t = 0\n    bwd_t = 0\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n // 2, device=device)\n    for i in range(10000):\n        t1 = _time()\n        output = F.glu(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\nTest device: **skx-8180.**\nBefore:\n```\ninput size(128, 10) forward time is 0.04 (ms); backwad avg time is 0.08 (ms).\ninput size(128, 100) forward time is 0.06 (ms); backwad avg time is 0.14 (ms).\ninput size(128, 1000) forward time is 0.11 (ms); backwad avg time is 0.31 (ms).\ninput size(128, 10000) forward time is 1.52 (ms); backwad avg time is 2.04 (ms).\n```\nAfter:\n```\ninput size(128, 10) forward time is 0.02 (ms); backwad avg time is 0.05 (ms).\ninput size(128, 100) forward time is 0.04 (ms); backwad avg time is 0.09 (ms).\ninput size(128, 1000) forward time is 0.07 (ms); backwad avg time is 0.17 (ms).\ninput size(128, 10000) forward time is 0.13 (ms); backwad avg time is 1.03 (ms).\n```\nFix https://github.com/pytorch/pytorch/issues/24707, https://github.com/pytorch/pytorch/issues/24708.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33179\n\nDifferential Revision: D19839835\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: e4d3438556a1068da2c4a7e573d6bbf8d2a6e2b9", "pr_number": "33179", "files_changed": [".circleci/scripts/cpp_doc_push_script.sh", ".github/workflows/lint.yml", ".gitignore", "CONTRIBUTING.md", "aten/CMakeLists.txt", "aten/src/ATen/gen.py", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/GatedLinearUnit.cpp", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/README.md", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THVector.h", "aten/src/TH/generic/THVectorDefault.cpp", "aten/src/TH/generic/THVectorDispatch.cpp", "aten/src/TH/vector/AVX.cpp", "aten/src/TH/vector/NEON.cpp", "aten/src/TH/vector/VSX.cpp", "aten/src/THCUNN/README.md", "aten/src/THCUNN/doc/api_reference.md", "aten/src/THCUNN/doc/style_guidelines.md", "aten/src/THNN/CMakeLists.txt", "aten/src/THNN/README.md", "aten/src/THNN/THNN.h", "aten/src/THNN/doc/api_reference.md", "aten/src/THNN/doc/style_guidelines.md", "aten/src/THNN/generic/GatedLinearUnit.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp", "caffe2/CMakeLists.txt", "cmake/Codegen.cmake", "docs/cpp/source/check-doxygen.sh", "setup.py"], "labels": ["merged", "open source", "topic: porting", "triaged"]}, "69d2741480": {"title": "Add list of view ops to public doc. (#32560)", "body": "Summary:\nThis PR comes from discussion with albanD in https://fb.quip.com/npBHAXaPfnbu. Main goal is to clarify view ops with general outplace/inplace ops and remind users about the difference.\nFor reference this information is only available in code which is internal and hard to find. Also changes to this list actually affect users so we think it's better to expose it as public information. It's also helpful for new backend like XLA when implementing PyTorch ops. https://github.com/pytorch/pytorch/blob/19bbb4fccb15fdd120ff00697887331f5a2394ef/tools/autograd/gen_autograd.py#L32-L68\nPlease feel free to comment!\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32560\n\nDifferential Revision: D20161069\n\nPulled By: ailzhang\n\nfbshipit-source-id: b5f1fd4353fe7594a427784db288aeb5a37dc521", "pr_number": "32560", "files_changed": ["docs/source/index.rst", "docs/source/tensor_view.rst", "docs/source/tensors.rst", "tools/autograd/gen_autograd.py"], "labels": ["merged"]}, "991f7a20f2": {"title": "Use clog from cpuinfo/deps instead of downloading (#33947)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33947\n\nXNNPACK was downloading clog because we weren't setting CLOG_SOURCE_DIR.\nActually, it was downloading cpuinfo and pointing to the copy of clog\nwithin that.  So let's just point to the copy of clog within the cpuinfo\nsubmodule we already have.\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\nRan cmake and didn't see any downloading.\nVerified that our clog is the same as the one that was being downloaded\nwith `diff -Naur`.\n\nDifferential Revision: D20169656\n\nPulled By: suo\n\nfbshipit-source-id: ba0f7d1535f702e504fbc4f0102e567f860db94b", "pr_number": "33947", "files_changed": ["cmake/Dependencies.cmake"], "labels": ["merged"]}, "4fb8679218": {"title": "[caffe2] fix field initialization after base Clang errors (#33556)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33556\n\nFix several places exposed by Clang where order of member initializer list doesn't actually match the actual initialization order. The fix is to simply reorder member initializer lists.\n\nAlso accepted formatting changes suggested by clang-format linter.\n\nTest Plan:\n```lang=bash\nbuck build mode/opt -c fbcode.cuda_use_clang=true //fblearner/flow/projects/dper:workflow\nbuck build mode/opt //fblearner/flow/projects/dper:workflow\n```\nExecute tests on devgpu:\n```\nbuck test mode/dev-nosan -j 8 //caffe2/caffe2/python/operator_test/... //caffe2/test:cuda\n```\n\nReviewed By: ngimel\n\nDifferential Revision: D20004834\n\nfbshipit-source-id: b61c7c3f1fe8413bbb3512f6b62177a3ddf67682", "pr_number": "33556", "files_changed": ["aten/src/ATen/native/cuda/Reduce.cuh", "caffe2/operators/channelwise_conv3d_op_cudnn.cu", "caffe2/operators/depthwise_3x3_conv_op_cudnn.cu"], "labels": ["fb-exported", "merged"]}, "4377061baf": {"title": "[caffe2] fix atomicAdd redeclaration Clang error (#33559)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33559\n\nFor sm_60+ CUDA supports `atomicAdd(double*, double*)` function and for lower compute capabilities the CUDA C Programming Guide [1] suggest a user implementation as in this code. On the other side, Clang's CUDA wrappers unconditionally define this function, regardless of compute capability, and merit an error if it actually get's used.\n\nSo the problem is: when Clang is used for < sm_60, CUDA's `atomicAdd(double*, double*)` cannot be used and it cannot be redeclared in standard compliant C++.\n\nWorkaround the problem by using Clang's `enable_if` attribute [2], which has a side effect of function redeclaration.\n\n1. https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions\n2. https://clang.llvm.org/docs/AttributeReference.html#enable-if\n\nTest Plan:\n```lang=bash\nbuck build mode/opt -c fbcode.cuda_use_clang=true //fblearner/flow/projects/dper:workflow\nbuck build mode/opt //fblearner/flow/projects/dper:workflow\n```\nExecute tests on devgpu:\n```\nbuck test mode/dev-nosan -j 8 //caffe2/caffe2/python/operator_test/... //caffe2/test:cuda\n```\n\nReviewed By: ngimel\n\nDifferential Revision: D20005113\n\nfbshipit-source-id: d0d4bd6514f201af9cdeba1229bd9b798df0d02e", "pr_number": "33559", "files_changed": ["aten/src/THC/THCAtomics.cuh"], "labels": ["fb-exported", "merged"]}, "38b6cb479b": {"title": "Check fuser results when profiling (#33944)", "body": "Summary:\nWith the profiling executor enabled the fuser won't be invoked until the second pass over a script function, so some of these tests weren't correctly comparing the fused output with the interpreter output.  I've used the `checkScript` method where applicable, which seems to do the right thing.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33944\n\nTest Plan: Locally inject obvious errors into the fuser and verify that the updated tests fail when they're supposed to.\n\nDifferential Revision: D20162320\n\nPulled By: bertmaher\n\nfbshipit-source-id: 4a2f3f2d2ff1d81f23db504dc8cd0d5417bdcc50", "pr_number": "33944", "files_changed": ["test/test_jit_fuser.py"], "labels": ["merged"]}, "de55e47a4b": {"title": "Pass all ops to XLA with additional info about whether it's compound (#33908)", "body": "Summary:\nThis PR prepares us to allow XLA use `XLAPreAutograd` to override compound ops.\nTo do this we'll need to pass all ops, with additional infomation about whether it's compound or not for XLA to parse.\nCompanion PR: https://github.com/pytorch/xla/pull/1698\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33908\n\nDifferential Revision: D20149585\n\nPulled By: ailzhang\n\nfbshipit-source-id: a93140e8a34548fcabcea454386d15df58177c1d", "pr_number": "33908", "files_changed": ["tools/autograd/gen_variable_type.py"], "labels": ["merged"]}, "55b44f6746": {"title": "Throw an exception when method cannot be found from mobile module. (#33972)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33972\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20168965\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 2efe5dcb1fb80407cd88a47c50cb382ecd8aa275", "pr_number": "33972", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/mobile/module.cpp"], "labels": ["jit", "merged"]}, "2f6ffe8c39": {"title": "[jit] Resolve type annotation names to types (#29623)", "body": "Summary:\nThis adds some machinery so that we use Python to resolve types to a value and the corresponding resolution logic in `annotations.py` instead of using the string.\n\nThis PR also `slowTests` a random test since it was taking > 1 min whereas all the other tests take < 10 seconds.\n\nFixes #31864\nFixes #31950\n](https://our.intern.facebook.com/intern/diff/20144407/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29623\n\nPulled By: driazati\n\nDifferential Revision: D20144407\n\nfbshipit-source-id: ef3699f6b86039d8b4646ffc42c21bd1132d1681", "pr_number": "29623", "files_changed": ["test/jit/test_models.py", "test/test_jit_py3.py", "torch/_jit_internal.py", "torch/csrc/jit/python/script_init.cpp", "torch/jit/__init__.py", "torch/jit/_recursive.py", "torch/jit/annotations.py", "torch/jit/frontend.py"], "labels": ["jit", "merged"]}, "6e70b2da62": {"title": "Fix mobile build (#33985)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33985\n\nThis was broken by https://github.com/pytorch/pytorch/pull/32521 but only showed up in master CI builds\nghstack-source-id: 99220995\n\nTest Plan: CI\n\nDifferential Revision: D20172782\n\nfbshipit-source-id: e4bfca2a6076f1bc1c562fca9c7dfcb156bfbf3e", "pr_number": "33985", "files_changed": ["tools/jit/templates/register_aten_ops.cpp"], "labels": ["jit", "merged"]}, "2111c4ff0c": {"title": "[jit] Add missing tensor properties (#33906)", "body": "Summary:\nFixes #30775\n\nThis adds TorchScript implementations (copied from `python_variable.cpp`) for the remainin `Tensor` properties that were missing from the jit, in addition to a test that ensures new properties will trigger a failure so we can decide whether we want to add them as well.\n\nFor `some_tensor`, adds:\n\n* `some_tensor.T`\n* `some_tensor.ndim`\n* `some_tensor.is_leaf`\n* `some_tensor.name`\n](https://our.intern.facebook.com/intern/diff/20153288/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33906\n\nPulled By: driazati\n\nDifferential Revision: D20153288\n\nfbshipit-source-id: 2ddc48a14267077bc176065267e5ce52181b3d6b", "pr_number": "33906", "files_changed": ["test/jit/test_builtins.py", "test/test_jit.py", "torch/csrc/jit/frontend/builtin_functions.cpp", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["jit", "merged"]}, "85b1c45a45": {"title": "[JIT] fix alias assertion (#33952)", "body": "Summary:\nThis bug has been hit a couple times recently. We need to handle all bivariant types, not just optional, when asserting mutability/immutability of pointed-to elements in alias analysis.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33952\n\nDifferential Revision: D20166025\n\nPulled By: eellison\n\nfbshipit-source-id: cf3df9897a639641ef8303a08ba2b13523d01ef1", "pr_number": "33952", "files_changed": ["test/test_jit.py", "torch/csrc/jit/ir/alias_analysis.cpp"], "labels": ["jit", "merged"]}, "0e52627358": {"title": "Fixing pthreadpool symbol conflict issue. (#33869)", "body": "Summary:\nMainly renaming pthread_create of C2, the only one referred internally in NNPACK, that\nis conflicting, to pthread_create_c2.\nRemoved 2 other conflicting symbols that are not used internally at all.\nPointing XNNPACK to original repo instead of the fork.\n\nCopy pasted the new interface and implementation to\ncaff2/utils/threadpool, so that for internal builds we compile against\nthis.\n\nWhen threadpool is unified this will be removed.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33869\n\nDifferential Revision: D20140580\n\nPulled By: kimishpatel\n\nfbshipit-source-id: de70df0af9c7d6bc065e85ede0e1c4dd6a9e6be3", "pr_number": "33869", "files_changed": [".gitmodules", "caffe2/CMakeLists.txt", "caffe2/utils/CMakeLists.txt", "caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/ThreadPoolMobile.cc", "caffe2/utils/threadpool/ThreadPoolMobile.h", "caffe2/utils/threadpool/ThreadPoolXNNPACK.cc", "caffe2/utils/threadpool/ThreadPoolXNNPACK.h", "caffe2/utils/threadpool/pthreadpool.h", "caffe2/utils/threadpool/pthreadpool_new_if_impl.c", "caffe2/utils/threadpool/pthreadpool_utils_new_if.h", "cmake/Dependencies.cmake", "third_party/XNNPACK"], "labels": ["merged"]}, "1494005cfd": {"title": "C++ tensor indexing: more indexing tests (#30427)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30427\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18695899\n\nPulled By: yf225\n\nfbshipit-source-id: 74455fe52ef922556fabe65aefca9ec93fe2346d", "pr_number": "30427", "files_changed": ["aten/src/ATen/ATen.h", "aten/src/ATen/TensorIndexing.cpp", "aten/src/ATen/TensorIndexing.h", "aten/src/ATen/native/TensorIndexing.cpp", "aten/src/ATen/native/TensorIndexing.h", "aten/src/ATen/native/native_functions.yaml", "test/cpp/api/support.h", "test/cpp/api/tensor_indexing.cpp", "torch/csrc/autograd/python_variable_indexing.cpp"], "labels": ["merged", "module: cpp"]}, "5a8562a6af": {"title": "Fix mobile build (#34000)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34000\n\n-\nghstack-source-id: 99241400\n\nTest Plan: liujiakai\n\nDifferential Revision: D20178827\n\nfbshipit-source-id: 980ac3d1ab3d47c12613c20ee9b8dc7d083f56a9", "pr_number": "34000", "files_changed": ["tools/jit/gen_jit_dispatch.py", "tools/jit/templates/register_aten_ops.cpp"], "labels": ["jit", "merged"]}, "c596ec7eb3": {"title": "[pytorch] update code analyzer script to cover new c10::Module::def API (#33975)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33975\n\nCurrently the code analysis script doesn't go beyond the scope of the\nregistration API call, i.e. calling registration via a wrapper will not\nbe covered by the analysis - currently the new API is essentially a\nwrapper around old API.\n\nSimply adding the new API signature to the registration API pattern can\nsolve the problem for now. We might need change the analyzer code if\nthings change significantly in the future.\n\nTest Plan:\n- update test project to use the new API;\n- run analyzer against pytorch codebase;\n\nDifferential Revision: D20169549\n\nPulled By: ljk53\n\nfbshipit-source-id: c7925fa0486eee18f07e791a38c32152fee59004", "pr_number": "33975", "files_changed": ["test/mobile/op_deps/simple_ops.cpp", "tools/code_analyzer/run_analyzer.sh"], "labels": ["merged"]}, "595445e889": {"title": "Revert D20178827: Fix mobile build", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20178827\n\nOriginal commit changeset: 980ac3d1ab3d\n\nfbshipit-source-id: 9af6cb319e80c9b6a916bbdeffd69920075c7aec", "pr_number": null, "files_changed": ["tools/jit/gen_jit_dispatch.py", "tools/jit/templates/register_aten_ops.cpp"], "labels": []}, "3acfccafbb": {"title": "Revert D20172782: Fix mobile build", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20172782\n\nOriginal commit changeset: e4bfca2a6076\n\nfbshipit-source-id: 3093efd4a135f8d6c3174887ad1e3362aad9aa7c", "pr_number": null, "files_changed": ["tools/jit/templates/register_aten_ops.cpp"], "labels": []}, "7f7ea685c0": {"title": "Revert D18672405: Use codegen'ed unboxing wrappers", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD18672405\n\nOriginal commit changeset: bf2a7056082d\n\nfbshipit-source-id: b7ef1529fc266b4856e49e4dbd1fe8c7ba3d455d", "pr_number": null, "files_changed": ["aten/src/ATen/core/OpsAlreadyMovedToC10.h", "aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/dispatch/DispatchTable.h", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen.py", "aten/src/ATen/native/README.md", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "aten/src/ATen/templates/OpsAlreadyMovedToC10.cpp", "test/backward_compatibility/check_backward_compatibility.py", "test/jit/test_custom_operators.py", "tools/autograd/gen_variable_type.py", "tools/jit/gen_jit_dispatch.py", "tools/jit/templates/register_aten_ops.cpp", "torch/csrc/jit/runtime/operator.h", "torch/csrc/jit/runtime/register_c10_ops.cpp"], "labels": []}, "7747fe81c4": {"title": "reuse named tensor error message in generated code (#33536)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33536\n\nSimple fix, merge the identical string literals that were being inlined into every wrapper for ops that don't support named tensors. E.g.\n```\nTensor all(const Tensor & self, int64_t dim, bool keepdim) {\n    if (self.has_names()) {\n        AT_ERROR(\n            \"all is not yet supported with named tensors. Please drop names via \"\n            \"`tensor = tensor.rename(None)`, call the op with an unnamed tensor, \"\n            \"and set names on the result of the operation.\");\n    }\n    const OptionalDeviceGuard device_guard(device_of(self));\n    return at::native::all(self, dim, keepdim);\n}\n```\nbecomes\n```\nTensor all(const Tensor & self, int64_t dim, bool keepdim) {\n    if (self.has_names()) {\n        AT_ERROR(\"all\", named_tensors_unsupported_error);\n    }\n    const OptionalDeviceGuard device_guard(device_of(self));\n    return at::native::all(self, dim, keepdim);\n}\n```\n\nAlso updated the generated file comments to include the source template names, e.g.\n```\n// generated by aten/src/ATen/gen.py from TypeDefault.cpp\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19993407\n\nPulled By: bhosmer\n\nfbshipit-source-id: 88395a649e6ba53191332344123555c217c5eb40", "pr_number": "33536", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen.py", "aten/src/ATen/templates/LegacyTHFunctions.cpp", "aten/src/ATen/templates/SparseTypeDerived.cpp", "aten/src/ATen/templates/TypeDefault.cpp", "aten/src/ATen/templates/TypeDerived.cpp"], "labels": ["merged"]}, "ace2b4f37f": {"title": "[resubmit] try to infer rref type from python (#33992)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33992\n\nresubmit of https://github.com/pytorch/pytorch/pull/33369 with tweaks on when the rref type being created to ensure ivalue->type() hold the correct RRef type inside of inner element type.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20175043\n\nPulled By: wanchaol\n\nfbshipit-source-id: a08b178e989c995632374e6c868d23c5a85526ae", "pr_number": "33992", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "torch/csrc/jit/python/pybind_utils.h", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["jit", "merged"]}, "84ec5357d3": {"title": "Make HashNode visible (#34045)", "body": "Summary:\nHashNode and CompareNode are useful functions for hanlding jit::Node. This is to unblock https://github.com/pytorch/glow/pull/4235.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34045\n\nReviewed By: houseroad\n\nDifferential Revision: D20184733\n\nPulled By: yinghai\n\nfbshipit-source-id: 6c829f2f111a490fd2d85017475c1731cd97fb20", "pr_number": "34045", "files_changed": ["torch/csrc/jit/ir/node_hashing.h"], "labels": ["jit", "merged"]}, "15caf3b516": {"title": "move test helper functions out of test funciton (#33960)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33960\n\ntest helper functions should be out of test function. it is possible process 2 launches test functions slower than process 1, and process 1 sends request to run a helper function on process 2. process 2 may have not compile the helper function yet when process 2 starts to serve processs 1's request, and thus may return error like \"attempted to get undefined function\"\nghstack-source-id: 99205620\n\nTest Plan: test_remote_script_module was flaky for thrift backend in my local stress test runs, due to error \"attempted to get undefined function\". With fix in this diff, stress runs passed\n\nDifferential Revision: D20167969\n\nfbshipit-source-id: 8a2b9cd7bd62462e24bdbcb69ad32dca745d6956", "pr_number": "33960", "files_changed": ["torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["jit", "merged"]}, "9f7708eecb": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebookincubator/fizz/commit/8c1badaa4a17d856cdc85d44590c68d23721a95e\nhttps://github.com/facebook/fb303/commit/ce1ee4219952104c9770b4fd996fb1e47dc6c3bd\nhttps://github.com/facebook/proxygen/commit/b23caba0730f4b33031cb1cb0332f2c1cef9bd86\nhttps://github.com/facebook/wangle/commit/aa48f50c9a87c2640efc1100307ab9ccf49a481e\nhttps://github.com/facebookincubator/katran/commit/f7695cddae3e90666acde73ad538f21d9a2d5aae\nhttps://github.com/facebookincubator/mvfst/commit/8a386d95498aef40da98812117b6a0ef6edff868\nhttps://github.com/pytorch/fbgemm/commit/baab5386e2b632d054cf43796631bb0338fd8c9f\n\nTest Plan: n/a\n\nReviewed By: wittgenst\n\nfbshipit-source-id: 6c036499de97418afd9337979e89365ce13ceee7", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "b98bce8cd4": {"title": "Add MemoryFormat to TensorOptions, but not codegen. (#33704)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33704\n\nThis diff adds MemoryFormat field to TensorOptions, and teaches\nall kernels that take TensorOptions to respect it, but doesn't\nteach the codegen about it.  As such, it is now possible to specify\nmemory_format using TensorOptions syntax, e.g.,\nat::empty_like(tensor, at::memory_format(MemoryFormat::Contiguous))\nin the C++ API, but there isn't any other user visible effect.\n\nThe intended end state of this diff stack is to eliminate the\nexplicit MemoryFormat? arguments from native functions, but\nas this change has BC implications I'd prefer to do it separately.\nSo this starts things off with a non-BC breaking addition to the\nAPI.  For all internal functions that are not bound by codegen,\nI switch them to exclusively using TensorOptions (eliminating\nMemoryFormat); there's only a few, mostly quantized and to().\n\nTo keep things screwed down in the short term, it is a HARD ERROR\nto specify both the explicit MemoryFormat argument as well as\nTensorOptions.  This caught a few errors in my diff where I needed\nto modify memory format settings and then call code later, esp\nin empty_like.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20073356\n\nPulled By: bhosmer\n\nfbshipit-source-id: 18d310d7ee7cf2ee182994104652afcfc9d613e2", "pr_number": "33704", "files_changed": ["aten/src/ATen/native/TensorConversions.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/mkldnn/TensorFactories.cpp", "aten/src/ATen/native/quantized/TensorFactories.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "aten/src/ATen/quantized/Quantizer.h", "c10/core/TensorOptions.h"], "labels": ["merged"]}, "ad769d74d9": {"title": "Collapse _like overloads into a single overload. (#33705)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33705\n\nThe fact that there were two overloads appears to be a historical\nartifact that dates back to when goldsborough originally added these\nbindings in the first place.  If TensorOptions is made optional,\nthen you only need one overload, not two, as they are exactly redundant\nwith each other.  When MemoryFormat was added, it was made a little\nharder to do this, as the C++ syntax at::empty_like(t, memory_format) would\nnot work if you collapsed the overload; but now it works because TensorOptions\nsupports MemoryFormat.\n\nThe upshot is, I can get rid of all the overloads and just have one overload.\nAmazingly, this change is backwards compatible, as the test attests.  While\nI was at it, I also deleted the overload name from the functions entirely.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20073355\n\nPulled By: bhosmer\n\nfbshipit-source-id: c6a8908213b32ccf6737ea864d135e2cce34f56b", "pr_number": "33705", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/native_functions.yaml", "c10/core/TensorOptions.h", "docs/source/jit_unsupported.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/jit/test_unsupported_ops.py", "test/test_jit.py", "tools/autograd/gen_variable_factories.py", "tools/autograd/gen_variable_type.py", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/remove_inplace_ops.cpp", "torch/csrc/jit/passes/shape_analysis.cpp"], "labels": ["jit", "merged"]}, "e017b1e9fb": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/pytorch/fbgemm/commit/af57f36db0f283ab377843784681394daf0e6bff\n\nTest Plan: n/a\n\nReviewed By: wittgenst\n\nfbshipit-source-id: 4bd71218aee5e2a20a3496f2a51d464a19c0f879", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "f857fe18cd": {"title": "[ATen] Remove `AT_ASSERTM` from Blob::free_() (#33929)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33929\n\n`Blob::~Blob()` calls `Blob::free_()`. `Blob::free_()` throws and destructors should not throw.\n\nA few other minor tweaks include:\n- Remove `static_cast<void*>()` in `ShareExternal`\n- Remove default values of `pointer_` and `has_ownership_`\n\nTest Plan:\n```\nbuck test caffe2/caffe2:caffe2_test_cpu\n```\n\nhttps://our.intern.facebook.com/intern/ads/canary/424941782651397826\nhttps://our.intern.facebook.com/intern/ads/canary/424941799628450155\n\nReviewed By: yinghai\n\nDifferential Revision: D19153199\n\nfbshipit-source-id: f93983d5bf324b9a464ad2d1ed0dba13f807d2f6", "pr_number": "33929", "files_changed": ["aten/src/ATen/core/blob.h"], "labels": ["fb-exported", "merged"]}, "a500491cbc": {"title": "Fix index_put when tensor length > int_max (#33753)", "body": "Summary:\nThis PR would fix https://github.com/pytorch/pytorch/issues/33345.\n\nThe original CUDA kernel looks good. I changed most appearances of `int` to `int64_t` to avoid the CUDA memory access issue. Removed the two `TORCH_CHECK`. Added a unit test.\n\ncc csarofeen ngimel ptrblck\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33753\n\nDifferential Revision: D20185005\n\nPulled By: ngimel\n\nfbshipit-source-id: ef0abdc12ea680e10fe6b85266e2773c7a272f0d", "pr_number": "33753", "files_changed": ["aten/src/ATen/native/cuda/Indexing.cu", "test/test_indexing.py"], "labels": ["merged", "open source", "triaged"]}, "6631c2a627": {"title": "[doc] Add grad context manager doc to toplevel torch module. (#33877)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/32014\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33877\n\nDifferential Revision: D20141801\n\nPulled By: albanD\n\nfbshipit-source-id: bac713382a71666dd5e2499f710c51a55cc579ba", "pr_number": "33877", "files_changed": ["docs/source/torch.rst", "test/test_docs_coverage.py", "torch/autograd/grad_mode.py"], "labels": ["merged", "open source", "triaged"]}, "71f8624ecb": {"title": "Revert D19153199: [ATen] Remove `AT_ASSERTM` from Blob::free_()", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD19153199\n\nOriginal commit changeset: f93983d5bf32\n\nfbshipit-source-id: d79cf659f3cb26427196b9d9d1fe44e15874ad79", "pr_number": null, "files_changed": ["aten/src/ATen/core/blob.h"], "labels": []}, "f4532d7542": {"title": "Fix typo (#33925)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33925\n\nDifferential Revision: D20171970\n\nPulled By: vincentqb\n\nfbshipit-source-id: 5c1a8553760f74cecebaea7e88463b767ab81211", "pr_number": "33925", "files_changed": ["torch/csrc/jit/OVERVIEW.md"], "labels": ["jit", "merged", "open source"]}, "ad17dafc50": {"title": "[caffe2] Remove python2 from operator_test (#33977)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33977\n\nRemoving python2 from operator_test so we can retire python2 support for PyTorch.\n\nTest Plan: waitforsandcastle\n\nReviewed By: seemethere\n\nDifferential Revision: D20129500\n\nfbshipit-source-id: d4c82e4acfc795be9bec6a162c713e37ffb9f5ff", "pr_number": "33977", "files_changed": ["caffe2/python/rnn_cell.py"], "labels": ["fb-exported", "merged"]}, "31737e989d": {"title": "[aten] remove shadowed declaration warning (#34014)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34014\n\nRemove warning\n```\ncaffe2/aten/src/ATen/core/op_registration/op_registration.h: In lambda function:\ncaffe2/aten/src/ATen/core/op_registration/op_registration.h:704:47: warning: declaration of \u2018c10::DeviceType t\u2019 shadows a parameter [-Wshadow=compatible-local]\n   auto deviceTypeToDispatchKey = [](DeviceType t){\n                                               ^\ncaffe2/aten/src/ATen/core/op_registration/op_registration.h:703:21: note: shadowed declaration is here\n inline CppFunction dispatch(DeviceType t, Func&& raw_f) {\n          ~~~~~~~~~~~^\n```\n\nTest Plan: CI\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D20181155\n\nfbshipit-source-id: 41947d171369b9bd7a87e3e367492f9b2165fd6b", "pr_number": "34014", "files_changed": ["aten/src/ATen/core/op_registration/op_registration.h"], "labels": ["fb-exported", "merged"]}, "e54b8e1a47": {"title": "[CUDNN NHWC CONVOLUTION] Re-stride input tensors for wgrad in cudnn_convolution (#33784)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33784\n\nDifferential Revision: D20127485\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 9d893ffe7ff9499e7e9a7e8bed720e9441d1018e", "pr_number": "33784", "files_changed": ["aten/src/ATen/native/ConvUtils.h", "aten/src/ATen/native/cudnn/Conv.cpp", "test/test_nn.py"], "labels": ["merged", "open source", "triaged"]}, "e73d4286b0": {"title": "Fix conflict between XNNPACK's clog dependency and our cpuinfo dependency (#33922)", "body": "Summary:\nCurrently if we run\n\n```bash\nDEBUG=1 ONNX_ML=0 MAX_JOBS=8 CMAKE_CXX_COMPILER_LAUNCHER=ccache CMAKE_C_COMPILER_LAUNCHER=ccache CMAKE_CUDA_COMPILER_LAUNCHER=ccache USE_OPENMP=0 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_NCCL=0 USE_CUDA=1 USE_CUDNN=0 USE_STATIC_CUDNN=0 USE_NNPACK=0 USE_QNNPACK=0 USE_FBGEMM=0 BUILD_TEST=0 TORCH_CUDA_ARCH_LIST=\"6.1\" python setup.py develop --cmake-only\n```\n\nthen `touch build/CMakeCache.txt` (which adjusting build options will\ndo), then `python setup.py develop`, the following error message will\nshow up:\n\n```\nCMake Error at build/clog-source/CMakeLists.txt:249 (ADD_SUBDIRECTORY):\nADD_SUBDIRECTORY not given a binary directory but the given source\ndirectory \"/home/hong/wsrc/pytorch/build/clog-source\" is not a subdirectory\nof \"/home/hong/wsrc/pytorch/build/clog-source\".  When specifying an\nout-of-tree source a binary directory must be explicitly specified.\n```\n\nThis is due to a conflict between our cpuinfo submodule and XNNPACK's\nexternal clog dependency. Moving our cpuinfo upward and setting\nCLOG_SOURCE_DIR resolves the issue.\n\n ---\n\nAlso reverted https://github.com/pytorch/pytorch/issues/33947 , where `CLOG_SOURCE_DIR` as an option is not quite appropriate (given that cpuinfo uses its included clog subdir) and the setting of this variable should be a bit later when the dir of cpuinfo is known.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33922\n\nDifferential Revision: D20193572\n\nPulled By: ezyang\n\nfbshipit-source-id: 7cdbdc947a6c7e0ef10df33feccb5b20e1b3ba43", "pr_number": "33922", "files_changed": ["cmake/Dependencies.cmake"], "labels": ["merged", "module: build", "open source", "triaged"]}, "cab8772c6c": {"title": "Freezing Torchscript modules (#32178)", "body": "Summary:\nThis patch enables folding GetAttr nodes with their corresponding\nvalues. _jit_pass_freeze_module API returns a new TorchScipt module\nwhere all function calls and get attributes are inlined.\nUsage:\n\nfrozen_model = torch._C._freeze_module(scrited_model._c)\nfrozen_model.forward(...)\n\nThis API currently optimizes the forward method. We will follow up to\nto preserve and optimize methods and attributes that are annotated as\n torch.jit.interface.\n\nSeveral future improvements to JIT optimizations are required to maximize\nclean up/de-sugar the graph and eliminate redundancies.\nIdeally, we want to produce a graph that can easily be lowered to\nGLOW and other low-level backends.\n__\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32178\n\nDifferential Revision: D19419640\n\nPulled By: bzinodev\n\nfbshipit-source-id: 52baffaba9bca2cd60a8e747baa68d57711ad42b", "pr_number": "32178", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/jit_type.h", "caffe2/CMakeLists.txt", "test/jit/test_freezing.py", "test/jit/test_module_interface.py", "test/test_jit.py", "tools/build_variables.bzl", "torch/csrc/jit/api/compilation_unit.h", "torch/csrc/jit/api/module.h", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/alias_analysis.h", "torch/csrc/jit/ir/class_type.cpp", "torch/csrc/jit/passes/freeze_module.cpp", "torch/csrc/jit/passes/freeze_module.h", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/graph_executor_impl.h"], "labels": ["jit"]}, "ba4cff2ffc": {"title": "[dtype inference] Following pytorch default for float vs double (#33713)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33713\n\nDifferential Revision: D20193387\n\nPulled By: anjali411\n\nfbshipit-source-id: d802ec395df4e75e2be02e91d7288ae6fb7cf8e0", "pr_number": "33713", "files_changed": ["test/test_torch.py", "torch/csrc/utils/tensor_new.cpp"], "labels": ["merged", "module: complex"]}, "ec0f2184ba": {"title": "clang intrinsics targeting (#33958)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33958\n\nlook for clang intrinsic headers on windows\n\nTest Plan: CI green\n\nDifferential Revision: D20153573\n\nfbshipit-source-id: c87da3b0e9950d3df0bf8350df8ae592064d6613", "pr_number": "33958", "files_changed": ["aten/src/ATen/cpu/vec256/intrinsics.h", "aten/src/ATen/native/cpu/Intrinsics.h"], "labels": ["fb-exported", "merged"]}, "9956a231b9": {"title": "Fix backward compatibility tests (#34071)", "body": "Summary:\n1. As RRef has been added as a JIT type in https://github.com/pytorch/pytorch/issues/32992, we no longer need to skip them\n2. Nightly now knows about Any\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34071\n\nReviewed By: houseroad\n\nDifferential Revision: D20196963\n\nPulled By: mrshenli\n\nfbshipit-source-id: 1ea79c5682e8be9087b9cb74104e1b914c3fc456", "pr_number": "34071", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "87b3f87f27": {"title": "Migrate prelu from CUDA_tensor_apply2 to TensorIterator (#34003)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34003\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20196994\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 1749a968b1ec6636e08c11c93de43b5599e7cf4b", "pr_number": "34003", "files_changed": ["aten/src/ATen/native/cuda/Activation.cu"], "labels": ["merged", "open source", "triaged"]}, "9239608037": {"title": "fix windows clang attributes (#33959)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33959\n\nmake sure clang on windows uses correct attributes.\nadd support for cl.exe style pragma attributes\n\nTest Plan: CI green\n\nDifferential Revision: D20153548\n\nfbshipit-source-id: bfbfd374e8f5e7d7b8598453c3ca2b6693a425f1", "pr_number": "33959", "files_changed": ["caffe2/core/common.h"], "labels": ["fb-exported", "merged"]}, "4b3ae7e0af": {"title": "Enable -Werror=format compile errors on torch exception types (#34019)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33899\n\nIn the issue, we have\n```\nTypeError(\"expected %s (got %s)\", dispatch_key, toString(other.key_set()).c_str());\n```\nwhich results in `dispatch_key` being interpreted as a c-string by `sprintf`. Adding `__attrbute__((format))` to the `TypeError` constructor allows gcc or clang to detect this at compile time. Then `-Werror=format` makes it a hard error at compile time.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34019\n\nDifferential Revision: D20194842\n\nPulled By: ezyang\n\nfbshipit-source-id: fa4448916c309d91e3d949fa65bb3aa7cca5c6a8", "pr_number": "34019", "files_changed": ["CMakeLists.txt", "binaries/make_mnist_db.cc", "torch/csrc/DataLoader.cpp", "torch/csrc/Exceptions.h", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_numpy.cpp"], "labels": ["merged", "open source"]}, "51d969e86a": {"title": "preprocessor cleanup (#33957)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33957\n\nlots of small preprocessor warning cleanup for windows\n\nTest Plan: CI green\n\nReviewed By: malfet, albanD\n\nDifferential Revision: D20153582\n\nfbshipit-source-id: 18fd61c466fd1f55ededdae4448b3009a9cedc04", "pr_number": "33957", "files_changed": ["aten/src/ATen/test/scalar_test.cpp", "caffe2/observers/runcnt_observer.cc", "caffe2/operators/gelu_op.cc", "caffe2/operators/roi_align_rotated_op.cc", "caffe2/operators/sinusoid_position_encoding_op.h", "caffe2/sgd/learning_rate_functors.h", "caffe2/utils/signal_handler.cc", "caffe2/utils/smart_tensor_printer.cc", "caffe2/utils/threadpool/ThreadPool.cc", "tools/autograd/templates/Functions.cpp"], "labels": ["fb-exported", "merged"]}, "45c45195cd": {"title": "Remove warning about building from source to use the NCCL backend (#34051)", "body": "Summary:\nI think this warning isn't true anymore, and the NCCL backend works without PyTorch needing to be built from source.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34051\n\nDifferential Revision: D20195310\n\nPulled By: ezyang\n\nfbshipit-source-id: 14f879a8c43ea5efdbdf0f638792ea2b90011f4a", "pr_number": "34051", "files_changed": ["torch/distributed/distributed_c10d.py"], "labels": ["merged", "open source"]}, "11843049d5": {"title": "[jit] Fix flipped PackedSequence outputs in script (#32955)", "body": "Summary:\nStacked PRs\n * **#32955 - [jit] Fix flipped PackedSequence outputs in script**\n * #32953 - [jit] Support properties on `Device`\n\nFixes #32605\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32955\n\nPulled By: driazati\n\nDifferential Revision: D20165514\n\nfbshipit-source-id: a130c438b40e51ec27d36f021b0dc7869570aa6a", "pr_number": "32955", "files_changed": ["test/test_jit.py", "torch/nn/utils/rnn.py"], "labels": ["merged"]}, "384a4feab6": {"title": "Fix bad math typesetting (#34027)", "body": "Summary:\nFixing documentation.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34027\n\nDifferential Revision: D20195235\n\nPulled By: ezyang\n\nfbshipit-source-id: 0281bc0e8718e700e0982ced1342969b367ba57c", "pr_number": "34027", "files_changed": ["torch/nn/modules/activation.py", "torch/nn/modules/adaptive.py", "torch/nn/modules/rnn.py"], "labels": ["merged", "open source"]}, "e568c039bd": {"title": "Enable Tensor.random_(from, to) for half on CPU (#34030)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34030\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20182412\n\nPulled By: pbelevich\n\nfbshipit-source-id: b7439e6d66e1c0b9ffa8b397cab057c9146f5714", "pr_number": "34030", "files_changed": ["aten/src/ATen/native/cpu/DistributionTemplates.h", "test/test_torch.py"], "labels": ["merged"]}, "15bf4892f2": {"title": "prevent crash on exit from static destructor race (#33955)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33955\n\nunit tests on windows (clang and cl) were crashing on exit due to racing with static variable destruction.\n\nTest Plan: CI green\n\nDifferential Revision: D20153587\n\nfbshipit-source-id: 22e35e591660d49f3a755f93d0c14d7a023ebb2a", "pr_number": "33955", "files_changed": ["c10/util/Logging.cpp"], "labels": ["fb-exported", "merged"]}, "a57a7b4c29": {"title": "Change input value in examples of `BCEWithLogitsLoss` (#34053)", "body": "Summary:\nIn the examples of `BCEWithLogitsLoss`, `0.999` is passed as the prediction value. The value `0.999` seems to be a probability, but actually it's not. I think it's better to pass a value that is greater than 1, not to confuse readers.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34053\n\nDifferential Revision: D20195456\n\nPulled By: ezyang\n\nfbshipit-source-id: 3abbda6232ee1ab141d202d0ce1177526ad59c53", "pr_number": "34053", "files_changed": ["torch/nn/modules/loss.py"], "labels": ["merged", "open source"]}, "c206b4398d": {"title": "Show errors from the tasks in the thread pool (#33938)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33938\n\nMaking sure we don't silently ignore exceptions from the tasks in the\nthread pool\n\nTest Plan: python setup.py clean && python setup.py develop install\n\nDifferential Revision: D20178603\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 34971032205a1a53fb7419ed84ebb986f9e959ad", "pr_number": "33938", "files_changed": ["c10/core/thread_pool.cpp"], "labels": ["merged"]}, "a4716d0e26": {"title": "Fix lint (#34094)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34094\n\nPulled By: driazati\n\nDifferential Revision: D20201433\n\nfbshipit-source-id: d8292b329aebd232556db517b71daeee3f266bfc", "pr_number": "34094", "files_changed": ["torch/nn/utils/rnn.py"], "labels": ["merged"]}, "b874c039f6": {"title": "Allow checking for cached module before asserting (#33954)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33954\n\nfixes caffe2/core/module_test.cc on windows\nmisc lint fixes.\n\nTest Plan: CI green\n\nReviewed By: malfet\n\nDifferential Revision: D20153512\n\nfbshipit-source-id: aeae84a028e26edd65c7218611e3c49a8d9bb8c0", "pr_number": "33954", "files_changed": ["caffe2/core/module.cc"], "labels": ["fb-exported", "merged"]}, "bb4465f9f5": {"title": ".circleci: Add CUDA 10.2 to our CI pipeline (#33471)", "body": "Summary:\nAdds support for CUDA 10.2 builds on our nightly pipelines / regular test pipeliens.\n\nDepends on https://github.com/pytorch/builder/pull/404\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33471\n\nTest Plan: sandcastle_will_deliver\n\nReviewed By: ezyang\n\nDifferential Revision: D20169501\n\nPulled By: seemethere\n\nfbshipit-source-id: 43b7ca680200a67fa88ad4f7b5a121954c9f089d", "pr_number": "33471", "files_changed": [".circleci/cimodel/data/dimensions.py", ".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/scripts/binary_linux_test.sh", ".circleci/scripts/setup_ci_environment.sh", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-docker-builder.yml", ".circleci/verbatim-sources/workflows-ecr-gc.yml", ".circleci/verbatim-sources/workflows-nightly-android-binary-builds.yml", ".circleci/verbatim-sources/workflows-pytorch-ge-config-tests.yml", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml", ".jenkins/pytorch/common.sh", "cmake/public/cuda.cmake"], "labels": ["releng"]}, "77b9016a8e": {"title": "Migrate gamma grad from CUDA_tensor_apply3 to TensorIterator (#34020)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34020\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20196083\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 8659bc004678a656071263c94e929f2e1a686812", "pr_number": "34020", "files_changed": ["aten/src/ATen/native/Distributions.h", "aten/src/ATen/native/cuda/Distributions.cu"], "labels": ["merged", "open source", "triaged"]}, "ff1fc402a8": {"title": "Migrate dirichlet from CUDA_tensor_apply3 to TensorIterator (#34021)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34021\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20196082\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 9736a0ebbc529975e95a4f996dbc28e070cf1e63", "pr_number": "34021", "files_changed": ["aten/src/ATen/native/cuda/Distributions.cu"], "labels": ["merged", "open source", "triaged"]}, "1ed950e1b6": {"title": "[distributed] skip use_ignore_output tests in c10d if not built with gloo (#33513)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33513\n\nThese tests require gloo so like the other tests, they should be\nskipped if not building with gloo. Otherwise they crash on Mac if not built\nwith gloo currently.\n\nverified that it does not crash anymore with this PR.\nghstack-source-id: 99303707\n\nTest Plan: Built on Mac and verified that the tests do not fail.\n\nDifferential Revision: D19976908\n\nfbshipit-source-id: 6a2a70c3eab83efd0e188e86cabe56de4a869f4d", "pr_number": "33513", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["merged"]}, "ad3f4a32bd": {"title": "[pytorch][buck] fix selective buck build (#34090)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34090\n\nUpdate the per-op-registration template file to use the new c10 registration API.\nghstack-source-id: 99318973\n\nTest Plan:\n```\nbuck build -c pt.selective_build=1 \\\nfbandroid/mode/dev_clang_libcxx fbandroid/mode/server \\\nxplat/caffe2/fb/lite_predictor:lite_predictor_resnet\n```\n\nDifferential Revision: D20200452\n\nfbshipit-source-id: dc619cb6bdfc0c787b87475eb24b6a2da29e70e2", "pr_number": "34090", "files_changed": ["aten/src/ATen/templates/PerOpRegistration.cpp"], "labels": ["merged"]}, "3b93928ada": {"title": ".circleci: Add filter to run nightly builds on tag (#34078)", "body": "Summary:\n## What this will do:\n\nWhen the repository is tagged the current nightly build pipelines will run and upload to the `test` subdirectory in our S3 bucket for `download.pytorch.org`. Will also upload to the correct organization on anaconda [pytorch-nightly](https://anaconda.org/pytorch-test)\n\nThis is only meant for release candidates and will actually not run on any tag that does not match the release candidate regex.\n\nThis has been tested on a small scale with: https://github.com/seemethere/test-repo/commit/3ebe0ff2f8ae679ac2da6922df7b1b62458f65a2\n\n## Related PRs:\n* `.circleci: Divert packages to test channel on tag`: https://github.com/pytorch/pytorch/pull/33842\n* `.cirlceci: Swap PYTORCH_BUILD_VERSION if on tag`: https://github.com/pytorch/pytorch/pull/33326\n\n## Work to be done later:\n- [ ] Figure out how to remove manual step of updating s3 html indices.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34078\n\nDifferential Revision: D20204104\n\nPulled By: seemethere\n\nfbshipit-source-id: 685630e8a04b19fc17374585e9228a13a8c3e407", "pr_number": "34078", "files_changed": [".circleci/cimodel/data/binary_build_definitions.py", ".circleci/config.yml"], "labels": ["merged", "releng"]}, "0759191f12": {"title": "blacklist spatialBN until bitwise matching (#34092)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34092\n\nDisable op in transform map until we get bitwise matching to ice-ref\n\nTest Plan: CI\n\nReviewed By: hyuen\n\nDifferential Revision: D20177936\n\nfbshipit-source-id: e316384184cb264852e63e5edce721a8614742d1", "pr_number": "34092", "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc"], "labels": ["fb-exported", "merged"]}, "f909b5535e": {"title": "[autograd] fix allow_unused checking for C++ API (#34035)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34035\n\nBug for the conditon check in https://github.com/pytorch/pytorch/pull/24342, realized we don't have tests in either\npython or cpp to catch this, so added testes for both python and cpp.\n\nThanks hczhu on capturing it!\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20198837\n\nPulled By: wanchaol\n\nfbshipit-source-id: 33846a14c0a8e7aac2e8328189d10c38a0d7e6ee", "pr_number": "34035", "files_changed": ["test/cpp/api/autograd.cpp", "test/test_autograd.py", "torch/csrc/autograd/autograd.cpp"], "labels": []}, "0729ad733d": {"title": "Change lint from python2 -> python3 (#34107)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34107\n\nUpdates linter to only lint for python3 instead of linting for python2\n\nTest Plan: good_testplan\n\nReviewed By: orionr\n\nDifferential Revision: D20205395\n\nfbshipit-source-id: 1fa34e5fdf15f7aed96a66d2ce824a7337ee6218", "pr_number": "34107", "files_changed": [".python2", ".python3"], "labels": ["fb-exported", "merged"]}, "8a14b41617": {"title": "fix warnings reported by PVS (#33868)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33868\n\nDifferential Revision: D20169059\n\nPulled By: ailzhang\n\nfbshipit-source-id: ec12226ae27ddd89fa5bacdd35151981ebfedcfd", "pr_number": "33868", "files_changed": ["c10/core/Device.cpp", "c10/util/reverse_iterator.h", "c10/util/sparse_bitset.h", "caffe2/core/nomnigraph/include/nomnigraph/Generated/OpClasses.h", "caffe2/core/nomnigraph/include/nomnigraph/Graph/Algorithms.h", "caffe2/core/nomnigraph/include/nomnigraph/Graph/Graph.h", "caffe2/opt/annotations.cc", "caffe2/opt/converter.cc", "torch/csrc/jit/tensorexpr/eval.h"], "labels": ["jit", "merged", "open source", "triaged"]}, "0689cf8fc1": {"title": "[c10] Make __assert_fail CUDA definition compilable with clang host compiler (#34102)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34102\n\nif nvcc is invoked with clang host compiler, it will fail with the following error due to the decorators mismatch defined in cuda and c10:\n```\n error: attribute \"noreturn\" did not appear on original declaration\n```\n\nTest Plan: Build pytorch with clang\n\nReviewed By: EscapeZero\n\nDifferential Revision: D20204951\n\nfbshipit-source-id: ff7cef0db43436e50590cb4bbf1ae7302c1440fa", "pr_number": "34102", "files_changed": ["c10/macros/Macros.h"], "labels": ["fb-exported", "merged"]}, "0afee0c20b": {"title": "[rpc][metrics] add initial metric handler classes. (#33153)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33153\n\nTest Plan: Added unit tests.\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D19615364\n\nfbshipit-source-id: e0447463651390b08ad48e134cb73764d8dcf4f3", "pr_number": "33153", "files_changed": ["torch/csrc/distributed/rpc/metrics/RpcMetricsHandler.h"], "labels": ["merged"]}, "e0b90b87a4": {"title": "[C2] Fix slowness of the ReshapeOp. (#33729)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33729\n\nReshapeOp is doing some useless movements of data between CPU and GPU, which results in crazy amount of kernel calls from this operator. Which makes this operator ridiculosly slow compared to BatchMatMul for cases of pretty cheap models (for example on some versions of GAT).\n\nThis diff is moving ReshapeOp to leverage CPU storage and reduce amount of kernel calls from num_dims + 3 calls for case of 3-D\ntensor to 2 calls.\n\nTest Plan:\nUnit-tests are still passing.\n\nTODO: perf testing\n\nReviewed By: akyrola\n\nDifferential Revision: D19659491\n\nfbshipit-source-id: 2341b21e57208b988169f2df5fb598be3dc8acb2", "pr_number": "33729", "files_changed": ["caffe2/operators/reshape_op.h"], "labels": ["fb-exported", "merged"]}, "2ce9d26809": {"title": "Support cdf for mixture_same_family distribution (#33408)", "body": "Summary:\nThe new added mixture_same_family should support cdf if the family has cdf implemented.\n\nThis is very useful for flow models where cdf of mixture of gassian/logistic is used to model flow\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33408\n\nDifferential Revision: D20191552\n\nPulled By: ezyang\n\nfbshipit-source-id: 0bfd7973aa335c162919398a12ddec7425712297", "pr_number": "33408", "files_changed": ["torch/distributions/mixture_same_family.py"], "labels": ["merge-this-please", "merged", "module: distributions", "open source", "triaged"]}, "a23e8099dd": {"title": "Fix typo (#34008)", "body": "Summary:\nThis PR removes apparently unnecessary dots in the documentation of `torch.t`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34008\n\nDifferential Revision: D20195084\n\nPulled By: ezyang\n\nfbshipit-source-id: a34022de6b7a32d05a0bb3da197ee3507f4b8d8e", "pr_number": "34008", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "5be8a4e027": {"title": "find mkl installed by nuget (#34031)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34031\n\nDifferential Revision: D20221807\n\nPulled By: ezyang\n\nfbshipit-source-id: 827e2775956f408febb287676bbf9a96a70fe2d4", "pr_number": "34031", "files_changed": ["cmake/Modules/FindMKL.cmake", "tools/setup_helpers/cmake.py"], "labels": ["merge-this-please", "merged", "open source", "triaged"]}, "1aff3e2dd3": {"title": "Revert D20204104: [pytorch][PR] .circleci: Add filter to run nightly builds on tag", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20204104\n\nOriginal commit changeset: 685630e8a04b\n\nfbshipit-source-id: 1f4c890b0b199b406bac51e30febb8c6482e7e31", "pr_number": null, "files_changed": [".circleci/cimodel/data/binary_build_definitions.py", ".circleci/config.yml"], "labels": []}, "b1fd7ba019": {"title": "Revert D20169501: [pytorch][PR] .circleci: Add CUDA 10.2 to our CI pipeline", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20169501\n\nOriginal commit changeset: 43b7ca680200\n\nfbshipit-source-id: dbeb0315ccc06b8e082d019cd1ffcd97e1d38e04", "pr_number": null, "files_changed": [".circleci/cimodel/data/dimensions.py", ".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/scripts/binary_linux_test.sh", ".circleci/scripts/setup_ci_environment.sh", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-docker-builder.yml", ".circleci/verbatim-sources/workflows-ecr-gc.yml", ".circleci/verbatim-sources/workflows-nightly-android-binary-builds.yml", ".circleci/verbatim-sources/workflows-pytorch-ge-config-tests.yml", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml", ".jenkins/pytorch/common.sh", "cmake/public/cuda.cmake"], "labels": []}, "f29110fdf8": {"title": "[pytorch] blas gemm fix for k=0 (#33819)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33819\n\nThese conditions are for the specific implementation, the fallback implementation works without these checks. So use that if any of these checks isn't true.\n\nResubmit of https://github.com/pytorch/pytorch/pull/33419 (which got reverted due to a problem with XLA, but which now has been fixed)\nghstack-source-id: 99333280\n\nTest Plan: Test included\n\nDifferential Revision: D20121460\n\nfbshipit-source-id: c1056b8e26751e24078bbe80c7cb4b223bcca7cb", "pr_number": "33819", "files_changed": ["aten/src/TH/generic/THBlas.cpp", "test/test_torch.py"], "labels": ["merged"]}, "3def76583a": {"title": "[RESUBMIT] [pytorch] Migrating index_add cuda to ATen (#33548)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33548\n\nMostly just moved code.\nIndex dim and number of indices checks are added to make checks idential to index_add_cpu_\n\nThis is a resubmit of #30573, which got reverted.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20002248\n\nPulled By: gchanan\n\nfbshipit-source-id: 46df4047cb3fc1dff37a15b83c70b2cbb7a6460b", "pr_number": "33548", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/cuda/detail/IndexUtils.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorIndex.h"], "labels": ["merged"]}, "4074d559e4": {"title": "Migrate kl_div_backward from CUDA_tensor_apply3 to TensorIterator (#34022)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34022\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20196080\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 265884dc01c3260197776ee5baaadbe6b523fede", "pr_number": "34022", "files_changed": ["aten/src/ATen/native/cuda/Loss.cu"], "labels": ["merged", "open source", "triaged"]}, "5082839de5": {"title": "Migrate Lerp from CUDA_tensor_apply4 to TensorIterator (#33994)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33994\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20196788\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: e5e281460e8cca7ea3911fe56549e1ab62d50e76", "pr_number": "33994", "files_changed": ["aten/src/ATen/native/cuda/Lerp.cu"], "labels": ["merged", "open source", "triaged"]}, "1702152ef9": {"title": "fixup unit tests (#34105)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34105\n\nmake parallel_net_test.cc chronos conforming.\nexclude gtest asserts that check thrown exceptions when exceptions are disabled.\n\nTest Plan: CI green\n\nDifferential Revision: D20153525\n\nfbshipit-source-id: 7371e559da948f46773fed09e3a23a77411d59e0", "pr_number": "34105", "files_changed": ["caffe2/core/net_test.cc", "caffe2/core/parallel_net_test.cc", "caffe2/core/transform_test.cc"], "labels": ["fb-exported", "merged"]}, "7289e8e865": {"title": "[caffe2] std::numeric_limits<double>::quiet_NaN() use instead of ::nan(\"\") (#33566)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33566\n\nTest Plan:\n```lang=bash\nbuck build mode/opt -c fbcode.cuda_use_clang=true //fblearner/flow/projects/dper:workflow\nbuck build mode/opt //fblearner/flow/projects/dper:workflow\n```\nExecute tests on devgpu:\n```\nbuck test mode/dev-nosan -j 8 //caffe2/caffe2/python/operator_test/... //caffe2/test:cuda\n```\n\nReviewed By: ngimel\n\nDifferential Revision: D20006447\n\nfbshipit-source-id: ec522bc2065ad033ee2eeedd26d4a8a7a27e5f56", "pr_number": "33566", "files_changed": ["aten/src/THCUNN/ClassNLLCriterion.cu", "aten/src/THCUNN/SpatialClassNLLCriterion.cu"], "labels": ["fb-exported", "merged"]}, "5b9f1ada30": {"title": "[quant][graphmode] Observing input/output values in call site (#33277)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33277\n\nCurrently we insert observer in the called graph, which is incorrect since graphs can be shared\nand the decision of whether to insert observer or not might dependend on where the graph is called.\nFor example, for a call sequence `self.conv1(self.conv2(x))`, we can't inserting observer correctly\nif `self.conv1` and `self.conv2` are sharing the same type in the current implementation, because we insert\nobserver in the graph of the forward method of Conv2d right now and this call sequence requires us to insert\nonly one observer for the output of self.conv1/input of self.conv2.\nWe'll need to insert observers for input/output values of the graph in call site instead.\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D20208787\n\nfbshipit-source-id: 739e1d877639c0d0ed24e573bbd36211defa6836", "pr_number": "33277", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp", "torch/testing/_internal/jit_utils.py"], "labels": ["jit", "merged"]}, "51936c5ea4": {"title": "[pytorch][CI] end-to-end custom build script (#34012)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34012\n\nToday some mobile simulator tests only run on landed PRs and it requires\nsetting up special build environment to repro errors locally.\n\nThe goal of the PR is to do end-to-end mobile custom build & integration\ntests with host toolchain (using same CMake options as mobile build). This\nway, non-mobile engineers can capture & debug mobile related build issues\nmuch more easily.\n\nThere are three custom build types that this script supports:\n\n1. `TEST_DEFAULT_BUILD=1 ./build.sh` - it is similar to the prebuilt libtorch\nlibraries released for Android and iOS (same CMake build options + host\ntoolchain), which doesn't contain autograd function nor backward ops thus is\nsmaller than full LibTorch.\n\n2. `TEST_CUSTOM_BUILD_STATIC=1 ./build.sh` - it further optimizes libtorch\nsize by only including ops used by a specific model.\n\n3. `TEST_CUSTOM_BUILD_DYNAMIC=1 ./build.sh` - similar as 2) except that it\nrelies on the op dependency graph (instead of static dispatch) to calculate\nand keep all transitively dependent ops by the model.\n\nType 2) will be deprecated by type 3) in the future.\nType 3) custom build has not been fully supported yet so it's expected to fail.\n\nReplacing existing mobile build CI to run Type 1) build & integration test.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20193328\n\nPulled By: ljk53\n\nfbshipit-source-id: 48c14cae849fde86e27123f00f9911996c1cf40e", "pr_number": "34012", "files_changed": [".jenkins/pytorch/build-mobile.sh", ".jenkins/pytorch/build.sh", "test/mobile/custom_build/CMakeLists.txt", "test/mobile/custom_build/build.sh", "test/mobile/custom_build/expected_output.txt", "test/mobile/custom_build/predictor.cpp", "test/mobile/custom_build/prepare_model.py"], "labels": ["merged"]}, "0cf34cf672": {"title": "[pytorch][mobile] make sure mobile build work with dynamic dispatch (#34038)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34038\n\nMobile build doesn't include autograd/VariableType dispatch. As the\nresult AutoNonVariableTypeMode needs to be set in mobile runtime.\n\nWith static dispatch this works is done inside generated jit-dispatch\ncode - AutoNonVariableTypeMode needs to be set on per-op basis. Setting\nit globally or setting it for wrong ops might break some `is_variable()`\nchecks in the codebase.\n\nThanks to the unification of Variable class and Tensor class, all\nis_variable() checks have been removed, so AutoNonVariableTypeMode can\nbe set globally now.\n\nWe never tested inference-only mobile build with dynamic dispatch. It\nseems that dynamic dispatch also requires setting AutoNonVariableTypeMode\nfor our mobile build (where VariableType functions are not registered).\n\nVerified the end-to-end test works with this change:\n```\nTEST_CUSTOM_BUILD_DYNAMIC=1 test/mobile/custom_build/build.sh\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20193329\n\nPulled By: ljk53\n\nfbshipit-source-id: cc98414d89d12463dc82b0cdde0b6160dafc0349", "pr_number": "34038", "files_changed": ["test/mobile/custom_build/predictor.cpp"], "labels": ["merged"]}, "9b527b35bb": {"title": "CUDA Vectorized Dropout (#33879)", "body": "Summary:\nAdd vectorization to dropout kernels for both reads & writes. Moved the `masked_scale_kernel` implementation to `TensorIterator` to pick up recent autovectorization additions by zasdfgbnm , and wrote a vectorized specialization of the dropout training kernel (along with some fairly conservative dispatch logic).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33879\n\nDifferential Revision: D20222853\n\nPulled By: ngimel\n\nfbshipit-source-id: 711f56ca907fbc792a10d4bf069c28adab7d6ad7", "pr_number": "33879", "files_changed": ["aten/src/ATen/native/cuda/Dropout.cu"], "labels": ["merged", "open source", "triaged"]}, "74a0663afd": {"title": "In torch_test, mark every test that takes >5s on a DEBUG CPU-only build as slow test (#33901)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33901\n\nAfter this change, the pytest profile looks like:\n\n4.83s call     test/test_torch.py::TestTorch::test_fft_ifft_rfft_irfft\n4.23s call     test/test_torch.py::TestTorch::test_var_dim\n4.22s call     test/test_torch.py::TestTorch::test_std_dim\n4.19s call     test/test_torch.py::TestTorch::test_max\n4.06s call     test/test_torch.py::TestTorch::test_min\n3.60s call     test/test_torch.py::TestTorchDeviceTypeCPU::test_cdist_norm_batch_cpu\n2.62s call     test/test_torch.py::TestTorchDeviceTypeCPU::test_pow_cpu\n2.60s call     test/test_torch.py::TestTorch::test_matmul_small_brute_force_1d_Nd\n\nAnd the entire CPU-only test suite can be run in 88s on my Intel(R) Xeon(R) CPU\nE5-2650 v4 @ 2.20GHz\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20222288\n\nPulled By: ezyang\n\nfbshipit-source-id: 4224a9117f42566e290ae202881d76f1545cebec", "pr_number": "33901", "files_changed": ["test/test_torch.py"], "labels": ["merged"]}, "358450e02b": {"title": "improved TorchScript traceback (#33834)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33834\n\nThis changes how we report Tracebacks to make them more clear when\nthere are both serialized and non-serialized ranges. It now looks like:\n\n```\nTraceback (most recent call last):\n  File \"foo.py\", line 25, in <module>\n    s2(a, b)\n  File \"/scratch/zdevito/pytorch/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__.py\", line 7, in forward\n    x: Tensor,\n    y: Tensor) -> Tensor:\n    return (self).bar(x, y, )\n            ~~~~~~~~~ <--- HERE\n  def bar(self: __torch__.Moo,\n    x: Tensor,\n  File \"code/__torch__.py\", line 11, in bar\n    x: Tensor,\n    y: Tensor) -> Tensor:\n    _0 = (self).baz(x, y, )\n          ~~~~~~~~~ <--- HERE\n    _1 = torch.ones([3], dtype=None, layout=None, device=None, pin_memory=None)\n    return torch.add(_0, _1, alpha=1)\n  File \"code/__torch__.py\", line 17, in baz\n    x: Tensor,\n    y: Tensor) -> Tensor:\n    return torch.add(x, y, alpha=1)\n           ~~~~~~~~~ <--- HERE\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"foo.py\", line 11, in forward\n    def forward(self, x, y):\n        return self.bar(x, y)\n               ~~~~~~~~ <--- HERE\n  File \"foo.py\", line 9, in bar\n    def bar(self, x, y):\n        return self.baz(x, y) + torch.ones(3)\n               ~~~~~~~~ <--- HERE\n  File \"foo.py\", line 7, in baz\n    def baz(self, x, y):\n        return x + y\n               ~~~~~ <--- HERE\nRuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1\n```\n\nIt follows Python convension of having the most important information last\nand reading from the bottom up.\n\nChanges:\n* Moved the error message to the end, to copy Python\n* Report original traceback separate from serialized traceback\n* Make sure root functions have names in the interpreter trace.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20126136\n\nPulled By: zdevito\n\nfbshipit-source-id: fd01f9985e5d74e04c4d064c02e8bc320f4fac13", "pr_number": "33834", "files_changed": ["binaries/dump_operator_names.cc", "test/cpp/jit/test_graph_executor.cpp", "test/cpp/jit/test_interpreter.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/test_utils.cpp", "test/test_jit.py", "torch/csrc/jit/api/function.h", "torch/csrc/jit/codegen/fuser/kernel_spec.h", "torch/csrc/jit/frontend/source_range.cpp", "torch/csrc/jit/frontend/source_range.h", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/graph_executor.h", "torch/csrc/jit/runtime/graph_executor_impl.h", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/interpreter.h", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.h", "torch/csrc/jit/serialization/export_module.cpp"], "labels": ["jit", "merged"]}, "ad2825a2c9": {"title": "Add API for listing functions overridable by __torch_function__ (#33791)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33182\n\nThis adds private API functions that developers of types that implement `__torch_function__` can use to ensure full coverage of the subset of the PyTorch API that can be overrided.\n\nI've refactored some of the code in the tests into a new `torch._overrides.get_overridable_functions` function. I've also changed `TENSOR_LIKE_TORCH_OVERRIDES` into `torch._overrides.get_testing_overrides` and `IGNORED_TORCH_FUNCTIONS` into `torch._overrides.get_ignored_functions`. Making these two static global variables in the tests into functions should allow rewriting their implementation to construct their return values instead of just statically defining the return value as is done here. Currently that is blocked on not being able to inspect function signatures of compiled kernels in PyTorch (see https://github.com/pytorch/pytorch/issues/28233). See the docs I've added for usage examples of these new functions. I also refactored the existing override tests to make use of these new functions, which should be a good forcing function to make sure they're kept up-to-date.\n\nFinally, while working on this I discovered that `TestTorchFunctionOverrides.test_mean` and `TestTorchFunctionOverrides.test_mm` weren't ever being run because they were getting clobbered by the other dynamically generated override tests. I fixed that by renaming the tests and then fixing the actual test code. I've verified that all the subclassing semantics is correct and that the updated test answers are correct. I'm happy to put the fixes to the existing tests in as a separate pull request if that would be easier to review.\n\nping cpuhrsch since the feature request originally came from them.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33791\n\nDifferential Revision: D20195053\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 1585f4e405f5223932b410eae03a288dc8eb627e", "pr_number": "33791", "files_changed": ["docs/source/notes/extending.rst", "test/test_overrides.py", "torch/_overrides.py"], "labels": ["merged", "open source", "triaged"]}, "fbc9c61c81": {"title": "randn and normal_ for complex tensors (#34037)", "body": "Summary:\n1. randn and normal_ methods will work for complex tensors after this PR\n2. added an internal function for viewing complex tensors as float tensors which enables us to reuse functions defined for float tensors for complex tensors with change in arguments passed(like size, standard deviation in case of normal_). currently the resultant new float tensor doesn't share the storage with the input complex tensor which means that the version counter wouldn't be updated if any function is called on this resultant tensor, but once the dtype entry is removed from the storage class, this issue will be resolved.\n\nSide notes:\n1. didn't add a separate header for the util functions because of this issue https://github.com/pytorch/pytorch/issues/20686#issuecomment-593002293\n2. we should eventually have a public API method view_complex_as_float once (2) mentioned above gets resolved\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34037\n\nDifferential Revision: D20221793\n\nPulled By: anjali411\n\nfbshipit-source-id: a78f5e83d6104e2f55e0b250c4ec32e8d29a14eb", "pr_number": "34037", "files_changed": ["aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "test/test_torch.py"], "labels": ["merged", "module: complex"]}, "49921cad28": {"title": "Minimum build should also exclude XNNPACK (#34110)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34110\n\nDifferential Revision: D20228129\n\nPulled By: ezyang\n\nfbshipit-source-id: 24e1482f6a6ff423de966bb7a7a45ad3815791e9", "pr_number": "34110", "files_changed": ["CONTRIBUTING.md"], "labels": ["merged", "open source", "triaged"]}, "49586a2a7e": {"title": "fix sph batchnorm to use sph fma", "body": "Summary: make use of springhill's fma on SpatialBatchnorm\n\nTest Plan:\nre-enabled the unit test, ran it a couple of times\npending: net runner\n\nReviewed By: amylittleyang\n\nDifferential Revision: D20227767\n\nfbshipit-source-id: 7c601f185940249c0a32bdf95d74a20552cd2625", "pr_number": null, "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc"], "labels": []}, "9650253d70": {"title": "[caffe2] fix ambiguous call to 'fmaxType' THCHalfAutoNumerics.cuh (#33569)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33569\n\nClang reported a few places where a call to `fmaxType` is ambiguous. In all cases one of the arguments is `double` and another is `float`. Fix the error by creating a proper value 0 and remove the unneeded `ZERO_MACRO` code.\n\nTest Plan:\n```lang=bash\nbuck build mode/opt -c fbcode.cuda_use_clang=true //fblearner/flow/projects/dper:workflow\nbuck build mode/opt //fblearner/flow/projects/dper:workflow\n```\nExecute tests on devgpu:\n```\nbuck test mode/dev-nosan -j 8 //caffe2/caffe2/python/operator_test/... //caffe2/test:cuda\n```\n\nReviewed By: ngimel\n\nDifferential Revision: D20006926\n\nfbshipit-source-id: ca6cfacd57459b1c48eb5080b822d9509b03544d", "pr_number": "33569", "files_changed": ["aten/src/THCUNN/LogSigmoid.cu"], "labels": ["fb-exported", "merged"]}, "a8fc3d8c2a": {"title": "Fix HistogramObserver to not do detach on input (#34114)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33545, added a unittest\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34114\n\nDifferential Revision: D20224719\n\nPulled By: dzhulgakov\n\nfbshipit-source-id: 053d3b3b0c86340027ba1b95b5f3c247aa151aee", "pr_number": "34114", "files_changed": ["test/test_quantization.py", "torch/quantization/observer.py"], "labels": ["merged"]}, "f26bbb5f86": {"title": "[fix] flake8 lint error (#34146)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34146\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D20228830\n\nfbshipit-source-id: 41de3c27c10256939ae6309d25b0499f708a3dca", "pr_number": "34146", "files_changed": ["test/test_jit.py"], "labels": ["merged"]}, "6a97777f72": {"title": "Remove use of `.data` from optimizers (#33640)", "body": "Summary:\nRemoves all uses of `.data` from optimizers.\n\nOr tries to.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33640\n\nReviewed By: vincentqb\n\nDifferential Revision: D20203216\n\nPulled By: albanD\n\nfbshipit-source-id: 9bfe78bbed00fd4aaa690801cff0201f0bd680a0", "pr_number": "33640", "files_changed": ["torch/optim/adadelta.py", "torch/optim/adagrad.py", "torch/optim/adam.py", "torch/optim/adamax.py", "torch/optim/adamw.py", "torch/optim/asgd.py", "torch/optim/lbfgs.py", "torch/optim/rmsprop.py", "torch/optim/rprop.py", "torch/optim/sgd.py", "torch/optim/sparse_adam.py"], "labels": ["merged", "module: autograd", "module: optimizer", "open source", "topic: bc-breaking", "triaged"]}, "c93b1d427c": {"title": "[profiler] fix chrome tracing for profiler run with cuda (#33987)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33987\n\nThere was an error in\nhttps://github.com/pytorch/pytorch/pull/30724/files that resulted in\n`export_chrome_trace` generating invalid JSON. This only came up when the\nprofiler is run with `use_cuda=True` from what it looks like. In the future, we\nshould have tests that ensure we generate valid JSON because we no longer use\nthe json library.\n\nTest Plan: Add UT to validate JSON.\n\nDifferential Revision: D20171428\n\nfbshipit-source-id: ec135a154ce33f62b78d98468174dce4cf01fedf", "pr_number": "33987", "files_changed": ["test/test_autograd.py", "torch/autograd/profiler.py"], "labels": ["fb-exported", "merged"]}, "92083f31b5": {"title": "[gloo] dont hold locks in calls to buffer in ProcessGroupGloo:RecvWork::wait() and (#33926)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33926\n\nThe UnboundBuffer calls here are already protected by a mutex. We only\nneed to hold the lock while writing the shared structures completed_ and\nexception_.\nghstack-source-id: 99315427\n\nTest Plan:\nCI\n\nCI\n\nDifferential Revision: D20154546\n\nfbshipit-source-id: d1b74508c917b21acdcd0f6a914eb0455437ca0e", "pr_number": "33926", "files_changed": ["torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["merged"]}, "27f56632a4": {"title": "Migrate bce loss from CUDA_tensor_apply3 to TensorIterator (#34023)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34023\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20196084\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: bd000f09139cb848562e5310f10067db85e1b935", "pr_number": "34023", "files_changed": ["aten/src/ATen/native/cuda/Loss.cu"], "labels": ["merged", "open source", "triaged"]}, "1affaf8d10": {"title": "Migrate lerp from CUDA_tensor_apply3 to TensorIterator (#34025)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34025\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20196079\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 150d1de6632c58850020b73ee72e0ed380072926", "pr_number": "34025", "files_changed": ["aten/src/ATen/native/cuda/Lerp.cu"], "labels": ["merged", "open source", "triaged"]}, "f299c2d6e1": {"title": "Completely kill CUDA_tensor_apply3 (#34026)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34026\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20196078\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 502184f412edee90a4f4c030def277a99a7369d4", "pr_number": "34026", "files_changed": ["aten/src/ATen/cuda/CUDAApplyUtils.cuh"], "labels": ["merged", "open source", "triaged"]}, "c579976603": {"title": "Revert D20171428: [profiler] fix chrome tracing for profiler run with cuda", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20171428\n\nOriginal commit changeset: ec135a154ce3\n\nfbshipit-source-id: 51ef4351a0df33fd087edbca1b7cd753cdbf1fdf", "pr_number": null, "files_changed": ["test/test_autograd.py", "torch/autograd/profiler.py"], "labels": []}, "ba1bd41767": {"title": "Turn on strict dtype checking for test_torch.py (#33825)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33825\n\nPartially addresses #20376\n\nI do this by overriding assertEqual in classes that opt into\nthis.  This means I have to fix #33821.  The fix is a little\nunsatisfactory as idiomatic Python 2 super() calls don't work\n(since the class is no longer in scope); hopefully this will just\nwork when we go to Python 3.\n\nGeneral approach taken:\n- A lot of dtype mismatches are because we specified tensor constants\n  that infer to some dtype, but the actual dtype needed is something else.\n  Those are easy, just annotate the tensor() constructor (often a legacy\n  Tensor/FloatTensor call) with dtype\n- There are a few cases where the promotion rules are nontrivial.  Some of them\n  I just typed out the expected promotion rules manually (based on trial\n  and error)\n- There are some more complex cases; if it gets too hairy I just\n  set exact_dtype=False and nope the fuck out\n\nI don't have time to do it for all the other classes.  But the setup\nshould work if people just incrementally add the overrides to classes,\nand then eventually flip the default.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20125791\n\nPulled By: ezyang\n\nfbshipit-source-id: 389c2d1efbd93172af02f13e38ac5e92fe730c57", "pr_number": "33825", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged"]}, "04378eb618": {"title": "[JIT] Add modulelist indexing for integer literal (#29236)", "body": "Summary:\nAllow indexing into modulelists for integer literals.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29236\n\nDifferential Revision: D19583935\n\nPulled By: eellison\n\nfbshipit-source-id: 24d54051422a69769dac5e82f3bf622ded2bd8a6", "pr_number": "29236", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/sugared_value.h", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_sugared_value.h"], "labels": ["jit", "merged"]}, "7cda964e20": {"title": "Remove deprecated codepath for old-style autograd.Function (#30696) (#33956)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33956\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20167359\n\nPulled By: glaringlee\n\nfbshipit-source-id: 9b323bd29eca97bce0475225ad2b3b2ded29005d", "pr_number": "33956", "files_changed": ["test/test_autograd.py", "torch/autograd/function.py", "torch/csrc/autograd/python_function.cpp"], "labels": ["merged", "topic: bc-breaking"]}, "cb3905e8cf": {"title": ".circleci: Re-do run nightly pipelines on tag (#34148)", "body": "Summary:\nCommit that this commit relied on was found to be causing issues with\nvalgrind https://github.com/pytorch/pytorch/issues/33471\n\nRe-does https://github.com/pytorch/pytorch/issues/34078 after revert.\n\nThis reverts commit 1aff3e2dd3c3937aa1fedbfeee2143cfca25abcc.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34148\n\nDifferential Revision: D20234451\n\nPulled By: seemethere\n\nfbshipit-source-id: cb5e496a3f761beeeb0cc8df71f9ebc0b271737b", "pr_number": "34148", "files_changed": [".circleci/cimodel/data/binary_build_definitions.py", ".circleci/config.yml"], "labels": ["merged"]}, "1beb309e03": {"title": "Make DEBUG == REL_WITH_DEB_INFO on CUDA build (#34153)", "body": "Summary:\nRelated issue: https://github.com/pytorch/pytorch/issues/34079\n\nI don't know how much we care about the difference between `-G` and `-lineinfo` in `DEBUG` vs `REL_WITH_DEB_INFO`, but since `-G` never worked, let's just use `-lineinfo` on both `DEBUG` and `REL_WITH_DEB_INFO`. This would resolve the failure in `DEBUG=1` build. Locally tested to work.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34153\n\nReviewed By: ljk53\n\nDifferential Revision: D20232049\n\nPulled By: ngimel\n\nfbshipit-source-id: 4e48ff818850ba911298b0cc159522f33a305aaa", "pr_number": "34153", "files_changed": ["CMakeLists.txt"], "labels": ["merged", "open source"]}, "9d1c971b11": {"title": "[Aten] Suppress valgrind leaks in libcuda (#34169)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34169\n\nValgrind have no insight how memory is being initialized by ioctls()\n\nTest Plan: CI\n\nReviewed By: seemethere\n\nDifferential Revision: D20235974\n\nfbshipit-source-id: 46413afa4842e7d42582bbbda903438b1d98691f", "pr_number": "34169", "files_changed": ["aten/tools/valgrind.sup"], "labels": ["fb-exported", "merged"]}, "f1085a8e41": {"title": "Improve ProcessGroup RpcBackendOptions Constructor API (#34081)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34081\n\nBefore this commit, applications have to do the following to configure\nnumber of threads in ProcessGroup RPC backend:\n\n```\nop = ProcessGroupRpcBackendOptions()\nop.rpc_timeout = rpc_timeout\nop.init_method = init_method\nop.num_send_recv_threads = 32\ninit_rpc(...., rpc_backend_options=op)\n```\n\nAfter this commit, it can be simplified to:\n\n```\ninit_rpc(...., rpc_backend_options=ProcessGroupRpcBackendOptions(num_send_recv_threads=32))\n```\n\nFixes #34075\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20227344\n\nPulled By: mrshenli\n\nfbshipit-source-id: def4318e987179b8c8ecca44d7ff935702c8a6e7", "pr_number": "34081", "files_changed": ["docs/source/rpc.rst", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/backend_registry.py", "torch/distributed/rpc/constants.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "3af0dffe84": {"title": "Use double quotes in C++ to stay consistent with Python RPC docs (#34095)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34095\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20227343\n\nPulled By: mrshenli\n\nfbshipit-source-id: 69c556beee1f9e944eb1053b5ff0ac368dd99c60", "pr_number": "34095", "files_changed": ["docs/source/rpc.rst", "torch/csrc/distributed/rpc/init.cpp"], "labels": ["merged"]}, "7da24b36b1": {"title": "Apply clang-format to RPC files (#34139)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34139\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20227342\n\nPulled By: mrshenli\n\nfbshipit-source-id: 01b478bde1f6a51f69eb5277fa90ba6ac2d4b5dc", "pr_number": "34139", "files_changed": ["torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_impl.h"], "labels": ["merged"]}, "99e211e661": {"title": "[jit] Add type tags to lists/dicts in pickle (#33255)", "body": "Summary:\nStacked PRs\n * #33474 - [jit] Remove list specializations from pickler\n * **#33255 - [jit] Add type tags to lists/dicts in pickle**\n\nThis adds a global call to `torch.jit._pickle.restore_type_tags` for\nlists and dicts so that we can preserve their types after serialization.\n](https://our.intern.facebook.com/intern/diff/19868637/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33255\n\nPulled By: driazati\n\nReviewed By: xman1979, Tianshu-Bao\n\nDifferential Revision: D19868637\n\nfbshipit-source-id: 2f1826e6679a786ca209198690269f399a542c04", "pr_number": "33255", "files_changed": ["caffe2/serialize/inline_container.h", "test/cpp/jit/test_save_load.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/mobile/type_parser.cpp", "torch/csrc/jit/serialization/import.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/pickler.h", "torch/csrc/jit/serialization/unpickler.cpp", "torch/csrc/jit/serialization/unpickler.h", "torch/jit/_pickle.py"], "labels": ["jit", "merged"]}, "4edff32f81": {"title": "[c10] Fix typo in __assert_fail noreturn modifier guard (#34157)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34157\n\n`[[noreturn]` only conficts with CUDA __asert_fail defition if clang is used if host compiler\n\nTest Plan: CI\n\nReviewed By: EscapeZero\n\nDifferential Revision: D20232088\n\nfbshipit-source-id: 7182c28a15278e03175865cd0c87410c5de5bf2c", "pr_number": "34157", "files_changed": ["c10/macros/Macros.h"], "labels": ["fb-exported", "merged"]}, "31cc311143": {"title": "Expose `CUDACachingAllocator` `raw_alloc` and `raw_delete` to python (#33860)", "body": "Summary:\nThis PR aims to improve the interoperability with [CuPy](https://github.com/cupy/cupy/pulls).\n\nInstead of having two separate and conflicting memory pools. With this PR, CuPy can directly alloc memory from the PyTorch allocator by means of this proposal https://github.com/cupy/cupy/pull/3126\n\nWe would like to gather feedback to know if this approach makes sense for PyTorch, or other alternative designs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33860\n\nDifferential Revision: D20212788\n\nPulled By: ngimel\n\nfbshipit-source-id: bc1e08a66da1992d26021147bf645dc65239581c", "pr_number": "33860", "files_changed": ["c10/cuda/CUDACachingAllocator.cpp", "c10/cuda/CUDACachingAllocator.h", "test/test_cuda.py", "torch/csrc/cuda/Module.cpp", "torch/cuda/memory.py"], "labels": ["merged", "open source", "triaged"]}, "a19db54b36": {"title": "[Redo][ATen] Remove AT_ASSERTM from Blob::free_() (#34168)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34168\n\nRedo D19153199. It was reverted because it broke CI, due to the change of `AT_ASSERTM` to `TORCH_INTERNAL_ASSERT_DEBUG_ONLY`. Two problems:\n1) bug in `TORCH_INTERNAL_ASSERT_DEBUG_ONLY` about MSVC. I'm sending another diff to fix this bug.\n2) BlobTest was expecting `Blob::template Get<T>()` to throw when there is a type mismatch.\n\nFor now I'll leave `AT_ASSERTM` as it is.\n\nTest Plan:\n```\nbuck test mode/dev //caffe2/caffe2:caffe2_test_cpu -- 'BlobTest' --run-disabled\nbuck test mode/opt //caffe2/caffe2:caffe2_test_cpu -- 'BlobTest' --run-disabled\n```\n\nReviewed By: yinghai\n\nDifferential Revision: D20235225\n\nfbshipit-source-id: 594dad97c03c419afaa8f9023408bc5a119b3cfa", "pr_number": "34168", "files_changed": ["aten/src/ATen/core/blob.h"], "labels": ["fb-exported", "merged"]}, "7c20578794": {"title": "NNPI op mapping correct SpatialBN NNPI op name (#34176)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34176\n\nWrong operator name for the NNPI SpatialBN\n\nTest Plan: flow canary\n\nReviewed By: hyuen\n\nDifferential Revision: D20237933\n\nfbshipit-source-id: dfde658dcbf2482320e36d549f7d83c27df264a0", "pr_number": "34176", "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc"], "labels": ["fb-exported", "merged"]}, "7cee787a19": {"title": "[pytorch_ci] Python target determinator (#33577)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33577\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33221\n\nThis will make it so that if a pull request is just pure Python files, then we'll only run the Python tests that are connected to the dependency graph of the touched files.\n\nAssumptions made:\n- the Python code does not do dynamic imports\n- test_X.py never imports from test_Y.py\n\nRight now this is only done for test_nn (presumably the largest test entrypoint), but it's not much more work to do it for all the other test entrypoints too.\n\nTest Plan:\nCircleCI results when touching just a few Python files:\n- pytorch_macos_10_13_py3_test: 41 ->13 minutes https://circleci.com/gh/pytorch/pytorch/4550574?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link\n- pytorch_windows_vs2019_py36_cuda10.1_test1: 11 -> 2 minutes https://circleci.com/gh/pytorch/pytorch/4550846?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link\n- pytorch_windows_vs2019_py36_cuda10.1_test2: 51 -> 21 minutes https://circleci.com/gh/pytorch/pytorch/4550845?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link\n- pytorch_linux_xenial_py3_6_gcc5_4_test: 41 -> 14 minutes https://circleci.com/gh/pytorch/pytorch/4550543?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link\n\nDifferential Revision: D20009089\n\nfbshipit-source-id: 41708cc301d1c866eb92a04421d8346feb0e3cb5", "pr_number": "33577", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/pytorch-job-specs.yml", ".jenkins/pytorch/common.sh", ".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/test.sh", ".jenkins/pytorch/win-test-helpers/test_python_all_except_nn.bat", ".jenkins/pytorch/win-test-helpers/test_python_nn.bat", ".jenkins/pytorch/win-test.sh", "test/run_test.py", "test/test_determination.py"], "labels": ["fb-exported", "merged"]}, "e5bbd23ca7": {"title": "[quant][graphmode] Skip quantizing input and output in matched module (#32814)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32814\n\nWe skip quantization for the intermediate values for patterns like `Conv - ReLU`,\nbut currently we didn't skip quantizing the input/output of the graphs of matched modules,\nsince we now changed the way we add observers, this also needs to be updated.\n\nTest Plan:\npython test/test_jit.py -- 'TestJit.test_insert_observers_skip_values'\n\nImported from OSS\n\nDifferential Revision: D20208785\n\nfbshipit-source-id: ce30f2c4c8ce737500d0b41357c80ec8b33aecf9", "pr_number": "32814", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "3c042a6ab9": {"title": "[pytorch][mobile] support for custom mobile build with dynamic dispatch (#34055)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34055\n\nEnable custom mobile build with dynamic dispatch for OSS build.\n\nIt calls a python util script to calculate transitive dependencies from\nthe op dependency graph and the list of used root ops, then pass the\nresult as the op registration whitelist to aten codegen, so that only\nthese used ops are registered and kept at link time.\n\nFor custom build with dynamic dispatch to work correctly, it's critical\nto have the accurate list of used ops. Current assumption is that only\nthose ops referenced by TorchScript model are used. It works well if\nclient code doesn't call libtorch API (e.g.  tensor methods) directly;\notherwise the extra used ops need to be added to the whitelist manually,\nas shown by the HACK in prepare_model.py.\n\nAlso, if JIT starts calling extra ops independent of specific model,\nthen the extra ops need to be added to the whitelist as well.\n\nVerified the correctness of the whole process with MobileNetV2:\n```\nTEST_CUSTOM_BUILD_DYNAMIC=1 test/mobile/custom_build/build.sh\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D20193327\n\nPulled By: ljk53\n\nfbshipit-source-id: 9d369b8864856b098342aea79e0ac8eec04149aa", "pr_number": "34055", "files_changed": ["CMakeLists.txt", "cmake/Codegen.cmake", "test/mobile/custom_build/prepare_model.py", "tools/code_analyzer/gen_transitive_deps.py"], "labels": ["merged"]}, "9b39ad7f2c": {"title": "[jit] Fix iOS build (#34180)", "body": "Summary:\n`unpickler.cpp` depends on the mobile type parser all the time, so include it regardless of whether it's a mobile build or not\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34180\n\nPulled By: driazati\n\nDifferential Revision: D20241881\n\nfbshipit-source-id: a998dd2b3f1c7f58e55bb7851dc595c8ddf9eacb", "pr_number": "34180", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["merged"]}, "7d01888a75": {"title": "[JIT] Register rpc.rpc_async(..) as a JIT operator (#33329)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33329\n\n# Use case\n\n```\ntorch.jit.script\ndef send_rpc_async(dst_worker_name, user_callable_qual_name, tensor):\n    # type: (str, str, Tensor) -> None\n    rpc._rpc_async_torchscript(\n        dst_worker_name, user_callable_qual_name, args=(tensor,)\n    )\n```\n\n# Problem\n\n```\ntorch.jit.frontend.NotSupportedError: keyword-arg expansion is not supported:\n  File \"/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/rpc_spawn#binary,link-tree/torch/distributed/rpc/api.py\", line 722\n    args = args if args else ()\n    kwargs = kwargs if kwargs else {}\n    fut = _invoke_rpc_torchscript(to, qualified_name, *args, **kwargs)\n                                                               ~~~~~~ <--- HERE\n    return fut\n```\n\n# Solution\n\nRegister `rpc.rpc_async(..)` as a JIT operator to handle variable-length argument list.\n\n# Plan\n\nThis PR is the required changes to make `rpc.rpc_async(..)` a JIT prim operator, which can dynamically handle different number of arguments.\n\n- Register \"prim::rpc_async\" as a `Symbol` in \"interned_string.h\"\n- Add a if branch in \"python_sugared_value.cpp\" `toSugarValue(py::object, ..)` entry utility function to set up how JIT frontend convert `torch.distributed.rpc.rpc_async(..)` Python function (Python object) into a `SpecialFormValue` (IR SugaredValue).\n- Add a switch case for \"prim::rpc_aynsc\" Symbol in \"ir_emitter.cpp\" and `emitApplySpecialForm(..)` to set up how JIT compiler provides inputs to the \"prim::rpc_aynsc\" Operator.\n- Register \"prim::rpc_async\" as a `jit::Operator` and provide implementation in \"register_distributed_ops.cpp\".\n\nNotice, since the distributed module is an optional part when building PyTorch. The code to be added in this PR should be wrapped within preprocessing maco.\n```\n#ifdef USE_DISTRIBUTED\nnew code here\n#endif\n```\n\nTest Plan:\nItems that need to be confirmed in the test cases\n\nhttps://fb.quip.com/DCvdA9ZLjeO0\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork  \\\n\\\n&& buck-out/gen/caffe2/test/distributed/rpc/jit/rpc_fork\\#binary.par -r test_call_python_function_remotely_from_script_not_supported\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_spawn\n```\n\n```\nbuck test mode/dev-nosan //caffe2/caffe2/python/operator_test:layer_norm_op_test-2.7 -- test_layer_norm_op_jit\n```\n\nDifferential Revision: D5738300\n\nfbshipit-source-id: a4604fe762e00be062dc8232ca9790df31fb2074", "pr_number": "33329", "files_changed": ["aten/src/ATen/core/interned_strings.h", "caffe2/CMakeLists.txt", "tools/build_variables.bzl", "torch/CMakeLists.txt", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/frontend/sugared_value.h", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/alias_analysis.h", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/register_distributed_ops.cpp", "torch/csrc/jit/serialization/python_print.cpp", "torch/distributed/rpc/api.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["jit", "merged", "module: rpc"]}, "57c1b80ec2": {"title": "[pytorch]Migrate _th_ger to Aten and kill resize_scalar in codegen (#33792)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33792\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20107158\n\nPulled By: glaringlee\n\nfbshipit-source-id: bceddb2d39d3abf36f277daba537677312449c9c", "pr_number": "33792", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/native_functions.yaml"], "labels": ["merged"]}, "2ba74b741e": {"title": "Add backward Int8Quantize shape inference (#34152)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34152\n\nPropagate the input shape of Int8Quantize backwards.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/opt:bound_shape_inference_test\n```\n\nReviewed By: csummersea\n\nDifferential Revision: D20231521\n\nfbshipit-source-id: a77c61b0d5bc570241e62553cecd9ff38553ff44", "pr_number": "34152", "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h"], "labels": ["fb-exported", "merged"]}, "f097ca503d": {"title": "Add and test training in lite interpreter. (#32359)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32359\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19450614\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 6bafff39d7880a5b7fb9cd70c33a4e584812be12", "pr_number": "32359", "files_changed": ["caffe2/CMakeLists.txt", "test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h", "tools/autograd/templates/VariableType.cpp", "tools/build_variables.bzl", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h", "torch/csrc/jit/mobile/register_mobile_autograd.cpp"], "labels": ["jit", "merged"]}, "fc6dce6033": {"title": "[c10] Fix TORCH_INTERNAL_ASSERT_DEBUG_ONLY MSVC bug (#34173)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34173\n\nTest Plan:\nTemporarily change `AT_ASSERTM` to `TORCH_INTERNAL_ASSERT_DEBUG_ONLY` to test MSVC fix.\n\n```\nbuck test mode/opt //caffe2/caffe2:caffe2_test_cpu -- 'BlobTest'\n```\n\n& CI\n\nReviewed By: yinghai\n\nDifferential Revision: D20235886\n\nfbshipit-source-id: 2b7d618e924a0ede95f4a6b8f60cc08e9d58b09d", "pr_number": "34173", "files_changed": ["c10/util/Exception.h"], "labels": ["fb-exported", "merged"]}, "6d78882158": {"title": "Add layout.html to template for stable docs (#33770)", "body": "Summary:\nWhen docs are built, conf.py points to a _templates-stable/layout.html that does not exist.\nAdding this file here so future stable docs will build with Google Analytics tags and without the unstable able that is in _templates/layout.html\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33770\n\nDifferential Revision: D20164895\n\nPulled By: jlin27\n\nfbshipit-source-id: 5fca9f9b825b1484dab52e2b2d91f92ae6372371", "pr_number": "33770", "files_changed": ["docs/source/_templates-stable/layout.html"], "labels": ["merged"]}, "790274bff2": {"title": "[caffe2] Fix signed unsigned comparison warning (#34161)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34161\n\nTest Plan: CI\n\nReviewed By: EscapeZero\n\nDifferential Revision: D20232087\n\nfbshipit-source-id: 09dc8d452c5923cd2941e0cc01eac7a6677b38e8", "pr_number": "34161", "files_changed": ["caffe2/operators/square_root_divide_op.h"], "labels": ["fb-exported", "merged"]}, "fdd771c90f": {"title": "Make tracing in code gen optional (#33715)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33715\n\nTracing codes depend on the full JIT, which is not available in lite interpreter. Use `-c pt.disable_gen_tracing=1` to turn off generating tracing part.\nghstack-source-id: 99252322\n\nTest Plan:\n```\nbuck build xplat/caffe2:torch -c pt.disable_gen_tracing=1\n```\nThe tracing part of generated/VariableType_?.cpp will not be generated.\n\nReviewed By: smessmer\n\nDifferential Revision: D19684577\n\nfbshipit-source-id: a1e5b80eca5e51c7bf72b5cc8f0e36c2135fabc2", "pr_number": "33715", "files_changed": ["tools/autograd/gen_autograd.py", "tools/autograd/gen_variable_factories.py", "tools/autograd/gen_variable_type.py", "tools/setup_helpers/generate_code.py", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["merged"]}, "f6c883ccea": {"title": "TH: Defer to ATen's AVX detection code (#34088)", "body": "Summary:\nAs per https://github.com/pytorch/pytorch/issues/22338#issuecomment-593028168, this removes the AVX detection code from TH. Now the environment variable `ATEN_CPU_CAPABILITY` is the only setting needed to disable AVX/AVX2.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34088\n\nDifferential Revision: D20236039\n\nPulled By: ezyang\n\nfbshipit-source-id: eecec64b41a7a6ca7e42c1c2762032eb47af535c", "pr_number": "34088", "files_changed": ["aten/src/TH/README.md", "aten/src/TH/vector/simd.h"], "labels": ["merged", "open source", "triaged"]}, "5f4a01b2ea": {"title": "Update MAGMA to 2.5.2 for Windows (#34205)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34205\n\nDifferential Revision: D20248224\n\nPulled By: soumith\n\nfbshipit-source-id: f5e0fe06aa8f8ee551abe45db1d55d06e95ab928", "pr_number": "34205", "files_changed": [".jenkins/pytorch/win-test-helpers/installation-helpers/install_magma.bat", "docs/source/notes/windows.rst"], "labels": ["merged", "open source"]}, "c688eb28a2": {"title": "Minor fix for quantizing the Ads complex model", "body": "Summary:\nRemove Int8Relu in quantized model\nSuppress log warnings if verbose is false\n\nTest Plan: TBD\n\nReviewed By: yinghai\n\nDifferential Revision: D20202474\n\nfbshipit-source-id: 995ef8e665d8edeee810eedac831440b55271a7b", "pr_number": null, "files_changed": ["caffe2/quantization/server/dynamic_histogram.cc"], "labels": []}, "22506ae71d": {"title": "Reduce code duplication in OperatorEntry by keying hash map on optional<DispatchKey> (#33817)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33817\n\nThen, nullopt denotes catch all, whereas everything else is specific to\na DispatchKey.  I can delete the second copy of methods when I do this.\nThis refactor should be pushed all the way to the frontend but I am doing\nit one step at a time.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20125163\n\nPulled By: ezyang\n\nfbshipit-source-id: 026075a4bab81b0bd88b07f0800f6e6bbeb2166a", "pr_number": "33817", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/op_registration/op_registration.cpp", "c10/core/DispatchKey.h"], "labels": ["merged"]}, "6f52562e75": {"title": "[quant][graphmode] Add add_relu pattern in skip values (#32816)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32816\n\natt\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D20208786\n\nfbshipit-source-id: ef84b77f46f88b192a75c123aabaa203836a7dfb", "pr_number": "32816", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "78ad3dc174": {"title": "Fix Lint (#34218)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34218\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20249788\n\nPulled By: mrshenli\n\nfbshipit-source-id: 5ca2acaff5344fc4455c70af60576f8e93e54cbf", "pr_number": "34218", "files_changed": ["tools/autograd/gen_variable_factories.py", "tools/setup_helpers/generate_code.py"], "labels": ["merged"]}, "3a3fcbbc39": {"title": "Use templates instead of macros when defining bitwise operators. (#33835)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33835\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20131414\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: ec7eb7cb14e037a277cc8d71d5c9df27abf51752", "pr_number": "33835", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h"], "labels": ["merged", "open source"]}, "438f4ea0ac": {"title": "Cleaner implementation of bitwise operations of integeral types (#33849)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33849\n\nFor integral types, there is no need to manipulate with\n`reinterpret_cast` and therefore a cleaner implementation is available.\nThis might also be helpful on some less optimized compilers or on a less optimized arch (while a\ntest on gcc 8.3 x64 shows no difference in performance).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20222675\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 875890d1479f8abab4c4a19d934fe9807d12dfd2", "pr_number": "33849", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_int.h"], "labels": ["merged", "open source", "triaged"]}, "112cecc440": {"title": "Remove the use of macros when defining division between integers (#34104)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34104\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20222676\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: fb026ce7843e7931324ea82542fb07784e40efdb", "pr_number": "34104", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_int.h"], "labels": ["merged", "open source"]}, "39f78db7ec": {"title": "optimize UpSampleNearest 1d 2d and 3d performance on CPU (#31452)", "body": "Summary:\nThis PR aims at improving `UpSample` performance with `mode='nearest'` on 1D 2D and 3D, both inference and training are covered. Current implementation from 'ATen' doesn't have parallelization.\n\n1. single socket inference speedup for 1d, 2d and 3d: **63x, 57x, 46x**.\n2. single core inference speedup for 1d, 2d and 3d: **5.9x, 4.6x, 3.4x**.\n3. dual sockets training speedup for 1d, 2d and 3d: **38x, 33x, 65x**\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31452\n\nDifferential Revision: D20077828\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: a7815cf2ae344696067d2ec63bd4f4e858eaafff", "pr_number": "31452", "files_changed": ["aten/src/ATen/native/UpSample.h", "aten/src/ATen/native/UpSampleNearest1d.cpp", "aten/src/ATen/native/UpSampleNearest2d.cpp", "aten/src/ATen/native/UpSampleNearest3d.cpp", "aten/src/ATen/native/cpu/UpSampleKernel.cpp"], "labels": ["merged", "open source", "triaged"]}, "45b8c8dbcb": {"title": "[torch] Fix sign-compare warning in `torch::utils::rnn:pack_sequence` (#34185)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34185\n\nArrayRef<T>::size() is size_t\n\nTest Plan: CI\n\nReviewed By: EscapeZero\n\nDifferential Revision: D20241552\n\nfbshipit-source-id: 73cd062db810ebc5a4e34e094dfe6c7e6571ef2d", "pr_number": "34185", "files_changed": ["torch/csrc/api/include/torch/nn/utils/rnn.h"], "labels": ["fb-exported", "merged"]}, "78b81dad83": {"title": "[Dist Autograd][Better Engineering] Enhanced Error Reporting in Dist Autograd/RPC (#34179)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34179\n\nFixes: https://github.com/pytorch/pytorch/issues/27644\n\nTest Plan: Asserted `test_backward_autograd_engine_error` throws an exception with node information.\n\nDifferential Revision: D20238150\n\nfbshipit-source-id: a49b279b77416a7e0e09043aa44ed616023d8e70", "pr_number": "34179", "files_changed": ["torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/rpc/request_callback.cpp", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py"], "labels": ["fb-exported", "merged"]}, "ac6e75a165": {"title": "Revert D20195053: [pytorch][PR] Add API for listing functions overridable by __torch_function__", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20195053\n\nOriginal commit changeset: 1585f4e405f5\n\nfbshipit-source-id: 3c1aab9c60e3138d40d200ae4238bda0cddf8896", "pr_number": null, "files_changed": ["docs/source/notes/extending.rst", "test/test_overrides.py", "torch/_overrides.py"], "labels": []}, "8269c4f3d3": {"title": "Added nullptr check for pthradpool_get_threads_count (#34087)", "body": "Summary:\nWe get seg fault without this in using XNNPACK.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34087\n\nDifferential Revision: D20199787\n\nPulled By: kimishpatel\n\nfbshipit-source-id: d3d274e7bb197461632b21688820cd4c10dcd819", "pr_number": "34087", "files_changed": ["caffe2/utils/threadpool/pthreadpool_impl.cc"], "labels": ["merged"]}, "9dd5d51b01": {"title": "[ATen] Exclude CUDA tests when running `basic` under valgrind (#34181)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34181\n\nTest Plan: CI\n\nReviewed By: orionr, seemethere\n\nDifferential Revision: D20241021\n\nfbshipit-source-id: a7371afc45acc2c07a36c8216036338e14170a56", "pr_number": "34181", "files_changed": ["aten/tools/run_tests.sh"], "labels": ["fb-exported", "merged"]}, "67608cc018": {"title": "Fix MKLDNN conv2d 5d weight handling (#34115)", "body": "Summary:\nEffectively backporting https://github.com/pytorch/pytorch/pull/32422/commits/c5c00c119fd2631e8500747a04ea2ca9c9933741 before that PR lands\n\nThe bug didn't manifesting itself earlier because MkldnnConv2d constructor didn't reorder the weights. So the issue was arising only on second serialization/deserialization. This also fixes the constructor to deliver better perf right away.\n\nNote, that I still serialize 5d tensor - it was the previous behavior, we have to handle it anyway and with https://github.com/pytorch/pytorch/issues/32422 the output of `mkldnn_reorder_conv2d_weight` will always be 4d.\n\ncc pinzhenx\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34115\n\nReviewed By: wanchaol\n\nDifferential Revision: D20224685\n\nPulled By: dzhulgakov\n\nfbshipit-source-id: 24ca9227c4eb4c139096a64ae348808d7478d7dc", "pr_number": "34115", "files_changed": ["aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp", "test/test_mkldnn.py", "torch/utils/mkldnn.py"], "labels": ["merged"]}, "93990bab58": {"title": "Make use of our S3 mirror if Yann Lecunn's website is not accessible (#34215)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34215\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20251538\n\nPulled By: ezyang\n\nfbshipit-source-id: c419f0ce869aca4dede7e37ebd274a08632d10bf", "pr_number": "34215", "files_changed": ["tools/download_mnist.py"], "labels": ["merged"]}, "385067ed4f": {"title": "[pytorch][cmake] improve build mobile with host toolchain (#34187)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34187\n\nNoticed that a recent PR broke Android/iOS CI but didn't break mobile\nbuild with host toolchain. Turns out one mobile related flag was not\nset on PYTORCH_BUILD_MOBILE code path:\n```\n\"set(INTERN_DISABLE_MOBILE_INTERP ON)\"\n```\n\nFirst, move the INTERN_DISABLE_MOBILE_INTERP macro below, to stay with\nother \"mobile + pytorch\" options - it's not relevant to \"mobile + caffe2\"\nso doesn't need to be set as common \"mobile\" option;\n\nSecond, rename PYTORCH_BUILD_MOBILE env-variable to\nBUILD_PYTORCH_MOBILE_WITH_HOST_TOOLCHAIN - it's a bit verbose but\nbecomes more clear what it does - there is another env-variable\n\"BUILD_PYTORCH_MOBILE\" used in scripts/build_android.sh, build_ios.sh,\nwhich toggles between \"mobile + pytorch\" v.s. \"mobile + caffe2\";\n\nThird, combine BUILD_PYTORCH_MOBILE_WITH_HOST_TOOLCHAIN with ANDROID/IOS\nto avoid missing common mobile options again in future.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20251864\n\nPulled By: ljk53\n\nfbshipit-source-id: dc90cc87ffd4d0bf8a78ae960c4ce33a8bb9e912", "pr_number": "34187", "files_changed": ["CMakeLists.txt", "scripts/build_mobile.sh"], "labels": ["merged"]}, "17a5c67796": {"title": "Add support to dump unsupported ops. Add lite_interpter_load test. (#34072)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34072\n\nThis diff helps check all the ops not supported by lite_interpreter.\nHelpful mainly to find all the ops that need to be added instead of adding them\none by one.\n\nTest Plan:\nbuck run caffe2/binaries:lite_interpreter_model_load --\n--model=<bytecode-model-path>\n\nReviewed By: iseeyuan\n\nDifferential Revision: D20194092\n\nfbshipit-source-id: 0d596cd0204308027194af7ed738551d0c32a374", "pr_number": "34072", "files_changed": ["binaries/lite_interpreter_model_load.cc", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/function.h", "torch/csrc/jit/mobile/import.cpp"], "labels": ["fb-exported", "jit", "merged"]}, "d59e036f4d": {"title": "Revert D20194092: Add support to dump unsupported ops. Add lite_interpter_load test.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20194092\n\nOriginal commit changeset: 0d596cd02043\n\nfbshipit-source-id: 17b4bae27543f231bd6c12d90368d399ca55ebdf", "pr_number": null, "files_changed": ["binaries/lite_interpreter_model_load.cc", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/function.h", "torch/csrc/jit/mobile/import.cpp"], "labels": []}, "e236e15934": {"title": "[quant] Run weight_post_process for QAT (#33852)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33852\n\nThis fixes an issue for QAT models. During eval if we call `prepare_qat` and `convert` before calling `load_state_dict` it throws an error because the weight info (num channels) is not updated in the observer module.\nIt is not an issue for per-tensor case\n\nFixes issue #33830\n\nTest Plan:\npython test/test_quantization.py EagerModePostTrainingQuantTest.test_eval_after_train\npython test/test_quantization.py EagerModeQuantizationAwareTrainingTest.test_eval_after_train\n\nImported from OSS\n\nDifferential Revision: D20212996\n\nfbshipit-source-id: a04af8fe4df2e555270ae4d6693f5777d86f8a46", "pr_number": "33852", "files_changed": ["test/test_quantization.py", "torch/nn/quantized/modules/conv.py", "torch/nn/quantized/modules/linear.py"], "labels": ["merged"]}, "1546d2afeb": {"title": "[pytorch_ci] Don't run determination tests in py35", "body": "Test Plan: Can only really be tested in PyTorch master\n\nReviewed By: mrshenli\n\nDifferential Revision: D20260023\n\nfbshipit-source-id: b5444c376894bfccd6524cf04a71cf76eea72275", "pr_number": null, "files_changed": ["test/run_test.py"], "labels": []}, "e1c6f93f14": {"title": "Clean warning message (#34143)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34143\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20228174\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 7ab873e87be8621b0f72e8300942fd82cbc19b29", "pr_number": "34143", "files_changed": ["c10/core/TensorImpl.h"], "labels": ["merged"]}, "c62de4286e": {"title": "Add test to verify dist_autograd doesn't populate .grad field. (#33949)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33949\n\nghstack-source-id: 99419830\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D20165254\n\nfbshipit-source-id: ef4413637b1568d81e4aca053838230025df6bba", "pr_number": "33949", "files_changed": ["torch/testing/_internal/distributed/rpc/dist_autograd_test.py"], "labels": ["merged"]}, "e2ddf935bb": {"title": "Run RPC JIT tests with variable type hints only in Python >=3.6 (#34284)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34284\n\nPython 3.5 only supports function type hints.\nVariable type hints are introduced in Python 3.6.\nSo these tests with JIT type hints will fail with \"Syntax Error\" in Python 3.5 environment.\n\nghstack-source-id: 99542199\n\nTest Plan: `\n\nDifferential Revision: D7348891\n\nfbshipit-source-id: c4c71ac021f35b5e6f7ce4d3e6af10dd1d2600cc", "pr_number": "34284", "files_changed": ["test/run_test.py"], "labels": ["merged"]}, "e132047f1b": {"title": "[JIT] fix alias assertion (#34268)", "body": "Summary:\n[This check](https://github.com/eellison/pytorch/blob/019ffdca310d22fdcdc66154fbb79f1b028f24b3/torch/csrc/jit/ir/alias_analysis.cpp#L772) wasn't being triggered for None outputs of tuples, because `mustBeNone` would return false if `num_outputs != 1`.  This caused an assertion to fail in alias analysis. It's kind of a convoluted case to repro and I wasn't able to make a succinct one, but I tested internally and it fixed the bug.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34268\n\nDifferential Revision: D20261539\n\nPulled By: eellison\n\nfbshipit-source-id: 95edea10e2971727cfd3f3bc2b6bdf9dbadca6a9", "pr_number": "34268", "files_changed": ["torch/csrc/jit/ir/ir.cpp"], "labels": ["jit", "merged"]}, "2b79bab029": {"title": "[CUDA_FUSER] Fork CUDA fuser (#33527)", "body": "Summary:\nSeparating CUDA fuser from CPU fuser.\n\n1. New node in IR - prim::CudaFusionGroup:\n   This enables the cuda fuser to co-exist along side the old fuser. Allows us\n   to incrementally build and expand cuda fuser.\n\n2. copied FuseGraph optimization passes to CudaFuserGraph:\n   We will re-factor & reuse Chunk/Concat in the old fuser logic, which is\n   handled in the optimization pass at this moment. Unfortunately many code in\n   the pass is tightly binded with the legacy fuser, which makes code sharing\n   difficult.\n   The CudaFusionGraph will support only a subset of operations comparing to\n   legacy fuser (CUDA only). It is registered as a custom pass post fusion via\n     ```torch._C._jit_register_cuda_fuser()```\n   To have it in effect, you should also turn off fusion on GPU via\n     ```torch._C._jit_override_can_fuse_on_gpu(False)```\n\n3. We don't have codegen in this PR yet (WIP). Currently we just fall back to\n   the old fuser.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33527\n\nDifferential Revision: D20171598\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 9a3c0f06f46da7eaa80ae7551c04869f5b03ef71", "pr_number": "33527", "files_changed": ["aten/src/ATen/core/interned_strings.h", "caffe2/CMakeLists.txt", "tools/build_variables.bzl", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/passes/cuda_graph_fuser.cpp", "torch/csrc/jit/passes/cuda_graph_fuser.h", "torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/python/python_ir.cpp", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["jit", "merged", "open source", "triaged"]}, "4a194f89aa": {"title": "Disable MNIST test in test_xla() (#34261)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34261\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20260350\n\nPulled By: mrshenli\n\nfbshipit-source-id: b92a6b79e59bdfdf8e68b5dd73f87ea1dfd0daed", "pr_number": "34261", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["merged"]}, "d98bd5e1f5": {"title": "[test all] Back out \"Revert D20171428: [profiler] fix chrome tracing for profiler run with cuda\"", "body": "Summary:\nThere was an error in\nhttps://github.com/pytorch/pytorch/pull/30724/files that resulted in\nexport_chrome_trace generating invalid JSON. This only came up when the\nprofiler is run with use_cuda=True from what it looks like. In the future, we\nshould have tests that ensure we generate valid JSON because we no longer use\nthe json library.\nghstack-source-id: 99508836\n\nTest Plan: Added a unit test.\n\nDifferential Revision: D20237040\n\nfbshipit-source-id: 510befbdf4ec39632ac56544afcddee6c8cc3aca", "pr_number": null, "files_changed": ["test/test_autograd.py", "torch/autograd/profiler.py"], "labels": []}, "e642a65bea": {"title": "[pytorch][CI] add e2e mobile custom build jobs to CI (#34184)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34184\n\nAdd mobile custom build with static dispatch & dynamic dispatch to CI.\nMost of mobile code analysis CI should be covered by the custom build +\ndynamic dispatch flow, so changing it to running on master only.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20241774\n\nPulled By: ljk53\n\nfbshipit-source-id: f34c5748735c536ab6b42c8eb1429d8bbdaefd62", "pr_number": "34184", "files_changed": [".circleci/config.yml", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml", ".jenkins/pytorch/build-mobile.sh"], "labels": ["merged"]}, "8216d9ae64": {"title": "ONNX Export Support for NLLLoss (#33509)", "body": "Summary:\nAdding ONNX export support for torch.nn.NLLLoss().\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33509\n\nReviewed By: hl475\n\nDifferential Revision: D20052212\n\nPulled By: houseroad\n\nfbshipit-source-id: 62efcff4efa1e0e97c65ad1b670c2fc1da08d28f", "pr_number": "33509", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset12.py"], "labels": ["merged", "module: onnx", "open source", "triaged"]}, "e907128caf": {"title": "[ROCm] Enable BFloat16 type for pooling ops (#34166)", "body": "Summary:\nThis PR enables bfloat16 type for pooling ops on ROCm. Also adds bfloat16 implementation of atomicAdd since pooling ops use it.\n\nNote: Changes in the lambda function blocks is only indentation as it is now wrapped inside `AT_SKIP_BFLOAT16_IF_NOT_ROCM` macro.\n\niotamudelta ezyang bddppq\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34166\n\nDifferential Revision: D20263421\n\nPulled By: ezyang\n\nfbshipit-source-id: 3f4199ec57522e638ec29f45e22c6ec919b7816d", "pr_number": "34166", "files_changed": ["aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu", "aten/src/ATen/native/cuda/AveragePool2d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool2d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/THC/THCAtomics.cuh", "test/test_nn.py"], "labels": ["merged", "module: rocm", "open source"]}, "ed11e2536a": {"title": "[pytorch_ci] Skip determination tests in rocm", "body": "Summary: I don't know why, but this segfaults on rocm.\n\nTest Plan: Can only be tested on master\n\nReviewed By: mrshenli\n\nDifferential Revision: D20286011\n\nfbshipit-source-id: dde952449bf54ae459d36020f3e3db6fa087b39f", "pr_number": null, "files_changed": ["test/run_test.py"], "labels": []}, "3a4bac5c76": {"title": "Throw a proper error when parsing local variable annotations without assignments (#34133)", "body": "Summary:\nCurrently, putting `outputs: List[Tensor]` instead of `outputs: List[Tensor] = []` in your JITed code results in:\n```\nTraceback (most recent call last):\n  File \"custom_lstms.py\", line 453, in <module>\n    test_script_stacked_bidir_rnn(5, 2, 3, 7, 4)\n  File \"custom_lstms.py\", line 404, in test_script_stacked_bidir_rnn\n    rnn = script_lstm(input_size, hidden_size, num_layers, bidirectional=True)\n  File \"custom_lstms.py\", line 62, in script_lstm\n    other_layer_args=[LSTMCell, hidden_size * dirs, hidden_size]))\n  File \"/home/apaszke/pytorch/torch/jit/__init__.py\", line 1267, in script\n    return torch.jit._recursive.create_script_module(obj, torch.jit._recursive.infer_methods_to_compile)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 305, in create_script_module\n    return create_script_module_impl(nn_module, concrete_type, stubs_fn)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 348, in create_script_module_impl\n    script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)\n  File \"/home/apaszke/pytorch/torch/jit/__init__.py\", line 1612, in _construct\n    init_fn(script_module)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 340, in init_fn\n    scripted = create_script_module_impl(orig_value, sub_concrete_type, infer_methods_to_compile)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 348, in create_script_module_impl\n    script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)\n  File \"/home/apaszke/pytorch/torch/jit/__init__.py\", line 1612, in _construct\n    init_fn(script_module)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 340, in init_fn\n    scripted = create_script_module_impl(orig_value, sub_concrete_type, infer_methods_to_compile)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 348, in create_script_module_impl\n    script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)\n  File \"/home/apaszke/pytorch/torch/jit/__init__.py\", line 1612, in _construct\n    init_fn(script_module)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 340, in init_fn\n    scripted = create_script_module_impl(orig_value, sub_concrete_type, infer_methods_to_compile)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 348, in create_script_module_impl\n    script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)\n  File \"/home/apaszke/pytorch/torch/jit/__init__.py\", line 1612, in _construct\n    init_fn(script_module)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 340, in init_fn\n    scripted = create_script_module_impl(orig_value, sub_concrete_type, infer_methods_to_compile)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 317, in create_script_module_impl\n    stubs = stubs_fn(nn_module)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 511, in infer_methods_to_compile\n    stubs.append(make_stub_from_method(nn_module, method))\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 41, in make_stub_from_method\n    return make_stub(func)\n  File \"/home/apaszke/pytorch/torch/jit/_recursive.py\", line 34, in make_stub\n    ast = torch.jit.get_jit_def(func, self_name=\"RecursiveScriptModule\")\n  File \"/home/apaszke/pytorch/torch/jit/frontend.py\", line 173, in get_jit_def\n    return build_def(ctx, py_ast.body[0], type_line, self_name)\n  File \"/home/apaszke/pytorch/torch/jit/frontend.py\", line 206, in build_def\n    build_stmts(ctx, body))\n  File \"/home/apaszke/pytorch/torch/jit/frontend.py\", line 129, in build_stmts\n    stmts = [build_stmt(ctx, s) for s in stmts]\n  File \"/home/apaszke/pytorch/torch/jit/frontend.py\", line 129, in <listcomp>\n    stmts = [build_stmt(ctx, s) for s in stmts]\n  File \"/home/apaszke/pytorch/torch/jit/frontend.py\", line 181, in __call__\n    return method(ctx, node)\n  File \"/home/apaszke/pytorch/torch/jit/frontend.py\", line 294, in build_AnnAssign\n    rhs = build_expr(ctx, stmt.value)\n  File \"/home/apaszke/pytorch/torch/jit/frontend.py\", line 180, in __call__\n    raise UnsupportedNodeError(ctx, node)\n  File \"/home/apaszke/pytorch/torch/jit/frontend.py\", line 116, in __init__\n    source_range = ctx.make_range(offending_node.lineno,\nAttributeError: 'NoneType' object has no attribute 'lineno'\n```\n\nThis patch makes the error message more reasonable:\n```\ntorch.jit.frontend.UnsupportedNodeError: annotated assignments without assigned value aren't supported:\n  File \"custom_lstms.py\", line 221\n        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\n        inputs = reverse(input.unbind(0))\n        outputs: List[Tensor]\n        ~ <--- HERE\n        for i in range(len(inputs)):\n            out, state = self.cell(inputs[i], state)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34133\n\nDifferential Revision: D20249076\n\nPulled By: ezyang\n\nfbshipit-source-id: 40ec34ad38859f9fe56f379d3f8d08644b00fab9", "pr_number": "34133", "files_changed": ["torch/jit/frontend.py"], "labels": ["jit", "merge-this-please", "merged", "open source", "triaged"]}, "75d29f8d3e": {"title": "Allow converting IValue to vector<string> (#34269)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34269\n\nfollow up for https://github.com/pytorch/pytorch/pull/16519\n\nTest Plan: unit tests\n\nReviewed By: houseroad\n\nDifferential Revision: D20261495\n\nfbshipit-source-id: 947f3cbd469d9258ec2dbb36cb68efe15a3b19eb", "pr_number": "34269", "files_changed": ["caffe2/core/operator.h"], "labels": ["fb-exported", "merged"]}, "9ce833879f": {"title": "[JIT] Introduce a fake Tensor creation node for IR unit tests (#33914)", "body": "Summary:\n**Summary**\nThere is often a need to create a Tensor when writing IR by hand for JIT\noptimisation pass unit tests. The only options for this today are real\nTensor creation functions like `aten::ones`. Any test that uses these functions\nmust also use the same default arguments as the Python/C++ API, which means\nthat all of the tests have to be updated when the API is updated. This commit\nintroduces a new primitive, `prim::MakeTestTensor` with schema `() -> Tensor` that\nshould be used in unit tests instead of real Tensor creation functions. This new\nprimitive has no public-facing API, so the maintenance burden is much lower.\n\n**Testing**\nThis commit updates the alias analysis and DCE tests to use `prim::MakeTestTensor` instead of\n`aten::rand`, `aten::ones`, and `aten::zeros`.\n\n```\n$ ./bin/test_jit\nCUDA not available. Disabling CUDA and MultiCUDA tests\nNote: Google Test filter = *-*_CUDA:*_MultiCUDA\n[==========] Running 75 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 75 tests from JitTest\n[ RUN      ] JitTest.ADFormulas\n[       OK ] JitTest.ADFormulas (82 ms)\n[ RUN      ] JitTest.Attributes\n[       OK ] JitTest.Attributes (0 ms)\n...\n...\n...\n[ RUN      ] JitTest.LiteInterpreterPrim\n[       OK ] JitTest.LiteInterpreterPrim (0 ms)\n[ RUN      ] JitTest.LiteInterpreterLoadOrigJit\n[       OK ] JitTest.LiteInterpreterLoadOrigJit (2 ms)\n[----------] 75 tests from JitTest (150 ms total)\n\n[----------] Global test environment tear-down\n[==========] 75 tests from 1 test case ran. (150 ms total)\n[  PASSED  ] 75 tests.\n```\n\n**Fixes**\nThis pull request fixes https://github.com/pytorch/pytorch/issues/33500.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33914\n\nDifferential Revision: D20150304\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: c88f5289055a02dc20b7a5dcdf87469f9816d020", "pr_number": "33914", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_base.cpp", "test/cpp/jit/test_dce.cpp"], "labels": ["jit", "merged"]}, "9651088228": {"title": "Tuck the packing logic into Int8FCPackWeight op (#34289)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34289\n\nTest Plan:\n```\n buck test caffe2/caffe2/quantization/server:fully_connected_dnnlowp_op_test\n```\n\nReviewed By: csummersea\n\nDifferential Revision: D20275538\n\nfbshipit-source-id: 699ca2a145c7c9a50b0fdab7bd68d8557a031ac0", "pr_number": "34289", "files_changed": ["caffe2/quantization/server/fbgemm_pack_op.cc", "caffe2/quantization/server/fully_connected_dnnlowp_op.cc", "caffe2/quantization/server/fully_connected_dnnlowp_op_test.py"], "labels": ["fb-exported", "merged"]}, "ff2731b45c": {"title": "Revert \"Disable MNIST test in test_xla() (#34261)\" (#34316)", "body": "Summary:\nShould be passing now ;)\nThis reverts commit 4a194f89aadc7cd1d7e24622b53855cfb885da75.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34316\n\nReviewed By: mrshenli\n\nDifferential Revision: D20287196\n\nPulled By: ailzhang\n\nfbshipit-source-id: 1cc48a11edcc48a0ec4161c94487912eba63c9a5", "pr_number": "34316", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["merged"]}, "beb4309406": {"title": "[ONNX] Reduce ONNX test time on CI (#33242)", "body": "Summary:\nAmong all ONNX tests, ONNXRuntime tests are taking the most time on CI (almost 60%).\nThis is because we are testing larger models (mainly torchvision RCNNs) for multiple onnx opsets.\nI decided to divide tests between two jobs for older/newer opsets. This is now reducing the test time from 2h to around 1h10mins.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33242\n\nReviewed By: hl475\n\nDifferential Revision: D19866498\n\nPulled By: houseroad\n\nfbshipit-source-id: 446c1fe659e85f5aef30efc5c4549144fcb5778c", "pr_number": "33242", "files_changed": [".circleci/cimodel/data/caffe2_build_data.py", ".circleci/cimodel/data/caffe2_build_definitions.py", ".circleci/config.yml", ".circleci/scripts/should_run_job.py", "scripts/onnx/test.sh", "test/onnx/test_utility_funs.py"], "labels": ["jit", "merged", "open source", "triaged"]}, "479c3b0aa5": {"title": "[JIT] add support for torch.norm (#33783)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33783\n\nFix for https://github.com/pytorch/pytorch/issues/20113\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20121917\n\nPulled By: eellison\n\nfbshipit-source-id: ffedcc40678cd80f5529ff9323088eed544e5158", "pr_number": "33783", "files_changed": ["docs/source/jit_unsupported.rst", "test/jit/test_unsupported_ops.py", "test/test_jit.py", "torch/functional.py", "torch/jit/_builtins.py"], "labels": ["jit", "merged"]}, "f218842f2e": {"title": "[JIT] Add support for list() (#33818)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33818\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20121915\n\nPulled By: eellison\n\nfbshipit-source-id: c6c4ef444dbf1d4134dccb28c13315e225945b64", "pr_number": "33818", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/functional.py"], "labels": ["jit", "merged"]}, "765c5b1c95": {"title": ".circleci: Add CUDA 10.2 to CI (#34241)", "body": "Summary:\nBasically a re-do of https://github.com/pytorch/pytorch/pull/33471\n\nShould be safe to merge now that https://github.com/pytorch/pytorch/issues/34135 has been merged.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34241\n\nDifferential Revision: D20292711\n\nPulled By: seemethere\n\nfbshipit-source-id: c508b5ef58f52aa3a263fd33b0373f31719fa0a4", "pr_number": "34241", "files_changed": [".circleci/cimodel/data/dimensions.py", ".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/scripts/binary_linux_test.sh", ".circleci/scripts/setup_ci_environment.sh", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-docker-builder.yml", ".circleci/verbatim-sources/workflows-ecr-gc.yml", ".circleci/verbatim-sources/workflows-nightly-android-binary-builds.yml", ".circleci/verbatim-sources/workflows-pytorch-ge-config-tests.yml", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml", "cmake/public/cuda.cmake"], "labels": ["merged"]}, "ccf4d69b75": {"title": "[Lite Interpreter] Enable __setstate__ (#33294)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33294\n\n1. Serialize bytecode of __setstate__ and run it when loading the model.\n2. One use case is quantization. To test this use case a few operators are registered temporarily for lite interpreter. The \"_\" prefix registration will be removed when the operators are all migrated to mobile.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20162898\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 7a3180807bf38fbce594d86993896861f12bb58c", "pr_number": "33294", "files_changed": ["aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h", "torch/csrc/jit/mobile/register_mobile_ops.cpp", "torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/import.cpp"], "labels": ["jit", "merged"]}, "2af64ba3ed": {"title": "Allow output to zero-strided tensors if the size is <= 1 along that dim (#34100)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33812\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34100\n\nDifferential Revision: D20267778\n\nPulled By: ngimel\n\nfbshipit-source-id: 1b84c4f6e6bf5d29c3698daa3cb71554b25c1eee", "pr_number": "34100", "files_changed": ["aten/src/ATen/MemoryOverlap.cpp", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "78aebbcb88": {"title": "[JIT] add other module apis (#34106)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34106\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20283996\n\nPulled By: eellison\n\nfbshipit-source-id: 88e7bc4547e96717d6c8efe0b25ede0d198d9e68", "pr_number": "34106", "files_changed": ["test/jit/test_module_interface.py", "test/test_jit.py", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_sugared_value.h"], "labels": ["jit", "merged"]}, "5500c3de0a": {"title": "Revert D20150304: [pytorch][PR] [JIT] Introduce a fake Tensor creation node for IR unit tests", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20150304\n\nOriginal commit changeset: c88f5289055a\n\nfbshipit-source-id: 14ac0e46145e9fb4f200c6318b63edd541380aeb", "pr_number": null, "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_base.cpp", "test/cpp/jit/test_dce.cpp"], "labels": []}, "e4a883e601": {"title": "cuDNN convolution try multiple algo (#33073)", "body": "Summary:\nFixes: https://github.com/pytorch/pytorch/issues/31336 https://github.com/pytorch/pytorch/issues/1664\n\nSometimes cuDNN heuristics return algorithms that can not be used. Instead of just using the first algorithm returned, we should try these algorithms one by one until one of them succeed.\n\nBenchmark:\nhttps://github.com/zasdfgbnm/things/blob/master/2020Q1/conv-benchmark.ipynb\n```python\ni = torch.randn(256, 3, 256, 256).cuda()\nc = torch.nn.Conv2d(3, 3, 3, 3).cuda()\n\n%timeit c(i); torch.cuda.synchronize()\n```\nbefore vs after = 498 vs 490 \u00b5s\n\nThe performance is improved I guess because, before this PR, we always call the heuristics to get the algorithm, but after this PR, we only do at the first time.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33073\n\nDifferential Revision: D20284755\n\nPulled By: ngimel\n\nfbshipit-source-id: b03af37c75939ca50c2cb401c706ba26914dd10e", "pr_number": "33073", "files_changed": ["aten/src/ATen/cuda/Exceptions.h", "aten/src/ATen/native/cudnn/Conv.cpp"], "labels": ["merged", "open source", "triaged"]}, "1cf12b7e53": {"title": "[quant] Fix histogram observer to work with QAT on GPU (#34232)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34232\n\nBy default `torch.zeros` creates the tensor on GPU. Need to specify the device argument to get it to work correctly on GPU during QAT.\n\nTest Plan:\n1. Tested by running QAT on GPU\n\n2. python test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D20286351\n\nfbshipit-source-id: 745723c85d902870c56c1c7492f26cb027ae9dc6", "pr_number": "34232", "files_changed": ["torch/quantization/observer.py"], "labels": ["merged"]}, "89d314b5d5": {"title": "[pytorch] update mobile docker image version (#34337)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34337\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20296975\n\nPulled By: ljk53\n\nfbshipit-source-id: bc4a39689dca22e4530f25225f1884eda9bc74de", "pr_number": "34337", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml"], "labels": ["merged"]}, "d2b5eb2a45": {"title": "[ONNX] Fix for random generators export (#33789)", "body": "Summary:\nExport random generator with dynamic input size\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33789\n\nReviewed By: hl475\n\nDifferential Revision: D20121175\n\nPulled By: houseroad\n\nfbshipit-source-id: c16d11eb07678166d125759d97aadfcd7c80ef14", "pr_number": "33789", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source", "triaged"]}, "434af5d94a": {"title": "[quant] Speed up per-channel min-max observer (#34118)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34118\n\nPreviously calc_per_channel_qparams was using for loops and python primitives, which called `item` many times causing slowdown during training.\n    These changes uses torch primitives on the tensor to speed up the operation over 60x\n\n    Perf results on MobileNetV2 during training using autograd profiler\n\n    FP32 forward call -\n    Self CPU time total: 47.222ms\n    CUDA time total: 124.001ms\n\n    before change\n    FakeQuant Model -\n    Self CPU time total: 19.107s\n    CUDA time total: 27.177s\n\n    after change\n    FakeQuant Model -\n    Self CPU time total: 404.667ms\n    CUDA time total: 446.344ms\n\nTest Plan:\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D20287841\n\nfbshipit-source-id: 6b706b8206e0d0da3c3c217b014e8da5b71b870d", "pr_number": "34118", "files_changed": ["torch/quantization/observer.py"], "labels": ["merged"]}, "02478984d6": {"title": "Add support to dump unsupported ops. Add lite_interpter_load test. (#34278)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34278\n\nThis diff helps check all the ops not supported by lite_interpreter.\nHelpful mainly to find all the ops that need to be added instead of adding them\none by one.\n\nTest Plan:\nbuck run caffe2/binaries:lite_interpreter_model_load --\n--model=<bytecode-model-path>\n\nReviewed By: iseeyuan\n\nDifferential Revision: D20266341\n\nfbshipit-source-id: 5a6c7a5bc52f910cea82a72045870da8105ccb87", "pr_number": "34278", "files_changed": ["binaries/lite_interpreter_model_load.cc", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/function.h", "torch/csrc/jit/mobile/import.cpp"], "labels": ["fb-exported", "jit", "merged"]}, "f4da78f1b3": {"title": "Remove RPC TorchScript private API (#33978)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33978\n\nWe can directly pass user_callbale to rpc_async API in TorchScript. There is no need to have private API for taking qualified name.\nghstack-source-id: 99600360\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/jit/rpc_fork\\#binary.par -r test_torchscript_functions_not_supported\n```\n\nDifferential Revision: D7420993\n\nfbshipit-source-id: 228c15b21848e67418fab780e3fd6a1c6da5142d", "pr_number": "33978", "files_changed": ["torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["jit", "merged", "module: rpc"]}, "76035f050b": {"title": "[C++ API Parity] Adam: updated step and class design (#33730)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33730\n\nDifferential Revision: D20292073\n\nPulled By: anjali411\n\nfbshipit-source-id: a7b4a70f29027ab355aebb91873ea55d5cb51783", "pr_number": "33730", "files_changed": ["test/cpp/api/serialize.cpp", "torch/csrc/api/include/torch/optim/adam.h", "torch/csrc/api/include/torch/optim/optimizer.h", "torch/csrc/api/include/torch/optim/serialize.h", "torch/csrc/api/src/optim/adagrad.cpp", "torch/csrc/api/src/optim/adam.cpp", "torch/csrc/api/src/optim/optimizer.cpp", "torch/csrc/api/src/optim/sgd.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "38857734f0": {"title": "[JIT] fix py35 test (#34350)", "body": "Summary:\ntest_module_interfaces was using syntax only supported in >= 3.6\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34350\n\nReviewed By: mrshenli\n\nDifferential Revision: D20298869\n\nPulled By: eellison\n\nfbshipit-source-id: 22319ca403113cff2eedf57767bb34d9580e6db3", "pr_number": "34350", "files_changed": ["test/test_jit.py"], "labels": ["merged"]}, "b50825e011": {"title": "Make RecordFunction more robust for async use cases (#34122)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34122\n\nEarlier work added support for async rpc cases when RecordFunction's\nend callbacks might be called in a different thread; in addition some\nextra care was needed to handle pointer to parent function;\n\nThis PR makes RecordFunction aware of potentially multiple threads in\nuse, as well as removes unused parent() call and restricts current()\nRecordFunction to scope-based record functions (RECORD_FUNCTION macro)\n\nTest Plan: unit tests\n\nDifferential Revision: D20297709\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 46a59e1b2eea0bbd8a59630385e193b38d30f9d1", "pr_number": "34122", "files_changed": ["test/cpp/jit/test_misc.cpp", "torch/csrc/autograd/profiler.cpp", "torch/csrc/autograd/profiler.h", "torch/csrc/autograd/record_function.cpp", "torch/csrc/autograd/record_function.h", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/distributed/autograd/utils.cpp"], "labels": ["jit", "merged"]}, "9a5e9d8cec": {"title": "[pytorch][mobile] change mobile build scripts to build PyTorch by default (#34203)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34203\n\nCurrently cmake and mobile build scripts still build libcaffe2 by\ndefault. To build pytorch mobile users have to set environment variable\nBUILD_PYTORCH_MOBILE=1 or set cmake option BUILD_CAFFE2_MOBILE=OFF.\n\nPyTorch mobile has been released for a while. It's about time to change\nCMake and build scripts to build libtorch by default.\n\nChanged caffe2 CI job to build libcaffe2 by setting BUILD_CAFFE2_MOBILE=1\nenvironment variable. Only found android CI for libcaffe2 - do we ever\nhave iOS CI for libcaffe2?\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20267274\n\nPulled By: ljk53\n\nfbshipit-source-id: 9d997032a599c874d62fbcfc4f5d4fbf8323a12e", "pr_number": "34203", "files_changed": [".circleci/config.yml", ".circleci/scripts/binary_ios_build.sh", ".circleci/verbatim-sources/job-specs-custom.yml", ".jenkins/caffe2/build.sh", ".jenkins/pytorch/build.sh", "CMakeLists.txt", "android/build_test_app.sh", "caffe2/CMakeLists.txt", "ios/TestApp/README.md", "scripts/build_android.sh", "scripts/build_ios.sh", "scripts/build_mobile.sh", "scripts/build_pytorch_android.sh"], "labels": ["merged"]}, "b8fd88319a": {"title": "C++ make torch::nn::Sequential push_back(AnyModule) methods public (#34208)", "body": "Summary:\nIssue https://github.com/pytorch/pytorch/issues/33192\nMoves Sequential::push_back methods with AnyModule from private -> public\nAllows adding an existing AnyModule via something like:\n\n```\n  torch::nn::Sequential q;\n  auto a=torch::nn::AnyModule(torch::nn::Linear(1,2));\n  q->push_back(a);\n  q->push_back(\"fc\",a);\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34208\n\nDifferential Revision: D20300278\n\nPulled By: yf225\n\nfbshipit-source-id: 4525319bb7fb6667e43a006c9f446a2193781005", "pr_number": "34208", "files_changed": ["test/cpp/api/sequential.cpp", "torch/csrc/api/include/torch/nn/modules/container/sequential.h"], "labels": ["merged", "open source"]}, "415595ace4": {"title": "[C++ API] Remove init-list form of at::indexing::Slice (#34255)", "body": "Summary:\nThe init-list form of `at::indexing::Slice` (i.e. `tensor.index({{1, None, 2}, ...})` instead of `tensor.index({Slice(1, None, 2), ...})`) in C++ API can be easily confused with the list-form indexing in Python API (e.g. `tensor[[1, 3, 2], ...]`), which is not good from readability perspective. This PR removes the init-list form of `at::indexing::Slice` to make the API less confusing.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34255\n\nTest Plan: Imported from GitHub, without a `Test Plan:` line.\n\nDifferential Revision: D20290166\n\nPulled By: yf225\n\nfbshipit-source-id: abbcbeca0b179219e5e1f196a33ef8aec87ebb76", "pr_number": "34255", "files_changed": ["aten/src/ATen/TensorIndexing.h", "test/cpp/api/tensor_indexing.cpp", "torch/csrc/autograd/python_variable_indexing.cpp"], "labels": ["merged"]}, "f9f135c5d8": {"title": "ChannelsLast3d support is_contiguous, contiguous, suggest_memory_format, caching (#33033)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33033\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19759661\n\nPulled By: glaringlee\n\nfbshipit-source-id: 6c4798fa93589338c0c71c5308b9fd1151330245", "pr_number": "33033", "files_changed": ["aten/src/ATen/cudnn/Descriptors.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/templates/TensorBody.h", "c10/core/MemoryFormat.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "test/test_torch.py", "torch/csrc/utils/tensor_memoryformats.cpp"], "labels": ["merged"]}, "30680196e4": {"title": "Revert D20121915: [JIT] Add support for list()", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20121915\n\nOriginal commit changeset: c6c4ef444dbf\n\nfbshipit-source-id: 829adb58780f4d0f41acebb3e7640a9c68bdbc1b", "pr_number": null, "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/functional.py"], "labels": []}, "35b6d2945d": {"title": "Tensor.random_ check that from and to are in tensor dtype bounds (#34033)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34033\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20182414\n\nPulled By: pbelevich\n\nfbshipit-source-id: 3704570ead7de169ce13c81164be0aff0806fb46", "pr_number": "34033", "files_changed": ["aten/src/ATen/native/DistributionTemplates.h", "aten/src/ATen/test/tensor_iterator_test.cpp", "test/test_nn.py", "test/test_torch.py"], "labels": ["merged"]}, "9c5578fd0a": {"title": "Make sure Vec256 int32_t and int16_t loadu temprary arrays are properly initialized (#34281)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34281\n\nSeems like #32722 has missed two loadu functions\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20287731\n\nPulled By: albanD\n\nfbshipit-source-id: d959b2508de3f9f660368152d7260026d7fbccbe", "pr_number": "34281", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_complex_double.h", "aten/src/ATen/cpu/vec256/vec256_complex_float.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/cpu/vec256/vec256_int.h"], "labels": ["merged", "open source"]}, "a7da4490cc": {"title": "Clean up some legacy scalar/empty handling. (#34217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34217\n\nLegacyNoScalar variants cause 0-dim tensors to behave like 1-dim tensors.\nLegacyAll variants cause 0-dim tensors to behave like 1-dim tensors, and numel == 0 tensors to be treated like 0-dimensional tensors.\n\nThis this was done by codemod, these are often unneeded and often translated incorrectly to ATen.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20249577\n\nPulled By: gchanan\n\nfbshipit-source-id: 6f2876d3e479562c9323f3629357a73a47869150", "pr_number": "34217", "files_changed": ["aten/src/TH/generic/THTensorLapack.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/THCUNN/generic/ClassNLLCriterion.cu", "aten/src/THCUNN/generic/SpatialConvolutionMM.cu"], "labels": ["merged"]}, "17ceb6941f": {"title": "[RPC] Create local RRef<ModuleInterface> remotely in Python, use it remotely in TorchScript (#34183)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34183\n\nhttps://github.com/pytorch/pytorch/pull/33263 enhanced the RRef Python constructor to infer most types, by `jit::tryToInferType(..)`.\n\nBut this helper function can't infer `ScriptModule` type due to `ScriptModule`'s special per-Module type singleton logic, so it's still not possible for an Python-created RRef to know the JIT type of it's contained `ScriptModule`.\n\nInstead of inferring the specific type of a Module, which could leads to too many candidate types (due to Module's multiple inheritance possibility), it's more straightforward to set it's type as a user-specified `ModuleInterface` type.\n\nWe added an optional argument `type_hint` for users to mark an `RRef` for what `ModuleInterface` type it's holds.\n\nghstack-source-id: 99649379\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\nAspects that need to be confirmed in the test cases\n\nhttps://fb.quip.com/aGxRAh2lCg05\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/jit/rpc_fork\\#binary.par -r test_create_local_script_class_rref\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/jit/rpc_fork\\#binary.par -r test_create_local_script_module_rref\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/jit/rpc_fork\\#binary.par -r test_return_local_script_class_rref_in_py_and_use_in_script\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/jit/rpc_fork\\#binary.par -r test_return_local_script_module_rref_in_py_and_use_in_script\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/jit/rpc_fork\\#binary.par -r test_torchscript_function_exception\n```\n\nDifferential Revision: D7065050\n\nfbshipit-source-id: e10210c0996622969e499e4a35b0659b36787c1c", "pr_number": "34183", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["jit", "merged"]}, "82a177c07f": {"title": "[c10] remove warning attribute does not apply to any entity (#34018)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34018\n\nRemove warning\n```\ncaffe2/c10/util/ArrayRef.h(278): warning: attribute does not apply to any entity\n```\n\nTest Plan: CI\n\nReviewed By: jianyuh\n\nDifferential Revision: D20181191\n\nfbshipit-source-id: 58bd168a87a94fec925c7cde8b8d728a4257446c", "pr_number": "34018", "files_changed": ["c10/util/Deprecated.h"], "labels": ["fb-exported", "merged"]}, "4872b126fd": {"title": "[aten] remove stmt unreachable, variable never used warnings (#34017)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34017\n\nRemove warning\n```\ncaffe2/aten/src/THC/generic/THCTensorMathBlas.cu(437): warning: statement is unreachable\ncaffe2/aten/src/THC/generic/THCTensorMathBlas.cu(271): warning: variable \"transpose_m1\" was set but never used\ncaffe2/aten/src/THC/generic/THCTensorMathBlas.cu(271): warning: variable \"transpose_m2\" was set but never used\n```\n\nTest Plan: CI\n\nReviewed By: ngimel\n\nDifferential Revision: D20181179\n\nfbshipit-source-id: 3665912ba55bffbd8b4555f8a6803e57a502c103", "pr_number": "34017", "files_changed": ["aten/src/THC/generic/THCTensorMathBlas.cu"], "labels": ["fb-exported", "merged"]}, "4c99351de6": {"title": "[AMD] Remove num_gpu check for remote execution (#34318)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34318\n\nStop checking whether we have AMD GPU devices on the host, because we may be constructing a net on a machine without GPU, and run the net on another one with GPU\n\nReviewed By: ajauhri\n\nDifferential Revision: D20269562\n\nfbshipit-source-id: 1f561086cacdcead3ce7c03c2d02c25336c8b11a", "pr_number": "34318", "files_changed": ["caffe2/python/_import_c_extension.py"], "labels": ["fb-exported", "merged"]}, "6d8a0f6731": {"title": "[Aten] Init container iterators to an unsigned type (#34159)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34159\n\nThis fixes `comparison of integers of different sign` warnings\n\nTest Plan: CI\n\nReviewed By: EscapeZero\n\nDifferential Revision: D20232085\n\nfbshipit-source-id: 8f325be54395be54c704335cb7edf2ec7ef75e75", "pr_number": "34159", "files_changed": ["aten/src/ATen/native/NamedTensor.cpp"], "labels": ["fb-exported", "merged"]}, "98afce3c56": {"title": "Remove unnecessary assert in autograd engine (#34307)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34307\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20283401\n\nPulled By: albanD\n\nfbshipit-source-id: 34f6eb8955b7d9cb259260abc1056ddd9f354107", "pr_number": "34307", "files_changed": ["torch/csrc/autograd/engine.cpp"], "labels": ["merged"]}, "879a90b322": {"title": "[ModelLoading] Use byte encoding for uint8, fp16 etc. instead of int32 (#34343)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34343\n\nUse byte encoding for uint8, fp16 etc. instead of int32 in TensorProto serialization/deserialization\n\ntl;dr\n- fp16 tensor deserialization 12x faster, serialized size 25% lower\n- uint8 tensor deserialization 36x faster, serialized size 25% lower\n\nTest Plan:\n```\n============================================================================\ncaffe2/caffe2/fb/predictor/ModelLoaderBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nBlobProtoInt32DeserializationFloat16                        12.37ms    80.82\nBlobProtoByteDeserializationFloat16             1125.46%     1.10ms   909.64\n----------------------------------------------------------------------------\nBlobProtoInt32DeserializationUInt8                          17.57ms    56.92\nBlobProtoByteDeserializationUInt8               3629.45%   484.02us    2.07K\n============================================================================\n```\n\nReviewed By: yinghai\n\nDifferential Revision: D20137451\n\nfbshipit-source-id: 8ed4be2286a6d4c7e134fcb0832f22bc645039a1", "pr_number": "34343", "files_changed": ["caffe2/core/blob_serialization.cc", "caffe2/core/blob_test.cc"], "labels": ["fb-exported", "merged"]}, "0489b8da42": {"title": "Add scripts to promote S3 artifacts from test channels to stable channels (#34274)", "body": "Summary:\nCurrently testing against the older release `1.4.0` with:\n```\nPYTORCH_S3_FROM=nightly TEST_WITHOUT_GIT_TAG=1 TEST_PYTORCH_PROMOTE_VERSION=1.4.0 scripts/release/promote/libtorch_to_s3.sh\nPYTORCH_S3_FROM=nightly TEST_WITHOUT_GIT_TAG=1 TEST_PYTORCH_PROMOTE_VERSION=1.4.0 scripts/release/promote/wheel_to_s3.sh\n```\n\nThese scripts can also be used for `torchvision` as well which may make the release process better there as well.\n\nLater on this should be made into a re-usable module that can be downloaded from anywhere and used amongst all pytorch repositories.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34274\n\nTest Plan: sandcastle_will_deliver\n\nDifferential Revision: D20294419\n\nPulled By: seemethere\n\nfbshipit-source-id: c8c31b5c42af5096f09275166ac43d45a459d25c", "pr_number": "34274", "files_changed": ["scripts/release/promote/common_utils.sh", "scripts/release/promote/libtorch_to_s3.sh", "scripts/release/promote/wheel_to_s3.sh"], "labels": ["merged", "releng"]}, "5f641f93f1": {"title": "[aten] Don't deadlock in IValue::Future impl, tests. (#34099)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34099\n\nThis change effectively applies into IValue's future impl a few fixes\nwe discovered when using the torch::utils::Future<T> impl.\n\nThe parallel impls should probably eventually be merged, but until then:\n\n  - Don't hold the lock when invoking the callbacks. This makes\n    it effectively impossible (deadlocks) to call value() to get\n    the value from inside the callback.\n\n  - We discovered that it was slightly cleaner in practice to\n    notify condition variables prior to invoking callbacks\n    (best to unblock paused threads ASAP, before spawning new work).\n\n  - Fix some var naming inconsistency.\n  - Add a some caffe2 cpp test coverage.\nghstack-source-id: 99336569\n\nTest Plan:\n```\nbuck test mode/dev //caffe2/test/cpp/jit:jit -- 'JitTest\\.IValueFuture'\n\n```\n\nDifferential Revision: D20203278\n\nfbshipit-source-id: 6e805ba547899dab9aab458e4b23049db31f930e", "pr_number": "34099", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "test/cpp/jit/test_ivalue.cpp", "test/cpp/jit/tests.h"], "labels": ["jit", "merged"]}, "c6ea71b6e8": {"title": "Fix Conv.cpp, &&= is not a C++ operator (#34381)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34381\n\nDifferential Revision: D20310674\n\nPulled By: ngimel\n\nfbshipit-source-id: a453c1d07bcf7aead7402f091bccb4af7b1ec690", "pr_number": "34381", "files_changed": ["aten/src/ATen/native/cudnn/Conv.cpp"], "labels": ["merged", "open source"]}, "5608ffc46c": {"title": "[PyTorch] Remove const modifiers from passed by value integers in qbatch_norm_fn (#34378)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34378\n\nThis fixes strange symbol mangling mismatch beteen `DECLARE_DISPATCH(qbatch_norm_fn, qbatch_norm_stub)` and `REGISTER_DISPATCH(qbatch_norm_stub, &q_batch_norm_kernel<false>);` if code is build on Windows with clang\n\nTest Plan: CI + build PyTorch on Windows using clang\n\nReviewed By: EscapeZero\n\nDifferential Revision: D20309550\n\nfbshipit-source-id: e97c7c3b6fee2e41ea6b2f8167ce197aec404e3d", "pr_number": "34378", "files_changed": ["aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h"], "labels": ["fb-exported", "merged"]}, "2d3f6cbf03": {"title": ".circleci: Update default smoke tests from cuda 10.0 -> 10.2 (#34328)", "body": "Summary:\nNow that https://github.com/pytorch/pytorch/issues/34241 is merged, we can update these to the latest cuda version to get a better signal.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34328\n\nDifferential Revision: D20312552\n\nPulled By: seemethere\n\nfbshipit-source-id: 8e6bf797e067500d5dd9a607c6c19465028637bc", "pr_number": "34328", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows-binary-builds-smoke-subset.yml"], "labels": ["merged", "module: ci"]}, "079de7f376": {"title": ".circleci: Remove macOS builds related to CUDA (#34333)", "body": "Summary:\nWe don't release binaries for macOS with CUDA support so we should just\nremove it from our regular PR pipeline\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34333\n\nDifferential Revision: D20312565\n\nPulled By: seemethere\n\nfbshipit-source-id: 376228680aa0e814d1b37f1ff63b7d1262515e44", "pr_number": "34333", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-pytorch-macos-builds.yml"], "labels": ["merged", "module: ci"]}, "516a587438": {"title": "Enhance reproducibility documentation (#33795)", "body": "Summary:\nImproves explanation of non-determinism when running on GPUs. Adds info about `torch.nn.BCELoss` operating non-deterministically on GPUs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33795\n\nDifferential Revision: D20284880\n\nPulled By: ngimel\n\nfbshipit-source-id: d543959636d261a80c234150304344b19a37ba5d", "pr_number": "33795", "files_changed": ["docs/source/notes/randomness.rst"], "labels": ["merged", "open source", "triaged"]}, "37dfc6c498": {"title": "Reenable large conv tests (#34259)", "body": "Summary:\nPlease merge after https://github.com/pytorch/pytorch/pull/33073\n\nWith that PR, we are now trying different algorithms when OOM, so hopefully there will be some algo working at low memory.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34259\n\nDifferential Revision: D20310094\n\nPulled By: ngimel\n\nfbshipit-source-id: bccd8162bd06a0e54ac6f42a7fd9a5b766f92cd7", "pr_number": "34259", "files_changed": ["test/test_nn.py"], "labels": ["merged", "open source"]}, "96ca06cfce": {"title": "Add nhwc memory format test for dropout (#34379)", "body": "Summary:\ncc: ptrblck\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34379\n\nDifferential Revision: D20310118\n\nPulled By: ngimel\n\nfbshipit-source-id: a9bafd6b8fbcb57443e22181cf6bd9879b6f6051", "pr_number": "34379", "files_changed": ["test/test_nn.py"], "labels": ["merged", "open source"]}, "01edb7450f": {"title": "[Lite Trainer] Add necessary registrations for MNIST model (#33717)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33717\n\nBecause of the special treatment of operator names for lite interpreter, all the operators used in lite interpreter are still prepended by \"_\". Add the necessary registrations for MNIST model. All the ops with autograd capability are included in torch_mobile_train. After rebase the selective build from D19649074 can be utilized to strip the unused ops.\n\nNote that this diff is for feasibility test. The training accuracy are not covered in the test.\nghstack-source-id: 97780066\n\nTest Plan:\n```\nbuck run xplat/caffe2/fb/lite_trainer:lite_trainer -c pt.disable_gen_tracing=1 -c pt.static_dispatch=0 -- --model=/path/MnistModel.bc\n```\n{F227898221}\n\nReviewed By: dreiss\n\nDifferential Revision: D19743201\n\nfbshipit-source-id: cacadd76f3729faa0018d147a69466bbf54312fd", "pr_number": "33717", "files_changed": ["torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/register_mobile_autograd.cpp", "torch/csrc/jit/mobile/register_mobile_ops.cpp"], "labels": ["jit", "merged"]}, "ccf6fab65e": {"title": "Fix doc and type hints for \"torch.add\"; fix deprecated python calls in tests (#33935)", "body": "Summary:\nThis PR fixed documentation for `torch.add` with alpha. It also fixed these deprecated python calls `torch.add` and `torch.addmm` in tests, which may affect performance in *test/test_sparse.py* and *test/test_nn.py*.\n\ncc csarofeen ptrblck\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33935\n\nDifferential Revision: D20313320\n\nPulled By: ngimel\n\nfbshipit-source-id: fb08413d7e244865952e3fc0e1be7f1794ce4e9a", "pr_number": "33935", "files_changed": ["test/test_sparse.py", "tools/pyi/gen_pyi.py", "torch/_torch_docs.py", "torch/testing/_internal/common_nn.py"], "labels": ["merged", "open source"]}, "d98516026e": {"title": "[PyTorch BC] Clean up the BC whitelist (#34393)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34393\n\nClean up the list\n\nTest Plan: CI\n\nReviewed By: hl475\n\nDifferential Revision: D20300530\n\nfbshipit-source-id: 50e7da0a9f8295eff33590982f32f84abee96d9c", "pr_number": "34393", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["fb-exported", "merged"]}, "b0479506a8": {"title": "Add the 3d avg pool for video related model (#33339)", "body": "Summary:\n```\nimport torch, time\n\nfor dtype in [torch.qint8, torch.quint8, torch.qint32]:\n    print('****', str(dtype), '*****')\n    x = torch.rand(1, 5, 56, 56, 256)\n\n    q_x = torch.quantize_per_tensor(x, 0.5, 1, dtype)\n    q_x = q_x.permute([0, 4, 1, 2, 3])\n\n    x = x.permute([0, 4, 1, 2, 3])\n\n    NITER = 10\n\n    s = time.time()\n    for i in range(NITER):\n        float_out = torch.nn.functional.avg_pool3d(x, kernel_size=3, stride=None, padding=0)\n    time_per_iter_float = (time.time() - s) / NITER\n\n    s = time.time()\n    for i in range(NITER):\n        quant_out = torch.nn.quantized.functional.avg_pool3d(q_x, kernel_size=3, stride=None, padding=0)\n    time_per_iter_quant = (time.time() - s) / NITER\n    print('time/iter ms (float)', 'time/iter ms (quant)', 'quant/float', sep='\\t')\n    print(time_per_iter_float * 1000, time_per_iter_quant * 1000, time_per_iter_quant / time_per_iter_float, sep='\\t')\n```\n\n```\n**** torch.qint8 *****\ntime/iter ms (float)  time/iter ms (quant)  quant/float\n16.286182403564453  0.7308721542358398  0.04487682479080417\n**** torch.quint8 *****\ntime/iter ms (float)  time/iter ms (quant)  quant/float\n15.364313125610352  0.6497383117675781  0.042288796541418254\n**** torch.qint32 *****\ntime/iter ms (float)  time/iter ms (quant)  quant/float\n15.649032592773438  13.879132270812988  0.8869003363966556\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33339\n\nDifferential Revision: D19900904\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: 4522cc6b4a0751aeda6c7edc258e0cb3f55a8fe3", "pr_number": "33339", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp", "aten/src/ATen/native/quantized/cpu/q_avgpool3d.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/test_quantized.py", "torch/nn/quantized/functional.py"], "labels": ["merged"]}, "392afb9f8b": {"title": "Fix overlapping keywords (#34142)", "body": "Summary:\nThis commit fixes overlapping keywords in the CPP Docs\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34142\n\nTest Plan: Imported from GitHub, without a `Test Plan:` line.\n\nDifferential Revision: D20319949\n\nPulled By: yf225\n\nfbshipit-source-id: e7bb2efdc286c85792c6f18a260c3bba33c54008", "pr_number": "34142", "files_changed": ["docs/cpp/source/_static/cpp_theme.css"], "labels": ["merged", "open source"]}, "c5e822b7bb": {"title": "Back out \"[jit] Add type tags to lists/dicts in pickle\" (#34406)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34406\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34405\n\nOriginal commit changeset: 2f1826e6679a\n\nTest Plan: reverting, see S197156\n\nReviewed By: akyrola, volkhin\n\nDifferential Revision: D20317456\n\nfbshipit-source-id: 89298a9c022edba1d54bcdc7541804cb919e33f5", "pr_number": "34406", "files_changed": ["caffe2/serialize/inline_container.h", "test/cpp/jit/test_save_load.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/mobile/type_parser.cpp", "torch/csrc/jit/serialization/import.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/pickler.h", "torch/csrc/jit/serialization/unpickler.cpp", "torch/csrc/jit/serialization/unpickler.h", "torch/jit/_pickle.py"], "labels": ["fb-exported", "jit"]}, "65bad41cbe": {"title": "Fixed typos in quantization docs / docstrings (#34182)", "body": "Summary:\nRemoved extra back quote character.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34182\n\nDifferential Revision: D20320146\n\nPulled By: jerryzh168\n\nfbshipit-source-id: 33c347711a052cc55f7d1a41ed959dadf99a3d7d", "pr_number": "34182", "files_changed": ["docs/source/quantization.rst", "torch/nn/quantized/modules/functional_modules.py"], "labels": ["merged", "open source", "quantization", "triaged"]}, "bb1114258c": {"title": "[JIT] Move stuff out of class_type.cpp (#33900)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33900\n\nThese functions don't require any libtorch-specific functionality, so move them into the header so they're included in the ATen build\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20175874\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 1efab1b60e196a635e6c6afadb042b63771170f0", "pr_number": "33900", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "torch/csrc/jit/ir/class_type.cpp"], "labels": ["jit", "merged"]}, "60e8615a6d": {"title": "[JIT] Virtualize Function (#33921)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33921\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.intern.facebook.com/intern/diff/D20153092/)!\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20177227\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 87f3e484c4f873d60f76f50f6789c1b4a73bdfde", "pr_number": "33921", "files_changed": ["aten/src/ATen/core/function.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "caffe2/CMakeLists.txt", "tools/build_variables.bzl", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/jit/api/compilation_unit.h", "torch/csrc/jit/api/function.cpp", "torch/csrc/jit/api/function.h", "torch/csrc/jit/api/function_impl.cpp", "torch/csrc/jit/api/function_impl.h", "torch/csrc/jit/api/method.h", "torch/csrc/jit/api/object.h", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/ir/class_type.cpp", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/ir/scope.cpp", "torch/csrc/jit/jit_log.cpp", "torch/csrc/jit/passes/bailout_graph.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/unpickler.cpp"], "labels": ["jit", "merged"]}, "45a504dd2d": {"title": "[JIT] Introduce BuiltinOpFunction and integrate into torchbind (#34098)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34098\n\n* #33900 [JIT] Move stuff out of class_type.cpp\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20229166\n\nPulled By: jamesr66a\n\nfbshipit-source-id: d658a63a5d6e372e675f35b8456adc8de82b49f3", "pr_number": "34098", "files_changed": ["aten/src/ATen/core/boxing/kernel_functor.h", "aten/src/ATen/core/builtin_function.h", "aten/src/ATen/core/custom_class.cpp", "aten/src/ATen/core/function.h", "aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/op_registration/infer_schema.h", "aten/src/ATen/core/type.cpp", "caffe2/CMakeLists.txt", "test/cpp/jit/test_custom_class.cpp", "tools/build_variables.bzl", "torch/csrc/jit/api/custom_class.cpp", "torch/csrc/jit/api/custom_class.h", "torch/csrc/jit/api/function_impl.h", "torch/csrc/jit/frontend/schema_type_parser.cpp", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/passes/inliner.cpp", "torch/csrc/jit/python/python_custom_class.cpp", "torch/csrc/jit/python/script_init.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/serialization/import_source.cpp", "torch/csrc/jit/serialization/python_print.cpp", "torch/custom_class.h", "torch/custom_class_detail.h"], "labels": ["jit", "merged"]}, "8a17dc65af": {"title": "[quantization] Make FP16 RNN use new prepack op (#34339)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34339\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20297194\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 8bf6d0f2cb047e90bbdd184aaad337b143040d10", "pr_number": "34339", "files_changed": ["test/test_quantization.py", "torch/nn/quantized/dynamic/modules/rnn.py"], "labels": ["merged"]}, "7d9f611b64": {"title": "Add worker_name helper to dist_utils. (#34162)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34162\n\nThis avoids the \"worker{}\".format(..) in our unit tests to something\ncleaner.\nghstack-source-id: 99713074\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D20233533\n\nfbshipit-source-id: 5cff952ca68af5a6d26dc5cc01463cf7756d83d9", "pr_number": "34162", "files_changed": ["torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["jit", "merged"]}, "79d47c1c5f": {"title": "Fix the missing ';' in Conv.cpp (#34448)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/34415.\nBTW, isn't this tested on CI? Maybe we need to introduce some tests with legacy versions of cuDNN.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34448\n\nDifferential Revision: D20325104\n\nPulled By: ngimel\n\nfbshipit-source-id: f03dec30ffa6e50a28ee8103d7d49cd6fc0a6d69", "pr_number": "34448", "files_changed": ["aten/src/ATen/native/cudnn/Conv.cpp"], "labels": ["merged", "open source"]}, "7e55494502": {"title": "Warns on read-only Numpy array->tensor conversion (#33615)", "body": "Summary:\nAddresses https://github.com/pytorch/pytorch/issues/5442.\n\nPer title (and see issue). A test is added to test_torch.py to verify the behavior.\n\nUpdate (with new behavior):\n\nNumPy arrays can be non-writeable (read-only). When converting a NumPy array to a Torch tensor the storage is shared, but the tensor is always writable (PyTorch doesn't have a read-only tensor). Thus, when a non-writeable NumPy array is converted to a PyTorch tensor it can be written to.\n\nIn the past, PyTorch would silently copy non-writeable NumPy arrays and then convert those copies into tensors. This behavior violates the from_numpy contract, however, which promises that the tensor and the array share memory.\n\nThis PR adds a warning message when a non-writeable NumPy array is converted into a Torch tensor. This will not break any networks, but will make end users aware of the behavior. They can work-around the warning message by marking their NumPy arrays as writeable.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33615\n\nDifferential Revision: D20289894\n\nPulled By: mruberry\n\nfbshipit-source-id: b76df0077399eb91038b12a6bf1917ef38c2cafd", "pr_number": "33615", "files_changed": ["test/test_torch.py", "torch/csrc/utils/tensor_numpy.cpp"], "labels": ["merged", "module: numpy"]}, "6e2bb1c054": {"title": "End of the .data removal in torch/optim (#34211)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34211\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20248684\n\nPulled By: albanD\n\nfbshipit-source-id: 2294bfa41b82ff47f000bc98860780f59d7d4421", "pr_number": "34211", "files_changed": ["test/jit/test_data_parallel.py", "torch/optim/adadelta.py", "torch/optim/adagrad.py", "torch/optim/adam.py", "torch/optim/adamax.py", "torch/optim/adamw.py", "torch/optim/asgd.py", "torch/optim/lbfgs.py", "torch/optim/rmsprop.py", "torch/optim/rprop.py", "torch/optim/sgd.py", "torch/optim/sparse_adam.py"], "labels": ["jit", "merged", "module: autograd", "module: optimizer", "topic: bc-breaking", "triaged"]}, "b09e90af1e": {"title": "Fix C++ at::Tensor docs generation (#34467)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/25845.\n\n**Test Plan:**\nCheck `pytorch_cpp_doc_push` CI job, and see if there is `classat_1_1_tensor` generated (similar to `structat_1_1native_1_1_convolution_descriptor`).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34467\n\nDifferential Revision: D20338190\n\nPulled By: yf225\n\nfbshipit-source-id: 52dc05af5e0d742e740de5576d0d2b3e17ef28dd", "pr_number": "34467", "files_changed": ["docs/cpp/source/Doxyfile"], "labels": ["merged", "module: cpp", "module: docs"]}, "739d4609c3": {"title": "[C++ API] Fix ModuleList compile error: error: 'begin' was not declared in this scope (#34463)", "body": "Summary:\nOne example in the current docs for `torch::nn::ModuleList` doesn't compile, and this PR fixes it.\nFixes https://github.com/pytorch/pytorch/issues/32414.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34463\n\nTest Plan: Imported from GitHub, without a `Test Plan:` line.\n\nDifferential Revision: D20331120\n\nPulled By: yf225\n\nfbshipit-source-id: 50bb078fe1a900c9114d5434e92dc40ee13b52bf", "pr_number": "34463", "files_changed": ["test/cpp/api/modulelist.cpp", "torch/csrc/api/include/torch/nn/modules/container/modulelist.h"], "labels": ["merged", "module: cpp"]}, "b1bd950a4d": {"title": "Fixed stub for AdamW (#34299)", "body": "Summary:\nFixes [https://github.com/pytorch/pytorch/issues/33757](https://github.com/pytorch/pytorch/issues/33757)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34299\n\nDifferential Revision: D20337844\n\nPulled By: ezyang\n\nfbshipit-source-id: 54bf174a09b8db9bf6e0c3c717730dd7c795d76b", "pr_number": "34299", "files_changed": ["torch/optim/__init__.pyi"], "labels": ["merged", "module: typing", "open source", "triaged"]}, "f62a7e7efb": {"title": "Simplify implementation of newWithStorage1d. (#34382)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34382\n\nThe previous implementation was handling both newWithStorage and newWithSize, which doesn't make much sense.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20311056\n\nPulled By: gchanan\n\nfbshipit-source-id: 2696a4566e6203c98338c86cbf4c236bd18d7c49", "pr_number": "34382", "files_changed": ["aten/src/TH/generic/THTensor.cpp", "aten/src/THC/generic/THCTensor.cpp"], "labels": ["merged"]}, "e3d50c4dda": {"title": "Retain the order of parameters while generating ConcreteModuleTypes (#34131)", "body": "Summary:\n`ConcreteModuleTypeBuilder` used to keep parameters together with all others attributes in an `unordered_map` often leading to reordering them while building up the type. Parameter order is semantically meaningful, so we need to preserve it.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34131\n\nDifferential Revision: D20331542\n\nPulled By: suo\n\nfbshipit-source-id: 5b860025f7902654d6099751d3fb14b12f6f5a67", "pr_number": "34131", "files_changed": ["test/test_jit.py", "torch/csrc/api/include/torch/ordered_dict.h", "torch/csrc/jit/frontend/concrete_module_type.cpp", "torch/csrc/jit/frontend/concrete_module_type.h"], "labels": ["jit", "merged", "open source", "triaged"]}, "baeb359e7a": {"title": "Remove `using namespace torch::autograd` from header files (#34423)", "body": "Summary:\nThis PR prevents leaking symbols from `torch::autograd` namespace to the root namespace.\nFixes https://github.com/pytorch/pytorch/issues/34371.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34423\n\nDifferential Revision: D20338404\n\nPulled By: yf225\n\nfbshipit-source-id: e7ff3348193667a0cee5d38f9a003ae36cc704ca", "pr_number": "34423", "files_changed": ["test/cpp/api/CMakeLists.txt", "test/cpp/api/namespace.cpp", "torch/csrc/api/include/torch/nn/modules/_functions.h", "torch/csrc/api/src/nn/modules/_functions.cpp"], "labels": ["merged", "module: cpp"]}, "bcfd348858": {"title": "[ONNX] Export new_zeros (#34077)", "body": "Summary:\nONNX export for new_zeros op added.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34077\n\nReviewed By: hl475\n\nDifferential Revision: D20332074\n\nPulled By: houseroad\n\nfbshipit-source-id: 4235c4f2c279c37aa8dde6d13c1b26f621967768", "pr_number": "34077", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source"]}, "91e922a338": {"title": "[AI Bench] Add support for nlu model", "body": "Summary: add support for nlu specific input\n\nTest Plan:\ntested\n\n```\nbuck run aibench:run_bench -- -b aibench/specifications/models/pytorch/fbnet/assistant_mobile_inference.json --platform android/full_jit --framework pytorch --remote --devices  SM-G950U-7.0-24\n```\nmake sure it compatible with previous test\n```\nbuck run aibench:run_bench -- -b aibench/specifications/models/pytorch/fbnet/fbnet_mobile_inference.json --platform android/full_jit --framework pytorch --remote --devices  SM-G950U-7.0-24\n```\n\n```\n{\n  \"model\": {\n    \"category\": \"CNN\",\n    \"description\": \"Assistant Mobile Inference\",\n    \"files\": {\n      \"model\": {\n        \"filename\": \"model.pt1\",\n        \"location\": \"//everstore/GICWmAB2Znbi_mAAAB0P51IPW8UrbllgAAAP/model.pt1\",\n        \"md5\": \"c0f4b29c442bbaeb0007fb0ce513ccb3\"\n      },\n      \"data\": {\n        \"filename\": \"input.txt\",\n        \"location\": \"/home/pengxia/test/input.txt\",\n        \"md5\": \"c0f4b29c442bbaeb0007fb0ce513ccb3\"\n      }\n    },\n    \"format\": \"pytorch\",\n    \"framework\": \"pytorch\",\n    \"kind\": \"deployment\",\n    \"name\": \"Assistant Mobile Inference\"\n  },\n  \"tests\": [\n    {\n      \"command\": \"{program} --model {files.model}  --input_dims \\\"1\\\" --input_type NLUType --warmup {warmup} --iter {iter} --input_file {files.data} --report_pep true\",\n      \"identifier\": \"{ID}\",\n      \"metric\": \"delay\",\n      \"iter\": 5,\n      \"warmup\": 2,\n      \"log_output\": true\n    }\n  ]\n}\n\n```\ninput.txt\n```\nwhat is weather today\nwhat time it is\nset a reminder for tomorrow\n```\n\nresult\nhttps://our.intern.facebook.com/intern/aibench/details/137241352201417\n\nReviewed By: kimishpatel\n\nDifferential Revision: D20300947\n\nfbshipit-source-id: 7c1619541a2e9514a560a9acb9029cfc4669f37a", "pr_number": null, "files_changed": ["binaries/speed_benchmark_torch.cc"], "labels": []}, "6d3783a6bc": {"title": "Clean up unused newWithSize variants. (#34383)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34383\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20311065\n\nPulled By: gchanan\n\nfbshipit-source-id: 9fc2cc4377f32c865401b04868a7405c49929c64", "pr_number": "34383", "files_changed": ["aten/src/ATen/test/basic.cpp", "aten/src/TH/generic/THTensor.cpp", "aten/src/TH/generic/THTensor.h", "aten/src/THC/generic/THCTensor.cpp", "aten/src/THC/generic/THCTensor.h"], "labels": ["merged"]}, "70fe508c26": {"title": "[pytorch] fix BUILD_CAFFE2_MOBILE gating around caffe2/operators/experimental/c10/cpu (#34354)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34354\n\nThe condition `NOT INTERN_BUILD_MOBILE AND NOT BUILD_CAFFE2_MOBILE` was\nadded in #27086, but seems it's always false on current master:\n\nBUILD_CAFFE2_MOBILE is ON by default - the name is a little bit misleading -\nit is ON even when it's building non-mobile PyTorch/Caffe2. It is OFF only\nwhen it's building PyTorch mobile, where INTERN_BUILD_MOBILE is ON.\n\nAnd when it's building PyTorch mobile, it won't build caffe2/operators\nat all (by setting BUILD_CAFFE2_OPS OFF: https://github.com/pytorch/pytorch/blob/master/CMakeLists.txt#L345)\n\nSo I imagine the real intention is to skip when it's building Caffe2 mobile.\nWe can simply remove the deprecating BUILD_CAFFE2_MOBILE condition.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20345298\n\nPulled By: ljk53\n\nfbshipit-source-id: d2cb4e2248fc209d63b2843e0f12e577e323def4", "pr_number": "34354", "files_changed": ["caffe2/operators/CMakeLists.txt"], "labels": ["merged"]}, "e025677e3c": {"title": "Remove **kwargs from torch.meshgrid (#34356)", "body": "Summary:\nChangelog:\n- Remove **kwargs from torch.meshgrid as they serve no purpose\n\nCloses https://github.com/pytorch/pytorch/issues/34206\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34356\n\nDifferential Revision: D20310971\n\nPulled By: zou3519\n\nfbshipit-source-id: 97250051504aa3ec1e2a9af9296e7cc71872e5bf", "pr_number": "34356", "files_changed": ["torch/functional.py"], "labels": ["merged", "open source"]}, "2b45368e50": {"title": "Fix cudnn 64bit indexing issue (#34407)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/33143\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34407\n\nDifferential Revision: D20325106\n\nPulled By: ngimel\n\nfbshipit-source-id: 5aa52295f5491f189b7a8bea0987f28de0589d98", "pr_number": "34407", "files_changed": ["aten/src/ATen/native/cudnn/Conv.cpp"], "labels": ["merged", "open source"]}, "776d2a1e8f": {"title": "[quant][graphmode] Handling ops doesn't require observation in insertObservers (#33481)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33481\n\nWe have to propagate observed property of values through ops like max_pool2d, flatten and\navoid inserting duplicated observers.\nFor example:\n```\nx1 = self.conv(x)\nx2 = maxpool(x1)\nx3 = self.conv(x2)\n```\nIf x1 is observed, we should propagate this information through maxpool and\nwe should consider x2 as observed as well.\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D20261897\n\nfbshipit-source-id: 7de354a3ccb2b6e1708f5c743d4d9f7272691a93", "pr_number": "33481", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "8294db8f15": {"title": "[iOS][CI] Remove org-member from iOS Simulator Builds (#34410)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34410\n\n### Summary\n\nCurrently, the iOS jobs are not being run on PRs anymore. This is because all iOS jobs have specified the `org-member` as a context which used to include all pytorch members. But seems like recently this rule has changed. It turns out that only users from the admin group or builder group can have access right to the context values. https://circleci.com/gh/organizations/pytorch/settings#contexts/2b885fc9-ef3a-4b86-8f5a-2e6e22bd0cfe\n\nThis PR will remove `org-member` from the iOS simulator build which doesn't require code signing. For the arm64 builds, they'll only be run on master, not on PRs anymore.\n\n### Test plan\n\n- The iOS simulator job should be able to appear in the PR workflow\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20347270\n\nPulled By: xta0\n\nfbshipit-source-id: 23f37d40160c237dc280e0e82f879c1d601f72ac", "pr_number": "34410", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-pytorch-ios-builds.yml", "scripts/xcode_build.rb"], "labels": ["merged"]}, "15a7b9cf0a": {"title": "[RpcAgent] Metrics for current num active/async rpc calls. (#34398)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34398\n\nAs part of PR 34109, it was suggested that we track the number of outstanding\nasync calls for RPC DebugInfo, particularly if we move towards using\nat::launch() threads on occasion for continuations.\n\nThis particular aspect of the change was distinct from the main purpose of the\ndiff, and started getting bigger, so split this functionality out as a separate diff.\nFor completeness, we track client_active_calls, server_active_calls,\nserver_active_async_calls, and write some very basic unittest coverage.\nghstack-source-id: 99708836\n\nTest Plan: buck test mode/dev-nosan caffe2/torch/fb/distributed/thriftRpcBackend/...\n\nDifferential Revision: D20314994\n\nfbshipit-source-id: 2f7c75d5c511b27ed0c09c7b8a67b6fb49df31a5", "pr_number": "34398", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": ["merged"]}, "c218963270": {"title": "fix more errors (#34480)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34480\n\nDifferential Revision: D20345198\n\nPulled By: ezyang\n\nfbshipit-source-id: 583246acd02850ead96f1f0574d01ef6697c6352", "pr_number": "34480", "files_changed": ["caffe2/operators/text_file_reader_utils.cc", "torch/csrc/jit/tensorexpr/ir.h", "torch/lib/c10d/PrefixStore.cpp"], "labels": ["jit", "merged", "open source"]}, "2c0f3536b6": {"title": "[jit] Make `ModuleList`s a sugared value (#34320)", "body": "Summary:\nPreviously when emitting subscripts we only emitted actual values, but\nnow they may sometimes emit a `ModuleValue`, so it should stay as a\n`SugaredValue`. This allows for the result of the subscript to be\ntreated as a real module (i.e. you can just do `self.modlist[1](inputs)`\ninstead of `self.modlist[1].forward(inputs)`)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34320\n\nPulled By: driazati\n\nDifferential Revision: D20345642\n\nfbshipit-source-id: 2bedf9a454af747b704422f6bbb8370cbdf4bf61", "pr_number": "34320", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/python/python_sugared_value.cpp"], "labels": ["jit", "merged"]}, "7688ca631a": {"title": "Enable RTTI for mobile builds, to enable custom class via torchbind in mobile (#34368)", "body": "Summary:\nCustom classes via torchbind requires runtime type information.\nWe are trying to enable custom class based graph rewrite for XNNPACK in\nthis stacked PRs: https://github.com/pytorch/pytorch/pull/34047.\nThey require RTTI enabled for mobile. Mobile builds are failing\ncurrently without it.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34368\n\nDifferential Revision: D20306155\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 52c61ff5467a619e8f51708a05258eee35dd0a56", "pr_number": "34368", "files_changed": ["aten/src/ATen/core/ivalue.h"], "labels": ["merged"]}, "2e7eef41ac": {"title": "[quant][graphmode] Swap quantized functional linear with aten::linear (#33853)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33853\n\nQuant fusion relies on inline, but inline will break the CallFunction(\"linaer\", ...) into a if block\nit will be hard to recognize this block and swap it with quantized::linear, in order to\npreserve the op, we will swap all quantized functional linear into aten::linear.\nThey might produce different backward graph, but this is called in the step before we get quantized\nmodel, so it shouldn't affect anything.\nWe'll integrate this with convert_script later in the new \"finalize_quant\" API\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D20343873\n\nfbshipit-source-id: 423e03bf893b79267d2dc97bc997ee1bfe54ec0f", "pr_number": "33853", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization.h", "torch/csrc/jit/python/init.cpp"], "labels": ["jit", "merged"]}, "34688d2c48": {"title": "Add brand guidelines link (#34503)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34503\n\nDifferential Revision: D20349273\n\nPulled By: soumith\n\nfbshipit-source-id: 6b085377741ace5d200ca0d536de433b9bb7825c", "pr_number": "34503", "files_changed": ["README.md"], "labels": ["merged"]}, "fea618b524": {"title": "[JIT] remove list with default builtin (#34171)", "body": "Summary:\nI think this was added when we couldn't compile the function itself. now we can.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34171\n\nDifferential Revision: D20269960\n\nPulled By: eellison\n\nfbshipit-source-id: 0a60458d639995d9448789c249d405343881b304", "pr_number": "34171", "files_changed": ["torch/csrc/jit/runtime/register_special_ops.cpp", "torch/jit/_builtins.py", "torch/nn/modules/utils.py"], "labels": ["jit", "merged"]}, "4e357089b4": {"title": "Stop calling newWithSize directly. (#34384)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34384\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20311057\n\nPulled By: gchanan\n\nfbshipit-source-id: 1e1a1f9b757b62f20d8d806f21abdd70f07b12aa", "pr_number": "34384", "files_changed": ["aten/src/THC/generic/THCTensorMasked.cu"], "labels": ["merged"]}, "90ff3b56d0": {"title": "Kill some unused TH(C)Storage functions. (#34385)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34385\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20311064\n\nPulled By: gchanan\n\nfbshipit-source-id: 6dc50621dc417e9ea4624cdebd0970453fa75a77", "pr_number": "34385", "files_changed": ["aten/src/TH/generic/THStorage.cpp", "aten/src/TH/generic/THStorage.h", "aten/src/THC/generic/THCStorage.cpp", "aten/src/THC/generic/THCStorage.h"], "labels": ["merged"]}, "0a4a558c2c": {"title": "Dictionary Constants (#32869)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32869\n\nDifferential Revision: D19909339\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 6fe2a9b470768f84b957c69cdf9af3a1bd9b1ca9", "pr_number": "32869", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/jit/test_freezing.py", "test/test_jit.py", "torch/csrc/jit/ir/constants.cpp", "torch/csrc/jit/ir/node_hashing.cpp", "torch/csrc/jit/serialization/python_print.cpp"], "labels": ["jit", "merged"]}, "2c1a302d6a": {"title": "[ROCm] Enable double __shfl_down (#34103)", "body": "Summary:\nThis allows us to enable some double-based pdist tests running into accrued error from casting down to float previously.\n\nAddresses https://github.com/pytorch/pytorch/issues/33128\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34103\n\nDifferential Revision: D20343279\n\nPulled By: ezyang\n\nfbshipit-source-id: a2da768259fab34ef326976283b7a15bebbbb979", "pr_number": "34103", "files_changed": ["aten/src/THC/THCDeviceUtils.cuh", "test/test_nn.py", "test/test_torch.py"], "labels": ["merged", "module: rocm", "open source", "triaged"]}, "e16908cb1f": {"title": "profile block outputs; helps guard elimination (#33889)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33889\n\nReviewed By: zdevito\n\nDifferential Revision: D20294979\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 2a68710ec8f8f854c99dfe173f49da442a39e498", "pr_number": "33889", "files_changed": ["test/test_jit.py", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/profiling_record.cpp"], "labels": ["jit", "merged"]}, "79e1305519": {"title": "[net_runner] Get shape info from qtensors (#34321)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34321\n\nMostly cosmetic as we can infer the shape anyway. It can remove a lot of the noise in the log though.\n\nNote that weight sharing doesn't work yet. I'll add another diff to address this.\n\nReviewed By: houseroad\n\nDifferential Revision: D20290841\n\nfbshipit-source-id: fe6f9b60d05dbe150af15b5d9d7a69fd902e12cc", "pr_number": "34321", "files_changed": ["caffe2/utils/proto_utils.cc"], "labels": ["fb-exported", "merged"]}, "2de4fa702b": {"title": "[JIT] Preserve qualified names on traced modules (#34395)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34395\n\nfixes: https://github.com/pytorch/pytorch/issues/33913\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20347778\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 7b5a35b6f9678c34cb6127d531fa3bfe65703116", "pr_number": "34395", "files_changed": ["torch/_jit_internal.py", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "3b661eb84c": {"title": "Avoid copy contents in SerializedPyObj (#34490)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34490\n\nDifferential Revision: D20347465\n\nTest Plan: Imported from OSS\n\nPulled By: mrshenli\n\nfbshipit-source-id: d59e74e3ee9122992a5c50a083e43ab31b7a70f5", "pr_number": "34490", "files_changed": ["torch/csrc/distributed/rpc/python_remote_call.cpp", "torch/csrc/distributed/rpc/python_remote_call.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/types.cpp", "torch/csrc/distributed/rpc/types.h"], "labels": ["merged"]}, "6d1c4df660": {"title": "Consolidate Python Messages to use SerializedPyObj (#34491)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34491\n\nDifferential Revision: D20347467\n\nTest Plan: Imported from OSS\n\nPulled By: mrshenli\n\nfbshipit-source-id: efae4111d961f3a528cede77c863fb049cda9029", "pr_number": "34491", "files_changed": ["torch/csrc/distributed/rpc/python_call.cpp", "torch/csrc/distributed/rpc/python_call.h", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_resp.cpp", "torch/csrc/distributed/rpc/python_resp.h", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp"], "labels": ["merged"]}, "18ef09f5ac": {"title": "Remove _load_return_value from RPC internal.py (#34492)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34492\n\nDifferential Revision: D20347468\n\nTest Plan: Imported from OSS\n\nPulled By: mrshenli\n\nfbshipit-source-id: 92388d0d50a08fb895bacacf94c7b5495b4ae2b6", "pr_number": "34492", "files_changed": ["torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/python_call.cpp", "torch/csrc/distributed/rpc/python_call.h", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_resp.cpp", "torch/csrc/distributed/rpc/python_resp.h", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/distributed/rpc/internal.py"], "labels": ["merged"]}, "544fb64440": {"title": "Use SerializedPyObj in PythonRpcHandler (#34493)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34493\n\nDifferential Revision: D20347462\n\nTest Plan: Imported from OSS\n\nPulled By: mrshenli\n\nfbshipit-source-id: 9edda9eb95b1994464459271bb53ee77b760e474", "pr_number": "34493", "files_changed": ["torch/csrc/distributed/rpc/python_call.cpp", "torch/csrc/distributed/rpc/python_call.h", "torch/csrc/distributed/rpc/python_resp.cpp", "torch/csrc/distributed/rpc/python_resp.h", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp"], "labels": ["merged"]}, "b82658810e": {"title": "Split deserialize from _run_function in RPC internal.py (#34494)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34494\n\nDifferential Revision: D20347463\n\nTest Plan: Imported from OSS\n\nPulled By: mrshenli\n\nfbshipit-source-id: e6fd886622f26c46bb83ac118e67abb2f5b296b9", "pr_number": "34494", "files_changed": ["torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/distributed/rpc/internal.py"], "labels": ["merged"]}, "b9c32209db": {"title": "Use SerializedPyObj in PythonRpcHandler::generatePythonUDFResult (#34495)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34495\n\nDifferential Revision: D20347466\n\nTest Plan: Imported from OSS\n\nPulled By: mrshenli\n\nfbshipit-source-id: 79625adb4ac3c9c6da4f40016e973bf17466c693", "pr_number": "34495", "files_changed": ["torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp"], "labels": ["merged"]}, "e408d46477": {"title": "Print pytorch version before running ASAN tests (#34521)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34521\n\nTest Plan: CI\n\nDifferential Revision: D20357233\n\nPulled By: malfet\n\nfbshipit-source-id: 1c1b5a94a66d828383676a7a1403bbc13bb21c83", "pr_number": "34521", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["merged"]}, "3671036ef3": {"title": "Adds true_divide function, analogous to Python 's, JAX's, NumPy's (true) division (#34236)", "body": "Summary:\nSee NumPy's division documentation here: https://numpy.org/doc/1.18/reference/generated/numpy.divide.html#numpy.divide.\n\nTrue division is the same as PyTorch's default division except when both inputs are integer or bool tensors. In the latter case the inputs are (conceptually) cast to the default floating type before the division is performed.\n\nThe function is implemented for dense and sparse tensors and supports exporting to ONNX from PyTorch's eager mode or JIT traces. The function is inherently incompatible with exporting to ONNX via JIT script, and is another datapoint suggesting we should deprecate exporting scripted graphs to ONNX.\n\nTests are added for the type promotion, named tensor, and ONNX export behavior.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34236\n\nReviewed By: houseroad\n\nDifferential Revision: D20334087\n\nPulled By: mruberry\n\nfbshipit-source-id: 83d00d886f46f713215d7d9e02ffd043164c57f1", "pr_number": "34236", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "docs/source/onnx.rst", "docs/source/torch.rst", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_namedtensor.py", "test/test_overrides.py", "test/test_type_promotion.py", "tools/autograd/derivatives.yaml", "torch/_torch_docs.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged"]}, "965146b818": {"title": "[jit] delete netdef converter (#33807)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33807\n\nafaik this is unused, so removing it from the source tree. RIP :(\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20122118\n\nPulled By: suo\n\nfbshipit-source-id: cb45943f5b9f969482301a2f9fe540326dbc78f2", "pr_number": "33807", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_netdef_converter.cpp", "test/cpp/jit/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/netdef_converter.cpp", "torch/csrc/jit/netdef_converter.h"], "labels": ["jit", "merged"]}, "4f62cbe7de": {"title": "[ONNX] Support one_hot (#34454)", "body": "Summary:\nThis PR resolves https://github.com/pytorch/pytorch/issues/22534 by adding a converter for the `torch.nn.functional.one_hot` function, and covering it with a test.\n\nAre there other places this should be tested?\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34454\n\nReviewed By: hl475\n\nDifferential Revision: D20354255\n\nPulled By: houseroad\n\nfbshipit-source-id: 84224c1610b2cc7986c91441c65647ddc090750d", "pr_number": "34454", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source"]}, "d30fa4837e": {"title": "Unify gradient accumulation between distributed autograd and local autograd (#33214)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33214\n\nDistributed autograd had some custom logic in terms of how we\naccumulated gradients. This was mostly done early on to enable basic\nfunctionality. Although, in the long term we should merge this logic with what\nwe have in the local autograd engine. A lot of work has gone into ensuring we\naccumulate grads correctly and efficiently and we should reuse that as a\nstarting point.\n\nWe can investigate if we need further custom logic for distributed autograd\nlater on if we need additional optimizations.\n\nIn this PR I've merged the gradient accumulation logic and also the gradient\nhooks. As a result, now gradient hooks are called in distributed autograd as\nwell.\nghstack-source-id: 99838019\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19843284\n\nfbshipit-source-id: 7923d7e871fb6afd3e98dba7de96606264dcb5f3", "pr_number": "33214", "files_changed": ["torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/autograd/functions/accumulate_grad.cpp", "torch/csrc/autograd/functions/accumulate_grad.h", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_engine.h", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/context/context.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/utils/future.h", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py"], "labels": ["merged"]}, "3e6e2e9b7b": {"title": "Print the current Node name in anomaly mode (#33875)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33875\n\nFixes #33675.\n\nI added a `current_node_name` argument to AnomalyMetadata::print_stack.\nThis is a mandatory arg because I found only one callsite and making it\na default arg on a virtual function can be confusing.\n\nTest Plan:\n- Tested locally:\nhttps://gist.github.com/zou3519/09937387c83efc76e1700374d5c9c9d9\n- I don't know how to add a test for this: the message is printed to\nstderr but it isn't an exception nor a warning. I considered capturing\nthe stderr of a subprocess but that seems like asking for flakiness.\n\nDifferential Revision: D20349399\n\nPulled By: zou3519\n\nfbshipit-source-id: 7585ddffe2bf9e1081f4028a9c44de783978a052", "pr_number": "33875", "files_changed": ["torch/csrc/autograd/anomaly_mode.h", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/python_anomaly_mode.cpp", "torch/csrc/autograd/python_anomaly_mode.h"], "labels": ["merged"]}, "f5ee46f1cf": {"title": "Remove custom function in no_grad block error message (#33896)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33896\n\nFixes #32625. Previously, we'd receive an error message if we have a\ncustom function return a view of an input in a no_grad block:\n```\nclass Alias(Function):\n    staticmethod\n    def forward(ctx, x):\n        return x[:]\n\n    staticmethod\n    def backward(ctx, gx):\n        return gx\n\ninp = torch.rand(2, requires_grad=True)\n\nwith torch.no_grad():\n    # Used to error out\n    output = Alias.apply(inp)\n```\n\nAfter this change, the error no longer happens. The behavior changes to\nbecome consistent to if we had implemented an operator that does the\nsame thing as the custom function:\n- the output requires_grad\n- we are able to detect (and error out) if the user tries to modify the\noutput in-place outside of the no_grad block.\n\nTest Plan: - new test\n\nDifferential Revision: D20345601\n\nPulled By: zou3519\n\nfbshipit-source-id: 7f95b4254f52ddbf989d26f449660403bcde1c78", "pr_number": "33896", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/custom_function.cpp"], "labels": ["merged"]}, "20b18a58f1": {"title": "Update compiler warning about ABI compatibility (#34472)", "body": "Summary:\n3ac42677633a39c588c3fea19d2d4121f114edb3 already forces pytorch to use gcc>=5 everywhere\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34472\n\nDifferential Revision: D20345134\n\nPulled By: ezyang\n\nfbshipit-source-id: 3ce706405e8784cac5c314500466b5f988ad31bf", "pr_number": "34472", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["merged"]}, "29b673392f": {"title": "[ROCm] Enable BFloat16 type for loss functions and few misc ops required for resnet50 (#34469)", "body": "Summary:\nThis PR enables bfloat16 type for loss criterion ops(and the ops they depend on) and few miscellaneous ops required to train resnet50.\n\niotamudelta ezyang\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34469\n\nDifferential Revision: D20348856\n\nPulled By: ezyang\n\nfbshipit-source-id: 0a8f06c2169cfa3c9cf319120e27150170095f6c", "pr_number": "34469", "files_changed": ["aten/src/ATen/Dispatch.h", "aten/src/ATen/native/cuda/BinaryArithmeticKernel.cu", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/cuda/Loss.cu", "aten/src/ATen/native/cuda/PointwiseOpsKernel.cu", "aten/src/ATen/native/cuda/ReduceOpsKernel.cu", "aten/src/ATen/native/cuda/SoftMax.cu", "aten/src/ATen/native/cuda/UnaryLogKernels.cu", "aten/src/ATen/native/cuda/UnarySignKernels.cu", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/ClassNLLCriterion.cu", "aten/src/THCUNN/MultiLabelMarginCriterion.cu", "aten/src/THCUNN/SpatialClassNLLCriterion.cu", "aten/src/THCUNN/generic/ClassNLLCriterion.cu", "aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu", "test/test_nn.py", "test/test_torch.py", "torch/testing/_internal/common_nn.py"], "labels": ["merged", "module: rocm", "open source"]}, "9e94e46453": {"title": "Check if rnn weights need to be flattened (#34265)", "body": "Summary:\ncuDNN needs it, MIOpen doesn't. However, since it seems to be the PyTorch preference to not introduce ROCm-specific logic in the python layer, we need to add a C++ function to detect if rnn weight flattening is needed.\n\nThis PR will be needed to fix the rnn unit test errors arising for PR https://github.com/pytorch/pytorch/issues/33837.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34265\n\nDifferential Revision: D20345105\n\nPulled By: ezyang\n\nfbshipit-source-id: a2588a6e2ac6f7d1edf2b7872bc6a879a7df96ec", "pr_number": "34265", "files_changed": ["aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/native_functions.yaml", "torch/csrc/api/src/nn/modules/rnn.cpp", "torch/nn/modules/rnn.py"], "labels": ["merged", "module: cudnn", "open source", "triaged"]}, "c7dd5f89a2": {"title": "Fix #33562 (uncaught domain_error on macOS) (#34301)", "body": "Summary:\nTries to fix https://github.com/pytorch/pytorch/issues/33562 by raising `std::runtime_error` instead of `std::domain_error`.\n* The Python tests already expect `RuntimeError` so this shouldn't affect Python users of PyTorch.\n* If someone out there is using C10 or ATen from C++ and tries to catch `std::domain_error` specifically, this fix would break their code. Hopefully that's not the case.\n\nAlternative to this PR is someone try to really get to the bottom of why `std::domain_error` isn't being caught.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34301\n\nDifferential Revision: D20344579\n\nPulled By: ezyang\n\nfbshipit-source-id: d5f3045085a2f75b71b864335ebf44991d0cad80", "pr_number": "34301", "files_changed": ["aten/src/ATen/test/scalar_test.cpp", "c10/util/TypeCast.h"], "labels": ["merged", "module: internals", "module: operators", "open source", "topic: bc-breaking", "topic: crash", "triaged"]}, "cb689a5d68": {"title": "remove duplicated process group gloo timeout (#31342)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31342\n\nTest Plan: unit test\n\nDifferential Revision: D19131704\n\nfbshipit-source-id: 4e91d5933635ee2c7c301caf89a5a7009c5cb7c8", "pr_number": "31342", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["merged"]}, "0dbfb26e53": {"title": "Clean up include list of Shape.cu (#34528)", "body": "Summary:\nThe include list seems to be copied from somewhere else, and some totally unrelated files are included.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34528\n\nDifferential Revision: D20358622\n\nPulled By: ngimel\n\nfbshipit-source-id: d8a6260f5f77b0eabdbd68e3728873efd632d9bc", "pr_number": "34528", "files_changed": ["aten/src/ATen/native/cuda/Shape.cu"], "labels": ["merged", "open source"]}, "cd9d9a2235": {"title": "fix handling of replica parameters in DataParallel (#33907)", "body": "Summary:\nIn DataParallel, replica parameters are not leaves (because they are computed via broadcast from master parameters), and should be treated as such. Fixes https://github.com/pytorch/pytorch/issues/33552\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33907\n\nDifferential Revision: D20150199\n\nPulled By: ngimel\n\nfbshipit-source-id: 5965d4115b6b3a8433063126ff6269567872fbeb", "pr_number": "33907", "files_changed": ["test/distributed/test_data_parallel.py", "torch/nn/parallel/replicate.py"], "labels": ["merged"]}, "5f61f42c79": {"title": ".circleci: Switch should_run_job cuda 10.1 -> 10.2 (#34498)", "body": "Summary:\nWe updated the default jobs to run in a different PR but neglected to\nupdate this script as well.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34498\n\nDifferential Revision: D20368420\n\nPulled By: seemethere\n\nfbshipit-source-id: 240171b18f397095e3a8d57de3a29d1d2e891d85", "pr_number": "34498", "files_changed": [".circleci/scripts/should_run_job.py"], "labels": ["merged", "module: ci"]}, "b185359fb4": {"title": "Avoid clone for sparse tensors during accumulation of grads. (#33427)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33427\n\nThis PR is an attempt to avoid clone for sparse tensors similar to how\nwe avoid clone for dense tensors currently.\n\nAs per my understanding even if the 'indices' and 'values' of a sparse tensor\nare non-continguous, operations like 'add' are still supported. As a result,\nthe major change in this PR is to use create a shallow copy instead of clone()\nfor sparse tensors.\nghstack-source-id: 99838375\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19926698\n\nfbshipit-source-id: b5a3f36c2aa273e17f8b7a9f09c1ea00e7478109", "pr_number": "33427", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/functions/accumulate_grad.h", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py"], "labels": ["merged", "module: sparse"]}, "2cf344be4c": {"title": "Turn on exact_dtype by default on test_sparse.py (#34489) (#34542)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34542\n\nTurn on exact_dtype by default on test_sparse.py (#34489)\n\nPull Request resolved: #34489\n\nTest Plan:\n```\npython test/test_sparse.py\n```\n\nImported from OSS\n\nDifferential Revision: D20369764\n\nfbshipit-source-id: ade2434f77af8ae419bda653b4c46616c052a8b2", "pr_number": "34542", "files_changed": ["test/test_sparse.py"], "labels": ["merged"]}, "6f12145c60": {"title": "Change std::to_string call to c10::to_string", "body": "Summary: I'm using this code in an internal Android build, and std::to_string doesn't work in our internal Android builds yet.\n\nTest Plan: Internal build.\n\nReviewed By: ljk53\n\nDifferential Revision: D20234221\n\nfbshipit-source-id: 8fd61235bf9b487e07a1459c452830e732c7afb0", "pr_number": null, "files_changed": ["aten/src/ATen/native/TensorIterator.cpp"], "labels": []}, "2d24005d18": {"title": "[C++ API Parity] rmsprop optimizer update (#33450)", "body": "Summary:\n**This PR is BC-breaking in the following way:**\n\nIn RMSpropOptions:\n1. learning_rate is renamed to lr.\n\n**Test plan before 1.5 release:**\n\nTest that in 1.5 we can load a C++ RMSprop optimizer that was serialized in 1.4, and their states are the same.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33450\n\nDifferential Revision: D20366623\n\nPulled By: anjali411\n\nfbshipit-source-id: 83250be9b583a766927e0e22a4de8b0765379451", "pr_number": "33450", "files_changed": ["test/cpp/api/serialize.cpp", "torch/csrc/api/include/torch/optim/adagrad.h", "torch/csrc/api/include/torch/optim/adam.h", "torch/csrc/api/include/torch/optim/rmsprop.h", "torch/csrc/api/src/optim/adagrad.cpp", "torch/csrc/api/src/optim/adam.cpp", "torch/csrc/api/src/optim/rmsprop.cpp", "torch/csrc/api/src/optim/sgd.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "259d7299db": {"title": "[caffe2] do not declare __assert_fail in clang builds (#33893)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33893\n\nIt appears that when Clang drives CUDA compilation ` __assert_fail` is always defined as device function.\n\nTest Plan:\n```lang=bash\nbuck build mode/opt -c fbcode.cuda_use_clang=true -c cxx.untracked_headers=ignore //fblearner/flow/projects/dper:workflow\n```\n\nReviewed By: ngimel\n\nDifferential Revision: D20145034\n\nfbshipit-source-id: 23153411ed631e05421c7afcf41b7ea5619cdd96", "pr_number": "33893", "files_changed": ["c10/macros/Macros.h"], "labels": ["fb-exported", "merged"]}, "09296c34a4": {"title": "Add the build for runtime dispatch for AVX, AVX2 instruction set (#26125)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26125\n\nWe already had some optimization implementation using AVX2 for improve the quantized kernel performance. In this diff, we want to enable the runtime dispatch.\n\nTest Plan:\nSandcastle build and test\n\nAlso test with a python binary calling into vectorized op.\n\ntorch.__config__.show()\nPyTorch built with:\n  - GCC 4.2\n  - clang 8.0.20181009\n  - Intel(R) Math Kernel Library Version 2017.0.3 Product Build 20170413 for Intel(R) 64 architecture applications\n  - Intel(R) MKL-DNN v0.18.1 (Git Hash N/A)\n  - OpenMP 1\n  - **CPU capability usage: AVX2**\n  - Build settings:\n\nReviewed By: jamesr66a\n\nDifferential Revision: D17337251\n\nfbshipit-source-id: 8e22d10011a12a4eaf54cea3485353eb1811d828", "pr_number": "26125", "files_changed": ["aten/src/ATen/Version.cpp", "test/cpp/api/dispatch.cpp"], "labels": ["merged", "module: build", "module: ci", "module: operators"]}, "dd7cec680c": {"title": "Do not use clang if it can not parse system extensions (#34549)", "body": "Summary:\nAttempt to build pytorch with ASAN on system with gcc-8 fails due to the mismatch system compilation flags.\nAddress the issue by using original compiler to build `torch._C` extension\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34549\n\nTest Plan: Run `.jenkins/pytorch/build-asan.sh` on FC-30\n\nDifferential Revision: D20373781\n\nPulled By: malfet\n\nfbshipit-source-id: 041c8d25f96b4436385a5e0eb6fc46e9b5fdf3f1", "pr_number": "34549", "files_changed": ["setup.py"], "labels": ["merged"]}, "be3bc1deb1": {"title": "convert counter back to list #33229 (#33356)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33229\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33356\n\nDifferential Revision: D20003196\n\nPulled By: vincentqb\n\nfbshipit-source-id: 96f9e0fc7e99a7c2e202f932d1a2ffa158afad92", "pr_number": "33356", "files_changed": ["test/test_optim.py", "torch/optim/lr_scheduler.py"], "labels": ["merged"]}, "d0834c5b64": {"title": "Preserve memory format for torch.cat on CUDA (#34526)", "body": "Summary:\nfix https://github.com/pytorch/pytorch/issues/34084\n\ncc: ptrblck VitalyFedyunin\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34526\n\nDifferential Revision: D20371847\n\nPulled By: ngimel\n\nfbshipit-source-id: e3b1a34caff2db8099ad9afe91bf9b473d5da6e8", "pr_number": "34526", "files_changed": ["aten/src/ATen/native/cuda/Shape.cu", "test/test_torch.py"], "labels": ["merged", "open source"]}, "903ad90325": {"title": "[JIT] Introduce a fake Tensor creation node for IR unit tests (#34334)", "body": "Summary:\n**Summary**\nThere is often a need to create a Tensor when writing IR by hand for JIT\noptimisation pass unit tests. The only options for this today are real\nTensor creation functions like `aten::ones`. Any test that uses these functions\nmust also use the same default arguments as the Python/C++ API, which means\nthat all of the tests have to be updated when the API is updated. This commit\nintroduces a new primitive, `prim::MakeTestTensor` with schema `() -> Tensor` that\nshould be used in unit tests instead of real Tensor creation functions. This new\nprimitive has no public-facing API, so the maintenance burden is much lower.\n\n**Testing**\nThis commit updates the alias analysis and DCE tests to use `prim::MakeTestTensor` instead of\n`aten::rand`, `aten::ones`, and `aten::zeros`.\n\n```\n$ ./bin/test_jit\nCUDA not available. Disabling CUDA and MultiCUDA tests\nNote: Google Test filter = *-*_CUDA:*_MultiCUDA\n[==========] Running 75 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 75 tests from JitTest\n[ RUN      ] JitTest.ADFormulas\n[       OK ] JitTest.ADFormulas (82 ms)\n[ RUN      ] JitTest.Attributes\n[       OK ] JitTest.Attributes (0 ms)\n...\n...\n...\n[ RUN      ] JitTest.LiteInterpreterPrim\n[       OK ] JitTest.LiteInterpreterPrim (0 ms)\n[ RUN      ] JitTest.LiteInterpreterLoadOrigJit\n[       OK ] JitTest.LiteInterpreterLoadOrigJit (2 ms)\n[----------] 75 tests from JitTest (150 ms total)\n\n[----------] Global test environment tear-down\n[==========] 75 tests from 1 test case ran. (150 ms total)\n[  PASSED  ] 75 tests.\n```\n\n**Fixes**\nThis pull request fixes https://github.com/pytorch/pytorch/issues/33500.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34334\n\nDifferential Revision: D20296437\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: df4e7b0881ae4913424e5a409bfa171a61c3e568", "pr_number": "34334", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_base.cpp", "test/cpp/jit/test_dce.cpp"], "labels": ["jit", "merged"]}, "a09c4d3997": {"title": "[pt][quant] Vectorized qmul and more methods on qint data types (#34376)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34376\n\nVectorized implementation of qmul. qmul is now ~16x faster on my development machine. This implementation works for qint8, quint8 and qint32. Also added some commonly used operations, such as multiply operator, requantize operation etc., to qint vector classes for future use.\n\n```\n#!/usr/bin/env python\n\nimport time\nimport torch\nimport torch.nn as nn\ntorch.set_num_threads(1)\n# print(torch.__config__.parallel_info())\n\nA = torch.rand(1, 54, 54, 256)\nB = torch.rand(1, 54, 54, 256)\n\nscale = .05\nzero_point = 50\n\nfor dtype in [torch.quint8, torch.qint8]:\n\n    qA = torch.quantize_per_tensor(A, scale=scale, zero_point=zero_point,\n            dtype=dtype)\n    qB = torch.quantize_per_tensor(B, scale=scale, zero_point=zero_point,\n            dtype=dtype)\n\n    NITER = 1000\n    s = time.time()\n    for i in range(NITER):\n        out = torch.ops.quantized.mul(qA, qB, scale=scale, zero_point=zero_point)\n    time_per_iter = (time.time() - s) / NITER\n\n    print('dtype: {} time per iter ms: {:.3f}'.format(dtype, time_per_iter * 1000))\n```\n### Before\ndtype: torch.quint8 time per iter ms: 6.714\ndtype: torch.qint8 time per iter ms: 6.780\n\n### After\ndtype: torch.quint8 time per iter ms: 0.431\ndtype: torch.qint8 time per iter ms: 0.417\n\n### Test\nModified qmul tests to include qint8 and qint32 data types.\n\npython test/test_quantized.py TestQuantizedOps.test_qmul_relu_same_qparams\npython test/test_quantized.py TestQuantizedOps.test_qmul_relu_different_qparams\npython test/test_quantized.py TestQuantizedOps.test_qmul_broadcast\nghstack-source-id: 99862681\n\nDifferential Revision: D20308515\n\nfbshipit-source-id: 4fa65b2ba433cfd59260fc183a70f53a6fcc36b4", "pr_number": "34376", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_qint.h", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qmul.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "aten/src/ATen/quantized/Quantizer.cpp", "aten/src/ATen/quantized/Quantizer.h", "test/test_quantized.py"], "labels": ["merged"]}, "4167db11f7": {"title": "[pytorch][ci] add build_only flag to mobile CI jobs (#34560)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34560\n\nThese jobs don't have next phase so we don't really need commit the\ndocker images.\nShould also fix issue #34557.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20375308\n\nPulled By: ljk53\n\nfbshipit-source-id: 328cb428fcfb0fbb79b2a233b5f52607158c983c", "pr_number": "34560", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/pytorch-build-params.yml", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml"], "labels": ["merged"]}, "23b2fba79a": {"title": "[jit] Add type tags to lists/dicts in pickle (#33255)", "body": "Summary:\nStacked PRs\n * #33474 - [jit] Remove list specializations from pickler\n * **#33255 - [jit] Add type tags to lists/dicts in pickle**\n\nThis adds a global call to `torch.jit._pickle.restore_type_tags` for\nlists and dicts so that we can preserve their types after serialization.\n](https://our.intern.facebook.com/intern/diff/20346780/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33255\n\nPulled By: driazati\n\nDifferential Revision: D20346780\n\nfbshipit-source-id: c8534954ef4adb2e3c880401acbee30cd284f3db", "pr_number": "33255", "files_changed": ["caffe2/serialize/inline_container.h", "test/cpp/jit/test_save_load.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/mobile/type_parser.cpp", "torch/csrc/jit/serialization/import.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/pickler.h", "torch/csrc/jit/serialization/unpickler.cpp", "torch/csrc/jit/serialization/unpickler.h", "torch/jit/_pickle.py"], "labels": ["jit", "merged"]}, "ce77d4a316": {"title": "Set USE_RCCL cmake option (dependent on USE_NCCL) (#31341)", "body": "Summary:\nso that Gloo build has RCCL path enabled for ROCm\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31341\n\nDifferential Revision: D20379910\n\nPulled By: ezyang\n\nfbshipit-source-id: 981f924be93ddcc0705c1934f92d938c29aaf312", "pr_number": "31341", "files_changed": ["CMakeLists.txt"], "labels": ["merged", "open source"]}, "8d84c5f1c7": {"title": "Fix static data initialization deadlock on GIL (#34505)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34505\n\nA thread could hold GIL when calling PythonRpcHandler::getInstance(),\nmeantime another thread could have been doing static data\ninitialization by calling `new PythonRpcHandler()`, inside of which GIL is\nalso required. Static data initialization is thread-safe, so the thread\nholding the GIL will wait for the other thread to finish static data\ninitializating before going forward. Because the initialization can't\nproceed without GIL, there is a deadlock. We ask the calling thread to\nrelease GIL to avoid this situation.\nghstack-source-id: 99893858\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:dist_autograd_spawn -- 'test_backward_simple_script_call \\(test_dist_autograd_spawn\\.DistAutogradTestWithSpawn\\)' --stress-runs 100\n```\n\nDifferential Revision: D7490489\n\nfbshipit-source-id: 76f63cc7bedf088d3dbff288f53aa0bd33749255", "pr_number": "34505", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h"], "labels": ["merged"]}, "2e88a78d2e": {"title": "add quantized_hardtanh (#34097)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34097\n\nAdds quantized hardtanh.  Calls the clamp kernel behind the\nscenes.\n\nTest Plan:\n```\npython test/test_quantized.py\n```\n\nImported from OSS\n\nDifferential Revision: D20208860\n\nfbshipit-source-id: 165a6a1c22f1dcc479679e5ea0c990d0e9c3b6c5", "pr_number": "34097", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "benchmarks/operator_benchmark/pt/qactivation_test.py", "test/test_quantized.py", "torch/nn/quantized/functional.py"], "labels": ["merged", "quantization"]}, "ab2297dfe6": {"title": "Add Tensor overload for start in narrow. (#34317)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/31558\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34317\n\nDifferential Revision: D20294333\n\nPulled By: ailzhang\n\nfbshipit-source-id: 47c6646ae298e04a455923bd5048db026a5e3c7c", "pr_number": "34317", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py"], "labels": ["merged"]}, "2ce9513b0c": {"title": "AccumulateGrad: ensure sparse tensor indices and values refcount is always 1 (#34559)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34559\n\nWe check the use_count for indices and values when we avoid a clone\nfor sparse tensors. The sparse tensor grad itself might have a higher refcount\ndue to DDP hooks/dist autograd structures holding refs, but the indices and\nvalues inside the sparse tensor should always have a refcount of 1.\nghstack-source-id: 99900534\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D20375239\n\nfbshipit-source-id: 6a654549d13071ab3451cef94259caf7627b575c", "pr_number": "34559", "files_changed": ["torch/csrc/autograd/functions/accumulate_grad.h", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py"], "labels": ["merged"]}, "7aca9afdfb": {"title": "[pytorch] remove boilerplate setQEngine() from PyTorch mobile predictors (#34556)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34556\n\nAccording to\nhttps://github.com/pytorch/pytorch/pull/34012#discussion_r388581548,\nthis `at::globalContext().setQEngine(at::QEngine::QNNPACK);` call isn't\nreally necessary for mobile.\n\nIn Context.cpp it selects the last available QEngine if the engine isn't\nset explicitly. For OSS mobile prebuild it should only include QNNPACK\nengine so the default behavior should already be desired behavior.\n\nIt makes difference only when USE_FBGEMM is set - but it should be off\nfor both OSS mobile build and internal mobile build.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20374522\n\nPulled By: ljk53\n\nfbshipit-source-id: d4e437a03c6d4f939edccb5c84f02609633a0698", "pr_number": "34556", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_lite.cpp", "binaries/speed_benchmark_torch.cc", "ios/TestApp/TestApp/Benchmark.mm", "ios/TestApp/TestAppTests/TestAppTests.mm", "test/mobile/custom_build/predictor.cpp"], "labels": ["merged"]}, "fbbeee0983": {"title": "Port `remainder` from TH to ATen (CPU and CUDA) (#34136)", "body": "Summary:\nCPU issue https://github.com/pytorch/pytorch/issues/24753\nCUDA issue https://github.com/pytorch/pytorch/issues/24615\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34136\n\nDifferential Revision: D20375458\n\nPulled By: ezyang\n\nfbshipit-source-id: 1a9fb39a7e2d17a0d31bd14b211eaacea060e834", "pr_number": "34136", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryArithmeticKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/THC/THCTensorMathPairwise.cu", "aten/src/THC/THCTensorMathPointwise.cuh", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPairwise.h", "aten/src/THC/generic/THCTensorMathPointwise.cu", "aten/src/THC/generic/THCTensorMathPointwise.h", "test/test_torch.py"], "labels": ["merged", "open source", "topic: bc-breaking", "topic: porting", "triaged"]}, "b2344b70da": {"title": "Beef up documentation on Dispatcher.h, reorder methods for clarity. (#33838)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33838\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20227875\n\nPulled By: ezyang\n\nfbshipit-source-id: 319855b1f0fa436f9ed5256d2106b07f20e6b833", "pr_number": "33838", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": ["merged"]}, "9d42177a31": {"title": "Delete OperatorOptions, absorb AliasAnalysisKind into FunctionSchema. (#34160)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34160\n\nI constructed the patch by deleting OperatorOptions and then rerouting\nall queries for AliasAnalysisKind to FunctionSchema.  Some of the\nbehavior is kind of bogus: we really shouldn't be mutating FunctionSchema\nafter the fact, but that won't get fixed until we actually switch to\ntrue schema merging.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20282846\n\nPulled By: ezyang\n\nfbshipit-source-id: ba7bca6e8adc3365789639b88e54c4e881b1692e", "pr_number": "34160", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/dispatch/OperatorOptions.h", "aten/src/ATen/core/function_schema.h", "aten/src/ATen/core/op_registration/op_registration.cpp", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/test_schema_matching.cpp", "tools/jit/templates/register_aten_ops.cpp", "torch/csrc/jit/codegen/fuser/fallback.cpp", "torch/csrc/jit/ir/constants.cpp", "torch/csrc/jit/passes/batch_mm.cpp", "torch/csrc/jit/passes/decompose_ops.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/python/python_interpreter.cpp", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/operator.h", "torch/csrc/jit/runtime/register_distributed_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_special_ops.cpp", "torch/csrc/jit/runtime/register_string_ops.cpp"], "labels": ["jit", "merged"]}, "5fc5cf6571": {"title": "Stop using ctypes to interface with CUDA libraries. (#33678)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/33016, Continuation of https://github.com/pytorch/pytorch/issues/31160\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33678\n\nDifferential Revision: D20249187\n\nPulled By: ezyang\n\nfbshipit-source-id: 172ce4a0fee7fbe01436a421d1af22ef6173b6ed", "pr_number": "33678", "files_changed": ["tools/build_variables.bzl", "torch/CMakeLists.txt", "torch/backends/cudnn/__init__.py", "torch/backends/cudnn/rnn.py", "torch/csrc/Module.cpp", "torch/csrc/cuda/Module.cpp", "torch/csrc/cuda/shared/cudart.cpp", "torch/csrc/cuda/shared/cudnn.cpp", "torch/csrc/cuda/shared/nvtx.cpp", "torch/cuda/__init__.py", "torch/cuda/__init__.pyi", "torch/cuda/nvtx.py", "torch/cuda/profiler.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["merged", "open source", "triaged"]}, "2ec779d46c": {"title": "PCA and SVD for low-rank matrices, LOBPCG for positive-defined generalized eigenvalue problem (#29488)", "body": "Summary:\nThis PR implements the following linear algebra algorithms for low-rank matrices:\n- [x] Approximate `A` as `Q Q^H A` - using Algorithm 4.4 from [Halko et al, 2009](http://arxiv.org/abs/0909.4061).\n  + exposed as `torch.lowrank.get_approximate_basis(A, q, niter=2, M=None) -> Q`\n  + [x] dense matrices\n  + [x] batches of dense matrices\n  + [x] sparse matrices\n  + [x] documentation\n- [x] SVD - using Algorithm 5.1 from [Halko et al, 2009](http://arxiv.org/abs/0909.4061).\n  + uses `torch.lowrank.get_approximate_basis`\n  + exposed as `torch.svd_lowrank(A, q=6, niter=2, M=None) -> (U, S, V)`\n  + [x] dense matrices\n  + [x] batches of dense matrices\n  + [x] sparse matrices\n  + [x] documentation\n- [x] PCA - using `torch.svd_lowrank`\n  + uses `torch.svd_lowrank`\n  + exposed as `torch.pca_lowrank(A, center=True, q=None, niter=2) -> (U, S, V)`\n  + [x] dense matrices\n  + [x] batches of dense matrices\n  + [x] sparse matrices, uses non-centered sparse matrix algorithm\n  + [x] documentation\n- [x] generalized eigenvalue solver using the original LOBPCG algorithm [Knyazev, 2001](https://epubs.siam.org/doi/abs/10.1137/S1064827500366124)\n  + exposed as `torch.lobpcg(A, B=None, k=1, method=\"basic\", ...)`\n  + [x] dense matrices\n  + [x] batches of dense matrices\n  + [x] sparse matrices\n  + [x] documentation\n- [x] generalized eigenvalue solver using robust LOBPCG with orthogonal basis selection [Stathopoulos, 2002](https://epubs.siam.org/doi/10.1137/S1064827500370883)\n  + exposed as `torch.lobpcg(A, B=None, k=1, method=\"ortho\", ...)`\n  + [x] dense matrices\n  + [x] batches of dense matrices\n  + [x] sparse matrices\n  + [x] documentation\n- [x] generalized eigenvalue solver using the robust and efficient LOBPCG Algorithm 8 from [Duersch et al, 2018](https://epubs.siam.org/doi/abs/10.1137/17M1129830) that switches to orthogonal basis selection automatically\n  + the \"ortho\" method improves iterations so rapidly that in the current test cases it does not make sense to use the basic iterations at all. If users will have matrices for which basic iterations could improve convergence then the `tracker` argument allows breaking the iteration process at user choice so that the user can switch to the orthogonal basis selection if needed. In conclusion, there is no need to implement Algorithm 8 at this point.\n- [x] benchmarks\n  + [x] `torch.svd` vs `torch.svd_lowrank`, see notebook [Low-rank SVD](https://github.com/Quansight/pearu-sandbox/blob/master/pytorch/Low-rank%20SVD.ipynb). In conclusion, the low-rank SVD is going to be useful only for large sparse matrices where the full-rank SVD will fail due to memory limitations.\n  + [x] `torch.lobpcg` vs `scipy.sparse.linalg.lobpcg`, see notebook [LOBPCG - pytorch vs scipy](https://github.com/Quansight/pearu-sandbox/blob/master/pytorch/LOBPCG%20-%20pytorch%20vs%20scipy.ipynb). In conculsion, both implementations give the same results (up to numerical errors from different methods), scipy lobpcg implementation is generally faster.\n  + [x] On very small tolerance cases, `torch.lobpcg` is more robust than `scipy.sparse.linalg.lobpcg` (see `test_lobpcg_scipy` results)\n\nResolves https://github.com/pytorch/pytorch/issues/8049.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29488\n\nDifferential Revision: D20193196\n\nPulled By: vincentqb\n\nfbshipit-source-id: 78a4879912424595e6ea95a95e483a37487a907e", "pr_number": "29488", "files_changed": ["docs/source/torch.rst", "test/test_overrides.py", "test/test_torch.py", "torch/__init__.py", "torch/_linalg_utils.py", "torch/_lobpcg.py", "torch/_lowrank.py", "torch/_six.py", "torch/functional.py", "torch/serialization.py", "torch/testing/_internal/common_utils.py"], "labels": ["open source", "triaged"]}, "63964175b5": {"title": "Revert D20379910: [pytorch][PR] Set USE_RCCL cmake option (dependent on USE_NCCL)", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20379910\n\nOriginal commit changeset: 981f924be93d\n\nfbshipit-source-id: 2cfc2eebe6ebabf801f0ea6a183aad2342ada79f", "pr_number": null, "files_changed": ["CMakeLists.txt"], "labels": []}, "6f8a8e4e47": {"title": "Revert D20282846: Delete OperatorOptions, absorb AliasAnalysisKind into FunctionSchema.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20282846\n\nOriginal commit changeset: ba7bca6e8adc\n\nfbshipit-source-id: b9e15d2b2c3d1dbc6e971ab3c0bdf380e769dcf1", "pr_number": null, "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/dispatch/OperatorOptions.h", "aten/src/ATen/core/function_schema.h", "aten/src/ATen/core/op_registration/op_registration.cpp", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/test_schema_matching.cpp", "tools/jit/templates/register_aten_ops.cpp", "torch/csrc/jit/codegen/fuser/fallback.cpp", "torch/csrc/jit/ir/constants.cpp", "torch/csrc/jit/passes/batch_mm.cpp", "torch/csrc/jit/passes/decompose_ops.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/python/python_interpreter.cpp", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/operator.h", "torch/csrc/jit/runtime/register_distributed_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_special_ops.cpp", "torch/csrc/jit/runtime/register_string_ops.cpp"], "labels": []}, "4b929e5466": {"title": "Revert D20193196: [pytorch][PR] PCA and SVD for low-rank matrices, LOBPCG for positive-defined generalized eigenvalue problem", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20193196\n\nOriginal commit changeset: 78a487991242\n\nfbshipit-source-id: 8da4f8cb17c45af41e8c0ce80bc72581eb10dbb8", "pr_number": null, "files_changed": ["docs/source/torch.rst", "test/test_overrides.py", "test/test_torch.py", "torch/__init__.py", "torch/_linalg_utils.py", "torch/_lobpcg.py", "torch/_lowrank.py", "torch/_six.py", "torch/functional.py", "torch/serialization.py", "torch/testing/_internal/common_utils.py"], "labels": []}, "82cdd3abae": {"title": "Stop last usage of newWithSize. (#34386)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34386\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20311061\n\nPulled By: gchanan\n\nfbshipit-source-id: 1e90a90db2efa1a566d4a78a6d1b8d918b91cf66", "pr_number": "34386", "files_changed": ["aten/src/THC/generic/THCTensorMathPairwise.cu"], "labels": ["merged"]}, "2cf576e9ea": {"title": "small typos (#34589)", "body": "Summary:\nSpotted a couple of small typos \ud83d\ude4f\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34589\n\nDifferential Revision: D20387653\n\nPulled By: ngimel\n\nfbshipit-source-id: 3089fe606ccb8c8ee57cf7a900aba714fd0ce567", "pr_number": "34589", "files_changed": ["test/test_jit.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source"]}, "b553e6911a": {"title": "[distributed] quicker exit in the case of failed tests in distributed (#34150)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34150\n\nIn the distributed setting we commonly have tests in which there are errors where one process\nexits but the other do not (since they are for example waiting for work from\nthe process that exited). Currently, when this situation happens we do not\nhandle this well, and wait for process 0 to timeout. This results in wasted\ntime waiting for test errors and a less helpful \"Process 0 timed out...\" error\nmessage when the error was actually something else.\n\nThis diff fixes the issue by checking for exited subprocesses and terminating\nthe test when we see a subprocess that has exited uncleanly. We still enforce\ntimeouts and return when all processes have exited cleantly in the happy path.\nghstack-source-id: 99921462\n\nTest Plan:\nAll distributed tests + tested by writing tests that should trigger\nthe unclean subprocess detection, and verified that we exit quickly instead of\nwaiting for the entire timeout.\n\nDifferential Revision: D20231032\n\nfbshipit-source-id: 3e0d4a20925b7d1098ec4c40ffcc66845425dd62", "pr_number": "34150", "files_changed": ["torch/testing/_internal/common_distributed.py"], "labels": ["merged"]}, "12fb8148e4": {"title": "Disable ROCM when building mobile libtorch. (#34478)", "body": "Summary:\nWhen a system has ROCm dev tools installed, `scripts/build_mobile.sh` tried to use it.\nThis PR fixes looking up unused ROCm library when building libtorch mobile.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34478\n\nDifferential Revision: D20388147\n\nPulled By: ljk53\n\nfbshipit-source-id: b512c38fa2d3cda9ac20fe47bcd67ad87c848857", "pr_number": "34478", "files_changed": ["scripts/build_mobile.sh"], "labels": ["merged", "open source", "triaged"]}, "1f834b5c2a": {"title": "[JIT] Torchbind error if python instantiate class that doesnt exist (#34568)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34568\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20378106\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 395a3b05d23727b9cfd074440b2d0e8ef002ec09", "pr_number": "34568", "files_changed": ["test/test_jit.py", "torch/csrc/jit/python/python_custom_class.cpp"], "labels": ["jit", "merged"]}, "70f3298684": {"title": "Fix SELECTED_OP_LIST file path issue (#33942)", "body": "Summary:\nIf SELECTED_OP_LIST is specified as a relative path in command line, CMake build will fail.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33942\n\nDifferential Revision: D20392797\n\nPulled By: ljk53\n\nfbshipit-source-id: dffeebc48050970e286cf263bdde8b26d8fe4bce", "pr_number": "33942", "files_changed": ["scripts/build_android.sh", "scripts/build_ios.sh", "scripts/build_mobile.sh"], "labels": ["merged", "open source", "triaged"]}, "25e4e9eb86": {"title": "[On-device Benchmark] speed_benchmark_torch switch to log latency from dataset level to row level (#34598)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34598\n\nas above\n\nTest Plan:\ntest.txt\n```\nwhat time is it now\ncould you set a reminder at 7 am\nwaht is the weather today\n```\nexample json\n```\n{\n    \"model\": {\n      \"category\": \"CNN\",\n      \"description\": \"Assistant Mobile Inference\",\n      \"files\": {\n        \"model\": {\n          \"filename\": \"model.pt1\",\n          \"location\": \"//everstore/GICWmAB2Znbi_mAAAB0P51IPW8UrbllgAAAP/model.pt1\",\n          \"md5\": \"c0f4b29c442bbaeb0007fb0ce513ccb3\"\n        },\n        \"data\": {\n          \"filename\": \"input.txt\",\n          \"location\": \"/home/pengxia/test/input.txt\",\n          \"md5\": \"c0f4b29c442bbaeb0007fb0ce513ccb3\"\n        }\n      },\n      \"format\": \"pytorch\",\n      \"framework\": \"pytorch\",\n      \"kind\": \"deployment\",\n      \"name\": \"Assistant Mobile Inference\"\n    },\n    \"tests\": [\n      {\n        \"command\": \"{program} --model {files.model}  --input_dims \\\"1\\\" --input_type NLUType --warmup {warmup} --iter 5 --input_file {files.data} --report_pep true\",\n        \"identifier\": \"{ID}\",\n        \"metric\": \"delay\",\n        \"iter\": 15,\n        \"warmup\": 2,\n        \"log_output\": true\n      }\n    ]\n  }\n\n```\n\niter = 5 (--iter 5 ) *3(3 lintes in the test.txt)  = 15\n\narbabu123 I will provide a wrapper to compute the iter in future.\n\nrun following command\n```\nbuck run aibench:run_bench -- -b aibench/specifications/models/pytorch/fbnet/assistant_mobile_inference.json --platform android/full_jit --framework pytorch --remote --devices  SM-G960U-8.0.0-26\n```\n\nresults\nhttps://our.intern.facebook.com/intern/aibench/details/275259559594003\n\n**Note: this is compatible with the existing examples.**\n\nReviewed By: kimishpatel, ljk53\n\nDifferential Revision: D20389285\n\nfbshipit-source-id: 80165ef394439a307ac7986cf540a80fdf3d85d6", "pr_number": "34598", "files_changed": ["binaries/speed_benchmark_torch.cc"], "labels": ["fb-exported", "merged"]}, "2de4f245c6": {"title": "Fix typo in documentation (#34581)", "body": "Summary:\nUpdate the  parameter description of `total_steps` in `OneCycleLR`. References https://github.com/pytorch/pytorch/issues/34531\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34581\n\nDifferential Revision: D20386306\n\nPulled By: albanD\n\nfbshipit-source-id: f8b424a01760e8f5d4de5367b6c60fb342019689", "pr_number": "34581", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["merged", "open source"]}, "866505b100": {"title": "[ci] try to fix rocm builds (#34600)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34600\n\nThey are failing with:\n```\nE: The method driver /usr/lib/apt/methods/https could not be found.\n```\n\nTrying the solution recommended in: https://unix.stackexchange.com/questions/263801/apt-get-fails-the-method-driver-usr-lib-apt-methods-https-could-not-be-found\n\nThe long-term solution is to move all this to be pre-installed in the\ndocker image.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20391153\n\nPulled By: suo\n\nfbshipit-source-id: 959dff2ea9e77bb52739c0659e9d800cdbe4cb01", "pr_number": "34600", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["merged"]}, "56832bf7f3": {"title": "[JIT] Add support for tolist for GPU-resident Tensors (#34554)", "body": "Summary:\n**Summary**\nThis commit modifies the JIT implementation of `Tensor.tolist` so that it\ncan be called on GPU-resident Tensors as well. If the Tensors is not on the\nCPU when the operator is invoked, it is copied to the CPU before doing any\nof the rest of the work to convert it into a list.\n\n**Testing**\nThis commit adds GPU versions of some of the existing CPU tests for this\nfeature.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34554\n\nDifferential Revision: D20392604\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 69c17b98d866428c19d683588046169538aaf1e3", "pr_number": "34554", "files_changed": ["test/jit/test_list_dict.py", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["jit", "merged"]}, "9064fafb6e": {"title": "[C++ API] Update torch::nn layer docs (#34522)", "body": "Summary:\nThis PR updates C++ API torch::nn layer docs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34522\n\nTest Plan: Imported from GitHub, without a `Test Plan:` line.\n\nDifferential Revision: D20380832\n\nPulled By: yf225\n\nfbshipit-source-id: ee99a838ec05c6ce2a23aa97555707e507d09958", "pr_number": "34522", "files_changed": [".circleci/scripts/cpp_doc_push_script.sh", "torch/csrc/api/include/torch/nn/modules/activation.h", "torch/csrc/api/include/torch/nn/modules/batchnorm.h", "torch/csrc/api/include/torch/nn/modules/conv.h", "torch/csrc/api/include/torch/nn/modules/distance.h", "torch/csrc/api/include/torch/nn/modules/dropout.h", "torch/csrc/api/include/torch/nn/modules/embedding.h", "torch/csrc/api/include/torch/nn/modules/fold.h", "torch/csrc/api/include/torch/nn/modules/instancenorm.h", "torch/csrc/api/include/torch/nn/modules/linear.h", "torch/csrc/api/include/torch/nn/modules/loss.h", "torch/csrc/api/include/torch/nn/modules/normalization.h", "torch/csrc/api/include/torch/nn/modules/padding.h", "torch/csrc/api/include/torch/nn/modules/pixelshuffle.h", "torch/csrc/api/include/torch/nn/modules/pooling.h", "torch/csrc/api/include/torch/nn/modules/upsampling.h", "torch/csrc/api/include/torch/nn/options/activation.h", "torch/csrc/api/include/torch/nn/options/batchnorm.h", "torch/csrc/api/include/torch/nn/options/conv.h", "torch/csrc/api/include/torch/nn/options/distance.h", "torch/csrc/api/include/torch/nn/options/dropout.h", "torch/csrc/api/include/torch/nn/options/embedding.h", "torch/csrc/api/include/torch/nn/options/fold.h", "torch/csrc/api/include/torch/nn/options/instancenorm.h", "torch/csrc/api/include/torch/nn/options/linear.h", "torch/csrc/api/include/torch/nn/options/loss.h", "torch/csrc/api/include/torch/nn/options/normalization.h", "torch/csrc/api/include/torch/nn/options/padding.h", "torch/csrc/api/include/torch/nn/options/pixelshuffle.h", "torch/csrc/api/include/torch/nn/options/pooling.h", "torch/csrc/api/include/torch/nn/options/upsampling.h"], "labels": ["merged", "module: cpp"]}, "adb8e26182": {"title": "Fix for handling batch size 0. (#34599)", "body": "Summary:\nSeparating this out in a different diff, however since most of the\nxnnpack integration is not tested until the PR https://github.com/pytorch/pytorch/issues/34047, this was not\ncaught till then.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34599\n\nTest Plan: Tested in test/test_xnnpack_integration.py via https://github.com/pytorch/pytorch/issues/34047.\n\nDifferential Revision: D20391000\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 596a3e54445072ab63f700d425d07c7f44586683", "pr_number": "34599", "files_changed": ["aten/src/ATen/native/xnnpack/Common.h", "aten/src/ATen/native/xnnpack/Convolution.cpp"], "labels": ["merged"]}, "86fb522acd": {"title": "Remove cudaMemcpy on full memory overlap (#34548)", "body": "Summary:\nTensorIterator is already checking partial overlap, so there is no trivial UB, but TensorITerator allows full overlap, and it is not a bad idea to skip the memcpy in such case.\n\nfixes: https://github.com/pytorch/pytorch/issues/34525\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34548\n\nDifferential Revision: D20371643\n\nPulled By: ngimel\n\nfbshipit-source-id: ff9e2e872537010afe040204e008b2499af963ad", "pr_number": "34548", "files_changed": ["aten/src/ATen/native/cuda/Copy.cu"], "labels": ["merged", "open source"]}, "0dc0fffca1": {"title": "[net_transform] only skip ConstantFill for autogen_grad (#34628)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34628\n\nDifferential Revision: D20370564\n\nfbshipit-source-id: 854c8ab44ba262e5020383447ed6bb629064ec33", "pr_number": "34628", "files_changed": ["caffe2/python/core.py"], "labels": ["fb-exported", "merged"]}, "2fe7fc681d": {"title": "[PT] add macro to expose caffe2 ops to PyTorch mobile (#34578)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34578\n\nRight now C10_EXPORT_CAFFE2_OP_TO_C10_CPU didn't work on mobile since we disabled some code paths. This diff added a new macro to enable these code paths so we can register caffe2 ops in PT mobile.\n\nTest Plan:\nverified caffe2 ops are registered in PT mobile\n(on the whole stack)\n\n```\n_caffe2::BBoxConcatBatchSplits(Tensor[] input_list, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor output)\n_caffe2::BBoxTransform(Tensor rois, Tensor deltas, Tensor im_info, float[] weights, bool apply_scale, bool rotated, bool angle_bound_on, int angle_bound_lo, int angle_bound_hi, float clip_angle_thresh, bool legacy_plus_one, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor output_0, Tensor output_1)\n_caffe2::BoxWithNMSLimit(Tensor scores, Tensor boxes, Tensor batch_splits, float score_thresh, float nms, int detections_per_im, bool soft_nms_enabled, str soft_nms_method, float soft_nms_sigma, float soft_nms_min_score_thres, bool rotated, bool cls_agnostic_bbox_reg, bool input_boxes_include_bg_cls, bool output_classes_include_bg_cls, bool legacy_plus_one, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor scores, Tensor boxes, Tensor classes, Tensor batch_splits, Tensor keeps, Tensor keeps_size)\n_caffe2::GenerateProposals(Tensor scores, Tensor bbox_deltas, Tensor im_info, Tensor anchors, float spatial_scale, int pre_nms_topN, int post_nms_topN, float nms_thresh, float min_size, bool angle_bound_on, int angle_bound_lo, int angle_bound_hi, float clip_angle_thresh, bool legacy_plus_one, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor output_0, Tensor output_1)\n_caffe2::HeatmapMaxKeypoint(Tensor heatmaps, Tensor bboxes_in, bool should_output_softmax=True, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor keypoints)\n_caffe2::ResizeNearest(Tensor X, str order, float width_scale, float height_scale, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor Y)\n_caffe2::RoIAlign(Tensor features, Tensor rois, str order, float spatial_scale, int pooled_h, int pooled_w, int sampling_ratio, bool aligned, Tensor[]? _caffe2_preallocated_outputs=None) -> (Tensor)\n\nReviewed By: dreiss\n\nDifferential Revision: D20128254\n\nfbshipit-source-id: 49a837dddc431eb528b5c72ffdfe0d0131cd10b4", "pr_number": "34578", "files_changed": ["aten/src/ATen/core/op_registration/op_registration.h", "caffe2/core/export_caffe2_op_to_c10.h", "caffe2/core/operator.cc", "caffe2/core/operator.h", "caffe2/core/tensor.h"], "labels": ["fb-exported", "merged"]}, "b039bca4db": {"title": "Fix typo in data.rst (#34624)", "body": "Summary:\nFix minor typo\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34624\n\nDifferential Revision: D20401946\n\nPulled By: ngimel\n\nfbshipit-source-id: 0c6a7d838aa15120b3ecb8b9ba4b57550c9bcd32", "pr_number": "34624", "files_changed": ["docs/source/data.rst"], "labels": ["merged", "open source"]}, "cf8b728255": {"title": "Delete OperatorOptions, absorb AliasAnalysisKind into FunctionSchema. (#34588)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34588\n\nI constructed the patch by deleting OperatorOptions and then rerouting\nall queries for AliasAnalysisKind to FunctionSchema.  Some of the\nbehavior is kind of bogus: we really shouldn't be mutating FunctionSchema\nafter the fact, but that won't get fixed until we actually switch to\ntrue schema merging.\n\nReland of https://github.com/pytorch/pytorch/pull/34160\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20387079\n\nPulled By: ezyang\n\nfbshipit-source-id: d189f7a6ad8cd186b88b6fbfa3f189994eea14e8", "pr_number": "34588", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/dispatch/OperatorOptions.h", "aten/src/ATen/core/function_schema.h", "aten/src/ATen/core/op_registration/op_registration.cpp", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_base.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/test_schema_matching.cpp", "tools/jit/templates/register_aten_ops.cpp", "torch/csrc/jit/codegen/fuser/fallback.cpp", "torch/csrc/jit/ir/constants.cpp", "torch/csrc/jit/passes/batch_mm.cpp", "torch/csrc/jit/passes/decompose_ops.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/python/python_interpreter.cpp", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/operator.h", "torch/csrc/jit/runtime/register_distributed_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_special_ops.cpp", "torch/csrc/jit/runtime/register_string_ops.cpp"], "labels": ["jit", "merged"]}, "c235be42dd": {"title": "[jit] kill script namespace (#34515)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34515\n\nOnce upon a time we thought this was necessary. In reality it is not, so\nremoving it.\n\nFor backcompat, our public interface (defined in `api/`) still has\ntypedefs to the old `script::` names.\n\nThere was only one collision: `Pass` as a `Stmt` and `Pass` as a graph\ntransform. I renamed one of them.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20353503\n\nPulled By: suo\n\nfbshipit-source-id: 48bb911ce75120a8c9e0c6fb65262ef775dfba93", "pr_number": "34515", "files_changed": ["android/pytorch_android/generate_test_asset.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/src/main/java/org/pytorch/Module.java", "aten/src/ATen/core/function.h", "aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/jit_type.h", "binaries/dump_operator_names.cc", "caffe2/contrib/pytorch/script_module_op.cc", "caffe2/python/pybind_state.cc", "docs/source/jit.rst", "docs/source/notes/large_scale_deployments.rst", "docs/source/org/pytorch/Module.rst", "ios/TestApp/TestAppTests/TestAppTests.mm", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_autodiff.cpp", "test/cpp/jit/test_class_import.cpp", "test/cpp/jit/test_class_parser.cpp", "test/cpp/jit/test_class_type.cpp", "test/cpp/jit/test_constant_pooling.cpp", "test/cpp/jit/test_custom_operators.cpp", "test/cpp/jit/test_dce.cpp", "test/cpp/jit/test_fuser.cpp", "test/cpp/jit/test_inliner.cpp", "test/cpp/jit/test_interface.cpp", "test/cpp/jit/test_ir.cpp", "test/cpp/jit/test_irparser.cpp", "test/cpp/jit/test_ivalue.cpp", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/test_module_api.cpp", "test/cpp/jit/test_peephole_optimize.cpp", "test/cpp/jit/test_save_load.cpp", "test/cpp/jit/test_schema_matching.cpp", "test/cpp/jit/test_subgraph_matcher.cpp", "test/cpp/jit/test_subgraph_rewriter.cpp", "test/cpp/jit/test_utils.cpp", "test/custom_operator/test_custom_ops.cpp", "test/mobile/custom_build/predictor.cpp", "test/test_cpp_api_parity.py", "torch/csrc/api/include/torch/jit.h", "torch/csrc/api/include/torch/serialize.h", "torch/csrc/api/include/torch/serialize/input-archive.h", "torch/csrc/api/include/torch/serialize/output-archive.h", "torch/csrc/api/src/jit.cpp", "torch/csrc/api/src/serialize/input-archive.cpp", "torch/csrc/api/src/serialize/output-archive.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/jit/api/compilation_unit.h", "torch/csrc/jit/api/method.h", "torch/csrc/jit/api/module.cpp", "torch/csrc/jit/api/module.h", "torch/csrc/jit/api/module_save.cpp", "torch/csrc/jit/api/object.cpp", "torch/csrc/jit/api/object.h", "torch/csrc/jit/docs/serialization.md", "torch/csrc/jit/frontend/builtin_functions.cpp", "torch/csrc/jit/frontend/builtin_functions.h", "torch/csrc/jit/frontend/concrete_module_type.cpp", "torch/csrc/jit/frontend/concrete_module_type.h", "torch/csrc/jit/frontend/convert_to_ssa.cpp", "torch/csrc/jit/frontend/convert_to_ssa.h", "torch/csrc/jit/frontend/edit_distance.cpp", "torch/csrc/jit/frontend/edit_distance.h", "torch/csrc/jit/frontend/error_report.cpp", "torch/csrc/jit/frontend/error_report.h", "torch/csrc/jit/frontend/function_schema_parser.cpp", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/frontend/ir_emitter.h", "torch/csrc/jit/frontend/lexer.cpp", "torch/csrc/jit/frontend/lexer.h", "torch/csrc/jit/frontend/mini_environment.h", "torch/csrc/jit/frontend/parse_string_literal.h", "torch/csrc/jit/frontend/parser.cpp", "torch/csrc/jit/frontend/parser.h", "torch/csrc/jit/frontend/parser_constants.h", "torch/csrc/jit/frontend/resolver.h", "torch/csrc/jit/frontend/schema_matching.cpp", "torch/csrc/jit/frontend/schema_matching.h", "torch/csrc/jit/frontend/schema_type_parser.cpp", "torch/csrc/jit/frontend/schema_type_parser.h", "torch/csrc/jit/frontend/script_type_parser.cpp", "torch/csrc/jit/frontend/script_type_parser.h", "torch/csrc/jit/frontend/string_to_type.cpp", "torch/csrc/jit/frontend/strtod.cpp", "torch/csrc/jit/frontend/strtod.h", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/frontend/sugared_value.h", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/frontend/tracer.h", "torch/csrc/jit/frontend/tree.h", "torch/csrc/jit/frontend/tree_views.h", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/ir/ir.h", "torch/csrc/jit/ir/irparser.cpp", "torch/csrc/jit/ir/irparser.h", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/type_parser.cpp", "torch/csrc/jit/passes/decompose_ops.cpp", "torch/csrc/jit/passes/fixup_trace_scope_blocks.cpp", "torch/csrc/jit/passes/fixup_trace_scope_blocks.h", "torch/csrc/jit/passes/freeze_module.cpp", "torch/csrc/jit/passes/freeze_module.h", "torch/csrc/jit/passes/inline_forked_closures.cpp", "torch/csrc/jit/passes/inline_forked_closures.h", "torch/csrc/jit/passes/lift_closures.cpp", "torch/csrc/jit/passes/lift_closures.h", "torch/csrc/jit/passes/lower_graph.cpp", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp", "torch/csrc/jit/passes/pass_manager.cpp", "torch/csrc/jit/passes/pass_manager.h", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization.h", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/passes/subgraph_rewrite.cpp", "torch/csrc/jit/passes/subgraph_rewrite.h", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/python/module_python.h", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/python/python_custom_class.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/python/python_sugared_value.h", "torch/csrc/jit/python/python_tracer.cpp", "torch/csrc/jit/python/python_tracer.h", "torch/csrc/jit/python/python_tree_views.cpp", "torch/csrc/jit/python/python_tree_views.h", "torch/csrc/jit/python/script_init.cpp", "torch/csrc/jit/python/script_init.h", "torch/csrc/jit/runtime/operator.cpp", "torch/csrc/jit/runtime/register_distributed_ops.cpp", "torch/csrc/jit/runtime/symbolic_script.cpp", "torch/csrc/jit/serialization/export.cpp", "torch/csrc/jit/serialization/export.h", "torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/import.cpp", "torch/csrc/jit/serialization/import.h", "torch/csrc/jit/serialization/import_legacy.cpp", "torch/csrc/jit/serialization/import_legacy.h", "torch/csrc/jit/serialization/import_source.cpp", "torch/csrc/jit/serialization/import_source.h", "torch/csrc/jit/serialization/python_print.cpp", "torch/csrc/jit/serialization/python_print.h", "torch/csrc/jit/testing/hooks_for_testing.cpp", "torch/csrc/jit/testing/hooks_for_testing.h", "torch/csrc/utils/init.cpp", "torch/csrc/utils/throughput_benchmark.cpp", "torch/csrc/utils/throughput_benchmark.h", "torch/custom_class.h", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "f70945b1c3": {"title": "fix the quantized batchnorm2d (#34579)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34579\n\nDifferential Revision: D20382783\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: dadfc4974cb4c808f1eedf8cc4ec52ec8d3ea1b0", "pr_number": "34579", "files_changed": ["torch/nn/quantized/modules/batchnorm.py"], "labels": ["merged"]}, "3c76b2aeea": {"title": "Replace THPLayout with at::Layout in Python Argument Parser (#34543) (#34584)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34584\n\nTest Plan:\n```\npython setup.py develop\npython test/test_torch.py\n```\nOutput:\n```\n...\nRan 3834 tests in 198.825s\n\nOK (skipped=180)\n```\n\nImported from OSS\n\nDifferential Revision: D20403330\n\nfbshipit-source-id: 41474d5e7001db070f98ac8379f909f0ac74deb6", "pr_number": "34584", "files_changed": ["tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_torch_functions.cpp", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": ["merged"]}, "a22008f91e": {"title": "Prohibit copying autograd engines (#34567)", "body": "Summary:\nMake sure that there could not be more than one instance of either `torch::autograd::Engine` or `torch::autograd::python::PythonEngine`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34567\n\nTest Plan: CI\n\nDifferential Revision: D20390622\n\nPulled By: malfet\n\nfbshipit-source-id: c90595032afc88f552dee52901361b58b282dc1a", "pr_number": "34567", "files_changed": ["torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_engine.h"], "labels": ["merged"]}, "962e362427": {"title": "Fix _cat operator (#34591)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34591\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20388000\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 8ae7593dbddc1a96a03193a99afc9a4ce46203ad", "pr_number": "34591", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["merged"]}, "514cba0661": {"title": "[JIT] remove builtin interpolate functions (#34514)", "body": "Summary:\n`torch.nn.functional.interpolate` was written as a builtin op when we scripted the standard library, because it has four possible overloads. As a result, whenever we make a change to `interpolate`, we need to make changes in two places, and it also makes it impossible to optimize the interpolate op. The builtin is tech debt.\n\nI talked with ailzhang, and the symbolic script changes are good to remove (i guess that makes a third place we needed to re-implement interpolate).\n\nI'm trying to get rid of unneccessary builtin operators because we're standardizing mobile bytecode soon, so we should try to get this landed as soon as possible.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34514\n\nDifferential Revision: D20391089\n\nPulled By: eellison\n\nfbshipit-source-id: abc84cdecfac67332bcba6b308fca4db44303121", "pr_number": "34514", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/symbolic_script.cpp", "torch/jit/_builtins.py", "torch/nn/functional.py"], "labels": ["jit", "merged"]}, "43c9cc7a9c": {"title": "add quantized ELU activation (#34267)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34267\n\nAdds quantized ELU.\n\nTest Plan:\n```\npython test/test_quantized.py TestQuantizedOps.test_qelu\n```\n\nstill need to benchmark, saving that for after the review comments\n\nImported from OSS\n\nDifferential Revision: D20370953\n\nfbshipit-source-id: fe941bf966f72dd9eee2c4b2ef45fe7afb50c866", "pr_number": "34267", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qelu.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "benchmarks/operator_benchmark/pt/qactivation_test.py", "test/test_quantized.py", "torch/nn/quantized/functional.py"], "labels": ["merged", "quantization"]}, "157d2d7825": {"title": "Fix version check for grad_fn for views (#34145)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34145\n\nThis fix the following behavior:\n```python\nimport torch\n\nclass MyFn(torch.autograd.Function):\n    staticmethod\n    def forward(ctx, inp, inplace):\n        view = inp.clone()[:3]\n        if inplace:\n            view += 2\n        return view\n\n    staticmethod\n    def backward(ctx, grad):\n        return grad, None\n\nbase = torch.rand(10, requires_grad=True)\nfoo = MyFn.apply(base, False)\n\nprint(foo.grad_fn)\n# <torch.autograd.function.MyFnBackward object at 0x7f5fd28c4d18>\n\nfoo = MyFn.apply(base, True)\n\nprint(foo.grad_fn)\n# <AsStridedBackward object at 0x7f601c0c3cf0>\n```\n\nWhere both should be printing `MyFnBackward`.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20229907\n\nPulled By: albanD\n\nfbshipit-source-id: 5ebd315d459023017d51760c5bafe43acd5fc3e2", "pr_number": "34145", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/variable.cpp"], "labels": ["merged"]}, "e95657b87e": {"title": "[C++ API] AdaptiveLogSoftmaxWithLoss (#29076)", "body": "Summary:\nImplemented AdaptiveLogSoftmaxWithLoss and some tests for modules. Reference https://github.com/pytorch/pytorch/issues/25883\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29076\n\nDifferential Revision: D20404588\n\nPulled By: yf225\n\nfbshipit-source-id: edbadf432b8173cbcc6caf83c9c03dd92dc31a37", "pr_number": "29076", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/api/modules.cpp", "test/cpp_api_parity/parity-tracker.md", "tools/build_variables.bzl", "torch/csrc/api/include/torch/nn/modules.h", "torch/csrc/api/include/torch/nn/modules/adaptive.h", "torch/csrc/api/include/torch/nn/modules/container/sequential.h", "torch/csrc/api/include/torch/nn/options/adaptive.h", "torch/csrc/api/src/nn/modules/adaptive.cpp", "torch/csrc/api/src/nn/options/adaptive.cpp"], "labels": ["merged", "module: cpp", "open source", "triaged"]}, "a54416d208": {"title": "[C++ API] Remove deprecated torch::nn::BatchNorm / FeatureDropout / modules_ordered_dict and torch::nn::init::Nonlinearity / FanMode (#34508)", "body": "Summary:\nThis PR is BC-breaking in the following way:\n- The deprecated `torch::nn::BatchNorm` is removed in favor of `torch::nn::BatchNorm{1,2,3}d`\n- The deprecated `torch::nn::FeatureDropout` is removed in favor of `torch::nn::Dropout{2,3}d`\n- The deprecated `torch::nn::modules_ordered_dict` is removed. User should do `Sequential sequential({{\"m1\", MyModule(1)}, {\"m2\", MyModule(2)}})` instead.\n- The deprecated `torch::nn::init::Nonlinearity` is removed, in favor of the following enums:\n    - `torch::kLinear`\n    - `torch::kConv1D`\n    - `torch::kConv2D`\n    - `torch::kConv3D`\n    - `torch::kConvTranspose1D`\n    - `torch::kConvTranspose2D`\n    - `torch::kConvTranspose3D`\n    - `torch::kSigmoid`\n    - `torch::kTanh`\n    - `torch::kReLU`\n    - `torch::kLeakyReLU`\n- The deprecated `torch::nn::init::FanMode` is removed, in favor of the following enums:\n    - `torch::kFanIn`\n    - `torch::kFanOut`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34508\n\nDifferential Revision: D20351601\n\nPulled By: yf225\n\nfbshipit-source-id: cca0cd112f29a31bb023e348ca8f82780e42bea3", "pr_number": "34508", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/api/init.cpp", "test/cpp/api/integration.cpp", "test/cpp/api/modulelist.cpp", "test/cpp/api/modules.cpp", "test/cpp/api/sequential.cpp", "tools/build_variables.bzl", "torch/csrc/api/include/torch/nn/init.h", "torch/csrc/api/include/torch/nn/modules/batchnorm.h", "torch/csrc/api/include/torch/nn/modules/container/functional.h", "torch/csrc/api/include/torch/nn/modules/container/modulelist.h", "torch/csrc/api/include/torch/nn/modules/container/named_any.h", "torch/csrc/api/include/torch/nn/modules/container/sequential.h", "torch/csrc/api/include/torch/nn/modules/dropout.h", "torch/csrc/api/include/torch/nn/options/dropout.h", "torch/csrc/api/src/nn/init.cpp", "torch/csrc/api/src/nn/modules/batchnorm.cpp", "torch/csrc/api/src/nn/modules/container/named_any.cpp", "torch/csrc/api/src/nn/modules/dropout.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "9fd08b9c37": {"title": "Get rid of newWithSize. (#34387)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34387\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20311058\n\nPulled By: gchanan\n\nfbshipit-source-id: b62653fd31a181d06aa73cda68abe75614cea0a9", "pr_number": "34387", "files_changed": ["aten/src/TH/generic/THTensor.cpp", "aten/src/THC/generic/THCTensor.cpp"], "labels": ["merged"]}, "518e9f94c2": {"title": "Kill newWithStorage. (#34388)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34388\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20311059\n\nPulled By: gchanan\n\nfbshipit-source-id: 4619a99c7bea76b54b7938b798eedc5bc2983dd5", "pr_number": "34388", "files_changed": ["aten/src/TH/generic/THTensor.cpp", "aten/src/THC/generic/THCTensor.cpp", "aten/src/THC/generic/THCTensor.hpp"], "labels": ["merged"]}, "dd313f314e": {"title": "Stop creating unnecessary Storage with newWithStorage1d. (#34389)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34389\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20311060\n\nPulled By: gchanan\n\nfbshipit-source-id: 6d681e0a78e3ea3982d11cfd2eedca843f48302a", "pr_number": "34389", "files_changed": ["aten/src/TH/generic/THTensor.cpp", "aten/src/THC/generic/THCTensor.cpp"], "labels": ["merged"]}, "4e07c35679": {"title": "Delete all user forks tracked in RRefContext before graceful shutting down (#31893)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31893\n\nIn order to resolve the issue summarized in https://github.com/pytorch/pytorch/issues/31325.\n\nThe overal solution is to proactively send out delete fork messages from user nodes, before user nodes detecting rref leaks.\n\nAs the first step, we want to have a weak ref tracker to track all user rrefs.\nghstack-source-id: 100023142\n\nTest Plan:\nV22 is the version that make User to wait on delete UseerRRef message.\n\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork\n\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork -- test_nested_rref_stress --stress-runs 100\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/rpc_fork\\#binary.par -r test_nested_rref_stress\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/rpc_fork\\#binary.par - r test_rref_forward_chain\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/rpc_fork\\#binary.par -r test_non_garbage_collected_user_rref_due_to_local_circular_dependency\n```\n\nReviewed By: mrshenli\n\nDifferential Revision: D19292254\n\nfbshipit-source-id: 92c3e8d0b00f183c5e22f163bdca482cc25a1ce9", "pr_number": "31893", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/distributed/rpc/api.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "3f1ba3c465": {"title": "Redo of \"Add API for listing functions overridable by __torch_function__\" (#34240)", "body": "Summary:\nThis is a redo of https://github.com/pytorch/pytorch/pull/33791, which was reverted because it introduced a flaky test. The test was flaky and only flaky on Python3.5 because of dict order randomization.\n\nI've fixed the issue with tests clobbering each other in b539fec and removed the override tests for `torch.nn.functional.tanh` and `torch.nn.functional.sigmoid`, which are deprecated and shouldn't be overridable in e0d7402. I also verified that no more test clobbering is happening.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34240\n\nDifferential Revision: D20252442\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 069568e342a41c90e1dc76cbf85ba4aed47f24be", "pr_number": "34240", "files_changed": ["docs/source/notes/extending.rst", "test/test_overrides.py", "torch/_overrides.py"], "labels": ["merged", "open source"]}, "f9f8424386": {"title": "[JIT] remove specialized list ops (#34520)", "body": "Summary:\nNow that lists are no longer specialized, we can register only one operator for list ops that are generic to their element type.\nThis PR reorgs lists into three sets of ops:\n- CREATE_GENERIC_LIST_OPS\n- CREATE_SPECIALIZED_LIST_OPS\n- CREATE_COMPARATOR_LIST_OPS_SPECIALIZED (we didn't bind certain specialized ops to Tensor)\n\nThis is important to land quickly because mobile is finalizing its bytecode soon, after which we could not remove these ops.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34520\n\nDifferential Revision: D20368543\n\nPulled By: eellison\n\nfbshipit-source-id: ad0c6d70d2a6be6ff0e948d6786052167fc43e27", "pr_number": "34520", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/test_lite_interpreter.cpp", "torch/csrc/jit/passes/canonicalize_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["jit", "merged"]}, "d81d65b2f7": {"title": "Add entry for distributed tests to CODEOWNERS. (#34637)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34637\n\nghstack-source-id: 100003837\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D20404552\n\nfbshipit-source-id: a7f35beb8b78ad25e5cd000cd940dd7e94cc65de", "pr_number": "34637", "files_changed": ["CODEOWNERS"], "labels": ["merged"]}, "1e6c47413a": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/proxygen/commit/87f3feae5a8aa9ec9cb17327a53fde69b9fe8e6c\nhttps://github.com/pytorch/fbgemm/commit/cd6c8897f5680b5e936c715b09b9f1b37bb9ec6a\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: 0c961541c715da74ae417ad25bf29f48e74e45d1", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "2f32b92763": {"title": "[ROCm] Enable BFloat16 type for EmbeddingBag ops et al (#34630)", "body": "Summary:\nThis PR enables bfloat16 type for\n\n- Embedding, Index, Sigmoid Ops used in [DLRM](https://github.com/facebookresearch/dlrm)\n- Miscellaneous ops like comparison ops, arange op used in unit tests\n- Rename types list with the pattern `*_with_bfloat16` in `test_torch.py` to avoid confusion\n\niotamudelta ezyang\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34630\n\nDifferential Revision: D20405093\n\nPulled By: ezyang\n\nfbshipit-source-id: aa9538acf81b3a5a9a46ce5014529707fdf25687", "pr_number": "34630", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu", "aten/src/ATen/native/cuda/CompareEQKernel.cu", "aten/src/ATen/native/cuda/CompareGEKernel.cu", "aten/src/ATen/native/cuda/CompareGTKernel.cu", "aten/src/ATen/native/cuda/CompareLEKernel.cu", "aten/src/ATen/native/cuda/CompareLTKernel.cu", "aten/src/ATen/native/cuda/CompareNEKernel.cu", "aten/src/ATen/native/cuda/Embedding.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/RangeFactories.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "aten/src/THC/THCTensorIndex.cu", "aten/src/THC/THCTensorMath.h", "aten/src/THC/generic/THCTensorIndex.cu", "test/test_nn.py", "test/test_torch.py"], "labels": ["merged", "open source"]}, "fff6fe83a7": {"title": "[pytorch-rpc] WireSerializer should check has_storage() (#34626)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34626\n\nWe need to check has_storage() before looking at it in\ncloneSparseTensors(), to avoid gratuitously throwing.\n\nIdeally, we'd add a test for this (I wrote one up but had to disable it),\nbut won't work until JIT Pickler supports sparse tensors.\nghstack-source-id: 100018077\n\nTest Plan: buck test mode/dev-nosan caffe2/torch/fb/distributed/thriftRpcAgent/...\n\nDifferential Revision: D20399971\n\nfbshipit-source-id: 5debfa8140eb1f949d37336330223962cc320abc", "pr_number": "34626", "files_changed": ["test/cpp/rpc/test_wire_serialization.cpp", "torch/csrc/distributed/rpc/utils.cpp"], "labels": ["merged"]}, "352e9b11e0": {"title": "Attempt to resolve inconsistent dll linkage warnings on MSVC (#34639)", "body": "Summary:\nContinue the work in https://github.com/pytorch/pytorch/pull/19242.\nRemove the template declarations that implies different dll linkage.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34639\n\nDifferential Revision: D20419400\n\nPulled By: ezyang\n\nfbshipit-source-id: 5c7c30f0a4c3ba555589629f352ddb1c006c0c54", "pr_number": "34639", "files_changed": ["torch/csrc/jit/frontend/tracer.h"], "labels": ["jit", "merged", "open source"]}, "944ea4c334": {"title": "ONNX Export Support for CrossEntropyLoss (#33767)", "body": "Summary:\nAdd ONNX export support for torch.nn.CrossEntropyLoss.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33767\n\nReviewed By: hl475\n\nDifferential Revision: D20121169\n\nPulled By: houseroad\n\nfbshipit-source-id: 7b56617e8c60617b922949fc8b4ecc626eedf7ed", "pr_number": "33767", "files_changed": ["aten/src/ATen/core/interned_strings.h", "caffe2/python/onnx/tests/onnx_backend_test.py", "test/onnx/expect/TestOperators.test_softmaxcrossentropy.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_ignore_index.expect", "test/onnx/test_operators.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "third_party/onnx", "torch/csrc/jit/passes/onnx/peephole.cpp"], "labels": ["merged", "open source", "triaged"]}, "a74fbea345": {"title": "Continuous bernoulli distribution (take 2) (#34619)", "body": "Summary:\nWe recently had a NeurIPS paper (https://arxiv.org/abs/1907.06845 and https://papers.nips.cc/paper/9484-the-continuous-bernoulli-fixing-a-pervasive-error-in-variational-autoencoders) where we introduce a new [0,1]-supported distribution: the continuous Bernoulli. This pull request implements this distribution in pytorch.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34619\n\nDifferential Revision: D20403123\n\nPulled By: ngimel\n\nfbshipit-source-id: d807c7d0d372c6daf6cb6ef09df178bc7491abb2", "pr_number": "34619", "files_changed": ["docs/source/distributions.rst", "test/test_distributions.py", "torch/distributions/__init__.py", "torch/distributions/continuous_bernoulli.py", "torch/distributions/kl.py"], "labels": ["merged", "module: distributions", "open source"]}, "cb06cb7b9f": {"title": "Remove hotpatches that circumvent MAGMA bug (#34357)", "body": "Summary:\nChangelog:\n- The magma implementation of small singular square batch matrices had a bug that resulted in nan values in the LU factorization result. This has been fixed in MAGMA 2.5.2. This PR removes the existing patch that was a temporary workaround for this bug.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34357\n\nTest Plan: - Existing tests for det and lu should pass\n\nDifferential Revision: D20422879\n\nPulled By: seemethere\n\nfbshipit-source-id: 8dd7a30b5c31fc5b844e0a11965efd46067e936a", "pr_number": "34357", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu"], "labels": ["merged", "open source"]}, "31cd893899": {"title": "remove some TH dead code (#34644)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34644\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20423063\n\nPulled By: ngimel\n\nfbshipit-source-id: 2783345ea9b3ed65e51a7d0e17cfa29f2c12cc43", "pr_number": "34644", "files_changed": ["aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/TH/generic/THVectorDefault.cpp"], "labels": ["merged", "open source"]}, "9e6cd98c3f": {"title": "Ensure torch_cuda is linked against on Windows (#34288)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31611.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34288\n\nDifferential Revision: D20314251\n\nPulled By: seemethere\n\nfbshipit-source-id: 15ab2d4de665d553a1622a2d366148697deb6c02", "pr_number": "34288", "files_changed": ["aten/src/ATen/cuda/CUDAContext.cpp", "caffe2/CMakeLists.txt", "torch/utils/cpp_extension.py"], "labels": ["merge-this-please", "merged", "open source"]}, "fe9b4e3cba": {"title": "[DPER3] Blob Reorder (#33579)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33579\n\nDifferential Revision: D20008865\n\nfbshipit-source-id: f35aded311d9d1d7d438d828ccabd2bab5575e5c", "pr_number": "33579", "files_changed": ["caffe2/python/predictor/predictor_py_utils.py"], "labels": ["fb-exported", "merged"]}, "8c332ff84f": {"title": "[JIT] EliminateDeadCode shouldn't remove custom operator node that has untracked mutation (#34635)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34635\n\nFor custom op, it's removed in EliminateDeadCode IR optimization step, causing wrong training result.\n\nEliminateDeadCode decides to remove it, because it has no output, so output is used. Also, it has no side effect, and has no untracked mutation, which is not true, custom op can have untracked mutation.\n\nThe if statement here only allows aten and prim operator to have untracked mutation, which should be removed.\nghstack-source-id: 100001319\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/torch/fb/distributed/pytorch/tests:test_jit\n\nbuck build mode/dev-nosan //caffe2/torch/fb/distributed/pytorch/tests:test_jit \\\n&& buck-out/gen/caffe2/torch/fb/distributed/pytorch/tests/test_jit\\#binary.par -r test_use_dense_adagrad_step\n```\n\nReviewed By: wanchaol\n\nDifferential Revision: D7440221\n\nfbshipit-source-id: e424417ab397d90075884c7050c59dfc5c84cf77", "pr_number": "34635", "files_changed": ["torch/csrc/jit/passes/dead_code_elimination.cpp"], "labels": ["jit", "merged"]}, "787c307e63": {"title": "Revert D20368543: [pytorch][PR] [JIT] remove specialized list ops", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20368543\n\nOriginal commit changeset: ad0c6d70d2a6\n\nfbshipit-source-id: b8b1a64ac830d5f544567714b940c57274194d3f", "pr_number": null, "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/test_lite_interpreter.cpp", "torch/csrc/jit/passes/canonicalize_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": []}, "721bd11cc3": {"title": "[caffe2] Refactor out common util functions from tvm_transformer (#34652)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34652\n\nSplit from D20006007 because it needs to synced to open source and also for easy testing & landing.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/fb/tvm:test_tvm_transform\n```\nCI\n\nReviewed By: yinghai\n\nDifferential Revision: D20414037\n\nfbshipit-source-id: 6e17dd9f8cffe87bc59c6e3cc6fd1f8d8def926b", "pr_number": "34652", "files_changed": ["caffe2/opt/backend_transformer_base.h", "caffe2/opt/tvm_transformer.cc", "caffe2/opt/tvm_transformer.h"], "labels": ["fb-exported", "merged"]}, "9f05fc9322": {"title": "[Aten] First argument of check_names_valid_for() should be an unsigned value (#34158)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34158\n\nTest Plan: CI\n\nReviewed By: EscapeZero\n\nDifferential Revision: D20232089\n\nfbshipit-source-id: d74b5e36a139998e6967b7b6339001c49d9d58e8", "pr_number": "34158", "files_changed": ["aten/src/ATen/core/NamedTensor.cpp", "aten/src/ATen/core/NamedTensor.h"], "labels": ["fb-exported", "merged"]}, "90ca7a1feb": {"title": "[quant][graphmode] Add Finalize function that inlines graph and produce quantized ops (#33927)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33927\n\nTest Plan:\ntest will be added in later PRs\n\nImported from OSS\n\nDifferential Revision: D20354879\n\nfbshipit-source-id: 03976f4b86c46dbdc4e45764a1e72f1a3855a404", "pr_number": "33927", "files_changed": ["test/test_jit.py", "test/test_quantization.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization.h", "torch/csrc/jit/python/init.cpp", "torch/quantization/_quantize_script.py"], "labels": ["jit", "merged"]}, "52787388d2": {"title": "[tools] Add clang_format_new.py to download, verify and run clang-format binary (#34566)", "body": "Summary:\n**Summary**\nThis commit adds `tools/clang_format_new.py`, which downloads a platform-appropriate\nclang-format binary to a `.gitignored` location, verifies the binary by comparing its\nSHA1 hash to a reference hash (also included in this commit), and runs it on all files\nmatched a specific regex in a list of whitelisted subdirectories of pytorch.\n\nThis script will eventually replace `tools/clang_format.py`.\n\n**Testing**\nRan the script.\n\n*No Args*\n```\npytorch > ./tools/clang_format.py\nDownloading clang-format to /Users/<user>/Desktop/pytorch/.clang-format-bin\n0% |################################################################| 100%\nUsing clang-format located at /Users/<user>/Desktop/pytorch/.clang-format-bin/clang-format\n> echo $?\n0\n> git status\n<bunch of files>\n```\n\n`--diff` *mode*\n```\n> ./tools/clang_format.py --diff\nUsing clang-format located at /Users/<user>/Desktop/pytorch/.clang-format-bin/clang-format\nSome files are not formatted correctly\n> echo $?\n1\n\n<format files using the script>\n\n> ./tools/clang_format.py --diff\nUsing clang-format located at /Users/<user>/Desktop/pytorch/.clang-format-bin/clang-format\nAll files are formatted correctly\n> echo $?\n0\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34566\n\nDifferential Revision: D20431290\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 3966f769cfb923e58ead9376d85e97127415bdc6", "pr_number": "34566", "files_changed": [".gitignore", "scripts/get_clang_format.py", "tools/clang_format_hash/mac/clang-format-mojave", "tools/clang_format_new.py"], "labels": ["merged"]}, "c78eacb5ee": {"title": "scripts: Add promotion script for s3 to pypi (#34500)", "body": "Summary:\nIs reliant on scripts for promotion from s3 to s3 to have already run.\n\nA continuation of the work done in https://github.com/pytorch/pytorch/issues/34274\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34500\n\nTest Plan: yeah_sandcastle\n\nDifferential Revision: D20389101\n\nPulled By: seemethere\n\nfbshipit-source-id: 5e5b554cff964630c5414d48be35f14ba6894021", "pr_number": "34500", "files_changed": ["scripts/release/promote/wheel_to_pypi.sh"], "labels": ["merged", "releng"]}, "44256199a9": {"title": "[JIT] remove specialized list ops (#34520)", "body": "Summary:\nNow that lists are no longer specialized, we can register only one operator for list ops that are generic to their element type.\nThis PR reorgs lists into three sets of ops:\n- CREATE_GENERIC_LIST_OPS\n- CREATE_SPECIALIZED_LIST_OPS\n- CREATE_COMPARATOR_LIST_OPS_SPECIALIZED (we didn't bind certain specialized ops to Tensor)\n\nThis is important to land quickly because mobile is finalizing its bytecode soon, after which we could not remove these ops.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34520\n\nReviewed By: iseeyuan\n\nDifferential Revision: D20429775\n\nPulled By: eellison\n\nfbshipit-source-id: ae6519f9b0f731eaa2bf4ac20736317d0a66b8a0", "pr_number": "34520", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/test_lite_interpreter.cpp", "torch/csrc/jit/passes/canonicalize_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/nn/functional.py"], "labels": ["jit", "merged"]}, "0ff4d37933": {"title": "[quant][graphmode] Add quantized conv2d-relu fusion pattern (#33279)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33279\n\natt\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D20354878\n\nfbshipit-source-id: 2b19797d4b3fd96918164a58bfbd768211ad6c6d", "pr_number": "33279", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization_patterns.h"], "labels": ["jit", "merged"]}, "4ae74b3b25": {"title": "[DPER3][Shape Inference] Initial Shape Inference in DPER3 frontend (#33607)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33607\n\nDifferential Revision: D20025048\n\nfbshipit-source-id: 8b3a3bcfeb450de4d38c555bf2bb116ddedad3ec", "pr_number": "33607", "files_changed": ["caffe2/proto/metanet.proto"], "labels": ["fb-exported", "merged"]}, "d5f8c8f3ba": {"title": "Revert D20121169: [pytorch][PR] ONNX Export Support for CrossEntropyLoss", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20121169\n\nOriginal commit changeset: 7b56617e8c60\n\nfbshipit-source-id: d7f302d1e54f3c978c3be0a0ad1ee600790a5b27", "pr_number": null, "files_changed": ["aten/src/ATen/core/interned_strings.h", "caffe2/python/onnx/tests/onnx_backend_test.py", "test/onnx/expect/TestOperators.test_softmaxcrossentropy.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_ignore_index.expect", "test/onnx/test_operators.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "third_party/onnx", "torch/csrc/jit/passes/onnx/peephole.cpp"], "labels": []}, "5d65b5cd01": {"title": "Add the 3d upsample quantized op for video model (#34594)", "body": "Summary:\nas title, we are currently missing this 3d op, which is required for video related model.\n\nPerformance benchmark:\n```\nimport torch, time\n\nfor dtype in [torch.qint8, torch.quint8, torch.qint32]:\n    print('****', str(dtype), '*****')\n    x = torch.rand(1, 56, 64, 56, 256)\n\n    q_x = torch.quantize_per_tensor(x, 0.5, 1, dtype)\n    q_x = q_x.permute([0, 4, 1, 2, 3])\n\n    x = x.permute([0, 4, 1, 2, 3])\n\n    NITER = 100\n\n    s = time.time()\n    for i in range(NITER):\n        float_out = torch.nn.functional.interpolate(x, size=30, scale_factor=None, mode=\"nearest\", align_corners=None)\n    time_per_iter_float = (time.time() - s) / NITER\n\n    s = time.time()\n    for i in range(NITER):\n        quant_out = torch.nn.functional.interpolate(q_x, size=30, scale_factor=None, mode=\"nearest\", align_corners=None)\n    time_per_iter_quant = (time.time() - s) / NITER\n\n    ref_quantized = torch.quantize_per_tensor(float_out, 0.5, 1, dtype)\n    torch.testing.assert_allclose(ref_quantized.dequantize(), quant_out.dequantize())\n\n    print('time/iter ms (float)', 'time/iter ms (quant)', 'quant/float', sep='\\t')\n    print(time_per_iter_float * 1000, time_per_iter_quant * 1000, time_per_iter_quant / time_per_iter_float, sep='\\t')\n\n    bytes_float = (x.numel() + float_out.numel()) * x.element_size()\n    bytes_quant = (q_x.numel() + quant_out.numel()) * q_x.element_size()\n\n    float_bw_gbps = bytes_float / time_per_iter_float / 1e9\n    quant_bw_gbps = bytes_quant / time_per_iter_quant / 1e9\n\n    print('GB/s float', 'GB/s quant', sep='\\t')\n    print(float_bw_gbps, quant_bw_gbps, sep='\\t')\n```\n\n```\n**** torch.qint8 *****\ntime/iter ms (float)  time/iter ms (quant)  quant/float\n1136.8209528923035  1.294245719909668 0.0011384780660638283\nGB/s float  GB/s quant\n0.20510608588517917 45.03953391792442\n**** torch.quint8 *****\ntime/iter ms (float)  time/iter ms (quant)  quant/float\n827.9890131950378 1.11464262008667  0.0013462046021426\nGB/s float  GB/s quant\n0.28160868355034036 52.29678369508914\n**** torch.qint32 *****\ntime/iter ms (float)  time/iter ms (quant)  quant/float\n834.6958303451538 7.481417655944824 0.008963046638020456\nGB/s float  GB/s quant\n0.2793459455806586  31.16640544920269\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34594\n\nDifferential Revision: D20389106\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: d3a8c2cac58087d8b29e9cae64822f5b2d4c03ba", "pr_number": "34594", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/qupsample_nearest3d.cpp", "test/test_quantized.py", "torch/nn/quantized/functional.py"], "labels": ["merged", "quantization"]}, "e9a660a160": {"title": "Revert D20354878: [quant][graphmode] Add quantized conv2d-relu fusion pattern", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20354878\n\nOriginal commit changeset: 2b19797d4b3f\n\nfbshipit-source-id: 18f447074794af0d579e145df02af47d01746921", "pr_number": null, "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization_patterns.h"], "labels": []}, "673d56c838": {"title": "Use c10::str in process_group_agent.cpp (#34679)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34679\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20428467\n\nPulled By: mrshenli\n\nfbshipit-source-id: 2bfde4e383347c6e709109f074f55b9bc8068a49", "pr_number": "34679", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp"], "labels": ["merged"]}, "f9aa0c870f": {"title": "Use c10::str in py_rref.cpp (#34681)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34681\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20428827\n\nPulled By: mrshenli\n\nfbshipit-source-id: 847486b3114f0e9a2ad5f80c5e44db82d977c6a2", "pr_number": "34681", "files_changed": ["torch/csrc/distributed/rpc/py_rref.cpp"], "labels": ["merged"]}, "ad4bc8c9b8": {"title": "Best-effort Error Detection for Using Deleted UserRRefs (#34673)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34673\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20427839\n\nPulled By: mrshenli\n\nfbshipit-source-id: b1b12ca42a9ed5294806c53fa7d6f54e7dc8b188", "pr_number": "34673", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/distributed/rpc/api.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "808f84ee35": {"title": "[Shape Inference] Update shape inference in dper3 backend - C2 part (#34474)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34474\n\nAdd InferQuantization - set current_dim_type_ to CONSTANT for quantization ops.\n\nTest Plan: buck test mode/opt-clang caffe2/caffe2/opt:bound_shape_inference_test\n\nReviewed By: yinghai\n\nDifferential Revision: D20332703\n\nfbshipit-source-id: 36fa9bc81ae9f49dd00d8393d99ccce0884542df", "pr_number": "34474", "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h"], "labels": ["fb-exported", "merged"]}, "fd35596585": {"title": "[docs][1.5] Update distributed autograd note (#34657)", "body": "Summary:\n- Update API calls `backward` and `optim.step` now that we require `context_id`\n- Add notes to clarify purpose of distributed autograd context (this was a source of confusion in some feedback)\n- Add note that details why optimizer requires context_id\n- Clearly specify that we don't have SMART mode yet\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34657\n\nDifferential Revision: D20427667\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 5f8a3539ccf648a78e9e9a0dfdfe389c678b1606", "pr_number": "34657", "files_changed": ["docs/source/notes/distributed_autograd.rst"], "labels": ["merged"]}, "6791ae51a5": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/fb303/commit/e8f09733c79353a9b9db8f43da22d069d4c4af2e\nhttps://github.com/facebook/fbzmq/commit/7e1606a4070763fbe1ed96a327432ade5ffedbd1\nhttps://github.com/facebook/rocksdb/commit/674cf417325cfe78ac7ce6389f2685acce061e65\nhttps://github.com/facebook/wangle/commit/e961892c6c4c2240e3e579a379e8bb1ddaa02679\nhttps://github.com/pytorch/fbgemm/commit/a5dffd278402e10f0da73d71dd31be5c3e8d87a5\n\nTest Plan: n/a\n\nReviewed By: 2d2d2d2d2d\n\nfbshipit-source-id: eb2e20f65ba40bacbfeb1d0cb54ed373cca564ff", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "b93518a662": {"title": "Revert D20422879: [pytorch][PR] Remove hotpatches that circumvent MAGMA bug", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20422879\n\nOriginal commit changeset: 8dd7a30b5c31\n\nfbshipit-source-id: a44dda3220d426a92b0e158e9903566be8701374", "pr_number": null, "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu"], "labels": []}, "14c1ab049d": {"title": "[Codemod][FBSourceGoogleJavaFormatLinter] Daily `arc lint --take GOOGLEJAVAFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D20415422\n\nfbshipit-source-id: 860f8dd9dce0a2420792bafb7d3e58bd883ab7e4", "pr_number": null, "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/Module.java"], "labels": []}, "8f854fb9e2": {"title": "[1/n][multi-tower] add partition info in predictor construction (#34175)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34175\n\nto incorporate PartitionInfo added in D20015493\n\nTest Plan: unit tests\n\nReviewed By: yinghai\n\nDifferential Revision: D20133759\n\nfbshipit-source-id: 130db2d80bca3c05a7ec91292159f857046718e0", "pr_number": "34175", "files_changed": ["caffe2/python/predictor/predictor_py_utils.py"], "labels": ["fb-exported", "merged"]}, "8e8a37d746": {"title": "Fix bug in baddbmm corner case (#33467) (#33538)", "body": "Summary:\nEnsure `torch.baddbmm(c, a, b)` returns `beta*c` when `a @ b` has empty inner dimension.\n\nFixes https://github.com/pytorch/pytorch/issues/33467.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33538\n\nDifferential Revision: D20352352\n\nPulled By: albanD\n\nfbshipit-source-id: a7021c1979f82402ecea4784d6cc39783392ea16", "pr_number": "33538", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "27410318ad": {"title": "[PyTorch][Mobile] Fix the operator latency issue.", "body": "Summary: Last diff enabled operator stats for non-production build including AIBench. But the operator latency is off: https://our.intern.facebook.com/intern/aibench/details/414567479798816 as it is representing operator execution end time, and as the threadLocalDebugInfo was not set, the start time is 0. So this diff is fixing it by creating a new ThreadLocalDebugInfo object when op starts to run and store the model information for logging.\n\nTest Plan:\n```buck run mode/mac aibench:run_bench_macos -- -b aibench/specifications/models/pytorch/pytext/pytext_mobile_inference.json --platform android --framework pytorch --remote --devices SM-G960F-8.0.0-26```\nhttps://our.intern.facebook.com/intern/aibench/details/922804117425407\n\n```buck run mode/mac aibench:run_bench_macos -- -b aibench/specifications/models/pytorch/fbnet/fbnet_mobile_inference.json --platform android --framework pytorch --remote --devices SM-G960F-8.0.0-26```\nhttps://our.intern.facebook.com/intern/aibench/details/593403202250750\n\nReviewed By: xta0\n\nDifferential Revision: D20436388\n\nfbshipit-source-id: 740bc94c3f51daef6af9b45c1ed7a708f5fc8836", "pr_number": null, "files_changed": ["torch/csrc/jit/mobile/module.cpp"], "labels": []}, "3924c55f4c": {"title": "[C++ API] Update torch::nn functional docs (#34688)", "body": "Summary:\n- `torch::nn::functional` functions must provide example for how to use the corresponding functional options\n- `torch::nn::functional` functions must link to the corresponding functional options\n- remove `TORCH_NN_FUNCTIONAL_USE_MODULE_OPTIONS` macro, and put `torch::nn::functional` options docs inside the functional namespace, right above functional declaration\n- `torch::nn::functional` options docs should not link back to torch::nn layers. Instead, they should  have links to `torch::nn::functional::xxx`\n\n----\n\nThis PR is BC-breaking in the following way:\n`TORCH_NN_FUNCTIONAL_USE_MODULE_OPTIONS` macro is removed, and user should explicitly write\n```cpp\nnamespace functional {\nusing SomeFuncOptions = SomeModuleOptions;\n} // namespace functional\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34688\n\nDifferential Revision: D20431251\n\nPulled By: yf225\n\nfbshipit-source-id: 7d4f27dca3aad2a1e523690927d7afb261b9d308", "pr_number": "34688", "files_changed": ["torch/csrc/api/include/torch/nn/functional/activation.h", "torch/csrc/api/include/torch/nn/functional/batchnorm.h", "torch/csrc/api/include/torch/nn/functional/conv.h", "torch/csrc/api/include/torch/nn/functional/distance.h", "torch/csrc/api/include/torch/nn/functional/dropout.h", "torch/csrc/api/include/torch/nn/functional/embedding.h", "torch/csrc/api/include/torch/nn/functional/fold.h", "torch/csrc/api/include/torch/nn/functional/instancenorm.h", "torch/csrc/api/include/torch/nn/functional/loss.h", "torch/csrc/api/include/torch/nn/functional/normalization.h", "torch/csrc/api/include/torch/nn/functional/padding.h", "torch/csrc/api/include/torch/nn/functional/pixelshuffle.h", "torch/csrc/api/include/torch/nn/functional/pooling.h", "torch/csrc/api/include/torch/nn/functional/upsampling.h", "torch/csrc/api/include/torch/nn/functional/vision.h", "torch/csrc/api/include/torch/nn/options/activation.h", "torch/csrc/api/include/torch/nn/options/batchnorm.h", "torch/csrc/api/include/torch/nn/options/common.h", "torch/csrc/api/include/torch/nn/options/conv.h", "torch/csrc/api/include/torch/nn/options/distance.h", "torch/csrc/api/include/torch/nn/options/dropout.h", "torch/csrc/api/include/torch/nn/options/embedding.h", "torch/csrc/api/include/torch/nn/options/fold.h", "torch/csrc/api/include/torch/nn/options/instancenorm.h", "torch/csrc/api/include/torch/nn/options/loss.h", "torch/csrc/api/include/torch/nn/options/normalization.h", "torch/csrc/api/include/torch/nn/options/padding.h", "torch/csrc/api/include/torch/nn/options/pixelshuffle.h", "torch/csrc/api/include/torch/nn/options/pooling.h", "torch/csrc/api/include/torch/nn/options/upsampling.h", "torch/csrc/api/include/torch/nn/options/vision.h"], "labels": ["merged", "module: cpp", "module: docs", "topic: bc-breaking"]}, "40eff454ce": {"title": "Fix max_pool2d NHWC for large tensors; fix incorrect use of cudaGetLastError() (#34519)", "body": "Summary:\nThis PR would fix https://github.com/pytorch/pytorch/issues/33988 and fix https://github.com/pytorch/pytorch/issues/34083.\n\nPreviously, the max_pool2d_nhwc kernels used a shared memory with size proportional to the tensor size (c \\* h \\* w). When the tensor size is too large, the kernel launch fails.\n\nThis PR follows the guidance in AdaptiveAvgPool2d_nhwc by increasing the number of grid_x with split in \"C\" dimension. With that change, there will be a maximum limit in the shared memory size (which is less than 48 kb) regardless of tensor size.\n\nA benchmark can be found at [here](https://github.com/xwang233/code-snippet/blob/0b98146089ffca65d3d56968a9eafbb401a82493/max-pool2d/max-pool2d.ipynb). TL;DR barely any performance drop is found.\n\ncc csarofeen ptrblck jjsjann123 VitalyFedyunin\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34519\n\nDifferential Revision: D20388848\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 9454f385f9315afaab4a05303305578bbcd80b87", "pr_number": "34519", "files_changed": ["aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu", "aten/src/ATen/native/cuda/AveragePool2d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool2d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/Dropout.cu", "aten/src/ATen/native/cuda/Embedding.cu", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LossCTC.cu", "aten/src/ATen/native/cuda/MaxUnpooling.cu", "aten/src/ATen/native/cuda/Normalization.cuh", "aten/src/ATen/native/cuda/Shape.cu", "aten/src/ATen/native/cuda/SoftMax.cu", "aten/src/ATen/native/cuda/Unique.cu", "aten/src/ATen/native/cuda/WeightNorm.cu", "test/test_nn.py"], "labels": ["merged", "open source"]}, "b1dbe33056": {"title": "Skip `TestNN.test_spectral_norm_load_state_` if PyTorch is compiled w\u2026 (#34686)", "body": "Summary:\n\u2026ithout lapack\n\nLAPACK is needed for `at::svd``, which is called from `pinverse()`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34686\n\nTest Plan: CI + local run\n\nDifferential Revision: D20442637\n\nPulled By: malfet\n\nfbshipit-source-id: b3531ecc1197b0745ddcf50febb7fb4a7700d612", "pr_number": "34686", "files_changed": ["test/test_nn.py"], "labels": ["merged"]}, "4a599f47fb": {"title": "scripts: Add script to promote conda packages (#34659)", "body": "Summary:\nHow this actually works:\n  1. Get's a list of URLs from anaconda for pkgs to download, most\n  likely from pytorch-test\n  2. Download all of those packages locally in a temp directory\n  3. Upload all of those packages, with a dry run upload by default\n\nThis, along with https://github.com/pytorch/pytorch/issues/34500 basically completes the scripting work for the eventual promotion pipeline.\n\nCurrently testing with:\n```\nTEST_WITHOUT_GIT_TAG=1 TEST_PYTORCH_PROMOTE_VERSION=1.4.0 PYTORCH_CONDA_FROM=pytorch scripts/release/promote/conda_to_conda.sh\n```\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34659\n\nDifferential Revision: D20432687\n\nPulled By: seemethere\n\nfbshipit-source-id: c2a99f6cbc6a7448e83e666cde11d6875aeb878e", "pr_number": "34659", "files_changed": ["scripts/release/promote/conda_to_conda.sh"], "labels": ["merged", "module: ci", "releng"]}, "027d7f7ba5": {"title": "Delete AT_WARN and replace all AT_WARN with TORCH_WARN (#34623)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34623\n\nThe bandaid of \"AT_WARN\" keeps introducing new warnings. Let's get rid\nof it entirely.\n\nClose #34502\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20420112\n\nPulled By: albanD\n\nfbshipit-source-id: 7160c113cb4deb2d2f50a375356f423fe5e86f50", "pr_number": "34623", "files_changed": ["aten/src/ATen/native/DispatchStub.cpp", "aten/src/ATen/native/IndexingUtils.h", "aten/src/ATen/native/LegacyDefinitions.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/cuda/LegacyDefinitions.cpp", "aten/src/ATen/native/cudnn/RNN.cpp", "c10/util/Exception.h", "torch/csrc/autograd/functions/comm.cpp", "torch/csrc/autograd/python_anomaly_mode.cpp", "torch/csrc/jit/api/compilation_unit.h", "torch/csrc/jit/api/function_impl.h", "torch/csrc/jit/api/module.h", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_special_ops.cpp", "torch/csrc/jit/runtime/vararg_functions.cpp", "torch/csrc/jit/serialization/export.cpp"], "labels": ["jit", "merged", "open source", "triaged"]}, "c34ee4fb6e": {"title": "[JIT] disable test (#34722)", "body": "Summary:\nI opened https://github.com/pytorch/pytorch/issues/34658 but it didn't work.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34722\n\nDifferential Revision: D20444547\n\nPulled By: eellison\n\nfbshipit-source-id: 90aa06098587b48c9760a9c6df9bec01d642fcdb", "pr_number": "34722", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py"], "labels": ["merged"]}, "c9023e3b12": {"title": "Support left and right shift operators in JIT (#34563)", "body": "Summary:\nWith this PR, we can now support left and right shift operators in the JIT engine for <int, int> and <Tensor, int>.\n\nUpdated tests pass as expected:\n```\n> python test/test_jit.py\n...\nRan 2427 tests in 84.861s\n\nOK (skipped=139, expected failures=1)\n```\n\nRunning the following code with Python results in the output below:\n```\n> cat ~/expressions.py\nimport torch\n\ntorch.jit.script\ndef fn(a, b):\n    # type: (int, int)\n    return (\n        a << b,  # supported\n        b >> a,  # supported\n        a & b,\n        a | b,\n        a ^ b\n    )\nprint(fn.graph)\n```\n\n```\n> python ~/expressions.py\ngraph(%a.1 : int,\n      %b.1 : int):\n  %4 : int = aten::leftshift(%a.1, %b.1) # /home/ince/expressions.py:7:8\n  %7 : int = aten::rightshift(%b.1, %a.1) # /home/ince/expressions.py:8:8\n  %10 : int = aten::__and__(%a.1, %b.1) # /home/ince/expressions.py:9:8\n  %13 : int = aten::__or__(%a.1, %b.1) # /home/ince/expressions.py:10:8\n  %16 : int = aten::__xor__(%a.1, %b.1) # /home/ince/expressions.py:11:8\n  %17 : (int, int, int, int, int) = prim::TupleConstruct(%4, %7, %10, %13, %16)\n  return (%17)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34563\n\nDifferential Revision: D20434209\n\nPulled By: tugrulince\n\nfbshipit-source-id: 886386c59755106e17b84778b8e495b80a6269cd", "pr_number": "34563", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/frontend/lexer.cpp", "torch/csrc/jit/frontend/lexer.h", "torch/csrc/jit/frontend/tree_views.h", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/jit/frontend.py"], "labels": ["jit"]}, "f933fa3613": {"title": "[docs][1.5] update RPC docs to reflect correct use of dist_autograd backwards and dist_optim step() (#34670)", "body": "Summary:\n- Clarify that `torch.distributed.autograd.backwards()` does not use the current thread local autograd context, instead it looks it up based on the context_id passed in\n- Clarify the same for `torch.distributeed.optimizer.optim.step()`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34670\n\nDifferential Revision: D20427645\n\nPulled By: rohan-varma\n\nfbshipit-source-id: a1a88de346cdd4dbe65fb2b7627157f86fd2b6a3", "pr_number": "34670", "files_changed": ["torch/csrc/distributed/autograd/init.cpp", "torch/distributed/optim/optimizer.py"], "labels": ["merged"]}, "a730abd997": {"title": "[PyTorch][tools] Add linux64 clang-format hash", "body": "Summary:\nThis commit adds a reference hash for the linux64 clang-format binary and in\ndoing so, enables this script to be used on Linux machines.\n\nTest Plan:\nRan the script.\n\n```\nmeghanl@devvm1517:caffe2  (ff25240c|remote/master)$ export http_proxy=fwdproxy:8080\nmeghanl@devvm1517:caffe2  (ff25240c|remote/master)$ export https_proxy=fwdproxy:8080\nmeghanl@devvm1517:caffe2  (ff25240c|remote/master)$ python3 ./tools/clang_format_new.py --diff\nDownloading clang-format to /data/users/meghanl/fbsource/fbcode/caffe2/.clang-format-bin\n0% |################################################################| 100%\nUsing clang-format located at /data/users/meghanl/fbsource/fbcode/caffe2/.clang-format-bin/clang-format\nmeghanl@devvm1517:caffe2  (ff25240c|remote/master)$ echo $?\n1\n```\nA non-zero return code indicates that `clang-format` will make changes.\n\nReviewed By: suo\n\nDifferential Revision: D20434291\n\nfbshipit-source-id: fa13766e9d94720d4b0d8a540d2f1507e788f7a5", "pr_number": null, "files_changed": ["tools/clang_format_hash/linux64/clang-format-linux64", "tools/clang_format_new.py"], "labels": []}, "0f3b6f3dec": {"title": "Add min function to cuda math compat (#34723)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34723\n\nAdd min function to cuda math compat\n\nTest Plan: unittest\n\nReviewed By: houseroad\n\nDifferential Revision: D20444517\n\nfbshipit-source-id: 1a93343cc57249ef1101eeb7ef373266f6a2873a", "pr_number": "34723", "files_changed": ["c10/cuda/CUDAMathCompat.h"], "labels": ["fb-exported", "merged"]}, "c371c3aba7": {"title": "[rpc][profiler] add a test case to verify record_function context manager works (#34511)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34511\n\nWith https://github.com/pytorch/pytorch/pull/34122/files, issues\nwith using record_function context manager and profiling RPCs were fixed. This\nadds a test case to verify that we can use RPC with the `record_function`\ndecorator.\nghstack-source-id: 100109932\n\nTest Plan: Unit test change\n\nDifferential Revision: D20352242\n\nfbshipit-source-id: d6429e4352ad3b8d874dc0f27b23ecb6202e6b2b", "pr_number": "34511", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "c9ed111894": {"title": "[caffe2][quantization] Add initializer and precision as read-only property to QueryTensorQparam (#34706)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34706\n\nas title\n\nTest Plan: test in stacked diff\n\nReviewed By: csummersea\n\nDifferential Revision: D20436618\n\nfbshipit-source-id: e51ef0a22708425cd296c05f4089fe8c98eda90a", "pr_number": "34706", "files_changed": ["caffe2/quantization/server/pybind.cc"], "labels": ["fb-exported", "merged"]}, "aedffdf7d8": {"title": "Support for Tensor Shape Type Hint (#34595)", "body": "Summary:\nThis PR is related to [https://github.com/pytorch/pytorch/issues/33953](https://github.com/pytorch/pytorch/issues/33953).\nI've created a directory `type_hint_tests` for the example as suggested by zou3519 [here](https://github.com/pytorch/pytorch/issues/33953#issuecomment-597716405). This directory is supposed to contain examples over which mypy will run. I've added the test in `test/test_type_hints.py`.\nThe test can simply be invoked by\n```\n$ python3 test/test_type_hints.py\nFail to import hypothesis in common_utils, tests are not derandomized\n.b'test/type_hint_tests/size.py:7: error: Tuple index out of range\\ntest/type_hint_tests/size.py:8: error: Tuple index out of range\\n'\n.\n----------------------------------------------------------------------\nRan 2 tests in 13.660s\n\nOK\n\n```\nNote that I've not made the change of fixing the stub to show that the test works. The issue can be fixed by changing definition of Size in `class Size(Tuple[_int, ...]): ... ` in `/torch/__init__.pyi.in`.\nAfter changing the `Size` definition, the test passes.\n```\n$ python3 test/test_type_hints.py\nFail to import hypothesis in common_utils, tests are not derandomized\n.b''\n.\n----------------------------------------------------------------------\nRan 2 tests in 19.382s\n\nOK\n```\nI will do that once i get approval from zou3519. This is an initial implementation, please provide your suggestions.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34595\n\nDifferential Revision: D20441817\n\nPulled By: zou3519\n\nfbshipit-source-id: 00a434adf5bca813960f4efea38aa6d6953fe85f", "pr_number": "34595", "files_changed": ["test/test_type_hints.py", "test/type_hint_tests/size.py", "torch/__init__.pyi.in"], "labels": ["merged", "open source", "triaged"]}, "6d790c3611": {"title": "Mark PyTorch incompatible with python-3.6.0 (#34724)", "body": "Summary:\nPer https://github.com/pytorch/pytorch/issues/19161 PyTorch is incompatible with 3.6.0 due to the missing `PySlice_Unpack`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34724\n\nTest Plan: CI + try to load pytorch binary using python-3.6.0\n\nDifferential Revision: D20449052\n\nPulled By: malfet\n\nfbshipit-source-id: 2c787fc64f5d1377c7f935ad2f3c77f46723d7dd", "pr_number": "34724", "files_changed": ["setup.py"], "labels": ["merged"]}, "1734bd6871": {"title": "skip mask_rcnn test (#34734)", "body": "Summary:\nfix master\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34734\n\nDifferential Revision: D20447607\n\nPulled By: eellison\n\nfbshipit-source-id: 165c64f0484abf068b7d3a204a6bcb623ffe0910", "pr_number": "34734", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py"], "labels": ["merged"]}, "e7910aa9e5": {"title": "[fix] use non-inplace for insert observer pass (#34190)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34190\n\ninplace modification of ClassType might affect other tests, so we want to do non-inplace modifications.\nActually the inplace argument will be removed soon.\n\nTest Plan:\nci\n\nImported from OSS\n\nDifferential Revision: D20451765\n\nfbshipit-source-id: e87ad528c4e7f84f5774b94a8e3e85568269682d", "pr_number": "34190", "files_changed": ["test/test_jit.py"], "labels": ["merged"]}, "af3a7e2b50": {"title": "[jit] small cleanups after script:: removal (#34677)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34677\n\n1. Remove remaining uses of `script::` namespace from the codebase,\n2. Add one more typedef for `script::ExtraFilesMap` which is part of the\npublic interface.\n\nPull Request resolved: #34580\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D20431739\n\nPulled By: suo\n\nfbshipit-source-id: a29d369c755b6506c53447ca1f286b6339222c9a", "pr_number": "34677", "files_changed": ["torch/csrc/jit/api/module.h", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization.h", "torch/csrc/jit/python/init.cpp"], "labels": ["jit", "merged"]}, "ab76a8206f": {"title": "[JIT][mobile] Support built-in Function call in lite interpreter (#34676)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34676\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20427938\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 79eebfa858776f26da55ffd49d3f78fa7ae0df9b", "pr_number": "34676", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/runtime/instruction.cpp", "torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/custom_class.h"], "labels": ["jit", "merged"]}, "52005b551c": {"title": "invokeOperatorFromPython: support overloaded operator calling (#34671)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34671\n\nLike the python arg parser, this tries to convert to the schema in order.\nIt introduces schema_match_exception which gets thrown when the schema doesn't match,\nallowing the overload handler to try the next option.\n\nBehavior will not 100% match the schema argument parser but should work for\nsimple cases using custom binding.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20432206\n\nPulled By: zdevito\n\nfbshipit-source-id: 280839a2205ea3497db3a9b5741fccc1e2bff9a8", "pr_number": "34671", "files_changed": ["aten/src/ATen/core/function_schema.h", "aten/src/ATen/core/function_schema_inl.h", "test/test_jit.py", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/runtime/register_distributed_ops.cpp"], "labels": ["jit", "merged"]}, "7dee36a061": {"title": ".circleci: Remove CUDA 10.0, no longer needed (#34726)", "body": "Summary:\nSince we've added CUDA 10.2, it is time to retire CUDA 10.0\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34726\n\nDifferential Revision: D20453081\n\nPulled By: seemethere\n\nfbshipit-source-id: fd5bb35325a5f1577d0f0404d16cd7dfe34c86ad", "pr_number": "34726", "files_changed": [".circleci/cimodel/data/binary_build_definitions.py", ".circleci/cimodel/data/dimensions.py", ".circleci/cimodel/data/pytorch_build_data.py", ".circleci/config.yml", ".circleci/scripts/binary_linux_test.sh", ".circleci/verbatim-sources/binary-build-tests.yml", ".circleci/verbatim-sources/workflows-nightly-uploads-header.yml"], "labels": ["merged", "module: ci"]}, "da11646db1": {"title": "[C++ API] Link to module options doc for functional that has same options as module (#34752)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34752\n\nTest Plan: Imported from GitHub, without a `Test Plan:` line.\n\nDifferential Revision: D20452681\n\nPulled By: yf225\n\nfbshipit-source-id: 06b56a08bd480999353ebbff39c035225e4070df", "pr_number": "34752", "files_changed": ["torch/csrc/api/include/torch/nn/options/activation.h", "torch/csrc/api/include/torch/nn/options/distance.h", "torch/csrc/api/include/torch/nn/options/fold.h", "torch/csrc/api/include/torch/nn/options/loss.h", "torch/csrc/api/include/torch/nn/options/normalization.h", "torch/csrc/api/include/torch/nn/options/pixelshuffle.h", "torch/csrc/api/include/torch/nn/options/pooling.h"], "labels": ["merged", "module: cpp", "module: docs"]}, "68758b2fa0": {"title": "Add the quantized batch_norm3d and also batch_norm3d fused with relu operators (#34702)", "body": "Summary:\nas title, for bringing up the quantized video model. Will add the batch_norm_relu test in another PR.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34702\n\nDifferential Revision: D20436092\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: 116bd306f7880bfd763d8575654fbd6c92818338", "pr_number": "34702", "files_changed": ["aten/src/ATen/native/quantized/cpu/qbatch_norm.cpp", "test/test_quantized.py", "test/test_quantized_nn_mods.py", "torch/nn/quantized/modules/__init__.py", "torch/nn/quantized/modules/batchnorm.py", "torch/quantization/default_mappings.py"], "labels": ["merged", "quantization"]}, "d041d0784e": {"title": "[C++ API] RNNCell / LSTMCell / GRUCell layers (#34400)", "body": "Summary:\nThis PR adds `RNNCell` / `LSTMCell` / `GRUCell` layers to the C++ frontend, with implementations exactly matching the Python API equivalent.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34400\n\nDifferential Revision: D20316859\n\nPulled By: yf225\n\nfbshipit-source-id: bb7cee092622334043c0d0fd0fcb4e75e707699c", "pr_number": "34400", "files_changed": ["test/cpp/api/modules.cpp", "test/cpp/api/sequential.cpp", "test/cpp_api_parity/parity-tracker.md", "torch/csrc/api/include/torch/nn/modules/rnn.h", "torch/csrc/api/include/torch/nn/options/rnn.h", "torch/csrc/api/src/nn/modules/rnn.cpp", "torch/csrc/api/src/nn/options/rnn.cpp"], "labels": ["merged", "module: cpp"]}, "af28915164": {"title": "[quant][onnx] Add support to convert max_pool2d quantized pytorch op to C2 (#33945)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33945\n\nAdd mapping for this operator in symbolics\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_max_pool2d\n\nImported from OSS\n\nDifferential Revision: D20433681\n\nfbshipit-source-id: 88f02ade698262a6f8824671830bc1f7d40bbfa6", "pr_number": "33945", "files_changed": ["test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/onnx/symbolic_caffe2.py"], "labels": ["merged"]}, "8a395882ce": {"title": "[quant][onnx] Support conversion of quantized sigmoid operator from pytorch to caffe2 (#34629)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34629\n\nAdd support for sigmoid in the conversion flow through onnx\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_quantized_sigmoid\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_small_model\n\nImported from OSS\n\nDifferential Revision: D20433680\n\nfbshipit-source-id: 95943e14637d294122e4d102c5c19c06d27064c6", "pr_number": "34629", "files_changed": ["test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp", "torch/onnx/symbolic_caffe2.py"], "labels": ["jit", "merged"]}, "fb20621b3b": {"title": "Move torchbind out of jit namespace (#34745)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34745\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20450239\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 3f5597626f21d7b5e329b57da358c76b531bf806", "pr_number": "34745", "files_changed": ["aten/src/ATen/core/custom_class.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "test/cpp/jit/test_custom_class.cpp", "torch/custom_class.h", "torch/custom_class_detail.h"], "labels": ["jit", "merged"]}, "5710374e4e": {"title": "[reland][quant][graphmode] Add quantized conv2d-relu fusion pattern (#33279) (#34744)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34744\n\natt\n\nTest Plan: python test/test_jit.py\n\nDifferential Revision: D20449667\n\nPulled By: jerryzh168\n\nfbshipit-source-id: 01bbc26604fac421dcaacaf4fa1b57731f1f08b7", "pr_number": "34744", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization_patterns.h"], "labels": ["jit", "merged"]}, "e23a9dc140": {"title": "[C++ API] RNN / GRU / LSTM layer refactoring (#34322)", "body": "Summary:\nThis PR refactors RNN / GRU / LSTM layers in C++ API to exactly match the implementation in Python API.\n\n**BC-breaking changes:**\n- Instead of returning `RNNOutput`, RNN / GRU forward method now returns `std::tuple<Tensor, Tensor>`, and LSTM forward method now returns `std::tuple<Tensor, std::tuple<Tensor, Tensor>>`, matching Python API.\n- RNN / LSTM / GRU forward method now accepts the same inputs (input tensor and optionally hidden state), matching Python API.\n- RNN / LSTM / GRU now has `forward_with_packed_input` method which accepts `PackedSequence` as input and optionally hidden state, matching the `forward(PackedSequence, ...)` variant in Python API.\n- In `RNNOptions`\n    - `tanh()` / `relu()` / `activation` are removed. Instead, `nonlinearity` is added which takes either `torch::kTanh` or `torch::kReLU`\n    - `layers` -> `num_layers`\n    - `with_bias` -> `bias`\n- In `LSTMOptions`\n    - `layers` -> `num_layers`\n    - `with_bias` -> `bias`\n- In `GRUOptions`\n    - `layers` -> `num_layers`\n    - `with_bias` -> `bias`\n\nThe majority of the changes in this PR focused on refactoring the implementations in `torch/csrc/api/src/nn/modules/rnn.cpp` to match the Python API. RNN tests are then changed to reflected the revised API design.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34322\n\nDifferential Revision: D20311699\n\nPulled By: yf225\n\nfbshipit-source-id: e2b60fc7bac64367a8434647d74c08568a7b28f7", "pr_number": "34322", "files_changed": ["test/cpp/api/enum.cpp", "test/cpp/api/modulelist.cpp", "test/cpp/api/rnn.cpp", "test/cpp/api/sequential.cpp", "test/cpp_api_parity/parity-tracker.md", "torch/csrc/api/include/torch/enum.h", "torch/csrc/api/include/torch/nn/modules/rnn.h", "torch/csrc/api/include/torch/nn/options/rnn.h", "torch/csrc/api/src/enum.cpp", "torch/csrc/api/src/nn/modules/rnn.cpp", "torch/csrc/api/src/nn/options/rnn.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "4c30fc7238": {"title": "Integrate XNNPACK with custom class for packing weights. (#34047)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34047\n\nThis PR integrates the added xnnpack conv2d and linear op via\ncustom class registration for packed weights. The packed struct\nis serializable.\n\nTest Plan:\npython test test/test_xnnpack_integration.py\n\nImported from OSS\n\nDifferential Revision: D20185657\n\nfbshipit-source-id: fc7e692d8f913e493b293b02d92f4e78536d7698", "pr_number": "34047", "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/xnnpack/Common.h", "aten/src/ATen/native/xnnpack/Convolution.cpp", "aten/src/ATen/native/xnnpack/Convolution.h", "aten/src/ATen/native/xnnpack/Factory.cpp", "aten/src/ATen/native/xnnpack/Factory.h", "aten/src/ATen/native/xnnpack/Linear.cpp", "aten/src/ATen/native/xnnpack/Linear.h", "aten/src/ATen/native/xnnpack/OpContext.cpp", "aten/src/ATen/native/xnnpack/OpContext.h", "aten/src/ATen/native/xnnpack/RegisterOpContextClass.cpp", "aten/src/ATen/native/xnnpack/Shim.cpp", "test/test_xnnpack_integration.py", "torch/backends/xnnpack/__init__.py", "torch/csrc/Module.cpp"], "labels": ["merged"]}, "7dd5da2026": {"title": "JIT pass to insert XNNPACK ops (#34048)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34048\n\nRewrites the graph to insert xnnpack prepack and packed run ops for\nconv2d and linear.\n\nTest Plan:\npython test/test_xnnpack_integration.py\n\nImported from OSS\n\nDifferential Revision: D20185658\n\nfbshipit-source-id: c4c073c912ad33e822e7beb4ed86c9f895129d55", "pr_number": "34048", "files_changed": ["caffe2/CMakeLists.txt", "test/test_xnnpack_integration.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/graph_rewrite_helper.cpp", "torch/csrc/jit/passes/graph_rewrite_helper.h", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.h", "torch/csrc/jit/python/init.cpp"], "labels": ["jit", "merged"]}, "4da5569300": {"title": "Pass to remove prepacking ops. (#34319)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34319\n\nRemoves prepacking ops and install them as attributes of the top level\nmodule. Needs to run freezing as the first pass.\n\nTest Plan:\npython test/test_xnnpack_integration.py\n\nImported from OSS\n\nDifferential Revision: D20290726\n\nfbshipit-source-id: 633ceaa867ff7d5c8e69bd814c0362018394cb3a", "pr_number": "34319", "files_changed": ["aten/src/ATen/native/xnnpack/RegisterOpContextClass.cpp", "caffe2/CMakeLists.txt", "test/test_xnnpack_integration.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/prepack_folding.cpp", "torch/csrc/jit/passes/prepack_folding.h", "torch/csrc/jit/passes/xnnpack_rewrite.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.h", "torch/csrc/jit/python/init.cpp"], "labels": ["jit", "merged"]}, "84bd71dbd4": {"title": "Enable threading for XNNPACK ops. (#34547)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34547\n\nThis enables threading by passing a threadpool to xnnpack ops.\n\nTest Plan:\npython test/test_xnnpack_integration.py\n\nImported from OSS\n\nDifferential Revision: D20370553\n\nfbshipit-source-id: 4db08e73f8c69b9e722b0e11a00621c4e229a31a", "pr_number": "34547", "files_changed": ["CMakeLists.txt", "aten/src/ATen/native/xnnpack/Common.h", "aten/src/ATen/native/xnnpack/Convolution.cpp", "aten/src/ATen/native/xnnpack/Linear.cpp", "cmake/Dependencies.cmake"], "labels": ["merged"]}, "6c555e1508": {"title": "Revert D20311699: [pytorch][PR] [C++ API] RNN / GRU / LSTM layer refactoring", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20311699\n\nOriginal commit changeset: e2b60fc7bac6\n\nfbshipit-source-id: 72f4a762189490998d6b716857eeac053a11742d", "pr_number": null, "files_changed": ["test/cpp/api/enum.cpp", "test/cpp/api/modulelist.cpp", "test/cpp/api/rnn.cpp", "test/cpp/api/sequential.cpp", "test/cpp_api_parity/parity-tracker.md", "torch/csrc/api/include/torch/enum.h", "torch/csrc/api/include/torch/nn/modules/rnn.h", "torch/csrc/api/include/torch/nn/options/rnn.h", "torch/csrc/api/src/enum.cpp", "torch/csrc/api/src/nn/modules/rnn.cpp", "torch/csrc/api/src/nn/options/rnn.cpp"], "labels": []}, "373c80ee90": {"title": "Fix missing header (#34762)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34762\n\nSo far it's by luck that we somehow include \"caffe2/core/tensor.h\" before including \"caffe2/caffe2/quantization/server/fbgemm_pack_blob.h\". This is not safe and this diff fixes it.\n\nTest Plan: unittest\n\nReviewed By: jianyuh\n\nDifferential Revision: D20455352\n\nfbshipit-source-id: 777dae32a23d0ec75fd7e5e1627426b5a5f81f5a", "pr_number": "34762", "files_changed": ["caffe2/quantization/server/fbgemm_pack_blob.h"], "labels": ["fb-exported", "merged"]}, "1d81bd02cc": {"title": "Export roi_align_gradient_op to c10 (#34776)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34776\n\nExport roi_align_gradient_op to c10\n\nTest Plan: unittest\n\nReviewed By: houseroad\n\nDifferential Revision: D20459210\n\nfbshipit-source-id: 80bf065f83bb44b39a150bae25b3591c16f522fa", "pr_number": "34776", "files_changed": ["caffe2/operators/roi_align_gradient_op.cc", "caffe2/operators/roi_align_gradient_op.cu", "caffe2/operators/roi_align_gradient_op.h"], "labels": ["fb-exported", "merged"]}, "08bc3c6cbf": {"title": "Remove unnecessary import (#34778)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/34563 accidentally introduced a lint error due to an unused import. This PR removes this import.\n\nJit tests run as expected after this change:\n```\n> python test/test_jit.py\n.....\nRan 2435 tests in 100.077s\n\nOK (skipped=140, expected failures=1)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34778\n\nDifferential Revision: D20459708\n\nPulled By: tugrulince\n\nfbshipit-source-id: bb742085fafc849ff3d9507d1557556e01fbeb4b", "pr_number": "34778", "files_changed": ["test/test_jit.py"], "labels": ["merged"]}, "15c84c37b6": {"title": "[PyTorch BC] Clean up the BC whitelist (#34784)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34784\n\nRemove stale items\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D20461740\n\nfbshipit-source-id: 46dcc39f3a867165aadee182033b09ca65ee8551", "pr_number": "34784", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["fb-exported", "merged"]}, "528aabd373": {"title": "Fix backward compatibility check test for schemas containing (#34782)", "body": "Summary:\n\"torch.classes\".\nBC check tests skips adding torch.classes based schemas to existing\nschemas. Removed the skip.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34782\n\nTest Plan:\ncd test/backward_compatibility\npython dump_all_function_schemas.py --filename new_schemas.txt\npython check_backward_compatibility.py --new-schemas new_schemas.txt\n\nBefore this PR fails with:\n```\nMar 15 11:12:20 Broken ops: [\nMar 15 11:12:20 \t_xnnpack::conv2d_packed(Tensor X, __torch__.torch.classes.XNNPackConv2dOpContext W_prepack) -> (Tensor Y)\nMar 15 11:12:20 \t_xnnpack::conv2d_prepack(Tensor W, Tensor? B, int[2] stride, int[2] padding, int[2] dilation, int groups) -> (__torch__.torch.classes.XNNPackConv2dOpContext)\nMar 15 11:12:20 \t_xnnpack::linear_packed(Tensor X, __torch__.torch.classes.XNNPackLinearOpContext W_prepack) -> (Tensor Y)\nMar 15 11:12:20 \t_xnnpack::linear_prepack(Tensor W, Tensor? B=None) -> (__torch__.torch.classes.XNNPackLinearOpContext)\nMar 15 11:12:20 ]\n```\nAfter this PR, it passes.\n\nReviewed By: houseroad\n\nDifferential Revision: D20461994\n\nPulled By: kimishpatel\n\nfbshipit-source-id: de692644ee7d49accf2d8260cd3a10f6e147653a", "pr_number": "34782", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "f404537c26": {"title": "CUDA Loops: move address computation into policy, make policy.load load all arguments (#33720)", "body": "Summary:\nSo that in the future we can make policy accept an offset calculator in its constructor for the support of non-contiguous tensors.\n\nThe `elementwise_kernel_helper` is now very general and it can handle any cases:\n\n```C++\ntemplate<typename func_t, typename policy_t>\n__device__ inline void elementwise_kernel_helper(func_t f, policy_t policy) {\n  using traits = function_traits<func_t>;\n  using return_t = typename traits::result_type;\n  using args_t = typename traits::ArgsTuple;\n\n  int idx = blockIdx.x;\n\n  return_t results[thread_work_size];\n  cuda9::workaround::enable_default_constructor<args_t> args_[thread_work_size];\n  args_t *args = reinterpret_cast<args_t *>(&args_);\n\n  // load\n  policy.load(args, idx);\n\n  // compute\n  #pragma unroll\n  for (int i = 0; i < thread_work_size; i++) {\n    if (policy.check_inbounds(i)) {\n      results[i] = c10::guts::apply(f, args[i]);\n    }\n  }\n\n  // store\n  policy.store(results, idx);\n}\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33720\n\nDifferential Revision: D20459652\n\nPulled By: ngimel\n\nfbshipit-source-id: aa8b122e0e8c6e08ab354785e04753ff778882e2", "pr_number": "33720", "files_changed": ["aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/MemoryAccess.cuh", "aten/src/ATen/test/cuda_vectorized_test.cu"], "labels": ["merged", "open source", "triaged"]}, "f058c03b15": {"title": "Disallow sending CUDA tensors over RPC for current RPC agents. (#33604)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33604\n\nFor our current RPC agents, this PR disallows sending CUDA tensors\nover RPC and asks users to copy them explicitly to CPU. Currently, this seems\nto be the easiest contract to guarantee for our current RPC agents, otherwise\nif we do support this transparently it gets a little tricky in terms of whether\na CUDA tensor on the client should be sent to CPU/GPU of the remote end and\nalso which GPU device on the remote end.\n\nIn the future, the TensorPipe RPC agent can have its own specific handling of\nCUDA tensors.\n\nCloses https://github.com/pytorch/pytorch/issues/28881\nghstack-source-id: 100166120\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D20020183\n\nfbshipit-source-id: ca4d43d2a24e8fcd3a60b21e654aa0e953e756cb", "pr_number": "33604", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/distributed/rpc/api.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "7848c229b8": {"title": "Move min and max(reduce all) to Aten(CPU) (#33936)", "body": "Summary:\nThis PR is about port min and max(reduce all) to Aten.\nPerformance test script:\n```\nimport torch\nimport timeit\n\ntorch.manual_seed(0)\n#torch.set_num_threads(1)\n\ndevice = \"cpu\"\nprint(f'device: {device}')\nfor op in ('max', 'min'):\n    for dtype in ('torch.double', 'torch.float', 'torch.int16', 'torch.int32', 'torch.int64'):\n        for n, t in [(20_000, 200000),\n                     (200_000, 20000)]:\n            print(f'a.{op}(), numel() == {n} for {t} times, dtype={dtype}')\n            print(timeit.timeit(f'a.{op}()', setup=f'import torch; a =(torch.torch.randn({n}) * 100).to({dtype})', number=t))\n```\nTest device: **skx-8180, 2 sockets**\nBefore:\n```\na.max(), numel() == 20000 for 200000 times, dtype=torch.double\n2.773961597122252\na.max(), numel() == 200000 for 20000 times, dtype=torch.double\n2.3256353894248605\na.max(), numel() == 20000 for 200000 times, dtype=torch.float\n3.800648272037506\na.max(), numel() == 200000 for 20000 times, dtype=torch.float\n3.31692426931113\na.max(), numel() == 20000 for 200000 times, dtype=torch.int16\n2.735901520587504\na.max(), numel() == 200000 for 20000 times, dtype=torch.int16\n2.2510280115529895\na.max(), numel() == 20000 for 200000 times, dtype=torch.int32\n2.723656536079943\na.max(), numel() == 200000 for 20000 times, dtype=torch.int32\n2.228839812800288\na.max(), numel() == 20000 for 200000 times, dtype=torch.int64\n2.703160767443478\na.max(), numel() == 200000 for 20000 times, dtype=torch.int64\n2.3175809988752007\na.min(), numel() == 20000 for 200000 times, dtype=torch.double\n2.820106916129589\na.min(), numel() == 200000 for 20000 times, dtype=torch.double\n2.325718787498772\na.min(), numel() == 20000 for 200000 times, dtype=torch.float\n3.833602518774569\na.min(), numel() == 200000 for 20000 times, dtype=torch.float\n3.316444822587073\na.min(), numel() == 20000 for 200000 times, dtype=torch.int16\n2.7308286419138312\na.min(), numel() == 200000 for 20000 times, dtype=torch.int16\n2.198460517451167\na.min(), numel() == 20000 for 200000 times, dtype=torch.int32\n2.730219766497612\na.min(), numel() == 200000 for 20000 times, dtype=torch.int32\n2.2268200274556875\na.min(), numel() == 20000 for 200000 times, dtype=torch.int64\n2.7342184390872717\na.min(), numel() == 200000 for 20000 times, dtype=torch.int64\n2.320415544323623\n```\nAfter:\n```\na.max(), numel() == 20000 for 200000 times, dtype=torch.double\n1.7767417253926396\na.max(), numel() == 200000 for 20000 times, dtype=torch.double\n0.550495645031333\na.max(), numel() == 20000 for 200000 times, dtype=torch.float\n1.1113408291712403\na.max(), numel() == 200000 for 20000 times, dtype=torch.float\n0.44446005020290613\na.max(), numel() == 20000 for 200000 times, dtype=torch.int16\n0.5246349424123764\na.max(), numel() == 200000 for 20000 times, dtype=torch.int16\n0.47057845536619425\na.max(), numel() == 20000 for 200000 times, dtype=torch.int32\n0.6597231412306428\na.max(), numel() == 200000 for 20000 times, dtype=torch.int32\n0.40366593934595585\na.max(), numel() == 20000 for 200000 times, dtype=torch.int64\n1.767227927222848\na.max(), numel() == 200000 for 20000 times, dtype=torch.int64\n0.6187495030462742\na.min(), numel() == 20000 for 200000 times, dtype=torch.double\n1.7881382443010807\na.min(), numel() == 200000 for 20000 times, dtype=torch.double\n0.5440589748322964\na.min(), numel() == 20000 for 200000 times, dtype=torch.float\n1.1090848250314593\na.min(), numel() == 200000 for 20000 times, dtype=torch.float\n0.4293213738128543\na.min(), numel() == 20000 for 200000 times, dtype=torch.int16\n0.5207074657082558\na.min(), numel() == 200000 for 20000 times, dtype=torch.int16\n0.41422136034816504\na.min(), numel() == 20000 for 200000 times, dtype=torch.int32\n0.6145811947062612\na.min(), numel() == 200000 for 20000 times, dtype=torch.int32\n0.4172037309035659\na.min(), numel() == 20000 for 200000 times, dtype=torch.int64\n1.7397673893719912\na.min(), numel() == 200000 for 20000 times, dtype=torch.int64\n0.596766366623342\n```\nSingle thread:\nBefore:\n```\na.max(), numel() == 20000 for 200000 times, dtype=torch.double\n2.5068740313872695\na.max(), numel() == 200000 for 20000 times, dtype=torch.double\n2.234461876563728\na.max(), numel() == 20000 for 200000 times, dtype=torch.float\n3.5549037409946322\na.max(), numel() == 200000 for 20000 times, dtype=torch.float\n3.2497852174565196\na.max(), numel() == 20000 for 200000 times, dtype=torch.int16\n2.493077039718628\na.max(), numel() == 200000 for 20000 times, dtype=torch.int16\n2.171935741789639\na.max(), numel() == 20000 for 200000 times, dtype=torch.int32\n2.469274105504155\na.max(), numel() == 200000 for 20000 times, dtype=torch.int32\n2.273881389759481\na.max(), numel() == 20000 for 200000 times, dtype=torch.int64\n2.5818942049518228\na.max(), numel() == 200000 for 20000 times, dtype=torch.int64\n2.2394551979377866\na.min(), numel() == 20000 for 200000 times, dtype=torch.double\n2.5894540259614587\na.min(), numel() == 200000 for 20000 times, dtype=torch.double\n2.331936141476035\na.min(), numel() == 20000 for 200000 times, dtype=torch.float\n3.590122046880424\na.min(), numel() == 200000 for 20000 times, dtype=torch.float\n3.255849950015545\na.min(), numel() == 20000 for 200000 times, dtype=torch.int16\n2.5205496419221163\na.min(), numel() == 200000 for 20000 times, dtype=torch.int16\n2.168218174017966\na.min(), numel() == 20000 for 200000 times, dtype=torch.int32\n2.658622432500124\na.min(), numel() == 200000 for 20000 times, dtype=torch.int32\n2.3376982398331165\na.min(), numel() == 20000 for 200000 times, dtype=torch.int64\n2.496626536361873\na.min(), numel() == 200000 for 20000 times, dtype=torch.int64\n2.2504652086645365\n```\nAfter:\n```\na.max(), numel() == 20000 for 200000 times, dtype=torch.double\n1.9525171788409352\na.max(), numel() == 200000 for 20000 times, dtype=torch.double\n1.6108122132718563\na.max(), numel() == 20000 for 200000 times, dtype=torch.float\n1.2444602297618985\na.max(), numel() == 200000 for 20000 times, dtype=torch.float\n0.7705567870289087\na.max(), numel() == 20000 for 200000 times, dtype=torch.int16\n0.6575072864070535\na.max(), numel() == 200000 for 20000 times, dtype=torch.int16\n0.13242999743670225\na.max(), numel() == 20000 for 200000 times, dtype=torch.int32\n0.829406064003706\na.max(), numel() == 200000 for 20000 times, dtype=torch.int32\n0.35575105529278517\na.max(), numel() == 20000 for 200000 times, dtype=torch.int64\n1.6426756298169494\na.max(), numel() == 200000 for 20000 times, dtype=torch.int64\n1.4049720335751772\na.min(), numel() == 20000 for 200000 times, dtype=torch.double\n2.029639278538525\na.min(), numel() == 200000 for 20000 times, dtype=torch.double\n1.6363644907251\na.min(), numel() == 20000 for 200000 times, dtype=torch.float\n1.3821239182725549\na.min(), numel() == 200000 for 20000 times, dtype=torch.float\n0.834847847931087\na.min(), numel() == 20000 for 200000 times, dtype=torch.int16\n0.6913397628813982\na.min(), numel() == 200000 for 20000 times, dtype=torch.int16\n0.1370067736133933\na.min(), numel() == 20000 for 200000 times, dtype=torch.int32\n0.8190992185845971\na.min(), numel() == 200000 for 20000 times, dtype=torch.int32\n0.3640836915001273\na.min(), numel() == 20000 for 200000 times, dtype=torch.int64\n1.6516661625355482\na.min(), numel() == 200000 for 20000 times, dtype=torch.int64\n1.4111155439168215\n```\nFixes: https://github.com/pytorch/pytorch/issues/33197\n\nFix https://github.com/pytorch/pytorch/issues/24728, https://github.com/pytorch/pytorch/issues/24729\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33936\n\nDifferential Revision: D20461658\n\nPulled By: ngimel\n\nfbshipit-source-id: 5749260114ace3ea7b513e32edc805c844a19c8a", "pr_number": "33936", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/ReduceAllOps.cpp", "aten/src/ATen/native/ReduceAllOps.h", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReduceOpsUtils.h", "aten/src/ATen/native/cpu/Reduce.h", "aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cpu/zmath.h", "aten/src/ATen/native/native_functions.yaml"], "labels": ["merged", "open source", "triaged"]}, "c258e4732a": {"title": "solve conv3d backward get incorrect result problem (#34358)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/34344.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34358\n\nDifferential Revision: D20461698\n\nPulled By: ngimel\n\nfbshipit-source-id: 472624d0037ab65d9dcc221f647ec68818be5fc9", "pr_number": "34358", "files_changed": ["aten/src/ATen/native/ConvolutionMM3d.cpp"], "labels": ["merged", "open source", "topic: convolution", "topic: multithreading", "triaged"]}, "c86d1361b8": {"title": "Removes unused THCTensor_(triu), THCTensor_(div) (#34712)", "body": "Summary:\nPer title. Dead code removal.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34712\n\nDifferential Revision: D20442618\n\nPulled By: mruberry\n\nfbshipit-source-id: b03aa4984328f94021c1480e21375fd868d6d550", "pr_number": "34712", "files_changed": ["aten/src/THC/generic/THCTensorMath.h", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPairwise.h"], "labels": ["merged"]}, "d4f182d06b": {"title": "Add overloaded name to prim operators (#34280)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34280\n\nTo have prim ops searchable for lite interpreter, overloaded names need to be added for the operators with the same name but different schema. For example, aten::add in register_prim_ops.cpp. The difference is a combination of args and output type.\n`\"aten::add(str a, str b) ->str\"`\n`\"aten::add(int a, int b) ->int\"`\n`\"aten::add(float a, float b) ->float\"`\n`\"aten::add(int a, float b) ->float\"`\n`\"aten::add(float a, int b) ->float\"`\n`\"aten::add(Scalar a, Scalar b) ->Scalar\"`\n\nSolution:\nUse the argument type and/or output type (the same to the existing overloaded names). The overloaded name should be minimum as long as the operators can be differentiated. For other operators please look into the source code change for details.\n\n`\"aten::add.str(str a, str b) ->str\"`\n`\"aten::add.int(int a, int b) ->int\"`\n`\"aten::add.float(float a, float b) ->float\"`\n`\"aten::add.int_float(int a, float b) ->float\"`\n`\"aten::add.float_int(float a, int b) ->float\"`\n`\"aten::add.Scalar_Scalar(Scalar a, Scalar b) ->Scalar\"`\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20456997\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 2c3dc324b4a4e045559f62c6cc2a10fbb9a72dcf", "pr_number": "34280", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/tests.h", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["jit", "merged"]}, "bdd7dbfd4b": {"title": "[C++ API] RNN / GRU / LSTM layer refactoring (#34322)", "body": "Summary:\nThis PR refactors RNN / GRU / LSTM layers in C++ API to exactly match the implementation in Python API.\n\n**BC-breaking changes:**\n- Instead of returning `RNNOutput`, RNN / GRU forward method now returns `std::tuple<Tensor, Tensor>`, and LSTM forward method now returns `std::tuple<Tensor, std::tuple<Tensor, Tensor>>`, matching Python API.\n- RNN / LSTM / GRU forward method now accepts the same inputs (input tensor and optionally hidden state), matching Python API.\n- RNN / LSTM / GRU layers now have `forward_with_packed_input` method which accepts `PackedSequence` as input and optionally hidden state, matching the `forward(PackedSequence, ...)` variant in Python API.\n- RNN / LSTM / GRU layers no longer have these fields: `w_ih` / `w_hh` / `b_ih` / `b_hh`. Instead, to access the weights and biases of the gates, users should do e.g. `rnn->named_parameters()[\"weight_ih_l0\"]`, which mirrors the Python API `rnn.weight_ih_l0`.\n- In `RNNOptions`\n    - `tanh()` / `relu()` / `activation` are removed. Instead, `nonlinearity` is added which takes either `torch::kTanh` or `torch::kReLU`\n    - `layers` -> `num_layers`\n    - `with_bias` -> `bias`\n- In `LSTMOptions`\n    - `layers` -> `num_layers`\n    - `with_bias` -> `bias`\n- In `GRUOptions`\n    - `layers` -> `num_layers`\n    - `with_bias` -> `bias`\n\nThe majority of the changes in this PR focused on refactoring the implementations in `torch/csrc/api/src/nn/modules/rnn.cpp` to match the Python API. RNN tests are then changed to reflected the revised API design.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34322\n\nDifferential Revision: D20458302\n\nPulled By: yf225\n\nfbshipit-source-id: ffff2ae1ddb1c742c966956f6ad4d7fba03dc54d", "pr_number": "34322", "files_changed": ["test/cpp/api/enum.cpp", "test/cpp/api/modulelist.cpp", "test/cpp/api/rnn.cpp", "test/cpp/api/sequential.cpp", "test/cpp_api_parity/parity-tracker.md", "torch/csrc/api/include/torch/enum.h", "torch/csrc/api/include/torch/nn/modules/rnn.h", "torch/csrc/api/include/torch/nn/options/rnn.h", "torch/csrc/api/src/enum.cpp", "torch/csrc/api/src/nn/modules/rnn.cpp", "torch/csrc/api/src/nn/options/rnn.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "762be86e63": {"title": "[C++ API Parity] [Optimizers] added closure to optimizers (#34790)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34790\n\nDifferential Revision: D20468361\n\nPulled By: anjali411\n\nfbshipit-source-id: 1c6115d735b211dc2bedf002d58931cb32cf657a", "pr_number": "34790", "files_changed": ["test/cpp/api/optim.cpp", "test/cpp/api/serialize.cpp", "torch/csrc/api/include/torch/optim/adagrad.h", "torch/csrc/api/include/torch/optim/adam.h", "torch/csrc/api/include/torch/optim/optimizer.h", "torch/csrc/api/include/torch/optim/rmsprop.h", "torch/csrc/api/include/torch/optim/sgd.h", "torch/csrc/api/src/optim/adagrad.cpp", "torch/csrc/api/src/optim/adam.cpp", "torch/csrc/api/src/optim/rmsprop.cpp", "torch/csrc/api/src/optim/sgd.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "c3c0cf1591": {"title": "Migrate binary_cross_entropy_backward from CUDA_tensor_apply4 to (#33995)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33995\n\nTensorIterator\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20196790\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: c0c231a20e6e69fc3c68c3ac5082b20f2feb6158", "pr_number": "33995", "files_changed": ["aten/src/ATen/native/cuda/Loss.cu"], "labels": ["merged", "open source", "triaged"]}, "a66b837b19": {"title": "Migrate dirichlet_grad from CUDA_tensor_apply4 to TensorIterator (#33996)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33996\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20196789\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 69ee720f4f3d8a2df91874b77ee3918ce1b951b2", "pr_number": "33996", "files_changed": ["aten/src/ATen/native/Distributions.h", "aten/src/ATen/native/cuda/Distributions.cu"], "labels": ["merged", "open source", "triaged"]}, "b94d650868": {"title": "Remove unused newView declaration. (#34729)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34729\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20446077\n\nPulled By: gchanan\n\nfbshipit-source-id: b68471aeaf673851bdfc6bb0615aba8ebb883a4c", "pr_number": "34729", "files_changed": ["aten/src/THC/generic/THCTensor.hpp"], "labels": ["merged"]}, "8eaafbd99b": {"title": "Remove unused newWithSize declaration. (#34730)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34730\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20446078\n\nPulled By: gchanan\n\nfbshipit-source-id: 0effc088dcba4f60385e3b23fa656cb772a3b7bc", "pr_number": "34730", "files_changed": ["aten/src/THC/generic/THCTensor.hpp"], "labels": ["merged"]}, "cec9758afa": {"title": "[quant][graphmode] Add quantization pattern for quantized::add_relu (#33532)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33532\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D20354880\n\nfbshipit-source-id: ea608a5ace395a909851f9e577ffdcb51512a3af", "pr_number": "33532", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization_patterns.h"], "labels": ["jit", "merged"]}, "a57f92e4de": {"title": "[jit] copy unused/ignored methods to ScriptModule during compilation (#33981)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33981\n\nOkay it turns out that https://github.com/pytorch/pytorch/pull/29342\ndeletes actually useful things from the resulting Python module. In\nparticular, people like having `ignore`'d methods attached so that they\ncan invoke them from python.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20171650\n\nPulled By: suo\n\nfbshipit-source-id: 71862e932c6a56cd055d0cff6657887ee0ceb9a8", "pr_number": "33981", "files_changed": ["torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "1af6002321": {"title": "Initial implementation of NNPI Int8FC op", "body": "Test Plan:\n```\n buck test mode/no-gpu glow/fb/test/numerics:test_fc_nnpi_int8nnpi -- --print-passing-detail\n```\n\nReviewed By: hyuen\n\nDifferential Revision: D20450490\n\nfbshipit-source-id: c4811cdc994548b6e319d57115434dfc199e07c2", "pr_number": null, "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc"], "labels": []}, "24c9e61e79": {"title": "Enable JIT tests on Windows (#27029)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/27029\n\nReviewed By: eellison\n\nDifferential Revision: D20458664\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 22be918543703869f471e89b3478423198351bf3", "pr_number": "27029", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/CMakeLists.txt", "test/cpp/jit/torch_python_test.cpp", "test/test_jit.py", "torch/CMakeLists.txt", "torch/csrc/jit/python/init.cpp"], "labels": ["jit", "merged", "module: cpp", "module: pybind", "open source", "triaged"]}, "99b91ee2ad": {"title": "[fix][tiny][caffe2] Avoid triggering errors when allow ratio is 100% (#34757)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34757\n\nReviewed By: Wakeupbuddy\n\nDifferential Revision: D20451255\n\nfbshipit-source-id: 07997cf31dba653b61d082ec3f28357c3b90c4eb", "pr_number": "34757", "files_changed": ["caffe2/operators/gather_ranges_to_dense_op.h", "caffe2/python/operator_test/gather_ranges_op_test.py"], "labels": ["fb-exported", "merged"]}, "e31d462e92": {"title": "[TensorExpr] Pull changes to core classes for representing expressions and statements from the side branch. (#34224)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34224\n\nOur development has been happening on a side branch `pytorch_fusion` in\n`bertmaher/pytorch` fork. This PR moves changes to the core classes\nrepresenting expressions and transformations on them.\n\nAt this moment, the tensor expressions are only used in tests.\nSubsequent PRs add LLVM and CUDA codegen for tensor expressions and\nimplement fuser on top of these.\n\nThis PR is huge as it is a squashed version of changes in the side\nbranch. It is not practical to pull changes one by one from the branch,\nso here is the squashed version. If you're interested in seeing the\nhistory of changes, please refer to https://github.com/bertmaher/pytorch\n\nDifferential Revision: D20251835\n\nTest Plan: Imported from OSS\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 1a871acc09cf3c6f7fb4af40d408cdbb82dc7dab", "pr_number": "34224", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/tensorexpr/CMakeLists.txt", "test/cpp/tensorexpr/padded_buffer.cpp", "test/cpp/tensorexpr/padded_buffer.h", "test/cpp/tensorexpr/test_aten.cpp", "test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/test_ir_printer.cpp", "test/cpp/tensorexpr/test_schedule.cpp", "test/cpp/tensorexpr/test_type.cpp", "test/cpp/tensorexpr/tests.h", "tools/build_variables.bzl", "torch/csrc/jit/tensorexpr/DesignOverview.md", "torch/csrc/jit/tensorexpr/buffer.h", "torch/csrc/jit/tensorexpr/codegen.cpp", "torch/csrc/jit/tensorexpr/codegen.h", "torch/csrc/jit/tensorexpr/eval.h", "torch/csrc/jit/tensorexpr/expr.cpp", "torch/csrc/jit/tensorexpr/expr.h", "torch/csrc/jit/tensorexpr/function.cpp", "torch/csrc/jit/tensorexpr/function.h", "torch/csrc/jit/tensorexpr/ir.cpp", "torch/csrc/jit/tensorexpr/ir.h", "torch/csrc/jit/tensorexpr/ir_mutator.cpp", "torch/csrc/jit/tensorexpr/ir_mutator.h", "torch/csrc/jit/tensorexpr/ir_printer.cpp", "torch/csrc/jit/tensorexpr/ir_printer.h", "torch/csrc/jit/tensorexpr/ir_visitor.cpp", "torch/csrc/jit/tensorexpr/ir_visitor.h", "torch/csrc/jit/tensorexpr/mem_arena.cpp", "torch/csrc/jit/tensorexpr/schedule.cpp", "torch/csrc/jit/tensorexpr/schedule.h", "torch/csrc/jit/tensorexpr/stmt.h", "torch/csrc/jit/tensorexpr/tensor.cpp", "torch/csrc/jit/tensorexpr/tensor.h", "torch/csrc/jit/tensorexpr/types.cpp", "torch/csrc/jit/tensorexpr/types.h", "torch/csrc/jit/tensorexpr/unique_name_manager.cpp", "torch/csrc/jit/tensorexpr/unique_name_manager.h"], "labels": ["jit", "merged"]}, "42b2c8c65d": {"title": "[TensorExpr] Add a fuser pass based on tensor expressions. (#34226)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34226\n\nLLVM and Cuda backends are added in subsequent PRs, so at this point the fuser is pretty useless, but it still can be tested and its logic is not going to change with addition of the codegens.\n\nDifferential Revision: D20251838\n\nTest Plan: Imported from OSS\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 82b0d221fa89904ed526689d02a6c7676a8ce8de", "pr_number": "34226", "files_changed": ["caffe2/CMakeLists.txt", "test/run_test.py", "test/test_tensorexpr.py", "tools/build_variables.bzl", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.h", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/tensorexpr/eval.cpp", "torch/csrc/jit/tensorexpr/eval.h", "torch/csrc/jit/tensorexpr/execution_counter.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["jit", "merged"]}, "35e7efeb9a": {"title": "[TensorExpr] Add CUDA codegen. (#34227)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34227\n\nThis PR adds a CUDA support to tensor expressions.\n\nDifferential Revision: D20251836\n\nTest Plan: Imported from OSS\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: ab36a55834cceff30c8371fef6cca1054a32f017", "pr_number": "34227", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/tensorexpr/gtest.cpp", "test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/tests.h", "test/test_tensorexpr.py", "tools/build_variables.bzl", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.h", "torch/csrc/jit/tensorexpr/cuda_half_support.h", "torch/csrc/jit/tensorexpr/cuda_random.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["jit", "merged"]}, "ea5c86c276": {"title": "[TensorExpr] Add LLVM codegen. (#34228)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34228\n\nThis PR adds LLVM codegen to tensor expressions. LLVM is added as an\noptional build dependency specified with `USE_LLVM=<path_to_llvm>`\nvariable. If this variable is not set or LLVM is not found in the\nspecified path, the LLVM codegen is completely disabled.\n\nDifferential Revision: D20251832\n\nTest Plan: Imported from OSS\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 77e203ab4421eb03afc64f8da17e0daab277ecc2", "pr_number": "34228", "files_changed": ["caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "test/cpp/tensorexpr/gtest.cpp", "test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/tests.h", "test/test_tensorexpr.py", "tools/build_variables.bzl", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.h", "torch/csrc/jit/tensorexpr/llvm_jit.cpp", "torch/csrc/jit/tensorexpr/llvm_jit.h"], "labels": ["jit", "merged"]}, "e93e7b2795": {"title": "[TensorExpr] Add tensorexpr benchmarks. (#34230)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34230\n\nThis PR adds some benchmarks that we used to assess tensor expressions performance.\n\nDifferential Revision: D20251830\n\nTest Plan: Imported from OSS\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: bafd66ce32f63077e3733112d854f5c750d5b1af", "pr_number": "34230", "files_changed": ["benchmarks/fastrnns/bench.py", "benchmarks/tensorexpr/benchmark.py", "benchmarks/tensorexpr/broadcast.py", "benchmarks/tensorexpr/conv.py", "benchmarks/tensorexpr/elementwise.py", "benchmarks/tensorexpr/framework.py", "benchmarks/tensorexpr/matmul.py", "benchmarks/tensorexpr/normalization.py", "benchmarks/tensorexpr/pooling.py", "benchmarks/tensorexpr/pt_engine.py", "benchmarks/tensorexpr/reduction.py", "benchmarks/tensorexpr/softmax.py", "benchmarks/tensorexpr/swish.py", "benchmarks/tensorexpr/tensor_engine.py"], "labels": ["merged"]}, "ef78fa8668": {"title": "caffe2::OperatorBase do not need to be aware of at::Tensor functions (#34810)", "body": "Summary:\nReplacing <ATen/core/Tensor.h> with <<ATen/core/TensorBody.h> speeds up compilation of caffe2 operators by 15%\nFor example, it reduces pool_op.cu compilation from 18.8s to 16s\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34810\n\nTest Plan: CI\n\nDifferential Revision: D20472230\n\nPulled By: malfet\n\nfbshipit-source-id: e1b261cc24ff577f09e2d5f6428be2063c6d4a8b", "pr_number": "34810", "files_changed": ["caffe2/core/operator.h"], "labels": ["merged"]}, "976d6aaa51": {"title": "Revert D20251830: [TensorExpr] Add tensorexpr benchmarks.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20251830\n\nOriginal commit changeset: bafd66ce32f6\n\nfbshipit-source-id: d8aea4b26441d8aba90c11d7350d3424df494052", "pr_number": null, "files_changed": ["benchmarks/fastrnns/bench.py", "benchmarks/tensorexpr/benchmark.py", "benchmarks/tensorexpr/broadcast.py", "benchmarks/tensorexpr/conv.py", "benchmarks/tensorexpr/elementwise.py", "benchmarks/tensorexpr/framework.py", "benchmarks/tensorexpr/matmul.py", "benchmarks/tensorexpr/normalization.py", "benchmarks/tensorexpr/pooling.py", "benchmarks/tensorexpr/pt_engine.py", "benchmarks/tensorexpr/reduction.py", "benchmarks/tensorexpr/softmax.py", "benchmarks/tensorexpr/swish.py", "benchmarks/tensorexpr/tensor_engine.py"], "labels": []}, "8bae1ed144": {"title": "PCA and SVD for low-rank matrices, LOBPCG for positive-defined generalized eigenvalue problem - copy (#34721)", "body": "Summary:\nThis is a copy of PR https://github.com/pytorch/pytorch/issues/29488 to help the merging process.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34721\n\nDifferential Revision: D20444270\n\nPulled By: vincentqb\n\nfbshipit-source-id: 042c56c8c0dae37834f52b4aee2deae7dd6fa659", "pr_number": "34721", "files_changed": ["docs/source/torch.rst", "test/test_torch.py", "torch/__init__.py", "torch/_linalg_utils.py", "torch/_lobpcg.py", "torch/_lowrank.py", "torch/_overrides.py", "torch/_six.py", "torch/functional.py", "torch/serialization.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged", "open source", "topic: linear algebra", "triaged"]}, "31eaeba38a": {"title": "Increase the prec of test_baddbmm (#34764)", "body": "Summary:\nThis test is flaky on my computer, the error is:\n```\nAssertionError: tensor(1.3351e-05) not less than or equal to 1e-05\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34764\n\nDifferential Revision: D20476006\n\nPulled By: ezyang\n\nfbshipit-source-id: dad7e702275346070552c8a98765c37e6ca2c197", "pr_number": "34764", "files_changed": ["test/test_torch.py"], "labels": ["merged", "open source"]}, "6d8649dc53": {"title": "[caffe2] fix Transpose2D calls in NHWC<->NCHW (#34625)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34625\n\nThese templated function calls are not specifying the template args correctly.  The first arg is the index type, not the array data type.  That means, right now it's using `T` as the index type as well, which will break if we do a template specialization for uint8_t.  If we omit both, it will correctly infer that the index type is `int` and the data type is `T`.\n\nReviewed By: BIT-silence\n\nDifferential Revision: D20358728\n\nfbshipit-source-id: 8cbd8eeb14bce602c02eb6fce2cc141f0121fa24", "pr_number": "34625", "files_changed": ["caffe2/utils/math/transpose.cc"], "labels": ["fb-exported", "merged"]}, "1bac5fd0d3": {"title": "add hardsigmoid FP operator to PyTorch (#34545)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34545\n\nThis is for common operator coverage, since this is widely used.  A future PR\nwill add the quantized version.\n\nSome initial questions for reviewers, since it's my first FP operator\ndiff:\n* do we need a backwards.out method for this?\n* do we need CUDA? If yes, should it be this PR or is it ok to split\n\nTest Plan:\n```\n// test\npython test/test_torch.py TestTorchDeviceTypeCPU.test_hardsigmoid_cpu_float32\n\n// benchmark\npython -m pt.hardsigmoid_test\n...\nForward Execution Time (us) : 40.315\n\nForward Execution Time (us) : 42.603\n```\n\nImported from OSS\n\nDifferential Revision: D20371692\n\nfbshipit-source-id: 95668400da9577fd1002ce3f76b9777c6f96c327", "pr_number": "34545", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/native_functions.yaml", "benchmarks/operator_benchmark/benchmark_all_other_test.py", "benchmarks/operator_benchmark/pt/hardsigmoid_test.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_overrides.py", "torch/nn/functional.py", "torch/nn/modules/__init__.py", "torch/nn/modules/activation.py"], "labels": ["merged"]}, "480d1849b0": {"title": "[ONNX] Fix for expand -1 dim value (#34069)", "body": "Summary:\nPyTorch expand allows size with -1 dim value. -1 dim value means to infer the dimension from input tensor. This can be exported to ONNX expand with 1 dim value since ONNX expand supports two-way broadcast.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34069\n\nReviewed By: hl475\n\nDifferential Revision: D20195532\n\nPulled By: houseroad\n\nfbshipit-source-id: c90e7d51b9d7422c09c5ed6e135ca8263105b8c9", "pr_number": "34069", "files_changed": ["test/onnx/expect/TestOperators.test_expand.expect", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged", "open source", "triaged"]}, "ae0c88d6aa": {"title": ".circleci: Add manywheel builds for python 3.8 (#34732)", "body": "Summary:\nNot entirely sure why this wasn't here before but we definitely need to\ntest for this.\n\nCloses https://github.com/pytorch/pytorch/issues/34727\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34732\n\nDifferential Revision: D20480508\n\nPulled By: seemethere\n\nfbshipit-source-id: 43bcff679ca35993f6bf1b10980acd7c86f780b1", "pr_number": "34732", "files_changed": [".circleci/cimodel/data/binary_build_data.py", ".circleci/config.yml"], "labels": ["merged"]}, "38b2856c71": {"title": "Split deserialize from runPythonUdf and remove generatePythonUDFResult (#34496)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34496\n\nDifferential Revision: D20347469\n\nTest Plan: Imported from OSS\n\nPulled By: mrshenli\n\nfbshipit-source-id: b832a3a9e2ef61f149175f737b26f65d63bf797b", "pr_number": "34496", "files_changed": ["torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp"], "labels": ["merged"]}, "d876fef743": {"title": "Fix send count for local RPC (#34809)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34809\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20470495\n\nPulled By: mrshenli\n\nfbshipit-source-id: 2d6e2a2889be07fb074443f05db5089291daf8cf", "pr_number": "34809", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": ["merged"]}, "422e348619": {"title": "Don't run user function until all UserRRefs in the args are confirmed (#34497)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34497\n\nUse a thread_local table to intercept UserRRefs created during user\nfunction args deserialization, and then wait for confirmations of\nthose UserRRefs before launching the given user function.\n\nDifferential Revision: D20347464\n\nTest Plan: Imported from OSS\n\nPulled By: mrshenli\n\nfbshipit-source-id: 087484a2d2f03fbfb156752ab25653f39b412a07", "pr_number": "34497", "files_changed": ["aten/src/ATen/core/rref_interface.h", "test/backward_compatibility/check_backward_compatibility.py", "tools/build_variables.bzl", "torch/CMakeLists.txt", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.h", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/message.cpp", "torch/csrc/distributed/rpc/message.h", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/distributed/rpc/request_callback.cpp", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/request_callback_impl.h", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/distributed/rpc/unpickled_python_call.cpp", "torch/csrc/distributed/rpc/unpickled_python_call.h", "torch/csrc/distributed/rpc/unpickled_python_remote_call.cpp", "torch/csrc/distributed/rpc/unpickled_python_remote_call.h", "torch/csrc/jit/runtime/register_distributed_ops.cpp", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "1e140c353c": {"title": "[profiler][rpc] fix a race condition in the profiler when multiple threads call (#33719)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33719\n\nWe were seeing a strange error where gathering profiler events (specifically `parse_cpu_trace` in `profiler.py`) would fail with the error:\n`IndexError: pop from empty list`.\n\nIt turned out that this was because for one particular `Event`, there was a pop recorded but not a push. Instead of the `push` event being completely missing, it was overwritten by a completely different event.\n\nAfter a bunch of debugging, and trying several hypotheses, it turns out that this was a race condition in `RangeEventList::record`. What happened was that different threads would call into `RangeEventList::record` on the same event list instance, and one record would stomp over the data written by the other one. Somehow the data written was a valid `Event` so the error did not manifest itself until the profiler realized a `pop` was missing a matching `push` in the python code.\n\nI fixed this by adding a lock to serialize writes to `RangeEventList::record`.\n\nThis PR also makes a small change to pass in the `RecordFunction` name into `popRange`. It makes the debugging easier when investigating the events recorded.\n\nDifferential Revision: D20071125\n\nfbshipit-source-id: 70b51a65bcb833a7c88b7462a978fd3a39265f7e", "pr_number": "33719", "files_changed": ["benchmarks/profiler_benchmark/profiler_bench.py", "torch/csrc/autograd/profiler.h"], "labels": ["merged"]}, "b336deb6ee": {"title": "[quant][mobile] Not use qnnpack max_pool2d if ceil_mode is true (#34844)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34844\n\nQNNPACK max_pool2d operator does not support ceil_mode so this can cause crashes in the kernel when it is set to true.\nWe default to the server implementation when ceil_mode is set to true\n\nTest Plan:\npython test/test_quantized.py\n\nImported from OSS\n\nDifferential Revision: D20478701\n\nfbshipit-source-id: 7962444ac493f5c3c32a9aa1a7be465e8b84ccc2", "pr_number": "34844", "files_changed": ["aten/src/ATen/native/quantized/cpu/qpool.cpp"], "labels": ["merged"]}, "471ddacd8b": {"title": "Add retry decorator and use it for Hub tests. (#34829)", "body": "Summary:\nfix https://github.com/pytorch/pytorch/issues/34751\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34829\n\nDifferential Revision: D20476231\n\nPulled By: ailzhang\n\nfbshipit-source-id: eb38ee655e28250352b15e8e37b3b39310a7c378", "pr_number": "34829", "files_changed": ["test/test_utils.py", "torch/testing/_internal/common_utils.py"], "labels": ["merged"]}, "e70c28856f": {"title": "[Caffe2] Move more method implementations from tensor.h to tensor.cc (#34811)", "body": "Summary:\nTo speed up compilation time\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34811\n\nTest Plan: CI\n\nDifferential Revision: D20476992\n\nPulled By: malfet\n\nfbshipit-source-id: 922cde93783fbfc04854851d7a05a635d5239792", "pr_number": "34811", "files_changed": ["caffe2/core/tensor.cc", "caffe2/core/tensor.h"], "labels": ["merged"]}, "89cbc0edea": {"title": "fix tests that could have racy script module instantiation (#34792)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34792\n\nit is not thread safe to initiate script module in multiple threads.\n\nfor both test_remote_script_module and test_torchscript_functions_not_supported, it is possible that client thread is initiating MyScriptModule while server thread is initiating it as well in the same rank process.\n\nremoving MyScriptModule instatiation in client thread, it is not needed actually.\nghstack-source-id: 100266609\n\nTest Plan: unit tests\n\nDifferential Revision: D20463234\n\nfbshipit-source-id: 6ff70ad90fa50b0b44c78df2495b4bcaabb4487b", "pr_number": "34792", "files_changed": ["torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["jit", "merged"]}, "699a4ed8f5": {"title": "[testing][do not land] (#34605)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34605\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20393219\n\nPulled By: jamesr66a\n\nfbshipit-source-id: c74d886f5f01061294203a002b72b75a3c446f09", "pr_number": "34605", "files_changed": ["test/test_jit.py"], "labels": ["merged"]}, "089a0a2117": {"title": "[torchbind] Test moving custom classes to/from IValue (#34847)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34847\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20480512\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 87f5f8ea8764e26d383b17e4f72538166ddd0655", "pr_number": "34847", "files_changed": ["test/cpp/jit/test_custom_class.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/frontend/script_type_parser.cpp"], "labels": ["jit", "merged"]}, "d9b97a4ffd": {"title": "[caffe2] open source 2/4-bit SLS operators (#34783)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34783\n\nMoving 2/4-bit SLS and row-wise 2/4-bit conversion operator to open source to be used by DLRM\n\nTest Plan: CI\n\nReviewed By: yinghai\n\nDifferential Revision: D20461609\n\nfbshipit-source-id: b3ef73ff10f2433afe06ffa73fe1145282d9ec4c", "pr_number": "34783", "files_changed": ["caffe2/operators/fused_rowwise_nbit_conversion_ops.cc", "caffe2/operators/fused_rowwise_nbit_conversion_ops.h", "caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc", "caffe2/operators/fused_rowwise_nbitfake_conversion_ops.h", "caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.cc", "caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h", "caffe2/python/benchmarks/fused_rowwise_nbit_conversion_bench.py", "caffe2/python/benchmarks/sparse_lengths_sum_nbit_benchmark.py", "caffe2/python/operator_test/fused_nbit_rowwise_conversion_ops_test.py", "caffe2/python/operator_test/fused_nbit_rowwise_test.cc", "caffe2/python/operator_test/fused_nbit_rowwise_test_helper.py", "caffe2/python/operator_test/lengths_reducer_fused_nbit_rowwise_ops_test.py"], "labels": ["fb-exported", "merged"]}, "76d9e76b4a": {"title": "Default to erroring when failing to return from non-void function. (#34663)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34663\n\nBeen bitten by this so many times.  Never more.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20425480\n\nPulled By: ezyang\n\nfbshipit-source-id: c4489efacc4149c9b57d1b8207cc872970c2501f", "pr_number": "34663", "files_changed": ["CMakeLists.txt"], "labels": ["merged"]}, "a8ca340ad6": {"title": "Remove all uses of AT_CHECK and replace them with TORCH_CHECK (#34846)", "body": "Summary:\nAT_CHECK has been deprecated and provides no more features than\nTORCH_CHECK\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34846\n\nDifferential Revision: D20481339\n\nPulled By: mrshenli\n\nfbshipit-source-id: 1777e769a069a78e03118270294e5e273d516ca7", "pr_number": "34846", "files_changed": ["aten/src/ATen/native/miopen/RNN_miopen.cpp", "c10/util/Exception.h", "torch/csrc/utils/tensor_new.cpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["merged", "open source"]}, "acbca57d18": {"title": "improve batch_norm contiguous case's performance (#34530)", "body": "Summary:\nFor batch_norm inference contiguous case, we can get a better performance by manually vectorize it.\nTest script:\n```                                                                                                   X\n import torch\n import torch.nn as nn\n import time\n\n torch.manual_seed(0)\n\n for n in [1, 10, 100]:\n     for c in [1, 10, 100]:\n         for hw in [1, 10, 200]:\n             m = nn.BatchNorm2d(c, affine=False)\n             m.eval()\n             input = torch.randn(20, c, hw, hw)\n             # warm up\n             for i in range(200):\n                 output = m(input)\n             fwd_t = 0\n             for j in range(1000):\n                 t1 = time.time()\n                 output = m(input)\n                 t2 = time.time()\n                 fwd_t = fwd_t + (t2 -t1)\n\n             fwd_avg = fwd_t / 1000 * 1000\n             print(\"size = (%d, %d, %d, %d); compute time is %.4f(ms)\" % (n, c, hw, hw, fwd_avg))\n```\n\nBefore:\n```\nsize = (1, 1, 1, 1); compute time is 0.0110(ms)\nsize = (1, 1, 10, 10); compute time is 0.0123(ms)\nsize = (1, 1, 200, 200); compute time is 0.8166(ms)\nsize = (1, 10, 1, 1); compute time is 0.0107(ms)\nsize = (1, 10, 10, 10); compute time is 0.0257(ms)\nsize = (1, 10, 200, 200); compute time is 8.7533(ms)\nsize = (1, 100, 1, 1); compute time is 0.0122(ms)\nsize = (1, 100, 10, 10); compute time is 0.1619(ms)\nsize = (1, 100, 200, 200); compute time is 123.5674(ms)\nsize = (10, 1, 1, 1); compute time is 0.0109(ms)\nsize = (10, 1, 10, 10); compute time is 0.0123(ms)\nsize = (10, 1, 200, 200); compute time is 0.5629(ms)\nsize = (10, 10, 1, 1); compute time is 0.0107(ms)\nsize = (10, 10, 10, 10); compute time is 0.0253(ms)\nsize = (10, 10, 200, 200); compute time is 8.7817(ms)\nsize = (10, 100, 1, 1); compute time is 0.0120(ms)\nsize = (10, 100, 10, 10); compute time is 0.1655(ms)\nsize = (10, 100, 200, 200); compute time is 123.2488(ms)\nsize = (100, 1, 1, 1); compute time is 0.0109(ms)\nsize = (100, 1, 10, 10); compute time is 0.0123(ms)\nsize = (100, 1, 200, 200); compute time is 0.5740(ms)\nsize = (100, 10, 1, 1); compute time is 0.0108(ms)\nsize = (100, 10, 10, 10); compute time is 0.0257(ms)\nsize = (100, 10, 200, 200); compute time is 8.7201(ms)\nsize = (100, 100, 1, 1); compute time is 0.0122(ms)\nsize = (100, 100, 10, 10); compute time is 0.1628(ms)\nsize = (100, 100, 200, 200); compute time is 123.1739(ms)\n```\nAfter:\n```\nsize = (1, 1, 1, 1); compute time is 0.0105(ms)\nsize = (1, 1, 10, 10); compute time is 0.0114(ms)\nsize = (1, 1, 200, 200); compute time is 0.5771(ms)\nsize = (1, 10, 1, 1); compute time is 0.0105(ms)\nsize = (1, 10, 10, 10); compute time is 0.0160(ms)\nsize = (1, 10, 200, 200); compute time is 6.9851(ms)\nsize = (1, 100, 1, 1); compute time is 0.0122(ms)\nsize = (1, 100, 10, 10); compute time is 0.0848(ms)\nsize = (1, 100, 200, 200); compute time is 98.6758(ms)\nsize = (10, 1, 1, 1); compute time is 0.0105(ms)\nsize = (10, 1, 10, 10); compute time is 0.0115(ms)\nsize = (10, 1, 200, 200); compute time is 0.2690(ms)\nsize = (10, 10, 1, 1); compute time is 0.0105(ms)\nsize = (10, 10, 10, 10); compute time is 0.0159(ms)\nsize = (10, 10, 200, 200); compute time is 6.6946(ms)\nsize = (10, 100, 1, 1); compute time is 0.0123(ms)\nsize = (10, 100, 10, 10); compute time is 0.0854(ms)\nsize = (10, 100, 200, 200); compute time is 98.7327(ms)\nsize = (100, 1, 1, 1); compute time is 0.0107(ms)\nsize = (100, 1, 10, 10); compute time is 0.0116(ms)\nsize = (100, 1, 200, 200); compute time is 0.2681(ms)\nsize = (100, 10, 1, 1); compute time is 0.0104(ms)\nsize = (100, 10, 10, 10); compute time is 0.0159(ms)\nsize = (100, 10, 200, 200); compute time is 6.7507(ms)\nsize = (100, 100, 1, 1); compute time is 0.0124(ms)\nsize = (100, 100, 10, 10); compute time is 0.0852(ms)\nsize = (100, 100, 200, 200); compute time is 98.6866(ms)\n```\nFor real modle Resnext101, we can also get **~20%** performance improvement for large batch size,\nTest script:\n```\n import torch\n import torchvision\n import torch\n import time\n\n torch.manual_seed(0)\n #torch.set_num_threads(1)\n\n model = torchvision.models.resnext101_32x8d().eval()\n\n for batch_size in [1, 64]:\n     input = torch.randn(batch_size, 3, 224, 224)\n     #warm up\n     with torch.no_grad():\n         for i in range(5):\n             output = model(input)\n\n         fwd_t = 0\n         for i in range(10):\n             t1 = time.time()\n             output = model(input)\n             t2 = time.time()\n             fwd_t = fwd_t + (t2 - t1)\n\n         time_fwd_avg = fwd_t / 10 * 1000\n         print(\"Throughput of resnext101 with batch_size = %d is %10.2f (imgs/s)\" % (batch_size, batch_size * 1000/              time_fwd_avg ))\n```\nBefore:\n```\nThroughput of resnext101 with batch_size = 1 is       7.89 (imgs/s)\nThroughput of resnext101 with batch_size = 64 is      13.02 (imgs/s)\n\nnum_threads =1\nThroughput of resnext101 with batch_size = 1 is       2.97 (imgs/s)\nThroughput of resnext101 with batch_size = 64 is       2.75 (imgs/s)\n```\nAfter:\n```\nThroughput of resnext101 with batch_size = 1 is       8.95 (imgs/s)\nThroughput of resnext101 with batch_size = 64 is      15.52 (imgs/s)\n\nnum_threads = 1\nThroughput of resnext101 with batch_size = 1 is       3.10 (imgs/s)\nThroughput of resnext101 with batch_size = 64 is       2.88 (imgs/s)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34530\n\nDifferential Revision: D20479560\n\nPulled By: ngimel\n\nfbshipit-source-id: 2e788ebcd814556116c90553ec61159eeffb3c16", "pr_number": "34530", "files_changed": ["aten/src/ATen/native/Normalization.cpp", "aten/src/ATen/native/batch_norm.h", "aten/src/ATen/native/cpu/batch_norm_kernel.cpp"], "labels": ["merged", "open source", "triaged"]}, "67cb018462": {"title": "Print cuda install logs for Windows CI (#34858)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/34821.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34858\n\nDifferential Revision: D20491248\n\nPulled By: ezyang\n\nfbshipit-source-id: c6ddd59197a7bce31c1a3ea5dc28b0ee95d5c216", "pr_number": "34858", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/pytorch-job-specs.yml"], "labels": ["merged", "open source"]}, "65889388d1": {"title": "Use randomtemp to resolve intermittent cuda build errors (#34777)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/25393.\nCore logic of randomtemp: https://github.com/peterjc123/randomtemp/blob/master/randomtemp/randomtemp.cpp\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34777\n\nDifferential Revision: D20491243\n\nPulled By: ezyang\n\nfbshipit-source-id: 76b0e1819ac1e3f760d5451197bd75ea13df1f0b", "pr_number": "34777", "files_changed": [".jenkins/pytorch/win-test-helpers/build_pytorch.bat"], "labels": ["merged", "open source"]}, "a0b7a39a92": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/litho/commit/eff7e6d11d0c5fb0a665832b2ff8c5e735b2f1dc\nhttps://github.com/pytorch/fbgemm/commit/7812ac2fa956b3c3edcb832f329fde433b79aff9\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: a3f94dd5b48240169296d773b2828cd97b0871dd", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "ecd7c0f84c": {"title": "[RPC] Use qualified name str directly in RPC torch script code path (#34733)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34733\n\nsimplify\nghstack-source-id: 100292435\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/jit/rpc_fork\\#binary.par \\\n-r test_return_local_script_class_rref_in_py_and_use_in_script\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc/jit:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/jit/rpc_fork\\#binary.par \\\n-r test_return_local_script_module_rref_in_py_and_use_in_script\n```\n\nDifferential Revision: D20442573\n\nfbshipit-source-id: 87f8b7d94adc03544f8e2955d01cd4702bb31a34", "pr_number": "34733", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "95833a49e6": {"title": "[TensorExpr] Pull changes from bertmaher/pytorch_fusion. (#34842)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34842\n\nThis PR (hopefully the last one of such kind) is merging changes from a\nside branch where tensor expessions based fuser work has been done so\nfar. This PR is is a squashed version of changes in the side branch,\nwhich is available here: https://github.com/bertmaher/pytorch\n\nDifferential Revision: D20478208\n\nTest Plan: Imported from OSS\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 21556e009f1fd88099944732edba72ac40e9b9c0", "pr_number": "34842", "files_changed": ["test/cpp/tensorexpr/test_aten.cpp", "test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/test_llvm.cpp", "test/cpp/tensorexpr/test_schedule.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tests.h", "test/te_utils.py", "test/test_tensorexpr.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/tensorexpr/DesignOverview.md", "torch/csrc/jit/tensorexpr/analysis.h", "torch/csrc/jit/tensorexpr/buffer.h", "torch/csrc/jit/tensorexpr/codegen.cpp", "torch/csrc/jit/tensorexpr/codegen.h", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.h", "torch/csrc/jit/tensorexpr/cuda_half_support.h", "torch/csrc/jit/tensorexpr/eval.cpp", "torch/csrc/jit/tensorexpr/eval.h", "torch/csrc/jit/tensorexpr/exceptions.h", "torch/csrc/jit/tensorexpr/execution_counter.h", "torch/csrc/jit/tensorexpr/expr.cpp", "torch/csrc/jit/tensorexpr/expr.h", "torch/csrc/jit/tensorexpr/function.cpp", "torch/csrc/jit/tensorexpr/function.h", "torch/csrc/jit/tensorexpr/hash_server.h", "torch/csrc/jit/tensorexpr/ir.cpp", "torch/csrc/jit/tensorexpr/ir.h", "torch/csrc/jit/tensorexpr/ir_mutator.cpp", "torch/csrc/jit/tensorexpr/ir_mutator.h", "torch/csrc/jit/tensorexpr/ir_printer.cpp", "torch/csrc/jit/tensorexpr/ir_printer.h", "torch/csrc/jit/tensorexpr/ir_simplifier.h", "torch/csrc/jit/tensorexpr/ir_visitor.cpp", "torch/csrc/jit/tensorexpr/ir_visitor.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.h", "torch/csrc/jit/tensorexpr/llvm_jit.cpp", "torch/csrc/jit/tensorexpr/llvm_jit.h", "torch/csrc/jit/tensorexpr/mem_arena.cpp", "torch/csrc/jit/tensorexpr/mem_arena.h", "torch/csrc/jit/tensorexpr/schedule.cpp", "torch/csrc/jit/tensorexpr/schedule.h", "torch/csrc/jit/tensorexpr/stmt.h", "torch/csrc/jit/tensorexpr/tensor.cpp", "torch/csrc/jit/tensorexpr/tensor.h", "torch/csrc/jit/tensorexpr/types.cpp", "torch/csrc/jit/tensorexpr/types.h", "torch/csrc/jit/tensorexpr/unique_name_manager.cpp"], "labels": ["jit", "merged"]}, "3e68d0c5d0": {"title": "Revert D20461609: [caffe2] open source 2/4-bit SLS operators", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20461609\n\nOriginal commit changeset: b3ef73ff10f2\n\nfbshipit-source-id: e90ee5e34b1feab5b0bd582ed7e96e37de7044b0", "pr_number": null, "files_changed": ["caffe2/operators/fused_rowwise_nbit_conversion_ops.cc", "caffe2/operators/fused_rowwise_nbit_conversion_ops.h", "caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc", "caffe2/operators/fused_rowwise_nbitfake_conversion_ops.h", "caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.cc", "caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h", "caffe2/python/benchmarks/fused_rowwise_nbit_conversion_bench.py", "caffe2/python/benchmarks/sparse_lengths_sum_nbit_benchmark.py", "caffe2/python/operator_test/fused_nbit_rowwise_conversion_ops_test.py", "caffe2/python/operator_test/fused_nbit_rowwise_test.cc", "caffe2/python/operator_test/fused_nbit_rowwise_test_helper.py", "caffe2/python/operator_test/lengths_reducer_fused_nbit_rowwise_ops_test.py"], "labels": []}, "cfab65d90d": {"title": "Fix CMake Dev warning in caffe2/CMakeLists.txt (#34886)", "body": "Summary:\nIf arguments of `ENDIF()` block are non-empty, they should match corresponding `IF()` BLOCK\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34886\n\nTest Plan: CI\n\nDifferential Revision: D20494631\n\nPulled By: malfet\n\nfbshipit-source-id: 5fed86239b4a0cb4b3aedd02c950c1b800199d2d", "pr_number": "34886", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["merged"]}, "3ad7dfa2cf": {"title": "move emulation libraries to contrib (#34861)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34861\n\nstart with unary ops\n\nTest Plan:\nbuck test //glow/fb/test/numerics/...\n\n```\n[hyz@devgpu019.snc1 ~/fbsource/fbcode/caffe2/caffe2/contrib] buck test //glow/fb/test/numerics/...\nAction graph will be rebuilt because files have been added or removed.\nParsing buck files: finished in 2.0 sec\nBuilding: finished in 9.8 sec (100%) 14826/14826 jobs, 23 updated\n  Total time: 11.9 sec\nTrace available for this run at /tmp/testpilot.20200316-143829.59858.log\nTestPilot test runner for Facebook. See https://fburl.com/testpilot for details.\nTestpilot build revision 7228e74a7f7e8e4934ab79a135930e665ca0e589 fbpkg e6db8251dbeb46b68a52a862744deff4 at Sun Mar  8 21:16:39 2020 by twsvcscm from /data/fbprojects/packages/testinfra.testpilot/795/t.par\n/proc/self/fd/4/__monkeytype_main_wrapper__.py:934: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\nDiscovering tests\nRunning 34 tests\nStarted new test run: https://our.intern.facebook.com/intern/testinfra/testrun/7036874425505432\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_slsw_all_one_tenth_mel_25 (glow.fb.test.numerics.test_operator_onnxifi.SLSTest) 0.000 1/34 (passed)\n      \u2713 glow/fb/test/numerics:test_batchnorm_nnpi_fp16nnpi - test_bn (glow.fb.test.numerics.test_batchnorm_nnpi_fp16.BatchnormTest) 1.974 2/34 (passed)\n      \u2713 glow/fb/test/numerics:test_fc_nnpi_fp16nnpi - test_clip (glow.fb.test.numerics.test_fc_nnpi_fp16.FCTest) 1.371 3/34 (passed)\n      \u2713 glow/fb/test/numerics:test_batchmatmul_nnpi_fp16nnpi - test_batch_matmul (glow.fb.test.numerics.test_batchmatmul_nnpi_fp16.TestBatchMatMul) 2.993 4/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_clip_graph (glow.fb.test.numerics.test_operator_onnxifi.CommonOpsTest) 0.536 5/34 (passed)\n      \u2713 glow/fb/test/numerics:test_numerics_nnpinnpi - test_accumulator_limits (glow.fb.test.numerics.test_numerics_nnpi.AccTest) 0.472 6/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_mat_mul_graph (glow.fb.test.numerics.test_operator_onnxifi.MatMulTest) 0.495 7/34 (passed)\n      \u2713 glow/fb/test/numerics:test_op_nnpi_fp16nnpi - test_tanh (glow.fb.test.numerics.test_op_nnpi_fp16.UnaryOpTest) 0.573 8/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_fc_graph (glow.fb.test.numerics.test_operator_onnxifi.FCTest) 0.793 9/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_concat_graph_sampe_shape (glow.fb.test.numerics.test_operator_onnxifi.ConcatTest) 0.441 10/34 (passed)\n      \u2713 glow/fb/test/numerics:test_sls_nnpi_fp16nnpi - test_small_sls (glow.fb.test.numerics.test_sls_nnpi_fp16.SparseLengthsSumTest) 0.463 11/34 (passed)\n      \u2713 glow/fb/test/numerics:test_op_nnpi_fp16nnpi - test_add_graph (glow.fb.test.numerics.test_op_nnpi_fp16.ArithmeticOpsTest) 0.772 12/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_fp16fc_graph (glow.fb.test.numerics.test_operator_onnxifi.Fp16FCTest) 0.481 13/34 (passed)\n      \u2713 glow/fb/test/numerics:test_fc_nnpi_fp16nnpi - test_fc_exercise (glow.fb.test.numerics.test_fc_nnpi_fp16.FCTest) 0.495 14/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_tanh_graph (glow.fb.test.numerics.test_operator_onnxifi.TanhTest) 0.538 15/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_add_graph (glow.fb.test.numerics.test_operator_onnxifi.ArithmeticOpsTest) 0.517 16/34 (passed)\n      \u2713 glow/fb/test/numerics:test_fc_nnpi_fp16nnpi - test_fc_numeric_cases (glow.fb.test.numerics.test_fc_nnpi_fp16.FCTest) 0.555 17/34 (passed)\n      \u2713 glow/fb/test/numerics:test_op_nnpi_fp16nnpi - test_sub_graph (glow.fb.test.numerics.test_op_nnpi_fp16.ArithmeticOpsTest) 0.692 18/34 (passed)\n      \u2713 glow/fb/test/numerics:test_op_nnpi_fp16nnpi - test_sigmoid (glow.fb.test.numerics.test_op_nnpi_fp16.UnaryOpTest) 1.038 19/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_sigmoid_graph (glow.fb.test.numerics.test_operator_onnxifi.SigmoidTest) 0.530 20/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_div_graph (glow.fb.test.numerics.test_operator_onnxifi.ArithmeticOpsTest) 0.590 21/34 (passed)\n      \u2713 glow/fb/test/numerics:test_sls_4bit_nnpi_fp16nnpi - test_slws_fused_4bit_rowwise_all_same (glow.fb.test.numerics.test_sls_4bit_nnpi_fp16.SparseLengthsSumTest) 0.607 22/34 (passed)\n      \u2713 glow/fb/test/numerics:test_op_nnpi_fp16nnpi - test_div_graph (glow.fb.test.numerics.test_op_nnpi_fp16.ArithmeticOpsTest) 0.583 23/34 (passed)\n      \u2713 glow/fb/test/numerics:test_op_nnpi_fp16nnpi - test_mul_graph (glow.fb.test.numerics.test_op_nnpi_fp16.ArithmeticOpsTest) 0.803 24/34 (passed)\n      \u2713 glow/fb/test/numerics:test_numerics_nnpinnpi - test_accumulator_simple (glow.fb.test.numerics.test_numerics_nnpi.AccTest) 0.484 25/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_slws_fused_8bit_rowwise_length1_graph (glow.fb.test.numerics.test_operator_onnxifi.SLSTest) 9.069 26/34 (passed)\n      \u2713 glow/fb/test/numerics:test_sls_nnpi_fp16nnpi - test_slws_fused_8bit_rowwise_intel2 (glow.fb.test.numerics.test_sls_nnpi_fp16.SparseLengthsSumTest) 1.741 27/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_mul_graph (glow.fb.test.numerics.test_operator_onnxifi.ArithmeticOpsTest) 0.902 28/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_sub_graph (glow.fb.test.numerics.test_operator_onnxifi.ArithmeticOpsTest) 0.678 29/34 (passed)\n      \u2713 glow/fb/test/numerics:test_sls_nnpi_fp16nnpi - test_slws_fused_8bit_rowwise_all_same (glow.fb.test.numerics.test_sls_nnpi_fp16.SparseLengthsSumTest) 0.726 30/34 (passed)\n      \u2713 glow/fb/test/numerics:test_fc_nnpi_fp16nnpi - test_fc_num0 (glow.fb.test.numerics.test_fc_nnpi_fp16.FCTest) 1.621 31/34 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_slws_fused_8bit_rowwise_graph (glow.fb.test.numerics.test_operator_onnxifi.SLSTest) 10.121 32/34 (passed)\n     \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - test_gather_graph (glow.fb.test.numerics.test_operator_onnxifi.CommonOpsTest) 99.675 33/34 (passed)\n      \u2713 glow/fb/test/numerics:fp16_op_test - FP16Test.4BitFusedSLS_NNPI 0.156 34/34 (passed)\n      {emoji:2702} glow/fb/test/numerics:fp16_op_test - FP16Test.4BitFusedSLS_Interpreter 0.000 (OMITTED)\nTest output:\n> This test was disabled.\n> To run this test locally, add the command line flag --run-disabled to your test command (prefix with -- if using buck).\n> To view why this is disabled or re-enable this test in the test console, visit https://our.intern.facebook.com/intern/testinfra/testdetail/281474992503783\n      \u2713 glow/fb/test/numerics:fp16_op_test - main 3.986 (passed)\n      \u2713 glow/fb/test/numerics:test_numerics_nnpinnpi - main 12.606 (passed)\n      \u2713 glow/fb/test/numerics:test_sls_nnpi_fp16nnpi - main 12.622 (passed)\n      \u2713 glow/fb/test/numerics:test_fc_nnpi_fp16nnpi - main 12.688 (passed)\n      \u2713 glow/fb/test/numerics:test_operator_onnxifinnpi - main 12.688 (passed)\n      \u2713 glow/fb/test/numerics:test_batchnorm_nnpi_fp16nnpi - main 12.744 (passed)\n      \u2713 glow/fb/test/numerics:test_batchmatmul_nnpi_fp16nnpi - main 12.763 (passed)\n      \u2713 glow/fb/test/numerics:test_op_nnpi_fp16nnpi - main 12.800 (passed)\n      \u2713 glow/fb/test/numerics:test_sls_4bit_nnpi_fp16nnpi - main 13.034 (passed)\nFinished test run: https://our.intern.facebook.com/intern/testinfra/testrun/7036874425505432\nSummary (total time 134.18s):\n  PASS: 43\n  FAIL: 0\n  SKIP: 0\n  FATAL: 0\n  TIMEOUT: 0\n  OMIT: 1\n    glow/fb/test/numerics:fp16_op_test - FP16Test.4BitFusedSLS_Interpreter\n\n```\n\nReviewed By: yinghai\n\nDifferential Revision: D20471053\n\nfbshipit-source-id: 0bd8e69fbb843a02dc031f45a060aa78c602b42c", "pr_number": "34861", "files_changed": ["caffe2/contrib/fakelowp/common.cc", "caffe2/contrib/fakelowp/unary_fp16_fake_op.cc", "caffe2/contrib/fakelowp/unary_fp16_fake_op.h"], "labels": ["fb-exported", "merged"]}, "7a3cf67fd8": {"title": "Implement channels last upsample2d/3d forward pass kernel. (#34597)", "body": "Summary:\nThi PR implement channel last upsampling nearest for 2D/3D.\nThis is supposed to be faster, plus, avoids converting formats going in\nand out of operator.\nWill post benchmarking numbers.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34597\n\nTest Plan: python test/test_nn.py TestNN.test_upsamplingNearest3d_channels_last\n\nDifferential Revision: D20390583\n\nPulled By: kimishpatel\n\nfbshipit-source-id: e0162fb97604a261887f38fc957d3f787c80954e", "pr_number": "34597", "files_changed": ["aten/src/ATen/native/UpSampleNearest2d.cpp", "aten/src/ATen/native/cpu/UpSampleKernel.cpp", "test/test_nn.py"], "labels": ["merged"]}, "940e678da9": {"title": "Add back cudaHostRegister to cudart API. (#34665)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34665\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20493861\n\nPulled By: ezyang\n\nfbshipit-source-id: 4215e3037a16be460f20cfc2859be5ee074128d3", "pr_number": "34665", "files_changed": ["torch/csrc/cuda/shared/cudart.cpp"], "labels": ["merged"]}, "275f5c8049": {"title": "setup.py: Add numpy as required for install_requires (#34510)", "body": "Summary:\nWas originally not a requirement but we should add it back here since\nit's required on import and we require it anyways for our conda\npackages.\n\nTested with:\n\n```\n\u276f pkginfo -f requires_dist *.whl\nrequires_dist: ['numpy']\n```\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34510\n\nDifferential Revision: D20352125\n\nPulled By: seemethere\n\nfbshipit-source-id: 383e396fe500ed7043d83c3df57d1772d0fff1e6", "pr_number": "34510", "files_changed": ["setup.py"], "labels": ["merged", "releng"]}, "6b701de130": {"title": "Add types argument to __torch_function__ (#34303)", "body": "Summary:\nThis PR adds the `types` argument to `__torch_function__` as per RFC 0001: https://github.com/pytorch/rfcs/pull/3\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34303\n\nDifferential Revision: D20474992\n\nPulled By: ezyang\n\nfbshipit-source-id: cdd40b3b38f3bda4ece8812a629f5db87e919d01", "pr_number": "34303", "files_changed": ["docs/source/notes/extending.rst", "test/test_overrides.py", "torch/_overrides.py", "torch/csrc/utils/python_arg_parser.cpp"], "labels": ["merged", "module: numpy", "open source", "triaged"]}, "a4224886f3": {"title": "Eliminate guards through max_pool ops. (#34512)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34512\n\nDifferential Revision: D20478962\n\nPulled By: resistor\n\nfbshipit-source-id: 86fc926305f95cae8b334ed344d8e0cdd1ef7b2b", "pr_number": "34512", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/guard_elimination.cpp"], "labels": ["jit", "merged"]}, "5857a125df": {"title": "Turn on exact_dtype by default on test_optim.py (#34825)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34825\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20498111\n\nPulled By: great-way\n\nfbshipit-source-id: e689ca40c496b6b4cccb0df30bdae89b2c024f31", "pr_number": "34825", "files_changed": ["test/test_optim.py"], "labels": ["merged"]}, "b227ea955e": {"title": ".circleci: Remove should_run_job, no longer needed (#34326)", "body": "Summary:\nDone at the recommendation of ezyang\n\nTODO:\n\n- [x] Sync `XImportant`\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34326\n\nDifferential Revision: D20496786\n\nPulled By: seemethere\n\nfbshipit-source-id: 8c84e097d81db28d7dcda8720973bce77f6eb4f7", "pr_number": "34326", "files_changed": [".circleci/cimodel/data/caffe2_build_data.py", ".circleci/cimodel/data/pytorch_build_data.py", ".circleci/config.yml", ".circleci/scripts/should_run_job.py", ".circleci/scripts/should_run_job.sh", ".circleci/verbatim-sources/binary-job-specs.yml", ".circleci/verbatim-sources/caffe2-job-specs.yml", ".circleci/verbatim-sources/commands.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/pytorch-job-specs.yml", ".circleci/verbatim-sources/workflows-binary-builds-smoke-subset.yml", ".circleci/verbatim-sources/workflows-pytorch-android-gradle-build.yml"], "labels": ["merged", "module: ci"]}, "72e3d66f50": {"title": "[ROCm] Fix for std::isnan regression in ROCm (#34664)", "body": "Summary:\nFiling this PR since we are in the process of migrating ROCm CI to ROCm version 3.1. This patch is to ensure the correct functionality of float <-> bfloat16 conversion in rocm3.1. `std::isnan` regresses with rocm3.1.\n\niotamudelta ezyang\n\ncc: ashishfarmer (original author of this patch)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34664\n\nDifferential Revision: D20440972\n\nPulled By: ezyang\n\nfbshipit-source-id: 1ccb911c88f05566d94e01878df6c70cf7f31242", "pr_number": "34664", "files_changed": ["c10/util/BFloat16.h"], "labels": ["merged", "open source"]}, "959a7138fd": {"title": "Support RowWiseSparseAdam on GPU (#34341)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34341\n\nImplement RowWiseSparseAdam on CUDA\n\nReviewed By: xianjiec\n\nDifferential Revision: D20289209\n\nfbshipit-source-id: a7a8a21bd18c1b9891f04f202d3ecaf183e30cad", "pr_number": "34341", "files_changed": ["caffe2/python/operator_test/adam_test.py", "caffe2/sgd/adam_op.cc", "caffe2/sgd/adam_op_gpu.cu"], "labels": ["fb-exported", "merged"]}, "0216c76e12": {"title": "SNIFAE Template Constructors of IValue (#34647) (#34843)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34843\n\nCurrently, we use not_ok_to_boxing to filter Dimname that can not be\nconverted/constructed to IValue. The correct way should be SNIFAE the\nconstructor of IValue.\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\nPyTorch compiled after the code change.\n\nAll unit test passed\n\nImported from OSS\n\nDifferential Revision: D20494886\n\nfbshipit-source-id: 91dfba6a41a3ae2d6ceba9d4124cbf612ea3f080", "pr_number": "34843", "files_changed": ["aten/src/ATen/core/boxing/boxing.h", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h"], "labels": ["merged"]}, "97757dca79": {"title": "Format register_ditributed_ops.cpp (#34922)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34922\n\nformat\n\nTest Plan: `\n\nDifferential Revision: D7717743\n\nfbshipit-source-id: 207bd46a6b0579adbd35f6417af239ec717c7a41", "pr_number": "34922", "files_changed": ["torch/csrc/jit/runtime/register_distributed_ops.cpp"], "labels": ["jit", "merged"]}, "58c5b6d306": {"title": "adds quantized implementation of hard sigmoid (#34607)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34607\n\nAdds quantized version of hardsigmoid activation.\n\nNote: not implementing the _ and .out versions is\ncurrently intended, because the implementation changes the scale and\nzp and it's nice to not allow the user to specify scale\nand zp.  Lmk if we should handle this differently.\n\nTest Plan:\ntests\nbenchmarks\n\nImported from OSS\n\nDifferential Revision: D20480546\n\nfbshipit-source-id: 9febcb44afd920125ed2ca4900492f0b712078ea", "pr_number": "34607", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "benchmarks/operator_benchmark/pt/qactivation_test.py", "test/test_quantized.py", "torch/nn/quantized/functional.py"], "labels": ["merged", "quantization"]}, "71f02a481b": {"title": "[RPC] Avoid polluting Python root logger on importing \"torch\" module (#34871)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34871\n\nWe used to configure root logger in RPC module. A stream handler is added to `root.handlers`. This is not desired behavior for pytorch users. We should instead keep the root logger handler list untouched.\n\nWe can configure the logger local to the rpc module, set it's log level, so it doesn't use it's ancestor, which is usually the root which has no stream handlers in most cases.\nhttps://docs.python.org/3/library/logging.html#logging.Logger.setLevel\n\nAnd add a stream handler to make it output to stdout, even if the root handlers is not configured and has an empty list.\nhttps://docs.python.org/3/library/logging.html#logging.Logger.addHandler\nhttps://docs.python.org/3/library/logging.handlers.html#logging.StreamHandler\nghstack-source-id: 100322141\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork\n\nbuck build mode/dev-nosan //caffe2/test/distributed/rpc:rpc_fork \\\n&& buck-out/gen/caffe2/test/distributed/rpc/rpc_fork\\#binary.par -r test_wait_all_workers\n```\n\nDifferential Revision: D7677493\n\nfbshipit-source-id: 88a66079e7348c79a7933e3527701917cbebb7ba", "pr_number": "34871", "files_changed": ["torch/distributed/rpc/api.py"], "labels": ["merged"]}, "841f7600bb": {"title": "[quant][graphmode] Quantization pattern for aten::linear (#33854)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33854\n\natt\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D20493031\n\nfbshipit-source-id: bafd0a3ba5d07327d451b3915f043db33b012b53", "pr_number": "33854", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization_patterns.h"], "labels": ["jit", "merged"]}, "f87cd83d11": {"title": "Append multiple arguments to list of flags as multiple items (#34899)", "body": "Summary:\nThis makes PyTorch compileable(but not linkable) with `CUDA_SEPARABLE_COMPILATION` option enabled.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34899\n\nTest Plan: CI\n\nDifferential Revision: D20501050\n\nPulled By: malfet\n\nfbshipit-source-id: 02903890a827fcc430a26f397d4d05999cf3a441", "pr_number": "34899", "files_changed": ["CMakeLists.txt", "cmake/Dependencies.cmake"], "labels": ["merged"]}, "0d857d55b9": {"title": "Add a warning for RRef serialization (#34884)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34884\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20491278\n\nPulled By: mrshenli\n\nfbshipit-source-id: fd00701fd0090639ffe392f40610426c78bc9269", "pr_number": "34884", "files_changed": ["torch/csrc/distributed/rpc/init.cpp"], "labels": ["merged"]}, "6446ccce76": {"title": "Adding warnings for async Tensor serialization in remote and rpc_async (#34885)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34885\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20491279\n\nPulled By: mrshenli\n\nfbshipit-source-id: 8c861e7c7e9ea39f9427f80bc4e75c72c0087366", "pr_number": "34885", "files_changed": ["torch/distributed/rpc/api.py"], "labels": ["merged"]}, "800bdcf000": {"title": "Removing experimental tag in for RPC and adding experimental tag for RPC+TorchScript (#34887)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34887\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20491409\n\nPulled By: mrshenli\n\nfbshipit-source-id: ce79c9706eb70a3a52a4032de4f0bd538b694332", "pr_number": "34887", "files_changed": ["docs/source/notes/distributed_autograd.rst", "docs/source/notes/rref.rst", "docs/source/rpc.rst"], "labels": ["merged"]}, "3c48aadd98": {"title": "Update descriptions for transmitting CUDA tensors (#34888)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34888\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20491408\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4ca35ac9edd4c1af4f2bae2cfb0f1f6060658d5c", "pr_number": "34888", "files_changed": ["docs/source/rpc.rst"], "labels": ["merged"]}, "552f9d3a68": {"title": "Minor fixes for RPC API docs (#34890)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34890\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20491788\n\nPulled By: mrshenli\n\nfbshipit-source-id: 95a9821d70e0afe51f586b891845b3106c7105ce", "pr_number": "34890", "files_changed": ["torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "e87db8a77b": {"title": "Fix example format in Distributed Autograd doc (#34914)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34914\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20500015\n\nPulled By: mrshenli\n\nfbshipit-source-id: 55715fd1ffce143952d3f6ffcf60ee83ade0efb4", "pr_number": "34914", "files_changed": ["torch/csrc/distributed/autograd/init.cpp"], "labels": ["merged"]}, "f05abd1259": {"title": "Fix example block format in Distributed Optimizer API doc (#34919)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34919\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20500013\n\nPulled By: mrshenli\n\nfbshipit-source-id: d28cbdd1ec207e1e8501ce389b7040fb764f12ca", "pr_number": "34919", "files_changed": ["torch/distributed/optim/optimizer.py"], "labels": ["merged"]}, "85c51a8c10": {"title": "Fix dist autograd context Example block format (#34921)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34921\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20500012\n\nPulled By: mrshenli\n\nfbshipit-source-id: 6c81123ad347726032c29630d7bf58feb6d8c5fd", "pr_number": "34921", "files_changed": ["torch/distributed/autograd/__init__.py"], "labels": ["merged"]}, "e43c2d59dd": {"title": "Reduce memory overhead of categorical.sample (#34900)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/34714 (using the discussed solution). Thanks to jjabo for flagging and suggesting this.\n\nInstead of expanding `probs` to prepend `sample_shape`, it is better  to use the `num_samples` argument to `torch.multinomial` instead, which is faster and consumes lesser memory.\n\nExisting tests should cover this. I have profiled this on different inputs and the change results in faster `.sample` (e.g. 100X faster on the example in the issue), or at worst is similar to what we have now with the default `sample_shape` argument.\n\ncc. fritzo, alicanb, ezyang\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34900\n\nDifferential Revision: D20499065\n\nPulled By: ngimel\n\nfbshipit-source-id: e5be225e3e219bd268f5f635aaa9bf7eca39f09c", "pr_number": "34900", "files_changed": ["torch/distributions/categorical.py"], "labels": ["merged", "module: distributions", "open source"]}, "74a28ff1dd": {"title": "Make checkInputs more robust (#34838)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34838\n\nDifferential Revision: D20500828\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 7eff720dff2698423f3e65b3809ff6f598f936d7", "pr_number": "34838", "files_changed": ["torch/csrc/jit/passes/guard_elimination.cpp"], "labels": ["jit", "merged"]}, "4bd3e9b41b": {"title": "fix barrier in jit test (#34901)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34901\n\ninit_pg is needed for dist.barrier call, otherwise default process group may not be found for some rpc backend\nghstack-source-id: 100319642\n\nTest Plan: unit  test\n\nDifferential Revision: D20495321\n\nfbshipit-source-id: a44241bd2ff6e1404eee9b241270a94e9fd114d0", "pr_number": "34901", "files_changed": ["torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["jit", "merged"]}, "1c8e086537": {"title": "[quant][graphmode][refactor] Change QParamMap to QParamVector (#34314)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34314\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D20493032\n\nfbshipit-source-id: fd945b861ae08e1d97f154aa2b1fb3099761882b", "pr_number": "34314", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "ff3d205ee5": {"title": "[rpc] handle exceptions in ProcessGroupAgent::enqueueRecv (#34413)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34413\n\nIn this diff we have made various improvements to ProcessGroupAgent in order to accomodate edge and error cases such as a \"non-clean\" shutdown (shutdowns in which we abort RPC as quickly as possible, and don't wait for all pending work across all RPC agents to be completed):\n\n1. Catch and log exceptions in `enqueueRecv`. This prevents us from calling `std::terminate()` in a different thread and logs an error message indicating the issue. With this we no longer have crashes caused by exceptions in this thread during non-graceful shutdown.\n\n2. Provide cleaner error messages everywhere (and use `c10::str` where possible). One example is in `agent::send()`.\n\n3. Add the ability to abort pending sends that cause blocking waits in `handleSend`. The reason we need to abort this is since during a non-graceful shutdown, we could become blocked waiting for these since there is no guarantee the remote end is still active and this would result in a long wait and eventual timeout. We abort these by adding them to a map, and go through this map during `shutdown()`.\n\n4. Fix flaky tests: `test_handle_send_exceptions` and `test_backward_node_failure` and `test_backward_node_failure_python_udf`. These tests were flaky since they dealt with non-graceful shutdown of workers which has chances for a bunch of edge cases explained above.\n\nWe have also refactored `createExceptionResponse`, `enqueueRecv`, and some test functions for the above reasons in this diff.\n\nFor testing:\nEnsured that the tests are no longer flaky with 500 tests runs. Previously, these tests were flaky and disabled. Also added a unit test in the internal `ProcessGroupAgentTest.cpp`.\nghstack-source-id: 100311598\n\nTest Plan: Ensured that the tests are no longer flaky with 500 tests runs. Previously, these tests were flaky and disabled. Also added a unit test in the internal `ProcessGroupAgentTest.cpp`.\n\nReviewed By: mrshenli\n\nDifferential Revision: D20269074\n\nfbshipit-source-id: de9cad7f7185f9864ffbb6b14cd8ca9f6ff8f465", "pr_number": "34413", "files_changed": ["torch/csrc/distributed/autograd/context/container.cpp", "torch/csrc/distributed/rpc/message.cpp", "torch/csrc/distributed/rpc/message.h", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/testing/_internal/dist_utils.py", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "95f1cb34b9": {"title": "Revert D20480546: adds quantized implementation of hard sigmoid", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD20480546\n\nOriginal commit changeset: 9febcb44afd9\n\nfbshipit-source-id: 4461b455e63448cf45237e23c988b492c3e0f1b0", "pr_number": null, "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "benchmarks/operator_benchmark/pt/qactivation_test.py", "test/test_quantized.py", "torch/nn/quantized/functional.py"], "labels": []}, "b5edf329f8": {"title": "[JIT] Make RPC RRef Owner WorkerInfo.name available to TorchScript (#34896)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34896\n\nMake TorchScript support calling ref.owner() to get owner worker id and calling ref.owner_name() to get owner worker name.\n\nDifferential Revision: D7652208\n\nfbshipit-source-id: a60125bb316ac2cf19a993cbd2affc933c0af7c9", "pr_number": "34896", "files_changed": ["aten/src/ATen/core/rref_interface.h", "test/backward_compatibility/check_backward_compatibility.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/jit/runtime/register_distributed_ops.cpp", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["jit", "merged"]}, "c4fdba326d": {"title": "Support using self as the destination in rpc.remote for builtin operators (#34931)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34931\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20503571\n\nPulled By: mrshenli\n\nfbshipit-source-id: ed1454a349798b18b9953bbf13c86bc43d3b559d", "pr_number": "34931", "files_changed": ["torch/csrc/distributed/rpc/python_functions.cpp", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["merged"]}, "09a7788a2f": {"title": "[torchbind] Improve IValue custom class API and remove most Capsule stuff (#34848)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34848\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20480514\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 1c595faf34e00aab0a6202a8902426bd310551c3", "pr_number": "34848", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "test/cpp/jit/test_custom_class.cpp", "torch/csrc/jit/python/pybind_utils.h", "torch/custom_class.h"], "labels": ["jit", "merged"]}, "130e720784": {"title": "[torchbind] Add more comprehensive docscrings (#34906)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34906\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20496221\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 3863ec77324564f6f0f1c54b0cbd6c29d12f3c74", "pr_number": "34906", "files_changed": ["torch/custom_class.h"], "labels": ["merged"]}, "df20f5b374": {"title": "Updating submodules", "body": "Summary:\nGitHub commits:\n\nhttps://github.com/facebook/mcrouter/commit/70331595ce14c5624dbe1ad583e8ba53e78b08ea\nhttps://github.com/pytorch/fbgemm/commit/51ae830b0055cd3f074f7f1507d2a8d445d1a1a8\n\nTest Plan: n/a\n\nReviewed By: zpao\n\nfbshipit-source-id: 045a70a24059fc1120d54d5b85ffe0e2831d2161", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "d7e4a379a0": {"title": "[C++ API Parity] LBFGS optimizer step() update and added closure to the Optimizer step() function (#34564)", "body": "Summary:\nFollow-ups after this PR:\n\n* Remove `LossClosureOptimizer`, and merge `Optimizer` into `OptimizerBase` (and rename the merged class to Optimizer)\n* Merge the LBFGS-specific serialize test function and the generic `test_serialize_optimizer` function, possibly by passing a bool `has_only_global_state` flag into the `test_serialize_optimizer` function to denote whether `size()` should be equal to 1 or 2?\n    * https://github.com/pytorch/pytorch/pull/34564#discussion_r393780303\n* It seems that we don't have the equivalent `XORConvergence_LBFGS` test like the other optimizers, and it would be good to add one\n* Remove mentions of `parameters_` in optimizer.cpp, de-virtualize all functions, and remove the `OptimizerBase(std::vector<Tensor> parameters)` constructor from `OptimizerBase`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34564\n\nTest Plan: Imported from GitHub, without a `Test Plan:` line.\n\nDifferential Revision: D20495701\n\nPulled By: anjali411\n\nfbshipit-source-id: 6d35286d2decb6f7dff93d9d3e57515770666622", "pr_number": "34564", "files_changed": ["test/cpp/api/optim.cpp", "test/cpp/api/optim_baseline.h", "test/cpp/api/optim_baseline.py", "test/cpp/api/serialize.cpp", "torch/csrc/api/include/torch/optim/lbfgs.h", "torch/csrc/api/include/torch/optim/optimizer.h", "torch/csrc/api/include/torch/optim/serialize.h", "torch/csrc/api/include/torch/utils.h", "torch/csrc/api/src/optim/adam.cpp", "torch/csrc/api/src/optim/lbfgs.cpp", "torch/csrc/api/src/optim/optimizer.cpp", "torch/csrc/api/src/optim/rmsprop.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "bcbdba450c": {"title": "[caffe2] open source 2/4-bit SLS operators (#34903)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34903\n\nReattempt of D20461609\n\nMoving 2/4-bit SLS and row-wise 2/4-bit conversion operator to open source to be used by DLRM\n\nTest Plan: CI\n\nReviewed By: jianyuh\n\nDifferential Revision: D20495304\n\nfbshipit-source-id: 66a99677583f50fd40e29c514710c7b1a8cdbc29", "pr_number": "34903", "files_changed": ["caffe2/operators/fused_rowwise_nbit_conversion_ops.cc", "caffe2/operators/fused_rowwise_nbit_conversion_ops.h", "caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc", "caffe2/operators/fused_rowwise_nbitfake_conversion_ops.h", "caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.cc", "caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h", "caffe2/python/benchmarks/fused_rowwise_nbit_conversion_bench.py", "caffe2/python/benchmarks/sparse_lengths_sum_nbit_benchmark.py", "caffe2/python/operator_test/fused_nbit_rowwise_conversion_ops_test.py", "caffe2/python/operator_test/fused_nbit_rowwise_test.cc", "caffe2/python/operator_test/fused_nbit_rowwise_test_helper.py", "caffe2/python/operator_test/lengths_reducer_fused_nbit_rowwise_ops_test.py"], "labels": ["fb-exported", "merged"]}, "b7129050e7": {"title": "Makes floor_divide a method, adds sparse floor division (#34552)", "body": "Summary:\n(Updated per review feedback)\n\n`torch.floor_divide` is currently a function that can operate on two tensors or a tensor and a scalar (scalar x scalar floor division is handled natively by Python and the JIT has a builtin function for it). This PR updates it to:\n\n- have an out variant: `floor_divide(x, y, out=z)`\n- be a method on a tensor: `x.floor_divide(y)`\n- have an in-place variant: `x.floor_divide_(y)`\n- work with sparse tensors\n\nTests are added to test_sparse.py and test_torch.py for these new behaviors.\n\nIn addition, this PR:\n\n- cleans up the existing sparse division and true_division code and improves their error message\n- adds testing of sparse true_division to test_sparse.py\n- extends existing floor_divide testing in test_torch to run on CUDA, too, not just the CPU\n\nUnfortunately, making floor_divide a method requires breaking backwards compatibility, and floor_divide has been added to the BC whitelist since this is international. The BC issue is that the first parameter name to torch.floor_divide is changing from input to self. If you previously called torch.floor_divide with keyword arguments, e.g. torch.floor_divide(input=x, other=y), you will need to update to torch.floor_divide(self=x, other=y), or the more common torch.floor_divide(x, y).\n\nThe intent of this PR is to allow floor_divide to be substituted for division (torch.div, /) wherever division was previously used. In 1.6 we expect torch.div to perform true_division, and floor_divide is how users can continue to perform integer division with tensors.\n\nThere are two potential follow-up issues suggested by this PR:\n\n- the test framework might benefit from additional tensor construction classes, like one to create dividends and divisors for multiple dtypes\n- the test framework might benefit from a universal function test class. while methods have reasonable coverage as part of test_torch.py's TestTensorOp tests, function coverage is spotty. Universal functions are similar enough it should be possible to generate tests for them.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34552\n\nDifferential Revision: D20497453\n\nPulled By: mruberry\n\nfbshipit-source-id: ac326f2007d8894f730d1278fef84d63bcb07b5d", "pr_number": "34552", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "docs/source/tensors.rst", "test/backward_compatibility/check_backward_compatibility.py", "test/test_sparse.py", "test/test_torch.py", "test/test_type_promotion.py", "tools/pyi/gen_pyi.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/utils/python_arg_parser.cpp"], "labels": ["merged", "topic: bc-breaking"]}, "a3de359464": {"title": "Do not throw from CUDAContext destructor (#34756)", "body": "Summary:\nThrowing from destructor leads to undefined behaviour (most often to segault)\nSo it's better to leak memory then segault\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34756\n\nTest Plan: Run `test_pytorch_onnx_caffe2`\n\nDifferential Revision: D20504228\n\nPulled By: malfet\n\nfbshipit-source-id: 7a05776fea9036f602e95b8182f8493cb5886dab", "pr_number": "34756", "files_changed": ["caffe2/core/context_gpu.cu", "caffe2/core/context_gpu.h"], "labels": ["merged"]}, "3585451469": {"title": "Release notes scripts", "body": "", "pr_number": null, "files_changed": ["release_notes/auto_categorize.py", "release_notes/categorize.py", "release_notes/common.py", "release_notes/create_data_json.py", "release_notes/create_filelist.sh", "release_notes/requirements.txt"], "labels": []}}