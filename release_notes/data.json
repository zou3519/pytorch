{"168570b0da": {"title": "move module_save.cpp to non-mobile build section in cmake (#30221)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30221\n\nPR #29881 moved Module::save() methods to a separate source file\nand removed C10_MOBILE gating logic. Seems it should stay with\nexport_module.cpp (which is in \"NOT INTERN_BUILD_MOBILE\" section).\nOtherwise it causes link error with build_mobile.sh.\n\nTest:\n- build locally\n- check CI\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18649234\n\nPulled By: ljk53\n\nfbshipit-source-id: b6c90a532d191c41ce10c1047a869d8f73854c4d", "pr_number": "30221", "files_changed": ["caffe2/CMakeLists.txt"], "labels": []}, "aa1e99e983": {"title": "Fix two links in RPC API doc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30259\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18644749\n\nPulled By: mrshenli\n\nfbshipit-source-id: ff515d2588cd59e0d87f020a01885156a6644450", "pr_number": "30259", "files_changed": ["docs/source/rpc.rst"], "labels": []}, "4609c626c5": {"title": "Enable test_call_method_on_rref in rpc_test (#30261)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30261\n\nWith #29827, the flakiness should disappear for test_call_method_on_rref\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18645036\n\nPulled By: mrshenli\n\nfbshipit-source-id: 44d759062fc78b1a797266096dbb4ddd104f07eb", "pr_number": "30261", "files_changed": ["test/rpc_test.py"], "labels": []}, "e5fc86130a": {"title": "Remove unnecessary linker flags from JNI host build (#30206)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30206\n\n- --whole-archive isn't needed because we link libtorch as a dynamic\n  dependency, rather than static.\n- --gc-sections isn't necessary because most (all?) of the code in our\n  JNI library is used (and we're not staticly linking libtorch).\n  Removing this one is useful because it's not supported by lld.\n\nTest Plan:\nBuilt on Linux.  Library size was unchanged.\nUpcoming diff enables Mac JNI build.\n\nDifferential Revision: D18653500\n\nPulled By: dreiss\n\nfbshipit-source-id: 49ce46fb86a775186f803ada50445b4b2acb54a8", "pr_number": "30206", "files_changed": ["android/pytorch_android/CMakeLists.txt"], "labels": []}, "30874b31a9": {"title": "Enable JNI build on Mac host (#30207)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30207\n\nThis should work now that we're not using gold-specific linker flags.\n\nTest Plan: CI\n\nDifferential Revision: D18653521\n\nPulled By: dreiss\n\nfbshipit-source-id: 31c3cdbefc37b87bfb4140ffbac781131fe72ab3", "pr_number": "30207", "files_changed": [".circleci/scripts/binary_populate_env.sh", ".jenkins/pytorch/build.sh"], "labels": []}, "a822a1d2a8": {"title": "Avoid overwriting output type in onnx graph (#25906)", "body": "Summary:\nWhen creating the onnx graph, we overwrite the output type with the output type of the PT graph.\nIn some special cases, when using scripting, the PT graph does not have type information. We want to avoid overwriting the input type is these cases.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25906\n\nReviewed By: hl475\n\nDifferential Revision: D18645903\n\nPulled By: houseroad\n\nfbshipit-source-id: 56acc43e0c15c74ac8ebd689e04f7371054e362e", "pr_number": "25906", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx.cpp", "torch/onnx/symbolic_opset9.py"], "labels": ["jit", "module: autograd", "module: internals", "module: onnx", "module: pybind", "open source", "triaged"]}, "73c9e6e6b6": {"title": "Rename function parameters to avoid [-Werror,-Wshadow] (#30276)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30276\n\n### Summary\n\nWhen building PyTorch for iOS in BUCK, the compiler complains about the ivar shadowing\n\n```\n/Users/taox/fbsource/xplat/caffe2/aten/src/ATen/core/dispatch/Dispatcher.h:184:144: error: declaration shadows a field of 'c10::Dispatcher' [-Werror,-Wshadow]\ninline Return Dispatcher::doCallUnboxed(const DispatchTable& dispatchTable, const LeftRight<ska::flat_hash_map<TensorTypeId, KernelFunction>>& backendFallbackKernels_, Args... args) const {\n                                                                                                                                               ^\n/Users/taox/fbsource/xplat/caffe2/aten/src/ATen/core/dispatch/Dispatcher.h:134:63: note: previous declaration is here\n  LeftRight<ska::flat_hash_map<TensorTypeId, KernelFunction>> backendFallbackKernels_;\n```\nThis happens because the internal iOS compiler enforces the `[-Werror, -Wshadow]` on every source file when compiling. Say in `benchmark.mm` we import `<torch/script.h>`, then it'll  leads all the way to `Dispatcher.h`\n\n```\n In file included from Apps/Internal/PyTorchPlayground/PyTorchPlayground/Application/Benchmark/Benchmark.mm:6:\nIn file included from /Users/taox/fbsource/xplat/caffe2/aten/src/ATen/ATen.h:5:\nIn file included from /Users/taox/fbsource/xplat/caffe2/aten/src/ATen/Context.h:4:\nIn file included from /Users/taox/fbsource/xplat/caffe2/aten/src/ATen/Tensor.h:12:\nIn file included from buck-out/cells/fbsource/gen/xplat/caffe2/TensorMethods.h/TensorMethods.h:10:\n/Users/taox/fbsource/xplat/caffe2/aten/src/ATen/core/dispatch/Dispatcher.h\n```\nIt'd be better to have a separate name for function parameters.\n\ncc shoumikhin\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18649116\n\nPulled By: xta0\n\nfbshipit-source-id: 19f50b7a23c11dedcafc2ac2d85b45ae4999be2f", "pr_number": "30276", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": []}, "5e19460ced": {"title": "cache tensor scalar_type in OperandInfo (#30065)", "body": "Summary:\nCaches result of `scalar_type` call in TensorIterator and TensorOptions, because the call is expensive.\nThis removes 120 - 150 ns of overhead (from 1.25 us to 1.12 us for out-of-place case, from 0.86 us to 0.73 us for inplace case)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30065\n\nTest Plan: Covered by existing tests\n\nDifferential Revision: D18576236\n\nPulled By: ngimel\n\nfbshipit-source-id: 17f63851a911fc572c2146f8a520b7f0dadfd14a", "pr_number": "30065", "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "c10/core/TensorOptions.h"], "labels": ["fb-exported"]}, "23650671a8": {"title": "add_hparams() NoneType error (#30286)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30286\n\nadd_hparams() in torch.utils.tensorboard.writer produced the following error\npython3.7/site-packages/torch/utils/tensorboard/writer.py\", line 294, in add_hparams\n    with SummaryWriter(log_dir=os.path.join(self.file_writer.get_logdir(), str(time.time()))) as w_hp:\nAttributeError: 'NoneType' object has no attribute 'get_logdir'\nOther methods such as add_scalar() and add_histogram() use self._get_file_writer() instead of self.file_writer directly.\n\nTest Plan:\n```\nwriter = summary_writer()\nwriter.add_hparams({\"a\": 0, \"b\": 0}, {\"hparam/test_accuracy\": 0.5}))\nwriter.flush()\nwriter.close()\n```\n\nReviewed By: J0Nreynolds, sanekmelnikov\n\nDifferential Revision: D18650610\n\nfbshipit-source-id: 1039dd2067d37913a8a131c8b372491a63154899", "pr_number": "30286", "files_changed": ["test/test_tensorboard.py", "torch/utils/tensorboard/writer.py"], "labels": ["fb-exported"]}, "48b943960e": {"title": "Add bfloat16 support in linear algebra on ROCm (#27719)", "body": "Summary:\nThis adds support for gemm-style matrix multiplications with data and output in bf16 to PyTorch on ROCm to the backend (i.e., bgemm).\n\nEnable operators depending on bgemm.\n\nWith this change, bf16 matrices on ROCm can be multiplied on the GPU.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27719\n\nDifferential Revision: D18653514\n\nPulled By: bddppq\n\nfbshipit-source-id: 805db923579bec6fc8fd1c51eeb5b1ef85a96758", "pr_number": "27719", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "aten/src/THC/THCBlas.cu", "aten/src/THC/THCBlas.h", "aten/src/THC/THCTensorMath.h", "aten/src/THC/THCTensorMathBlas.cu", "aten/src/THC/generic/THCTensorMathBlas.cu", "test/test_torch.py"], "labels": ["module: cublas", "module: cuda", "module: internals", "module: operators", "module: rocm", "open source", "triaged"]}, "1690feba9f": {"title": "add mobile build CI with host toolchain (#30292)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30292\n\nWe already have CI jobs to build Android/iOS libraries, but there are\ntwo issues:\n- It's no easy for people who are not regularly working on mobile to debug\nthese CI errors as they need setup Android/iOS build environment;\n- It's hard to run cross-compiled mobile libraries as it requires\nemulator. It happened a couple times recently that it can build but fail\nto load and run a model with mobile build.\n\nTo address these problems, create this new CI job to build mobile\nlibrary with linux host toolchain so that we can build & test without\ninvolving Android/iOS environment/simulator. Will add tests on top of it next.\n\nTest Plan: - check the new CI job\n\nDifferential Revision: D18654074\n\nPulled By: ljk53\n\nfbshipit-source-id: eb1baee97a7b52c44979dbf1719c3357e08f895e", "pr_number": "30292", "files_changed": [".circleci/config.yml", ".circleci/generate_config_yml.py", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml", ".jenkins/pytorch/build.sh"], "labels": []}, "f5ef3a6fb6": {"title": "disable JIT optimizer in Android wrapper for mobile custom build (#30285)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30285\n\nPR #30144 introduced custom build script to tailor build to specific\nmodels. It requires a list of all potentially used ops at build time.\n\nSome JIT optimization passes can transform the IR by replacing\noperators, e.g. decompose pass can replace aten::addmm with aten::mm if\ncoefficients are 1s.\n\nDisabling optimization pass can ensure that the list of ops we dump from\nthe model is the list of ops that are needed.\n\nTest Plan: - rerun the test on PR #30144 to verify the raw list without aten::mm works.\n\nDifferential Revision: D18652777\n\nPulled By: ljk53\n\nfbshipit-source-id: 084751cb9a9ee16d8df7e743e9e5782ffd8bc4e3", "pr_number": "30285", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "binaries/speed_benchmark_torch.cc"], "labels": []}, "95b451d386": {"title": "fixing test_tensorboard for py2 (#30298)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30298\n\nThis diff fixes test_tensorboard for python2:\n- proto serialization is different in py2 vs py3 (e.g. for bytes) -> simple string comparison will fail for test_pytorch_graph. Modified to make graph comparison field by field\n\nReviewed By: J0Nreynolds\n\nDifferential Revision: D18654691\n\nfbshipit-source-id: fdbca32e9a7fc2ea70a040bb825eab8a48d0dfe4", "pr_number": "30298", "files_changed": ["test/test_tensorboard.py"], "labels": ["fb-exported"]}, "a074080d57": {"title": "Mark `c10d::~NCCLUtils` as noexcept (#29118)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29118\n\nIt's never a good idea to throw from a destructor and per #28288 we\ncan't use `std::make_shared` on a class with a `noexcept(false)`\ndestructor.\n\nTo fix this, we `abort` instead of throw from the `NCCLComm` destructor.\n\nCloses #28288.\nghstack-source-id: 93182910\n\nTest Plan: ProcessGroupNCCLErrorsTest runs successfully.\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D18298271\n\nfbshipit-source-id: ccac37753fef64fb63cb304433f4f97dc5621379", "pr_number": "29118", "files_changed": ["torch/lib/c10d/NCCLUtils.hpp"], "labels": ["module: distributed"]}, "29887f813a": {"title": "Remove unused forward declaration (#30154)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30154\n\nThis doesn't seem to be used in thread_pool.cpp.\nghstack-source-id: 94264158\n\nTest Plan: Let's see if this compiles.\n\nDifferential Revision: D18614141\n\nfbshipit-source-id: c6ff3db56b55fcee7d8123d909ee275690163ece", "pr_number": "30154", "files_changed": ["c10/core/thread_pool.h"], "labels": []}, "183aa1534f": {"title": "Add --no_python flag (#29144)", "body": "Summary:\nAllows you to use a bash script wrapper in-between launch and your\ntraining script. e.g.\n```\npython -m torch.distributed.launch --nproc_per_node=8 --no_python --use_env \\\n    bash -c 'exec numactl --cpunodebind=$(( LOCAL_RANK / 4 )) \"$@\"' -- \\\n    python train.py ...\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29144\n\nDifferential Revision: D18345647\n\nPulled By: pietern\n\nfbshipit-source-id: f05849c38c82de782988d07d300e00cf9f37253a", "pr_number": "29144", "files_changed": ["torch/distributed/launch.py"], "labels": ["module: distributed"]}, "faacbfa8bf": {"title": "Migrate index_add cpu from TH to ATen (#28421)", "body": "Summary:\nMigrate index_add cpu from TH to ATen.\n\nI couldn't find replacement for get1d and set1d, so doing pointer arithmetic inplace.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28421\n\nTest Plan: existing tests\n\nDifferential Revision: D18060971\n\nPulled By: ggoossen\n\nfbshipit-source-id: 413719990cdb2fe578964cde14e93577e48a4342", "pr_number": "28421", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.h", "test/test_torch.py"], "labels": []}, "3455231e9c": {"title": "Expose configuration of Numa directories to setup.py (#30104)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/29968\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30104\n\nDifferential Revision: D18656882\n\nPulled By: ezyang\n\nfbshipit-source-id: f932a98674033f1a3184dc1c22faa6f8c2b50134", "pr_number": "30104", "files_changed": ["tools/setup_helpers/cmake.py"], "labels": ["module: build"]}, "f41422121e": {"title": "default construct rpc agent options based on the backend type (#30201)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30201\n\nProvide a default constructor so that users don't have to construct\nRPC agent options. Also rename this to RPCBackend Options as suggested.\nghstack-source-id: 94411768\n\nTest Plan: Unit tests pass.\n\nDifferential Revision: D18628698\n\nfbshipit-source-id: 81fb45f124ad1006e628f6045162308093c9d446", "pr_number": "30201", "files_changed": ["test/dist_utils.py", "test/rpc_agent_test_fixture.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py", "torch/distributed/rpc/backend_registry.py"], "labels": []}, "559b3b5a7a": {"title": "Use unboxed registration for most of operators used in lite interpreter. (#30239)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30239\n\nUse unboxed registration per smessmer 's request. For some ops with optional arg or tensor list that unboxed registration are not supported, we still use boxed.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18653846\n\nPulled By: iseeyuan\n\nfbshipit-source-id: c22ce8111dfff0ba63316a9bcfe2b712b2d31fc1", "pr_number": "30239", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py", "torch/csrc/jit/mobile/register_mobile_ops.cpp"], "labels": ["jit"]}, "c478a92b93": {"title": "Add local shutdown to process group agent (#30020)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30020\nThis is now possible due to previous changes made in `gloo` and `ProcessGroupGloo`. We `abort` the listener thread that is waiting for a message, and join all other threads. The destructor calls this same `localShutdown` method, but we ensure this is not called multiple times.\n\nghstack-source-id: 94415336\n\nTest Plan: Unit tests pass.\n\nDifferential Revision: D5578006\n\nfbshipit-source-id: 6258879fb44c9fca97fdfad64468c1488c16ac02", "pr_number": "30020", "files_changed": ["docs/source/notes/distributed_autograd.rst", "docs/source/rpc.rst", "test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/api.py"], "labels": []}, "5d7b2089e8": {"title": "Draft version: Make AliasAnalysisKind optional in Op Registration API (#30187)", "body": "Summary:\nDon't look into deep into the diff's implementation. The reason to send out this diff is to help sync on the design first. Once we agree on the design, I will update the implementation accordingly.\n\n**Here is the basic design for achieving this functionality:**\n\n**Q1: Do we need to tell apart case between the following:**\ncase 1:  registry 1: PURE -> registry 2: CONSERVATIVE\ncase 2:  registry 1: PURE -> registry 2: <not set>\n\nA: should be yes though, right now both cases have same value(due to defaulting to CONSERVATIVE) in operators_ and operatorLookupTable_.\ncase 1 should be denied while case 2 should be legal case where registry 1 will be PURE at the end.\n\n**How to tell apart both cases:**\n\nRight now, AliasAnalysisKind::CONSERVATIVE is by default (code pointer: https://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/caffe2/aten/src/ATen/core/dispatch/OperatorOptions.h?lines=22%2C52)\n\nCurrent approach: Introducing a boolean flag in OperatorOptions called isDefault, defaulting to value true. When manually call setAliasAnalysis(AliasAnalysisKind), it will be set too false.\nAnd then when findSchema() in Dispatcher.cpp,  we will check response's option's isDefault value.\nIf isDefault = true, then with some sanity check and if all checks passed, we can update the option info in both operators_ and operatorLookupTable_\n\nOther approaches:\n1. Introducing a new AliasAnalaysisKind maybe called NOT_SPECIFIED.  (I am not doing it this way since then I need to update other callosities related to AliasAnalaysisKind::CONSERVATIVE) Also, we will need to have additional logics to align between NOT_SPECIFIED and CONSERVATIVE\n\n**What data to be updated:**\ncorresponding entry in std::list<OperatorDef> operators_ and LeftRight<ska::flat_hash_map<OperatorName, OperatorHandle>> operatorLookupTable_\n\n(More things to be discussed here.)\n\n**Do we need to trigger listeners if an entry get updated:**\nI think no.\ncallOnOperatorRegistered(op) seems only to be using OperatorHandle.schema now from the only callsite from register_c10_ops.cpp\n(code pointers: https://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/caffe2/aten/src/ATen/core/dispatch/Dispatcher.cpp?commit=b4cefeaa98dca5b1ec5f7a0bca6028e368960244&lines=87-90\nand https://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/caffe2/torch/csrc/jit/register_c10_ops.cpp?lines=178&link_ref=biggrep)\n\nHowever, things can be much more complicated if future extensions may use options when some listeners want to use options value to register operators.\n\n**Future reading list + remaining questions:**\n1. How options get consumed on the other side.\n2. Usages for fields in OperatorEntry besides schema/options/kernals\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30187\n\nTest Plan:\n[xintchen@devvm6308.prn2 ~/fbsource/fbcode] buck test mode/dev //caffe2:ATen-core-test\n\nAll tests passed\n\nDifferential Revision: D18530964\n\nPulled By: charliechen0401\n\nfbshipit-source-id: 60c0560a63a36e54f09f397667bb7122b61d6a8e", "pr_number": "30187", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/dispatch/OperatorOptions.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp"], "labels": ["fb-exported"]}, "7903fb118f": {"title": "Move qkv_same, kv_same into branch (#30142)", "body": "Summary:\nPerf improvements to multi_head_attention_forward\n\n- qkv_same and kv_same were not used outside of that branch. Further, kv_same was calculated even though it is not used if qkv_same\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30142\n\nDifferential Revision: D18610938\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 19b7456f20aef90032b0f42d7da8c8a2d5563ee3", "pr_number": "30142", "files_changed": ["torch/nn/functional.py"], "labels": []}, "fa242246ee": {"title": "add unit tests to iOS CI jobs (#30133)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30133\n\n### Summary\n\nRecently we've found that the master branch was constantly broken due to some unwanted change being landed on mobile. The problem is that our CI was not able to detect the runtime errors.\n\n### Previous work\n\n- Add an unit test target to the iOS TestApp ( #29962 )\n- Update Fastlane to run tests ( #29963 )\n\n### What's been changed in CI\n\n1. XCode version has been updated to 11.2.1\n2. For iOS simulator build, we'll run some unit tests( currently only one) after the build test.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18641413\n\nPulled By: xta0\n\nfbshipit-source-id: 12942206f1dee045b2addba3ae618760e992752c", "pr_number": "30133", "files_changed": [".circleci/config.yml", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-nightly-ios-binary-builds.yml", ".circleci/verbatim-sources/workflows-pytorch-ios-builds.yml", "ios/TestApp/README.md", "ios/TestApp/TestApp.xcodeproj/project.pbxproj", "ios/TestApp/TestAppTests/TestAppTests.mm", "ios/TestApp/benchmark/setup.rb"], "labels": []}, "65f465050b": {"title": "Dont use SubgraphRewriter in FoldQuantizeCallIntoBuffer", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30264\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18645531\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 44fc0f0a3c8cabe62924baae0d556e43bbf637ec", "pr_number": "30264", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "1cc321deed": {"title": "Memoize parseIR calls in graph mode quantization", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30188\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18625743\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 88f9da8e79324ba91e3550a8fc1a05e85bb83a86", "pr_number": "30188", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "97fae401f0": {"title": "Use LinearPackedParams everywhere", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30198\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18628003\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 76ff0248fd859e805a15cde555d26dd2138636fa", "pr_number": "30198", "files_changed": ["test/test_jit.py", "test/test_quantized_nn_mods.py", "torch/nn/intrinsic/quantized/modules/linear_relu.py", "torch/nn/quantized/dynamic/modules/linear.py", "torch/nn/quantized/modules/linear.py", "torch/quantization/_quantize_script.py"], "labels": []}, "638f4c1fb3": {"title": "Update Cocoapods to 1.4.0 (#30326)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30326\n\nNote that this PR won't trigger the cocoapods build. We'll push the binary and release the cocoapods after the branch cut.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18660308\n\nPulled By: xta0\n\nfbshipit-source-id: 95dd97b7b67e70ecee3a65d8bbc125791872b7ca", "pr_number": "30326", "files_changed": ["ios/LibTorch.podspec"], "labels": []}, "a5272cb643": {"title": "Error instead of assertion failure for div by sparse (#30260)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30260\n\nfixes: https://github.com/pytorch/pytorch/issues/30044\n\nWithout this PR,\n\n```\n>>> torch.tensor(1.) / torch.tensor(1.).to_sparse()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: r.is_sparse() INTERNAL ASSERT FAILED at /Users/distiller/project/conda/conda-bld/pytorch_1570710797334/work/aten/src/ATen/native/sparse/SparseTensorMath.cpp:168, please report a bug to PyTorch.\n```\n\nTest Plan:\nRan the same code with this change:\n\n```\nIn [1]: import torch\nIn [2]: torch.tensor(1).to_sparse() / torch.tensor(1).to_sparse()\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-2-7177f54f30bb> in <module>\n----> 1 torch.tensor(1).to_sparse() / torch.tensor(1).to_sparse()\n\nRuntimeError: Unsupported tensor layout\n```\n\nDifferential Revision: D18657387\n\nPulled By: nairbv\n\nfbshipit-source-id: cd23570d46f5b26fd84049e5e63b61b19835603d", "pr_number": "30260", "files_changed": ["aten/src/ATen/native/sparse/SparseTensorMath.cpp", "test/test_sparse.py"], "labels": []}, "35e6c1763e": {"title": "Switch Docker image onda-cuda-cxx11-ubuntu1604 to new uniform name (#29943)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29943\n\nThis was apparently the same as \"pytorch/pytorch-binary-docker-image-ubuntu16.04:latest\",\nso standardize on that name.\n\nTest Plan:\nThis PR, which is stacked on top of a commit that puts one of the jobs\nusing that container into the set of PR builds.\n\nImported from OSS\n\nDifferential Revision: D18653554\n\nfbshipit-source-id: 40e6c52db02265d61e8166bb1211376faccfc53a", "pr_number": "29943", "files_changed": [".circleci/cimodel/data/binary_build_definitions.py", ".circleci/config.yml"], "labels": []}, "0c18de2623": {"title": "Add inferBoundShapeOp", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30101\n\nReviewed By: ipiszy\n\nDifferential Revision: D18387803\n\nfbshipit-source-id: 5edb6b949257370b62fa6da477bd6ed2f16a9bd1", "pr_number": "30101", "files_changed": ["caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h", "caffe2/proto/metanet.proto", "caffe2/proto/predictor_consts.proto"], "labels": ["fb-exported"]}, "90cb1e67ff": {"title": "Fix exception message in Java Tensor", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30205\n\nTest Plan: Imported from OSS\n\nReviewed By: linbinyu\n\nDifferential Revision: D18653568\n\nPulled By: dreiss\n\nfbshipit-source-id: a5fcb809eba641a7fbd0e99e835eceeb248e680c", "pr_number": "30205", "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/Tensor.java"], "labels": []}, "8c6f0c0587": {"title": "Detect TorchScript archives in torch.load (#29339)", "body": "Summary:\nThis PR looks for a `constants.pkl` file at the top level in a zip file\nin `torch.load`. If found, it calls `torch.jit.load` instead and issues\na warning to call `torch.jit.load` directly\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29339\n\nDifferential Revision: D18611095\n\nPulled By: driazati\n\nfbshipit-source-id: f070a02f6b5509054fc3876b3e8356bbbcc183e1", "pr_number": "29339", "files_changed": ["caffe2/serialize/inline_container.cc", "caffe2/serialize/inline_container.h", "test/test_jit.py", "torch/csrc/jit/init.cpp", "torch/serialization.py"], "labels": ["jit"]}, "2e709763a3": {"title": "add wrapper to exclude XLA when running device tests", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30316\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18659286\n\nPulled By: nairbv\n\nfbshipit-source-id: 86d035bb0c54c612868590c3188cfcd969c3f686", "pr_number": "30316", "files_changed": ["test/common_device_type.py"], "labels": []}, "99a2a0b1ca": {"title": "Implement torch.diagonal for named tensors (#30193)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30193\n\nFeaturing:\n- Added a NoNamesGuard::reset() function that sets NamesMode back to\nwhat it was before the guard. This makes it so that we don't have to\ncreate a new context to run code in an unnamed way.\n- Added a diagonal(Tensor, *, Dimname outdim, Dimname dim1, Dimname dim2, int64_t offset=0)\noverload. All of the non-tensor arguments are keyword only for\nreadability purposes; something like `tensor.diagonal(\"A\", \"B\", \"C\")`\nwould be really confusing.\n\nTest Plan: - Added new tests\n\nDifferential Revision: D18638363\n\nPulled By: zou3519\n\nfbshipit-source-id: ea37b52a19535f84a69be38e95e569e88f307381", "pr_number": "30193", "files_changed": ["aten/src/ATen/NamedTensorUtils.cpp", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/core/NamedTensor.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_namedtensor.py"], "labels": []}, "6c9b188262": {"title": "Support in-place update in IndexHashOp (#30275)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30275\n\n`IndexHash` did not support in-place update.\n\nReviewed By: kennyhorror\n\nDifferential Revision: D18612231\n\nfbshipit-source-id: adeccdf1ceb6107454555ff9cdf66fd5e5773f2a", "pr_number": "30275", "files_changed": ["caffe2/operators/index_hash_ops.cc", "caffe2/python/operator_test/index_hash_ops_test.py"], "labels": ["fb-exported"]}, "ac103a5d78": {"title": "Remove variable wrapping from register_c10_ops (#29207)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29207\n\nThe logic calling c10 ops from JIT did some variable wrapping to make sure all results are always variables.\nThanks to ezyang, this is not needed anymore because everything is a variable now.\nghstack-source-id: 93345590\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18327507\n\nfbshipit-source-id: 86512c5e19d6972d70f125feae172461c25e3cb6", "pr_number": "29207", "files_changed": ["torch/csrc/jit/register_c10_ops.cpp"], "labels": ["jit"]}, "c7f988b8c6": {"title": "transport open registration (#30167)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30167\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29164\n\n- Created GlooDeviceFactory to hide device creation details\n- Added transport option while on Python interface\n\nThe reason of making the factory class is to make it easier to extend gloo transport in the future\n\nTest Plan: Imported from OSS\n\nReviewed By: satgera, d4l3k\n\nDifferential Revision: D18596527\n\nfbshipit-source-id: e8114162ee8d841c0e0769315b48356b37d6ca0a", "pr_number": "30167", "files_changed": ["torch/lib/c10d/CMakeLists.txt", "torch/lib/c10d/GlooDeviceFactory.cpp", "torch/lib/c10d/GlooDeviceFactory.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["fb-exported"]}, "6a00191fc2": {"title": "Add RpcAgent::getWorkerInfos() (#30241)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30241\n\nWe need an API to get all worker infos. This will be used by backend-agnostic `rpc.wait_all_workers()` API.\nghstack-source-id: 94454935\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork -- test_get_worker_infos\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_get_worker_infos\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_get_worker_infos\n\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_get_worker_infos\n```\n\nDifferential Revision: D5693412\n\nfbshipit-source-id: 5123c8248b6d44fd36b8a5f381dbabb2660e6f0f", "pr_number": "30241", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h"], "labels": []}, "328ec5460f": {"title": "refactor the observer removal and quantize tensor", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30360\n\nDifferential Revision: D18670373\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: 1481d6e4d5ce40376577b8deb0a0f74d5559076e", "pr_number": "30360", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "ee20e66c48": {"title": "replace the SLSRQ for their right emulations in the replayer test (#30367)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30367\n\nuse the SLS emulations that match the hardware\n\nTest Plan: replayer test\n\nDifferential Revision: D18667605\n\nfbshipit-source-id: 89aee630184737b86ecfb09717437e5c7473e42c", "pr_number": "30367", "files_changed": ["caffe2/opt/custom/fakefp16_transform.cc"], "labels": ["fb-exported"]}, "2a7a39c1af": {"title": "(de)serialization of values between C++ and Python (#30108)", "body": "Summary:\nThis PR updates `torch::pickle_save` to use the new zipfile format introduced in #29232 and adds `torch::pickle_load` which can decode the zipfile format. Now that `torch.save/load` use this format as well (if the `_use_new_zipfile_serialization` flag is `True`), raw values saved in Python can be loaded in C++ and vice versa.\n\nFixes #20356\n](https://our.intern.facebook.com/intern/diff/18607087/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30108\n\nPulled By: driazati\n\nDifferential Revision: D18607087\n\nfbshipit-source-id: 067cdd5b1cf9c30ddc7e2e5021a8cceee62d8a14", "pr_number": "30108", "files_changed": ["test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests_setup.py", "torch/csrc/api/include/torch/serialize.h", "torch/csrc/api/src/serialize.cpp", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export.h", "torch/csrc/jit/import.cpp", "torch/csrc/jit/import.h", "torch/csrc/jit/pickle.cpp", "torch/csrc/jit/pickle.h", "torch/csrc/jit/pickler.cpp", "torch/serialization.py"], "labels": ["jit"]}, "59ca9b7430": {"title": "Graph-mode quantization for convolution from traced model (#30245)", "body": "Summary:\nIn the PR, we enhance the graph-mode quantization for aten::_convolution, which could be generated from tracing path.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30245\n\nDifferential Revision: D18671597\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: 78a2470fbb0fe0def55d63c6bda7cbb5c89f7848", "pr_number": "30245", "files_changed": ["test/test_quantization.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "7570b2798a": {"title": "updating citation (#30267)", "body": "Summary:\nNIPS -> NeurIPS\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30267\n\nDifferential Revision: D18672928\n\nPulled By: soumith\n\nfbshipit-source-id: c20f26a0547f94ff39f8ee40e5f0ccc5fcc814af", "pr_number": "30267", "files_changed": ["CITATION"], "labels": []}, "7c4b9042ab": {"title": "Updates to quantization documentation (#30288)", "body": "Summary:\nThis pull request includes fixes for six quantization doc bugs.\n\nhttps://github.com/pytorch/pytorch/issues/30283 - Rendering issue on QConfig\nhttps://github.com/pytorch/pytorch/issues/26305 - Minor doc issue on fuse_modules()\nhttps://github.com/pytorch/pytorch/issues/27451 - Issues with ConvReLU2d, ConvReLU3d, and LinearReLU doc issues\nhttps://github.com/pytorch/pytorch/issues/26899 - Missing docstrings in torch.nn.intrinsic fused functions\nhttps://github.com/pytorch/pytorch/issues/29735 - add discussion of QNNPack to quantization doc page\nhttps://github.com/pytorch/pytorch/issues/27938 - some of the quantized functions lack documentation\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30288\n\nDifferential Revision: D18653368\n\nPulled By: gottbrath\n\nfbshipit-source-id: 410b3dd81ff10909a7f1a7736ca42d7cabf0beb1", "pr_number": "30288", "files_changed": ["docs/source/quantization.rst", "torch/nn/intrinsic/modules/fused.py", "torch/nn/intrinsic/quantized/modules/conv_relu.py", "torch/quantization/fuse_modules.py", "torch/quantization/qconfig.py"], "labels": ["module: docs", "quantization"]}, "4aa692fc91": {"title": "Convert KernelTable to a flat-indexed array rather than a hashtable. (#30332)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30332\n\n-\nghstack-source-id: 94481315\n\nReviewed By: resistor\n\nDifferential Revision: D18660421\n\nfbshipit-source-id: 9f11434f1c3c234c45f586719182053fa81731f0", "pr_number": "30332", "files_changed": ["aten/src/ATen/core/dispatch/DispatchTable.h"], "labels": []}, "7b5045be9d": {"title": "Remove LeftRight from OperatorEntry and DispatchTable. (#30333)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30333\n\nre-export of https://github.com/pytorch/pytorch/pull/30328\nghstack-source-id: 94481321\n\nDifferential Revision: D18661518\n\nfbshipit-source-id: 5a35a1ed2fae3b21a43614957a91d648c21bcca1", "pr_number": "30333", "files_changed": ["aten/src/ATen/core/dispatch/DispatchTable.h", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h"], "labels": []}, "24aabe439a": {"title": "Make Dispatcher::backendFallbackKernels_ an array (#30340)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30340\n\nWe already made OperatorEntry::dispatchTable_ an array to be able to avoid the concurrency primitives there,\nbut Dispatcher::backendFallbackKernels_ has the same issue. Let's make it a table too.\n\nSince there is some code duplication here, we also factor out the concept of a KernelFunctionTable to be used in both places.\nghstack-source-id: 94481317\n\nTest Plan: unit tests\n\nDifferential Revision: D18663426\n\nfbshipit-source-id: ba82ca5c4cae581eea359d5c0c3a5e23b0f8838c", "pr_number": "30340", "files_changed": ["aten/src/ATen/core/dispatch/DispatchTable.h", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": []}, "583c288232": {"title": "Add a OperatorHandle argument to boxed kernels (#29201)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29201\n\nThis is required for boxed backend fallback kernels (e.g. lazy, AMP) because they need to know which op was actually called.\nghstack-source-id: 94481313\n\nTest Plan: I will add unit tests in a diff stacked on top\n\nDifferential Revision: D18282746\n\nfbshipit-source-id: 339a1bbabd6aff31a587b98f095c75104dfc6f99", "pr_number": "29201", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/kernel_functor.h", "aten/src/ATen/core/boxing/kernel_stackbased_test.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "caffe2/core/export_caffe2_op_to_c10.h", "torch/csrc/jit/mobile/register_mobile_ops.cpp"], "labels": []}, "fb8c17dde1": {"title": "Test cases for backend fallback kernels (#29214)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29214\n\n-\nghstack-source-id: 94481312\n\nTest Plan: unit tests\n\nDifferential Revision: D18329308\n\nfbshipit-source-id: 1dbae401f2255c69ed16d436f891b9b60c333d81", "pr_number": "29214", "files_changed": ["aten/src/ATen/core/op_registration/op_registration_test.cpp"], "labels": []}, "afdc0bd4ec": {"title": "OperatorHandle::callBoxed/callUnboxed (#29330)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29330\n\nThis makes for a nicer API, especially in backend fallback kernels who get an OperatorHandle instance and can directly call these methods on it.\nghstack-source-id: 94481322\n\nTest Plan: unit tests stacked on top\n\nDifferential Revision: D18357424\n\nfbshipit-source-id: fa8c638335f246c906c8e16186507b4c486afb3f", "pr_number": "29330", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/function_wrapper.py"], "labels": []}, "aa2862b843": {"title": "Hide the OperatorKernel* argument from the stack based kernel API (#29337)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29337\n\nThis argument is needed by boxing wrappers so they're able to get a pointer to the corresponding unboxed kernel and call into it.\nBut if a kernel is registered in a boxed way, we don't need it and should hide this from the API.\nThis is especially needed for the backend fallback API where users would only be left wondering why this argument is there and what it does.\nAlso, hiding it allows us to potentially totally remove it in a future refactoring if we find some way to do so.\nghstack-source-id: 94481316\n\nTest Plan: unit tests\n\nDifferential Revision: D18361991\n\nfbshipit-source-id: 5cef26c896fe3f2a5db730d3bc79dcd62e7ef492", "pr_number": "29337", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/kernel_functor.h", "aten/src/ATen/core/boxing/kernel_stackbased_test.cpp", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "caffe2/core/export_caffe2_op_to_c10.h", "torch/csrc/jit/mobile/register_mobile_ops.cpp"], "labels": []}, "959a849a23": {"title": "better boxing (#29681)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29681\n\nRemove callUnboxedOnly() and instead use metaprogramming to figure out if an operator can use a boxed fallback or not.\nThis enables boxed fallback for ops in native_functions.yaml even if they don't have `use_c10_dispatcher: full` set, as long as they're in the range of supported types.\nghstack-source-id: 94481320\n\nTest Plan: unit tests\n\nDifferential Revision: D18462653\n\nfbshipit-source-id: 2955e3c4949267520a1734a6a2b919ef5e9684a2", "pr_number": "29681", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/boxing.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/function_wrapper.py"], "labels": []}, "0c7e4c1d62": {"title": "backend fallback test (#29682)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29682\n\nThis PR re-introduces backend_fallback_test.cpp, which was previously called boxed_fallback_test.cpp and showed how to use the backend fallback API.\nghstack-source-id: 94481314\n\nTest Plan: unit tests\n\nDifferential Revision: D18462654\n\nfbshipit-source-id: 3e9b5c8f35c05f9cd795f44a5fefd1a0aaf03509", "pr_number": "29682", "files_changed": ["aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/backend_fallback_test.cpp"], "labels": []}, "3990e9d1ca": {"title": "Improve performance of LeftRight::read() (#30282)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30282\n\nThe atomic increment/decrements in LeftRight::read() were measurable in perf benchmarks. Let's improve their perf.\nghstack-source-id: 94443230\n\nTest Plan: unit tests, perf benchmarks\n\nDifferential Revision: D18650228\n\nfbshipit-source-id: d184ce8288510ab178e7c7da73562609d1ca3c9f", "pr_number": "30282", "files_changed": ["c10/util/LeftRight.h"], "labels": []}, "20dfae4099": {"title": "Fix the crashes for c++ not able to find java class through Jni (#30390)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30390\n\nFix the crashes for c++ not able to find java class through Jni\nghstack-source-id: 94499644\n\nTest Plan: buck install -r fb4a\n\nReviewed By: ljk53\n\nDifferential Revision: D18667992\n\nfbshipit-source-id: aa1b19c6dae39d46440f4a3e691054f7f8b1d42e", "pr_number": "30390", "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/IValue.java", "android/pytorch_android/src/main/java/org/pytorch/Tensor.java"], "labels": []}, "99a46b44ea": {"title": "Use correct API macro in VariableHooksInterface. (#30320)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30320\n\nFixes #30296\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18665704\n\nPulled By: ezyang\n\nfbshipit-source-id: f09a953137fcc105959382254f9b8886af5aea3b", "pr_number": "30320", "files_changed": ["aten/src/ATen/core/VariableHooksInterface.h"], "labels": ["merge-this-please"]}, "f994377d28": {"title": "Turn off scalar_check for lshift, rshift.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29878\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18521746\n\nPulled By: gchanan\n\nfbshipit-source-id: 11fd7db79ac8ae76b1a5df25fb0ff59d81fcf394", "pr_number": "29878", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "94ad7544ae": {"title": "Turn off scalar_check for __or__", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29879\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18521745\n\nPulled By: gchanan\n\nfbshipit-source-id: 93d17d5e9cad5dd6d2c20221d87408c838d74eca", "pr_number": "29879", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "0c9c62ba6e": {"title": "Turn off scalar_checks for __and__ and clone.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29880\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18521732\n\nPulled By: gchanan\n\nfbshipit-source-id: 7fdf5d8a7b93b43ac32067222cb8df5e790900de", "pr_number": "29880", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/function_wrapper.py", "test/test_torch.py"], "labels": []}, "ce5f1a1b25": {"title": "Turn off scalar_check for masked_select. (#29923)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29923\n\nNote that this changes the behavior of masked_select when both \"self\" and \"mask\" are 0-dimensional.\n\nIn previous versions of PyTorch, this would return a 0-dimensional tensor.  But the documentation reads:\n\"Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.\"\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18539560\n\nPulled By: gchanan\n\nfbshipit-source-id: 1637ed2c434fcf8ceead0073aa610581f4a19d21", "pr_number": "29923", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": ["topic: bc-breaking"]}, "6e88ddf352": {"title": "Turn off scalar_check for _th_addmv and _th_eig as they can never pass. (#29945)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29945\n\nBoth functions require at least 1 2-dimensional tensor, so can never return an inferred scalar.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548056\n\nPulled By: gchanan\n\nfbshipit-source-id: f99a41d490b9a5ab5717534c92e4f2e848c743e8", "pr_number": "29945", "files_changed": ["aten/src/ATen/Declarations.cwrap"], "labels": []}, "7c6cc1d6d4": {"title": "Turn off scalar_checks for _th_multinomial_alias_draw. (#29946)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29946\n\nit requires > 0-dimensional tensors.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548050\n\nPulled By: gchanan\n\nfbshipit-source-id: 4d1e3b53bd701137cc2cb674f95627a5e064a274", "pr_number": "29946", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "b8eba7aca9": {"title": "Turn off scalar_check for ormqr. (#29947)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29947\n\nIt requires > 0-dimensional tensors.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548049\n\nPulled By: gchanan\n\nfbshipit-source-id: ce80a42515b59513a0e5ef2b32e2c2b90b4d64f5", "pr_number": "29947", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "16606e1725": {"title": "Turn off scalar_check for mode; the underlying code is correct.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29948\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548053\n\nPulled By: gchanan\n\nfbshipit-source-id: 15cdfc24d3e5123497c72dc09c5e6b28cb5e1f88", "pr_number": "29948", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "7160300638": {"title": "Turn off scalar_check for reductions _th_max, _th_min. (#29949)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29949\n\nThe underlying functions handle this already.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548047\n\nPulled By: gchanan\n\nfbshipit-source-id: 123c9297db4e4315da9b1d996ac8b41aa1b4c7bc", "pr_number": "29949", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "0c67311878": {"title": "Turn off scalar_check for set_(Storage, ...) (#29950)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29950\n\nThe underlying code handles it correctly.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548052\n\nPulled By: gchanan\n\nfbshipit-source-id: 88b737572c816fb0026ac5e66da7e3f4ab686773", "pr_number": "29950", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "d7ac90e2ef": {"title": "Stop binding std_single and var_single from TH; they aren't used anymore.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29951\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548057\n\nPulled By: gchanan\n\nfbshipit-source-id: 0143f694517fa8229e53bd2bc636501804a3f80b", "pr_number": "29951", "files_changed": ["aten/src/ATen/Declarations.cwrap"], "labels": []}, "c12f9a12a8": {"title": "Fix quantized ConvReLU3d test (#30266)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30266\n\nFix quantized ConvReLU3d test\n\nTest Plan: buck test mode/dev-nosan //caffe2/test:quantized -- \"conv\"\n\nReviewed By: hl475\n\nDifferential Revision: D18645717\n\nfbshipit-source-id: bbe93f9daf5046f2aa05363efc7d0e59eaff37bf", "pr_number": "30266", "files_changed": ["test/test_quantized_nn_mods.py", "torch/nn/intrinsic/quantized/modules/conv_relu.py", "torch/quantization/quantize.py"], "labels": ["fb-exported"]}, "0517323dad": {"title": "Update osx CI to XCode 9.4 / CUDA 10.0, cudnn 7.6.5 (#30359)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30359\n\nWe need this for C++14 support\n\nghstack-source-id: 94519850\n\nTest Plan: unit tests\n\nDifferential Revision: D18668868\n\nfbshipit-source-id: 87e8eadf0e60a1699fba4524aea53b306b9a7f24", "pr_number": "30359", "files_changed": [".circleci/config.yml", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/binary-job-specs.yml", ".circleci/verbatim-sources/caffe2-job-specs.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-pytorch-macos-builds.yml"], "labels": []}, "d64e2581cc": {"title": "Add list of supported XCode/CUDA versions to README", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30407\n\nDifferential Revision: D18689043\n\nPulled By: smessmer\n\nfbshipit-source-id: cd772451ef31356ed3045ebb1a9c4f5e5e91bb45", "pr_number": "30407", "files_changed": ["README.md"], "labels": []}, "5c6705e62c": {"title": "add default arg for init_method (#30208)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30208\n\nAdds default arg for init_method so users don't have to pass this in,\nand moves it to `RpcBackendOptions` struct. Removes `init_method` arg from rpc.init_rpc. Also fixes some docs.\nghstack-source-id: 94500475\n\nTest Plan: Unit tests pass.\n\nReviewed By: mrshenli\n\nDifferential Revision: D18630074\n\nfbshipit-source-id: 04b7dd7ec96f4c4da311b71d250233f1f262135a", "pr_number": "30208", "files_changed": ["docs/source/notes/distributed_autograd.rst", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py", "torch/distributed/rpc/backend_registry.py", "torch/distributed/rpc/constants.py"], "labels": []}, "25f4ba7c1b": {"title": "Improve compare kernel (#29743)", "body": "Summary:\nCurrently, the way the compare kernels handle dtypes is very funny (this behavior is introduced in https://github.com/pytorch/pytorch/pull/28427 and I just realize it today):\n\nLet's say `a, b` are two float tensors on CUDA.\n\nIf you do `a < b`, this is what would happen inside the loop:\n- Step 1: Fetch `a` and `b`, dynamically cast them from `float` to `float`. (i.e. check the scalar type to figure out if it needs cast. it doesn't. so do nothing then.)\n- Step 2: compute `a < b`, get a `bool` result\n- Step 3: statically cast the result into `float`\n- Step 3: do a dynamic cast of the result from `float` to `bool` and store the value\n\nAnd if you do `a.lt_(b)`, this is what would happen:\n- Step 1: Fetch `a` and `b`, no casting\n- Step 2: compute `a < b`, get a `bool` result\n- Step 3: statically cast the result into `float`\n- Step 4: store the result to memory, no casting\n\nAlthough dynamic casting happens on registers, it still hurt the performance a bit (~8%).\n\nThis PR fixes this issue. Now for compare kernels, if the output is bool and inputs have the same dtype, then there is no dynamic casting. Otherwise, there will be dynamic casting for each input and output. That is, the dynamic casting behavior of the two cases described above are swapped.\n\nBenchmark on `a < b` for tensor of 1000000000 fp32 elements:\nBefore https://github.com/pytorch/pytorch/issues/28427 6.35 ms\nCurrent master: 6.88 ms\nWith this PR: 6.36 ms\nBenchmark on `a.lt_(b)` does not show any difference across versions.\n\nBesides this, what worries me most is, with type promotion, the logic for tensor iterator is becoming super complicated, and it is hard to see if one change causes the performance regression of others. I suggest we create scripts that could benchmark tensor iterator entirely, review that code and put it somewhere inside the repository (maybe under `/tools` or `/test/scripts`?), and whenever we are not certain about the performance we could run it to check. (I guess not on this PR but on PRs after the script is done. If there are worries about performance, the author of PRs should run the script manually, and the reviewer should remind PR author to do so if necessary) If this is a good idea, I will send a PR for the script.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29743\n\nDifferential Revision: D18671269\n\nPulled By: ngimel\n\nfbshipit-source-id: 89a9c1c8b5fd45d5ae8fe907d65c2fe1a7dfd2dc", "pr_number": "29743", "files_changed": ["aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/cuda/BinaryCompareKernel.cu", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/cuda/Copy.cu"], "labels": []}, "b8f50d9cc8": {"title": "Support to add dequant for each use of Value (#30145)", "body": "Summary:\nIn this PR, we mainly handle the case there are multiple usage of a Value when inserting the quant-dequant pair. This change will add one dequant for each usage of the Value.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30145\n\nDifferential Revision: D18671600\n\nPulled By: lly-zero-one\n\nfbshipit-source-id: 61324a98861da85b80dcf7e930381311118ae53b", "pr_number": "30145", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "0b71e7e1fd": {"title": "Refactor QAT Conv module for better extensibility (#30362)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30362\n\nRight now the qat modules(qat.ConvBn2d, qat.ConvBnReLU2d, qat.Conv2d)\nare not convinent to support other dimensions of Conv, this PR refactors\nthese modules so that we can support Conv1d/Conv3d better\n\nTest Plan:\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18691152\n\nfbshipit-source-id: 5b561e6b054eadd31b98cabdf1ac67a61ee9b805", "pr_number": "30362", "files_changed": ["test/test_jit.py", "torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/modules/conv.py", "torch/nn/qat/modules/conv.py"], "labels": []}, "ab2ec4d835": {"title": "Fix inexistent parameter in document (#24335)", "body": "Summary:\nThere is no `out` argument to `argsort` according to the source code.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/24335\n\nDifferential Revision: D16829134\n\nPulled By: vincentqb\n\nfbshipit-source-id: 8f91154984cd4a753ba1d6105fb8a9bfa0da22b3", "pr_number": "24335", "files_changed": ["torch/_torch_docs.py"], "labels": ["module: docs", "open source", "triaged"]}, "46e7f31fa3": {"title": "Document unsupported types (#30344)", "body": "Summary:\nThis adds a listing of the parts of the `typing` module that are unsupported\n\nThis is also a first pass decisions on features are 'unlikely to be implemented' vs 'not implemented' so they're open to discussion\n](https://our.intern.facebook.com/intern/diff/18665628/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30344\n\nPulled By: driazati\n\nDifferential Revision: D18665628\n\nfbshipit-source-id: 22b8ebbde23df03839306cdb4344ca18a44f2c29", "pr_number": "30344", "files_changed": ["docs/source/jit.rst"], "labels": []}, "661a6c8ef2": {"title": "Add `get_qparams` and revert the changes to `calculate_qparams` (#30262)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30262\n\n`get_qparams` returns all parameters that's needed to call quantize function\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D18645047\n\nfbshipit-source-id: e57c11a66dac2d589778d412a996796ad5b6f86a", "pr_number": "30262", "files_changed": ["torch/csrc/jit/passes/quantization.cpp", "torch/nn/quantized/modules/utils.py", "torch/quantization/fake_quantize.py", "torch/quantization/observer.py"], "labels": ["jit"]}, "8199596d7e": {"title": "Add missing std::move (#30411)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30411\n\n-\nghstack-source-id: 94526555\n\nTest Plan: unit tests\n\nDifferential Revision: D18690385\n\nfbshipit-source-id: fd348c0887c279694c2f6d287b361c8e07f02ffb", "pr_number": "30411", "files_changed": ["aten/src/ATen/core/dispatch/OperatorEntry.cpp"], "labels": []}, "085dde5965": {"title": "Fix for when PyTorch model trace has RecursiveScriptModules (#30430)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30430\n\nWhen a module isn't a TracedModule, attempt to get name information with `original_name` property on module and default to 'Module' when no such property exists.\n\nTest Plan:\n### Change child module to scripted module:\n```\nmodel = torchvision.models.alexnet()\nmodel.classifier = torch.jit.script(model.classifier)\n```\n### Add graph\n```\nw = SummaryWriter()\nw.add_graph(model, torch.rand((2, 3, 224, 224)))\nw.close()\n```\n### No errors\nHowever, graph is disconnected at parts and hard to understand.\n{F223327878}\n\nReviewed By: sanekmelnikov\n\nDifferential Revision: D18690836\n\nfbshipit-source-id: 42295d06b7c1d48d5401776dca1e0d12cd64b49d", "pr_number": "30430", "files_changed": ["torch/utils/tensorboard/_pytorch_graph.py"], "labels": ["fb-exported"]}, "ab5774547a": {"title": "Add info about transitive dependencies in case of using local aars (#30128)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30128\n\nPreview: https://github.com/pytorch/pytorch/tree/gh/IvanKobzarev/23/head/android\n\nBased on users issue: https://discuss.pytorch.org/t/android-somethings-went-wrong-with-pytorch-android-1-4-0-snapshot/61009/3\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18702658\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 14928baccd58ddbe633fad03038271d8333c4b49", "pr_number": "30128", "files_changed": ["android/README.md"], "labels": []}, "eccf42fd15": {"title": "Bug fix: Handle missing keys in observer state dict during load (#30357)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30357\n\nFix issue https://github.com/pytorch/pytorch/issues/29032 in loading from state dict for observers and fake quant.\nghstack-source-id: 94468814\n\nTest Plan: Ensures that load/save of fake quant and observers with missing keys works correctly.\n\nDifferential Revision: D18668517\n\nfbshipit-source-id: 0eda6f47c39102e55977fc548b9a03664f123ad7", "pr_number": "30357", "files_changed": ["torch/quantization/fake_quantize.py", "torch/quantization/observer.py"], "labels": []}, "584be86c3f": {"title": "Try exporting ONNX with force_outplace=False (#29466)", "body": "Summary:\nThis should resolve https://github.com/pytorch/pytorch/issues/29008. This flag has two effects on the tracer.\n- Remove the underscroll for inplace operators. E.g.: index_put_ ==> index_put. This is handled in utils.py separately as well.\n- Add out as input for backward computation.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29466\n\nReviewed By: hl475\n\nDifferential Revision: D18422815\n\nPulled By: houseroad\n\nfbshipit-source-id: 317b6a3c8a5751fe6fe49d7543e429d281ed0d6d", "pr_number": "29466", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/ir.h", "torch/csrc/jit/passes/remove_inplace_ops.cpp", "torch/onnx/symbolic_opset9.py", "torch/onnx/utils.py"], "labels": []}, "06db5ad707": {"title": "Provide names for operator nodes in ONNX exported graph. (#27342)", "body": "Summary:\nThe PyTorch exporter does not add any name to the ONNX operators in the exported graph. A common request is to add names to op nodes by default. This helps the readability of the graph in visualization tools such a Netron, or when the ONNX graph is printed as a string. Also, it helps with the debuggability of the ONNX graph.\n\nTherefore this PR adds name to operators in the exporters. The names follow a simple format, <op_type>_<index>. Expect files for tests in `test/onnx/test_operators.py` have been updated.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27342\n\nReviewed By: hl475\n\nDifferential Revision: D17790979\n\nPulled By: houseroad\n\nfbshipit-source-id: 1eaae88b5f51f152735a2ff96e22827837e34d9d", "pr_number": "27342", "files_changed": ["test/onnx/expect/TestOperators.test_acos.expect", "test/onnx/expect/TestOperators.test_add_broadcast.expect", "test/onnx/expect/TestOperators.test_add_left_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_right_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_singleton_broadcast.expect", "test/onnx/expect/TestOperators.test_addconstant.expect", "test/onnx/expect/TestOperators.test_addmm.expect", "test/onnx/expect/TestOperators.test_arange_dynamic.expect", "test/onnx/expect/TestOperators.test_argmax.expect", "test/onnx/expect/TestOperators.test_asin.expect", "test/onnx/expect/TestOperators.test_at_op.expect", "test/onnx/expect/TestOperators.test_atan.expect", "test/onnx/expect/TestOperators.test_avg_pool2d.expect", "test/onnx/expect/TestOperators.test_baddbmm.expect", "test/onnx/expect/TestOperators.test_basic.expect", "test/onnx/expect/TestOperators.test_batchnorm.expect", "test/onnx/expect/TestOperators.test_batchnorm_1d.expect", "test/onnx/expect/TestOperators.test_batchnorm_noaffine.expect", "test/onnx/expect/TestOperators.test_batchnorm_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_batchnorm_training.expect", "test/onnx/expect/TestOperators.test_bitshift.expect", "test/onnx/expect/TestOperators.test_c2_op.expect", "test/onnx/expect/TestOperators.test_chunk.expect", "test/onnx/expect/TestOperators.test_clip.expect", "test/onnx/expect/TestOperators.test_clip_max.expect", "test/onnx/expect/TestOperators.test_clip_min.expect", "test/onnx/expect/TestOperators.test_concat2.expect", "test/onnx/expect/TestOperators.test_conv.expect", "test/onnx/expect/TestOperators.test_conv_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_conv_onnx_irv4_opset8.expect", "test/onnx/expect/TestOperators.test_convtranspose.expect", "test/onnx/expect/TestOperators.test_cos.expect", "test/onnx/expect/TestOperators.test_cumsum.expect", "test/onnx/expect/TestOperators.test_det.expect", "test/onnx/expect/TestOperators.test_dict.expect", "test/onnx/expect/TestOperators.test_dict_str.expect", "test/onnx/expect/TestOperators.test_dropout.expect", "test/onnx/expect/TestOperators.test_elu.expect", "test/onnx/expect/TestOperators.test_embedding_bags.expect", "test/onnx/expect/TestOperators.test_empty_like.expect", "test/onnx/expect/TestOperators.test_empty_like_opset7.expect", "test/onnx/expect/TestOperators.test_equal.expect", "test/onnx/expect/TestOperators.test_erf.expect", "test/onnx/expect/TestOperators.test_exp.expect", "test/onnx/expect/TestOperators.test_expand.expect", "test/onnx/expect/TestOperators.test_flatten.expect", "test/onnx/expect/TestOperators.test_flatten2D.expect", "test/onnx/expect/TestOperators.test_fmod.expect", "test/onnx/expect/TestOperators.test_frobenius_norm.expect", "test/onnx/expect/TestOperators.test_full.expect", "test/onnx/expect/TestOperators.test_full_like.expect", "test/onnx/expect/TestOperators.test_gather.expect", "test/onnx/expect/TestOperators.test_gather_opset11.expect", "test/onnx/expect/TestOperators.test_ge.expect", "test/onnx/expect/TestOperators.test_gelu.expect", "test/onnx/expect/TestOperators.test_gt.expect", "test/onnx/expect/TestOperators.test_hardtanh.expect", "test/onnx/expect/TestOperators.test_implicit_expand.expect", "test/onnx/expect/TestOperators.test_index.expect", "test/onnx/expect/TestOperators.test_isnan.expect", "test/onnx/expect/TestOperators.test_le.expect", "test/onnx/expect/TestOperators.test_linear.expect", "test/onnx/expect/TestOperators.test_log_sigmoid.expect", "test/onnx/expect/TestOperators.test_logsoftmax.expect", "test/onnx/expect/TestOperators.test_lt.expect", "test/onnx/expect/TestOperators.test_master_opset.expect", "test/onnx/expect/TestOperators.test_max.expect", "test/onnx/expect/TestOperators.test_maxpool.expect", "test/onnx/expect/TestOperators.test_maxpool_dilations.expect", "test/onnx/expect/TestOperators.test_maxpool_indices.expect", "test/onnx/expect/TestOperators.test_mean.expect", "test/onnx/expect/TestOperators.test_meshgrid.expect", "test/onnx/expect/TestOperators.test_min.expect", "test/onnx/expect/TestOperators.test_mm.expect", "test/onnx/expect/TestOperators.test_narrow.expect", "test/onnx/expect/TestOperators.test_ne.expect", "test/onnx/expect/TestOperators.test_nonzero.expect", "test/onnx/expect/TestOperators.test_norm_p1.expect", "test/onnx/expect/TestOperators.test_norm_p2.expect", "test/onnx/expect/TestOperators.test_ones_like.expect", "test/onnx/expect/TestOperators.test_pad.expect", "test/onnx/expect/TestOperators.test_params.expect", "test/onnx/expect/TestOperators.test_params_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_permute2.expect", "test/onnx/expect/TestOperators.test_pixel_shuffle.expect", "test/onnx/expect/TestOperators.test_pow.expect", "test/onnx/expect/TestOperators.test_prelu.expect", "test/onnx/expect/TestOperators.test_prod.expect", "test/onnx/expect/TestOperators.test_rand.expect", "test/onnx/expect/TestOperators.test_randn.expect", "test/onnx/expect/TestOperators.test_reduce_sum_negative_indices.expect", "test/onnx/expect/TestOperators.test_reduced_mean.expect", "test/onnx/expect/TestOperators.test_reduced_mean_keepdim.expect", "test/onnx/expect/TestOperators.test_reduced_prod.expect", "test/onnx/expect/TestOperators.test_reduced_prod_keepdim.expect", "test/onnx/expect/TestOperators.test_reduced_sum.expect", "test/onnx/expect/TestOperators.test_reduced_sum_keepdim.expect", "test/onnx/expect/TestOperators.test_reducemax.expect", "test/onnx/expect/TestOperators.test_reducemin.expect", "test/onnx/expect/TestOperators.test_remainder.expect", "test/onnx/expect/TestOperators.test_repeat.expect", "test/onnx/expect/TestOperators.test_repeat_dim_overflow.expect", "test/onnx/expect/TestOperators.test_retain_param_name_disabled.expect", "test/onnx/expect/TestOperators.test_round.expect", "test/onnx/expect/TestOperators.test_rrelu.expect", "test/onnx/expect/TestOperators.test_rsqrt.expect", "test/onnx/expect/TestOperators.test_rsub.expect", "test/onnx/expect/TestOperators.test_scatter_add.expect", "test/onnx/expect/TestOperators.test_scatter_add_opset11.expect", "test/onnx/expect/TestOperators.test_selu.expect", "test/onnx/expect/TestOperators.test_sign.expect", "test/onnx/expect/TestOperators.test_sin.expect", "test/onnx/expect/TestOperators.test_slice.expect", "test/onnx/expect/TestOperators.test_slice_dynamic.expect", "test/onnx/expect/TestOperators.test_split.expect", "test/onnx/expect/TestOperators.test_split_with_sizes.expect", "test/onnx/expect/TestOperators.test_sqrt.expect", "test/onnx/expect/TestOperators.test_std.expect", "test/onnx/expect/TestOperators.test_sum.expect", "test/onnx/expect/TestOperators.test_tan.expect", "test/onnx/expect/TestOperators.test_topk.expect", "test/onnx/expect/TestOperators.test_topk_smallest_unsorted.expect", "test/onnx/expect/TestOperators.test_unfold.expect", "test/onnx/expect/TestOperators.test_unique.expect", "test/onnx/expect/TestOperators.test_unsqueeze.expect", "test/onnx/expect/TestOperators.test_upsample_nearest.expect", "test/onnx/expect/TestOperators.test_view.expect", "test/onnx/expect/TestOperators.test_view_flatten.expect", "test/onnx/expect/TestOperators.test_zeros_like.expect", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/verify.py", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export.h", "torch/csrc/jit/python_ir.cpp", "torch/onnx/utils.py"], "labels": ["jit", "module: onnx", "open source", "triaged"]}, "efe1859ad9": {"title": "By default ignore RRef leaks during shutdown (#30217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30217\n\nBefore this commit, RRefContext throws an error if it detects any\nRRef leak during shutdown. However, this requires applications to\nmake sure that is has freed all references to RRefs in application\ncode, which can be a bad debugging experience when for large\napplications. Besides, this also relies on Python GC to free things\nup in time, which might not always be true. After this commit,\nRRefContext would ignore leaking RRefs during shutdown, as shutdown\nis called when the application has finished training and no longer\ncare about local states. Hence, it should be OK to just ignore\nthose leaks and destroy OwnerRRefs. If application would like to\nenforce no leaks, just set torch.distributed.rpc.api._ignore_rref_leak\nto False.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18632546\n\nPulled By: mrshenli\n\nfbshipit-source-id: 2744b2401dafdd16de0e0a76cf8e07777bed0f38", "pr_number": "30217", "files_changed": ["test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/rref.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/distributed/rpc/api.py"], "labels": []}, "2599b9b551": {"title": "Add output_size argument to caffe2 Int8ResizeNearest (#30202)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30202\n\nPytorch Upsample operator has output_size as an argument.\nFor quantized tensor inputs we cannot get the input_size to calculate the width and height scale factor.\nInstead we pass the output_size directly to caffe2 to calculate the scale factors.\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_upsample\n\nImported from OSS\n\nDifferential Revision: D18631478\n\nfbshipit-source-id: 38a39129bc863f4ecf2293acc068e40ab7edc825", "pr_number": "30202", "files_changed": ["caffe2/operators/quantized/int8_resize_nearest_op.cc", "caffe2/operators/quantized/int8_resize_nearest_op.h", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/onnx/symbolic_caffe2.py"], "labels": []}, "0febff36ac": {"title": "Export dynamic unbind/split and __getitem__ (#29136)", "body": "Summary:\nIn ONNX opset 11, a series of sequence ops were added. Operators that are related to Tensor[] in PyTorch can be exported using these sequence ops.\nIn this PR, unbind/split that produces Tensor[], and __getitem__ that takes Tensor[] as input, are exported correctly to ONNX opset 11.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29136\n\nReviewed By: hl475\n\nDifferential Revision: D18309222\n\nPulled By: houseroad\n\nfbshipit-source-id: be12c96bf8d0a56900683ef579f1c808c0a1af21", "pr_number": "29136", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["jit", "module: onnx"]}, "79a830af56": {"title": "Turn off scalar_check for Tensor.set_(Tensor) (#29952)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29952\n\nThe underlying op handles the check correctly.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548048\n\nPulled By: gchanan\n\nfbshipit-source-id: 9ac6fde743408e59ccdfc61bd574ebe6e2862238", "pr_number": "29952", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "72ac45662b": {"title": "Turn off scalar_checks for torch.take. (#29953)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29953\n\nThe underlying function handles it correctly.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548055\n\nPulled By: gchanan\n\nfbshipit-source-id: cc2d0ae37d9689423363d115c6a653cb64840528", "pr_number": "29953", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "dbce53fe32": {"title": "Turn off scalar_check for _th_gather. (#29954)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29954\n\nThe underlying op handles scalar_check correctly.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548054\n\nPulled By: gchanan\n\nfbshipit-source-id: a1b44afa80c2928b78abbfba8b8b5d3608ac0fd3", "pr_number": "29954", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "dcd9f49809": {"title": "Specify ordering on singular values and eigenvalues output from torch\u2026 (#30389)", "body": "Summary:\n\u2026.svd/symeig respectively\n\nChangelog:\n- Adds a note to docstrings of the both functions specifying the ordering\n\nFixes https://github.com/pytorch/pytorch/issues/30301\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30389\n\nDifferential Revision: D18707608\n\nPulled By: zou3519\n\nfbshipit-source-id: b0f73631578f39a24fae9af4997c6491de8be9a8", "pr_number": "30389", "files_changed": ["torch/_torch_docs.py"], "labels": ["open source"]}, "45880f4246": {"title": "Change logging to remove the word \"error\" from info log", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30468\n\nReviewed By: xianjiec\n\nDifferential Revision: D18702959\n\nfbshipit-source-id: a777445bea735dce89182dd95f38907963fab556", "pr_number": "30468", "files_changed": ["caffe2/operators/gather_ranges_to_dense_op.h"], "labels": ["fb-exported"]}, "b0871f211b": {"title": "Make all optimizers consistent so that they don't change gradients inplace", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30257\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18665461\n\nPulled By: albanD\n\nfbshipit-source-id: cfdafef919468a41007881b82fd288b7128baf95", "pr_number": "30257", "files_changed": ["torch/optim/adam.py", "torch/optim/optimizer.py", "torch/optim/sgd.py"], "labels": ["topic: bc-breaking"]}, "fec903ce00": {"title": "Fix test case after get_qparams refactor (#30470)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30470\n\natt\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D18710775\n\nfbshipit-source-id: b1c7c0afbc538ff1d3e19c5d3d6bd425e4f94f06", "pr_number": "30470", "files_changed": ["test/test_jit.py"], "labels": []}, "e9cc4a5942": {"title": "Add @DoNotStrip to nativeNewTensor method. (#30472)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30472\n\nAdd DoNotStrip to nativeNewTensor method.\nghstack-source-id: 94596624\n\nTest Plan:\nTriggered build on diff for automation_fbandroid_fallback_release.\n\nbuck install -r fb4a\n\nTested BI cloaking using pytext lite interpreter.\n\nObverse that logs are sent to scuba table:\n\n{F223408345}\n\nReviewed By: linbinyu\n\nDifferential Revision: D18709087\n\nfbshipit-source-id: 74fa7a0665640c294811a50913a60ef8d6b9b672", "pr_number": "30472", "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/Tensor.java"], "labels": []}, "c5a6c4d6c9": {"title": "Adding elementwise kernel also operating on index (#28175)", "body": "Summary:\nThis PR add `gpu_kernel_with_index` as an addition to element-wise kernel template. It allows kernel to not only operate on input tensor value, but also each values index(view as 1d, so from 0 to numel) within the lambda.\nDirect use case here is to replace thrust::tabulate used in range/arange/linspace. Benifits are:\n- thrust::tabulate causes additional unneccessary synchronization on cpu.\n- Now it works with tensor iterator, output no longer needs to be contiguous and a memcpy is saved\n\nIt can also potentially be reused to add new function to pytorch later, if we see use case both value and index is needed.(for example unify tril/triu into tensor iterator element-wise? add other pattern?)\n\nKnown issues:\nhttps://github.com/pytorch/pytorch/pull/23586 is needed to enable non-contiguous case work properly, since overlapping needs to be checked. Currently non-contiguous tensor falls into TOO_HARD. I could write proper check in this file but I figured using exist method is better. jjsjann123\nIt does not work beyond 32bit indexing. But thrust was erroring on those case too. We could split tensor in caller to enable this. Index changes after split, so it is easier for caller to pass different lambda, and harder for the template to handle it in general.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28175\n\nDifferential Revision: D18708649\n\nPulled By: ngimel\n\nfbshipit-source-id: 382081c96f266ae7b61095fc1f2af41c6b210fa9", "pr_number": "28175", "files_changed": ["aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/RangeFactories.cu"], "labels": []}, "634f370c63": {"title": "Add comment to ops bound at python layer", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30419\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D18714000\n\nPulled By: eellison\n\nfbshipit-source-id: 22ccb941b2db24031921f378c600e68fe70e1346", "pr_number": "30419", "files_changed": ["tools/autograd/templates/python_torch_functions.cpp", "tools/autograd/templates/python_variable_methods.cpp", "torch/csrc/autograd/python_variable.cpp"], "labels": []}, "976d91d30a": {"title": "Comment on a set of ops bound at the python layer", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30420\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D18713999\n\nPulled By: eellison\n\nfbshipit-source-id: 3a8d6e4431cbfe6a78ca047217c1c53c47403841", "pr_number": "30420", "files_changed": ["tools/autograd/templates/python_torch_functions.cpp", "tools/autograd/templates/python_variable_methods.cpp"], "labels": []}, "05a1644ce3": {"title": "Fix BC for quantized linear", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30481\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18714602\n\nPulled By: jamesr66a\n\nfbshipit-source-id: d51206c22cf2446e98053446789c6324c0481321", "pr_number": "30481", "files_changed": ["test/test_quantized_nn_mods.py", "torch/nn/quantized/modules/linear.py"], "labels": []}, "4eff2f2007": {"title": "Fix missing closing quotes in docs", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30448\n\nDifferential Revision: D18711396\n\nPulled By: zou3519\n\nfbshipit-source-id: 6e35e0779716185791273eedca7a93667a6cda90", "pr_number": "30448", "files_changed": ["docs/source/name_inference.rst"], "labels": []}, "8bbafa0b32": {"title": "Add logical_and and logical_or (#28162)", "body": "Summary:\nSuperseding https://github.com/pytorch/pytorch/issues/24379 as type promotion has been implemented.\n\nClose https://github.com/pytorch/pytorch/issues/24379\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28162\n\nDifferential Revision: D18580867\n\nPulled By: ailzhang\n\nfbshipit-source-id: 7e4d7c37da4dc8df87314bd4f1f6a7539e46586a", "pr_number": "28162", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/README.md", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_namedtensor.py", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["module: operators"]}, "21d7532dfe": {"title": "Add more comment on NumPy detection in Python scripts.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30417\n\nDifferential Revision: D18716502\n\nPulled By: albanD\n\nfbshipit-source-id: 0b1b86f882e0e24cb6845e4a44708048e7e3b4a8", "pr_number": "30417", "files_changed": ["tools/setup_helpers/cmake.py", "tools/setup_helpers/numpy_.py"], "labels": ["module: build"]}, "6bd8937aee": {"title": "FunctionParameter::set_default_str replace || with &&", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30471\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18710958\n\nPulled By: pbelevich\n\nfbshipit-source-id: 7e5339175c7e16cd975a90bf6b123df728045e4d", "pr_number": "30471", "files_changed": ["torch/csrc/utils/python_arg_parser.cpp"], "labels": []}, "5ada5363fc": {"title": "GenericDict/List type use unshapedType() (#30428)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30428\n\nReported issue https://discuss.pytorch.org/t/incomprehensible-behaviour/61710\n\nSteps to reproduce:\n\n```\nclass WrapRPN(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, features):\n        # type: (Dict[str, Tensor]) -> int\n        return 0\n```\n\n```\n#include <torch/script.h>\n\nint main() {\n  torch::jit::script::Module module = torch::jit::load(\"dict_str_tensor.pt\");\n\n  torch::Tensor tensor = torch::rand({2, 3});\n  at::IValue ivalue{tensor};\n  c10::impl::GenericDict dict{c10::StringType::get(),ivalue.type()};\n  dict.insert(\"key\", ivalue);\n  module.forward({dict});\n}\n```\n\nValueType of `c10::impl::GenericDict` is from the first specified element as `ivalue.type()`\nIt fails on type check in` function_schema_inl.h` !value.type()->isSubtypeOf(argument.type())\nas `DictType::isSubtypeOf` requires equal KeyType and ValueType, while `TensorType`s are different.\n\nFix:\nUse c10::unshapedType for creating Generic List/Dict\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18717189\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 1e352a9c776a7f7e69fd5b9ece558f1d1849ea57", "pr_number": "30428", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp"], "labels": []}, "2d6b2f39e9": {"title": "Fix docs so that the example works (#30120)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30120\n\nThe example given for functional conv2d didn't work. This diff fixes the example in docs so that it works.\n\nFixes https://github.com/pytorch/pytorch/issues/29649\nghstack-source-id: 94601559\n\nTest Plan: Tried the example locally\n\nDifferential Revision: D18604606\n\nfbshipit-source-id: ff1a4f903e2843efe30d962d4ff00e5065cd1d7e", "pr_number": "30120", "files_changed": ["torch/nn/quantized/functional.py"], "labels": []}, "829499e626": {"title": "avoid Formatting::print() when STRIP_ERROR_MESSAGES is set (#30451)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30451\n\nTORCH_CHECK takes __VA_ARGS__ so there is no need to concatenate strings\nbefore calling it. This way it won't call Formatting::print() on the\ntensor when STRIP_ERROR_MESSAGES macro is set. Formatting::print() calls\nseveral specific tensor methods that brings in unnecessary inter-op\ndependencies for static code analysis.\n\nTest Plan: - builds\n\nDifferential Revision: D18703784\n\nPulled By: ljk53\n\nfbshipit-source-id: 1c0628e3ddcb2fd42c475cb161edbef09dfe8eb5", "pr_number": "30451", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp"], "labels": []}, "c1c8105de0": {"title": "Make the warning of using SparseTensor in JIT less noisy", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30499\n\nTest Plan: waitforsandcastle\n\nReviewed By: wanchaol\n\nDifferential Revision: D18705553\n\nfbshipit-source-id: d6e16e3285a74a1c031a5312f7a690f1baf392f8", "pr_number": "30499", "files_changed": ["torch/csrc/jit/pybind_utils.h"], "labels": ["fb-exported", "jit"]}, "512c2a2df5": {"title": "Enable constant folding (#29834)", "body": "Summary:\nSet default do_constant_folding = True\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29834\n\nReviewed By: hl475\n\nDifferential Revision: D18588037\n\nPulled By: houseroad\n\nfbshipit-source-id: b35c06161321629c886e177ea666eff31cebf06a", "pr_number": "29834", "files_changed": ["test/onnx/expect/TestOperators.test_arange_dynamic.expect", "test/onnx/expect/TestOperators.test_baddbmm.expect", "test/onnx/expect/TestOperators.test_batchnorm.expect", "test/onnx/expect/TestOperators.test_batchnorm_1d.expect", "test/onnx/expect/TestOperators.test_batchnorm_noaffine.expect", "test/onnx/expect/TestOperators.test_batchnorm_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_batchnorm_training.expect", "test/onnx/expect/TestOperators.test_bitshift.expect", "test/onnx/expect/TestOperators.test_prelu.expect", "test/onnx/expect/TestOperators.test_retain_param_name_disabled.expect", "test/onnx/expect/TestOperators.test_slice_dynamic.expect", "test/onnx/test_onnx_opset.py", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/onnx/test_verify.py", "test/onnx/verify.py", "torch/onnx/__init__.py", "torch/onnx/utils.py"], "labels": []}, "d2336edcfb": {"title": "Boxed variable dispatch (#29934)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29934\n\nPreviously, when doing boxed dispatch (e.g. custom ops), the dispatcher manually removed the VariableTensorId flag before dispatching\nbecause custom ops don't have variable kernels.\nThis is one of the blockers that prevented us from using the boxed dispatch mechanism for ops from native_functions.yaml because they define variable kernels and need them to be called for autograd.\n\nThis PR changes that. The dispatcher doesn't remove the VariableTensorId flag anymore.\nInstead, to make custom ops work, we implement a variable fallback kernel that is called whenever no other variable kernel was found.\nghstack-source-id: 94618474\n\nTest Plan: unit tests\n\nDifferential Revision: D18542342\n\nfbshipit-source-id: a30ae35d98f89f7ae507151f55c42cfbed54a451", "pr_number": "29934", "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h"], "labels": []}, "1d3f3a1a0c": {"title": "Add pybind11 trampoline class for c10d.Store (#30415)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30415\n\nThis enables subclassing of c10d.Store and implementing its interface in Python.\nghstack-source-id: 94586627\n\nTest Plan: New tests passes.\n\nReviewed By: vladbelous\n\nDifferential Revision: D18693018\n\nfbshipit-source-id: fa1eba4bd11cc09a3d6bf3f35369c885033c63c0", "pr_number": "30415", "files_changed": ["test/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp"], "labels": []}, "0282c5ae69": {"title": "Add helper to aggregate multiple process groups (#25768)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25768\n\nThe round robin process group can be constructed from multiple other\nprocess groups. Every collective call against this new process group\nis delegated to the specified process groups in a round robin fashion.\n\nDoing so may benefit performance when calling into multiple NCCL\nprocess groups. Instead of adding support for round-robin usage of\nNCCL communicators, we achieve the same without changing the NCCL\nprocess group and adding this wrapper class.\n\nThe API to create this round robin process group is a bit harsh. If we\nfind it adds significant benefit we can revisit and make this a first\nclass citizen in the torch.distributed module.\nghstack-source-id: 94578376\n\nTest Plan: The newly added test passes.\n\nReviewed By: chenyangyu1988\n\nDifferential Revision: D17226323\n\nfbshipit-source-id: ec9f754b66f33b983fee30bfb86a1c4c5d74767d", "pr_number": "25768", "files_changed": ["test/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/CMakeLists.txt", "torch/lib/c10d/ProcessGroupRoundRobin.cpp", "torch/lib/c10d/ProcessGroupRoundRobin.hpp"], "labels": ["module: distributed"]}, "1e8ed021c6": {"title": "Support logsoftmax with dim != -1 (#30433)", "body": "Summary:\nPyTorch dim and ONNX axis have different meanings.\nONNX only supports log_softmax with dim = -1. Transpose must be added before and after log_softmax to support other cases.\nThis requires input rank to be known at export time.\nFixes https://github.com/pytorch/pytorch/issues/17918\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30433\n\nReviewed By: hl475\n\nDifferential Revision: D18723520\n\nPulled By: houseroad\n\nfbshipit-source-id: d0ed3b3f051d08d46495a7abfa854edd120dca3a", "pr_number": "30433", "files_changed": ["test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": []}, "d0acc9c085": {"title": "Switch PyTorch/Caffe2 to C++14 (#30406)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30406\n\nghstack-source-id: 94642238\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D17908478\n\nfbshipit-source-id: 6e340024591ec2c69521668022999df4a33b4ddb", "pr_number": "30406", "files_changed": ["CMakeLists.txt", "CONTRIBUTING.md", "android/pytorch_android/CMakeLists.txt", "aten/src/ATen/cpu/tbb/CMakeLists.txt", "aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt", "aten/src/ATen/test/test_install/CMakeLists.txt", "c10/CMakeLists.txt", "c10/util/C++17.h", "cmake/Dependencies.cmake", "cmake/MiscCheck.cmake", "cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake", "cmake/TorchConfig.cmake.in", "cmake/public/cuda.cmake", "cmake/public/utils.cmake", "docs/cpp/source/installing.rst", "ios/LibTorch.podspec", "setup.py", "test/custom_operator/CMakeLists.txt", "torch/csrc/jit/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/fuser/cuda/fused_kernel.cpp", "torch/lib/c10d/CMakeLists.txt", "torch/lib/libshm/CMakeLists.txt", "torch/utils/cpp_extension.py"], "labels": ["jit"]}, "fcb7371e65": {"title": "Update docs for cpp_extension on Windows (#30392)", "body": "Summary:\nTargets https://github.com/pytorch/pytorch/issues/30379.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30392\n\nDifferential Revision: D18730438\n\nPulled By: albanD\n\nfbshipit-source-id: f718d006ee8aaaa356c1e15e53a0469f15e8ed41", "pr_number": "30392", "files_changed": ["docs/cpp/source/installing.rst"], "labels": []}, "106ab487eb": {"title": "fix typo in doc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30518\n\nDifferential Revision: D18729361\n\nPulled By: albanD\n\nfbshipit-source-id: 4e386b99e898b9cd8f9a21dff642d0f40355899f", "pr_number": "30518", "files_changed": ["c10/core/TensorTypeId.h"], "labels": []}, "a69be8123a": {"title": "Use `gettimeofday` on iOS (#30361)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30361\n\n### Summary\n\nBy default, the compiler will choose `clock_gettime` for the iOS build. However, that API is not available until iOS 10. Since the Facebook app still supports iOS 9.0,  we have to use `gettimeofday` instead.\n\n```shell\nxplat/caffe2/torch/csrc/autograd/profiler.h:86:3: error: 'clock_gettime' is only available on iOS 10.0 or newer [-Werror,-Wunguarded-availability]\n\nxplat/caffe2/torch/csrc/autograd/profiler.h:86:17: error: '_CLOCK_MONOTONIC' is only available on iOS 10.0 or newer [-Werror,-Wunguarded-availability]\n```\n\nP.S. the open-sourced version is iOS 12.0 and above, so we don't have this problem.\n\n### Test Plan\n\n- buck build works\n- Don't break CIs\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18730262\n\nPulled By: xta0\n\nfbshipit-source-id: fe6d954b8d3c23cbc9d1e25a2e72e0b0c1d4eaa9", "pr_number": "30361", "files_changed": ["torch/csrc/autograd/profiler.h"], "labels": []}, "c1c5622a6a": {"title": "Add katex to pytorch-linux-xenial-py3.6-gcc5.4 docker image (#30522)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30522\n\nThis is in preparation for moving the docs push CI jobs to depend on\n`pytorch-linux-xenial-py3.6-gcc5.4` rather than\n`pytorch-linux-xenial-cuda9-cudnn7-py3`.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18731108\n\nPulled By: zou3519\n\nfbshipit-source-id: fd753a5ca818fa73a14e4276c33368a247cc40e1", "pr_number": "30522", "files_changed": [".circleci/docker/build.sh"], "labels": []}, "7d2b0aa693": {"title": "add retries to network operations (curl, conda install, git clone) (#30479)", "body": "Summary:\nAddresses some of the top network-related flakiness occurrences.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30479\n\nDifferential Revision: D18736386\n\nPulled By: kostmo\n\nfbshipit-source-id: 9eb5dca0cd0281894a0b304fbaf59a0341d3ff58", "pr_number": "30479", "files_changed": [".circleci/config.yml", ".circleci/docker/common/install_android.sh", ".circleci/docker/common/install_cache.sh", ".circleci/docker/common/install_cmake.sh", ".circleci/scripts/binary_checkout.sh", ".circleci/scripts/binary_install_miniconda.sh", ".circleci/scripts/binary_ios_build.sh", ".circleci/scripts/binary_linux_test.sh", ".circleci/scripts/setup_ci_environment.sh", ".circleci/scripts/setup_linux_system_environment.sh", ".circleci/verbatim-sources/binary-job-specs.yml", ".circleci/verbatim-sources/caffe2-job-specs.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".jenkins/caffe2/build.sh", ".jenkins/pytorch/build.sh", ".jenkins/pytorch/macos-common.sh", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_magma.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_miniconda3.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_mkl.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_sccache.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat"], "labels": []}, "0b25371f5d": {"title": "Turn off scalar_check for _th_normal.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29955\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18548051\n\nPulled By: gchanan\n\nfbshipit-source-id: c652999ac9e37d2592aa85ef022040fe0700b5cf", "pr_number": "29955", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/cuda/Distributions.cu", "test/test_torch.py"], "labels": []}, "87f29557bd": {"title": "Ignore logical_and and logical_or in op BC check for now (#30537)", "body": "Summary:\nGet the CI happy.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30537\n\nReviewed By: hl475\n\nDifferential Revision: D18738567\n\nPulled By: houseroad\n\nfbshipit-source-id: f30a87e22653b83ebdb1b54851460ec245866ecf", "pr_number": "30537", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": []}, "7ac8efa689": {"title": "Skip undefined tensors when moving torch::nn module to a different device (#30523)", "body": "Summary:\nThis fixes high-pri issues such as https://github.com/pytorch/pytorch/issues/30508 and https://github.com/pytorch/pytorch/issues/30462.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30523\n\nDifferential Revision: D18732904\n\nPulled By: yf225\n\nfbshipit-source-id: fe5a7a43838000f5803bd9c01ecfba0c3f02df5d", "pr_number": "30523", "files_changed": ["test/cpp/api/module.cpp", "torch/csrc/api/include/torch/nn/module.h"], "labels": ["module: cpp"]}, "1350b99de4": {"title": "Add local shutdown to process group agent (#30330)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30330\n\nThis is now possible due to previous changes made in `gloo` and `ProcessGroupGloo`. We `abort` the listener thread that is waiting for a message, and join all other threads. The API is changed so that the previous `wait_all_workers` does not destroy the agent, and this is now done in a new `shutdown` method. All callsites are updated appropriately.\n\nghstack-source-id: 94673884\nghstack-source-id: 94673884\n\nTest Plan: Unit tests pass.\n\nReviewed By: mrshenli\n\nDifferential Revision: D18661775\n\nfbshipit-source-id: 5aaa7c14603e18253394224994f6cd43234301c2", "pr_number": "30330", "files_changed": ["docs/source/notes/distributed_autograd.rst", "docs/source/rpc.rst", "test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/api.py"], "labels": []}, "f4e7e9039d": {"title": "Improve process_group_agent() serialization speed (#29785)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29785\n\nTLDR: This change improves process_group's serialization speed:\n  Serialize_Tensor64:     12.38us ->   1.99us  (~-84%)\n  Deserialize_Tensor64:   33.89us ->   5.62us  (~-84%)\n  Serialize_Tensor1M:    525.74us -> 285.43us  (~-45%)\n  Deserialize_Tensor1M:  892.61us -> 273.68us  (~-70%)\n\nAfter speaking with the jit team, we had consensus that torch::save()/load()\nare somewhat high-overhead for RPC serialization, mostly intended for\npersistent disk data.\n\n(Particularly, for large tensors, 35% of the time is spent in CRC checking, even\nwith the fb-side changes to subsitute 40x faster SSE-accelerated crc checking;\nAlso, for small tensors, the zip container overhead is considerable, as is the\noverhead of lexing/parsing an embedded text python program for each RPC).\n\nThe jit team encouraged us to use jit::pickler, with the WriteableTensorData\nway of outputting result tensors (not the default side-tensor table, or\nwith pickling the actual tensors). This ends up just pickling some tensor\nmetadata, and giving us some tensor blobs that we can mindlessly\nblit over the wire (they copy to cpu memory if needed).\n\nThere is yet no standardized container format for the pickled data\n(there is jit::pickle_save() checked in, but but it's experimental,\nno load function is yet provided), but they encouraged us to just use\nsomething sensible for this, and possibly revisit later. For now, I made\nthe directory headers slightly http-inspired.\n\nNote that serialization is just one component of the pipeline, but that\nsaid, we also see reasonable reductions in end-to-end echo times (noisier):\n   ProcessGroupAgent_Echo(Tensor_Small)   855.25us -> 492.65us  (~-42%)\n   ProcessGroupAgent_Echo(Tensor_1M)       10.82ms -> 6.94ms    (~-35%)\n   ProcessGroupAgent_Echo(Small_NoTensor) 688.82us -> 301.72us  (~-56%)\n   ProcessGroupAgent_Echo(1MB_NoTensor)     4.65ms -> 3.71ms    (~-20%)\n\nI moved the \"wire serialization\" logic to a separate file to assist with\nunittesting.\nghstack-source-id: 94694682\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/test/cpp/api:serialize\n  buck test mode/dev-nosan caffe2/test/...\n\nDifferential Revision: D18493938\n\nfbshipit-source-id: 07ddfe87dbe56472bc944f7d070627052c94a8f4", "pr_number": "29785", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/rpc/CMakeLists.txt", "test/cpp/rpc/test_wire_serialization.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h"], "labels": []}, "ec5e471647": {"title": "Reorganize rpc API doc and add introduction (#30491)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30491\n\nOur RPC API docs presents the APIs well but misses a general\nintroduction to the APIs. Readers might be a little lost the first\ntime landing this page. This commits reorganizes the APIs into\nfour components from user's perspective, RPC, RRef, dist autograd,\nand dist optimizer. It also adds an intro to each and briefly\ndiscribes why we provide those.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18723294\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4aced4ab537b070aa780aaaf9724659fd47cb3cb", "pr_number": "30491", "files_changed": ["docs/source/rpc.rst"], "labels": []}, "30d70d5378": {"title": "Make doc source format consistent in rpc/init.cpp", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30515\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18728184\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7b643c7f8225943113fbd7130ff6aadb30c1d4e9", "pr_number": "30515", "files_changed": ["torch/csrc/distributed/rpc/init.cpp"], "labels": []}, "dd52f50fc8": {"title": "Add examples to RRef doc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30516\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18728183\n\nPulled By: mrshenli\n\nfbshipit-source-id: af472ebed0e6dd0a85653b080abd3ac4d482bd26", "pr_number": "30516", "files_changed": ["torch/csrc/distributed/rpc/init.cpp"], "labels": []}, "53785771a7": {"title": "Don't build test_cpp_rpc if torch is built without distributed support (#30587)", "body": "Summary:\nOn the latest master, I get link errors when building one of the tests:\n\n```sh\n/home/pbell/git/pytorch/build/../test/cpp/rpc/test_wire_serialization.cpp:23:\nundefined reference to `torch::distributed::rpc::wireDeserialize(void const*, unsigned long)'\n```\n\nThis seems to be caused by PR https://github.com/pytorch/pytorch/issues/29785 not working with `USE_DISTRIBUTED=0`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30587\n\nDifferential Revision: D18758625\n\nPulled By: jjlilley\n\nfbshipit-source-id: 0ad0703acdbbac22bb4b8317370fbe2606fcb67e", "pr_number": "30587", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["open source"]}, "c780610f2d": {"title": "Disable test_backward_per_tensor in test_fake_quant (#30594)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30594\n\nThis testcase started breaking, clean up for the build.\nghstack-source-id: 94736837\n\nTest Plan: Unittest disabling change\n\nDifferential Revision: D18758635\n\nfbshipit-source-id: 05df1158ff0ccd75e401f352da529fb663b1cae0", "pr_number": "30594", "files_changed": ["test/test_fake_quant.py"], "labels": []}, "e6000a7c04": {"title": "Temporarily disable test_numerical_consistency_per_tensor (#30600)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30600\n\ntest_numerical_consistency_per_tensor in test_fake_quant is failing on Windows.\nghstack-source-id: 94742124\n\nTest Plan: CircleCI tests\n\nDifferential Revision: D18760287\n\nfbshipit-source-id: 7f59355eab74e811bb370ad2836ed2f1def1f621", "pr_number": "30600", "files_changed": ["test/test_fake_quant.py"], "labels": []}, "8ee61e0be4": {"title": "Fix CPU_INTEL flag error on windows (#30564)", "body": "Summary:\n${CMAKE_HOST_SYSTEM_PROCESSOR} get processor name by `uname -p` on linux and `%PROCESSOR_ARCHITECTURE%` on windows\n1. %PROCESSOR_ARCHITECTURE% has value in (AMD64|IA64|ARM64) for 64-bit processor, and (x86) for 32-bit processor\n2. `uname -p` has value like \"(x86_64|i[3-6]+86)\"\nWe cannot tell intel cpu from other cpus by ${CMAKE_HOST_SYSTEM_PROCESSOR}. It is the architecture, not provider.\ni. e. Intel CPU i7-9700K CPU on windows get \"AMD64\"\n\nreference:\n[MSDN](https://docs.microsoft.com/zh-cn/windows/win32/winprog64/wow64-implementation-details?redirectedfrom=MSDN)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30564\n\nDifferential Revision: D18763031\n\nPulled By: ezyang\n\nfbshipit-source-id: 11ae20e66b4b89bde1dcf4df6177606a3374c671", "pr_number": "30564", "files_changed": ["CMakeLists.txt"], "labels": ["merge-this-please"]}, "b68d1fc316": {"title": "add small input shapes to some ops (#30617)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30617\n\nas title\n\nTest Plan: buck run //caffe2/benchmarks/operator_benchmark:benchmark_all_test -- --iterations 1 --operator add,as_strided,cat,chunk,fill,linear,matmul,split\n\nReviewed By: hl475\n\nDifferential Revision: D18764248\n\nfbshipit-source-id: 510cf83542822acfa1b7b5e475b0cc7432f7ac19", "pr_number": "30617", "files_changed": ["benchmarks/operator_benchmark/pt/add_test.py", "benchmarks/operator_benchmark/pt/as_strided_test.py", "benchmarks/operator_benchmark/pt/cat_test.py", "benchmarks/operator_benchmark/pt/chunk_test.py", "benchmarks/operator_benchmark/pt/fill_test.py", "benchmarks/operator_benchmark/pt/linear_test.py", "benchmarks/operator_benchmark/pt/matmul_test.py", "benchmarks/operator_benchmark/pt/split_test.py"], "labels": ["fb-exported"]}, "6deb41c88d": {"title": "Update magma to 2.5.1 for Windows and switch CUDA in CI to 9.2", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30513\n\nDifferential Revision: D18764184\n\nPulled By: ezyang\n\nfbshipit-source-id: 4992869fd6a89471a5d25eb6a9b44ad8eceb480f", "pr_number": "30513", "files_changed": [".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_magma.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", "docs/source/notes/windows.rst"], "labels": []}, "1111a6b810": {"title": "Use pybind11::gil_scoped_* functions instead of AutoGIL/AutoNoGIL (#30274)", "body": "Summary:\nReland of https://github.com/pytorch/pytorch/pull/29095\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30274\n\nDifferential Revision: D18762293\n\nPulled By: ezyang\n\nfbshipit-source-id: d3d50c2dd12bcb678ab25fa708eb6587cc4b66f9", "pr_number": "30274", "files_changed": ["tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_nn_functions.h", "tools/autograd/templates/python_nn_functions_dispatch.h", "tools/autograd/templates/python_torch_functions.cpp", "tools/autograd/templates/python_torch_functions_dispatch.h", "tools/autograd/templates/python_variable_methods.cpp", "tools/autograd/templates/python_variable_methods_dispatch.h", "tools/build_variables.py", "torch/csrc/Exceptions.cpp", "torch/csrc/Exceptions.h", "torch/csrc/README.md", "torch/csrc/autograd/python_anomaly_mode.cpp", "torch/csrc/autograd/python_anomaly_mode.h", "torch/csrc/autograd/python_cpp_function.cpp", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_function.cpp", "torch/csrc/autograd/python_function.h", "torch/csrc/autograd/python_hook.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/cuda/Event.cpp", "torch/csrc/cuda/Module.cpp", "torch/csrc/cuda/Stream.cpp", "torch/csrc/cuda/python_comm.cpp", "torch/csrc/cuda/python_nccl.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/jit/init.cpp", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/python_interpreter.cpp", "torch/csrc/jit/python_ir.cpp", "torch/csrc/jit/python_tracer.cpp", "torch/csrc/jit/script/init.cpp", "torch/csrc/utils/auto_gil.h", "torch/csrc/utils/cuda_lazy_init.cpp", "torch/csrc/utils/init.cpp", "torch/csrc/utils/tensor_list.cpp", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_numpy.cpp", "torch/csrc/utils/throughput_benchmark.cpp"], "labels": ["jit"]}, "d32f261f16": {"title": "make the order btw div and mul in adagrad update consistent (#30449)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30449\n\nThere was an inconsistency in the order of operation between scalar and SIMD code when we compute Adagrad.\nIn this diff we first compute effective_lr = lr / (sqrt(moment) + epsilon) and then multiply with gradient.\n\nTest Plan: CI\n\nReviewed By: protonu\n\nDifferential Revision: D18703416\n\nfbshipit-source-id: 2a8b2a3f5401466549561412bd22f07abac3c598", "pr_number": "30449", "files_changed": ["caffe2/perfkernels/adagrad.h", "caffe2/perfkernels/adagrad_avx.cc", "caffe2/sgd/adagrad_op.h"], "labels": ["fb-exported"]}, "3636cb0364": {"title": "windows build (#30556)", "body": "Summary:\nbased on https://github.com/pytorch/pytorch/pull/28677\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30556\n\nDifferential Revision: D18764040\n\nPulled By: mingbowan\n\nfbshipit-source-id: 53104636800f5887b74a82c154bc5e9603de9322", "pr_number": "30556", "files_changed": [".circleci/config.yml", ".circleci/generate_config_yml.py", ".circleci/verbatim-sources/header-section.yml", ".circleci/verbatim-sources/pytorch-job-specs.yml", ".circleci/verbatim-sources/windows-build-test.yml", ".jenkins/pytorch/win-build.sh", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", ".jenkins/pytorch/win-test.sh", "test/test_cuda.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "569729527b": {"title": "Turn off scalar_checks for exp, cos, cosh, tan, atan, tanh, erf, erfc. (#30434)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30434\n\nThese are all pointwise ops that are implemented correctly wrt shapes in THC.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18699087\n\nPulled By: gchanan\n\nfbshipit-source-id: 82cb91b00c77bfaca75be497c87fc7ae52daf46c", "pr_number": "30434", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "19b7d49fac": {"title": "Add TOC to CONTRIBUTING.md (#29671)", "body": "Summary:\nThis TOC is manually generated but `CONTRIBUTING.md` seems like its\nstable enough for that to be okay\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29671\n\nPulled By: driazati\n\nDifferential Revision: D18771604\n\nfbshipit-source-id: 0d6c9c6cf1083d3be413219d3cead79c2fe5050b", "pr_number": "29671", "files_changed": ["CONTRIBUTING.md"], "labels": []}, "9c02b88791": {"title": "Add pickler support for Device (#30131)", "body": "Summary:\nThis PR adds (un)pickling support for `c10::Device`. It also adds `torch.device` as a type annotation for device attributes.\n](https://our.intern.facebook.com/intern/diff/18664421/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30131\n\nPulled By: driazati\n\nDifferential Revision: D18664421\n\nfbshipit-source-id: 64378fb42b2d1bbe2bd86259e5ed10f24b5d1e49", "pr_number": "30131", "files_changed": ["aten/src/ATen/core/Dict_inl.h", "test/test_jit.py", "torch/csrc/jit/pickler.cpp", "torch/csrc/jit/pickler.h", "torch/csrc/jit/python_ir.cpp", "torch/csrc/jit/unpickler.cpp", "torch/csrc/jit/unpickler.h", "torch/jit/annotations.py"], "labels": ["jit"]}, "98ab55fc51": {"title": "PRAGMA missing for clang (#30351)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30351\n\nNot sure what proper fix is, clang is having trouble with the loop pragmas. This at least gets things compiling.\nghstack-source-id: 94458450\n\nTest Plan: CI passes\n\nDifferential Revision: D18665812\n\nfbshipit-source-id: b8a899ce4138010cbe308eaa2c0838dd9e15573f", "pr_number": "30351", "files_changed": ["aten/src/TH/generic/THTensorApply.hpp"], "labels": []}, "2d0a4e42e9": {"title": "Add barriers to fix flaky test_graph_for_py_nested_call and (#30624)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30624\n\nThese tests were flaky since we would end up calling the 'verify'\nmethods before some of the RPCs were done. The `check_rpc_done` function might\nnot guarantee this since set_rpc_done sets an appropriate flag in python which\ncauses `check_rpc_done` to pass. Although, there are a few steps after that\nlike attaching the send functions for the response of the RPC that might not\nhave executed by then.\nghstack-source-id: 94781954\n\nTest Plan: Run the tests 100 times.\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D18768786\n\nfbshipit-source-id: a14c3f4b27de14fe5ecc6e90854dc52652f769b8", "pr_number": "30624", "files_changed": ["test/dist_autograd_test.py"], "labels": []}, "968c0d4a46": {"title": "Add support for converting quantized AvgPool2d and Reshape operations (#30490)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30490\n\nAdd symbolic mapping to Int8AvgPool2d and Int8Reshape op in C2\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps\n\nImported from OSS\n\nDifferential Revision: D18740520\n\nfbshipit-source-id: 1606125500c4b549fbc984e7929b7fd5204396a0", "pr_number": "30490", "files_changed": ["caffe2/onnx/backend.cc", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/onnx/symbolic_caffe2.py"], "labels": []}, "9e3d19412b": {"title": "Disable implicit conversion warning (#30529)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30529\n\nWe started to see build failures for multiple services with top-of-trunk LLVM compiler. The failures point to a warning that was treated as error for implicit conversion from long to double. Per discussion on D18642524, I'm disabling this warning from the containing TARGET file. T58053069 opened for code owner to track this - a proper source code fix and more unit test is needed.\n\nTest Plan: local build, sandcastle\n\nReviewed By: smessmer\n\nDifferential Revision: D18668396\n\nfbshipit-source-id: 28c0ff3258c5ba3afd41a0053f9fe1b356a496a8", "pr_number": "30529", "files_changed": ["c10/util/Half.h"], "labels": ["fb-exported"]}, "db81e13d6b": {"title": "Fix TCPStoreTest and improve tcputils::connect() (#30354)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30354\n\nTCPStoreTest would timeout since the TCPStore constructor for the\nserver would block the main thread waiting for workers. The workers themselves\nwere spawned later on once the server store is created. As a result, this test\nwould always timeout.\n\nTo fix the test, I moved the server store to a thread so that the workers can\nregister with the server in parallel.\n\nIn addition to this made a few improvements to tcputils::connect. When\ntcputils::connect() encountered an exception, it always looked at `errno` for\nthe error code. In some cases `errno` could be overwritten and the real error\ncode would be stored in `std::system_error`. As a result, I've modified the\ncode to look at the error code in `std::system_error` if we catch an exception\nof that type.\nghstack-source-id: 94758939\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18668454\n\nfbshipit-source-id: d5a3c57b066b094bfecda9a79d9d31bfa32e17f0", "pr_number": "30354", "files_changed": ["torch/lib/c10d/Utils.cpp", "torch/lib/c10d/test/CMakeLists.txt", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": []}, "4dab29a2bd": {"title": "Fix serialization memory lifetime issue. (#30603)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30603\n\nPickler object needs to be kept in scope until data is written out to the\nfinal serialized string. tensorData in particular is a reference to memory\nowned by the descoped Pickle object.\n\nNoticed this by inspection. In practice, this potential read-after-free here\nis limited to non-cpu tensors, and any such use was very soon after free.\nghstack-source-id: 94756036\n\nTest Plan: existing test suite at buck test mode/dev-nosan caffe2/test:rpc_fork\n\nDifferential Revision: D18760463\n\nfbshipit-source-id: 9de890d66626aa48f13ca376dd9bd50b92e0cb00", "pr_number": "30603", "files_changed": ["torch/csrc/distributed/rpc/utils.cpp"], "labels": []}, "0bebfe2143": {"title": "Add the explicit per-tensor/per-channel quant info when we print the module (#30591)", "body": "Summary:\nAs Title says. We would like to explicitly distinguish per-tensor/per-channel scheme when we print the module.\n\nHere is an example for Lenet after applying the per-channel dynamic quantization:\n\nBefore this PR:\n```\nFloatModel(\n  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): DynamicQuantizedLinear(\n    in_features=800, out_features=500\n    (_packed_params): LinearPackedParams()\n  )\n  (fc2): DynamicQuantizedLinear(\n    in_features=500, out_features=10\n    (_packed_params): LinearPackedParams()\n  )\n)\n```\n\nAfter this PR:\n```\nFloatModel(\n  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): DynamicQuantizedLinear(\n    in_features=800, out_features=500, qscheme=torch.per_channel_affine\n    (_packed_params): LinearPackedParams()\n  )\n  (fc2): DynamicQuantizedLinear(\n    in_features=500, out_features=10, qscheme=torch.per_channel_affine\n    (_packed_params): LinearPackedParams()\n  )\n)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30591\n\nDifferential Revision: D18764366\n\nPulled By: jianyuh\n\nfbshipit-source-id: e897ab42ace6b82b2a90729ba788313c7873de1a", "pr_number": "30591", "files_changed": ["torch/nn/quantized/dynamic/modules/linear.py", "torch/nn/quantized/modules/linear.py"], "labels": []}, "e7fe64f6a6": {"title": "Fix typos (#30606)", "body": "Summary:\nShould be non-semantic.\n\nUses https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines to find likely typos.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30606\n\nDifferential Revision: D18763028\n\nPulled By: mrshenli\n\nfbshipit-source-id: 896515a2156d062653408852e6c04b429fc5955c", "pr_number": "30606", "files_changed": [".jenkins/caffe2/build.sh", "aten/src/ATen/core/MT19937RNGEngine.h", "aten/src/ATen/core/PhiloxRNGEngine.h", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/dlpack.h", "aten/src/ATen/native/Linear.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cpu/avx_mathfun.h", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/LossCTC.cu", "aten/src/ATen/native/cuda/SpectralOps.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/test/test_assert.h", "aten/src/TH/THTensorApply.h", "benchmarks/fastrnns/README.md", "binaries/benchmark_helper.cc", "binaries/convert_and_benchmark.cc", "c10/core/TensorTypeId.h", "c10/core/TensorTypeSet.h", "c10/test/util/bfloat16_test.cpp", "c10/util/Deprecated.h", "c10/util/Half.h", "c10/util/tempfile.h", "caffe2/contrib/tensorrt/tensorrt_op_trt.cc", "caffe2/core/blob_serialization.h", "caffe2/core/context_gpu.cu", "caffe2/core/context_gpu.h", "caffe2/core/net_dag_utils.cc", "caffe2/core/nomnigraph/include/nomnigraph/Transformations/SubgraphMatcher.h", "caffe2/core/operator.h", "caffe2/core/plan_executor.cc", "caffe2/mobile/contrib/ios/ios_caffe_predictor.h", "caffe2/mobile/contrib/ios/mpscnn/mpscnn.mm", "caffe2/operators/concat_split_op.h", "caffe2/operators/gather_ranges_to_dense_op.cc", "caffe2/operators/load_save_op_util.cc", "caffe2/operators/quantized/int8_roi_align_op.h", "caffe2/operators/reducer_functors.h", "caffe2/operators/roi_align_gradient_op.cc", "caffe2/operators/roi_align_gradient_op.cu", "caffe2/operators/roi_align_op.cc", "caffe2/operators/roi_align_op.cu", "caffe2/operators/roi_align_rotated_gradient_op.cu", "caffe2/operators/roi_align_rotated_op.cc", "caffe2/operators/roi_align_rotated_op.cu", "caffe2/operators/segment_reduction_op.h", "caffe2/operators/stats_put_ops.cc", "caffe2/operators/stats_put_ops.h", "caffe2/operators/text_file_reader.cc", "caffe2/operators/utility_ops.h", "caffe2/opt/converter.cc", "caffe2/opt/onnxifi_transformer.cc", "caffe2/predictor/emulator/data_filler.h", "caffe2/proto/caffe2.proto", "caffe2/python/crf.py", "caffe2/python/dlpack.h", "caffe2/python/hypothesis_test.py", "caffe2/python/layer_model_helper.py", "caffe2/python/layers/batch_lr_loss.py", "caffe2/python/layers/feature_sparse_to_dense.py", "caffe2/python/lstm_benchmark.py", "caffe2/python/memonger.py", "caffe2/python/model_helper.py", "caffe2/python/modeling/gradient_clipping.py", "caffe2/python/models/resnet.py", "caffe2/python/modifier_context.py", "caffe2/python/normalizer_context.py", "caffe2/python/onnx/backend.py", "caffe2/python/operator_test/dataset_ops_test.py", "caffe2/python/operator_test/gather_ops_test.py", "caffe2/python/operator_test/pooling_test.py", "caffe2/python/optimizer_context.py", "caffe2/python/optimizer_test_util.py", "caffe2/python/pipeline.py", "caffe2/python/regularizer.py", "caffe2/python/regularizer_context.py", "caffe2/python/schema.py", "caffe2/python/session.py", "caffe2/python/task.py", "caffe2/quantization/server/dnnlowp_test_utils.py", "caffe2/serialize/inline_container.h", "caffe2/sgd/clip_tensor_op.cc", "caffe2/video/video_decoder.cc", "docs/source/community/contribution_guide.rst", "docs/source/distributed.rst", "docs/source/hub.rst", "docs/source/name_inference.rst", "modules/detectron/smooth_l1_loss_op.cc", "scripts/fbcode-dev-setup/onnx_c2_setup.sh", "test/common_utils.py", "test/cpp/api/dataloader.cpp", "test/cpp/api/nn_utils.cpp", "test/cpp/jit/test_utils.h", "test/dist_autograd_test.py", "test/run_test.py", "test/test_cpp_api_parity.py", "test/test_nn.py", "test/test_quantization.py", "test/test_quantized.py", "tools/autograd/utils.py", "tools/clang_tidy.py", "tools/jit/gen_jit_dispatch.py", "torch/_jit_internal.py", "torch/_torch_docs.py", "torch/backends/cudnn/__init__.py", "torch/csrc/DataLoader.cpp", "torch/csrc/api/include/torch/data/dataloader/stateful.h", "torch/csrc/api/include/torch/ordered_dict.h", "torch/csrc/autograd/profiler.cpp", "torch/csrc/distributed/autograd/context/context.h", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/generic/StorageMethods.cpp", "torch/csrc/jit/argument_spec.cpp", "torch/csrc/jit/fuser/cpu/temp_file.h", "torch/csrc/jit/passes/alias_analysis.h", "torch/csrc/jit/passes/create_autodiff_subgraphs.cpp", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/python_ir.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/register_special_ops.cpp", "torch/csrc/jit/script/compiler.cpp", "torch/csrc/jit/script/module.h", "torch/csrc/jit/script/object.h", "torch/csrc/jit/script/parser.cpp", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/csrc/jit/unpickler.cpp", "torch/cuda/__init__.py", "torch/distributed/launch.py", "torch/jit/__init__.py", "torch/lib/libshm/err.h", "torch/multiprocessing/reductions.py", "torch/nn/functional.py", "torch/nn/modules/fold.py", "torch/nn/parallel/scatter_gather.pyi", "torch/nn/utils/prune.py", "torch/onnx/__init__.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset11.py", "torch/utils/checkpoint.py"], "labels": ["jit"]}, "18ec4632b3": {"title": "Exclude undefined tensors in the result of Module::parameters() / named_paramters() / buffers() / named_buffers() (#30626)", "body": "Summary:\nPR https://github.com/pytorch/pytorch/pull/30523 attempted to fix https://github.com/pytorch/pytorch/issues/30508 and https://github.com/pytorch/pytorch/issues/30462, but the fix wasn't complete. This PR makes the following improvements:\n1. Fixes https://github.com/pytorch/pytorch/issues/30508 and https://github.com/pytorch/pytorch/issues/30462 properly by excluding undefined tensors in the result of `Module::parameters()` / `named_parameters()` / `buffers()` / `named_buffers()`, which mirrors the Python API behavior.\n2. Audits all use sites of `Module::parameters_` / `buffers_` and change them to `Module::named_parameters(/*recurse=*/false)` / `named_buffers(/*recurse=*/false)` when appropriate, so that use sites of module parameters / buffers never need to worry about undefined tensors.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30626\n\nDifferential Revision: D18777507\n\nPulled By: yf225\n\nfbshipit-source-id: 55b64b69779e1186342efd3c44857f416334ed6b", "pr_number": "30626", "files_changed": ["test/cpp/api/module.cpp", "test/cpp/api/support.h", "torch/csrc/api/include/torch/nn/cloneable.h", "torch/csrc/api/include/torch/nn/module.h", "torch/csrc/api/include/torch/nn/parallel/data_parallel.h", "torch/csrc/api/src/nn/module.cpp"], "labels": ["module: cpp"]}, "e5b947a3a8": {"title": "Raise an error for is_signed on quantized types (#30527)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30527\n\nWhen we introduced dtype.is_signed we allowed for support of\nquantized types, but we're not sure what the correct result should be.\n\nSee discussion at https://github.com/pytorch/pytorch/pull/29511\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18765410\n\nPulled By: nairbv\n\nfbshipit-source-id: c87cfe999b604cfcbbafa561e04d0d5cdbf41e6d", "pr_number": "30527", "files_changed": ["c10/core/ScalarType.h", "test/test_torch.py", "torch/csrc/Dtype.cpp"], "labels": []}, "61798865e3": {"title": "Turn off scalar_checks for torch.clamp. (#30435)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30435\n\nThe underlying THC implementations are correct.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18699089\n\nPulled By: gchanan\n\nfbshipit-source-id: f5d1319bf48eae36903296dad0b98ed80661f732", "pr_number": "30435", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "8b29701ae5": {"title": "Turn off scalar_checks for _th_reciprocal. (#30436)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30436\n\nThe underlying TH implementation is correct.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18699088\n\nPulled By: gchanan\n\nfbshipit-source-id: e75a588ae4afb0506922ba98208546d5c0de623a", "pr_number": "30436", "files_changed": ["aten/src/ATen/Declarations.cwrap", "test/test_torch.py"], "labels": []}, "b446572997": {"title": "TestCppExtension now removes /tmp/torch_extensions folder so that it can be used by other users in a multi-user environment. (#30095)", "body": "Summary:\nPrevious behaviour: a user runs tests from `TestCppExtension` class so that `/tmp/torch_extensions` is created under her ownership and not removed afterwards,\nthen the other user's run of the same tests might result in 'Permission denied' exception upon deleting `/tmp/torch_extensions`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30095\n\nDifferential Revision: D18770234\n\nPulled By: ezyang\n\nfbshipit-source-id: 4c6b972e4c4327a94c8b4bf6b0b9998a01c218bb", "pr_number": "30095", "files_changed": ["test/test_cpp_extensions.py"], "labels": ["open source", "triaged"]}, "f9f54201d3": {"title": "Remove deprecated fromIvalue in RRefForkData", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30646\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18777610\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7a749c1035e36bbb464332d3829fd53e2c6cf727", "pr_number": "30646", "files_changed": ["torch/csrc/distributed/rpc/rref.cpp", "torch/csrc/distributed/rpc/rref.h"], "labels": []}, "5a484245d9": {"title": "Change test_invalid_names test to only test constructor of WorkerInfo (#30620)", "body": "Summary:\nThis tests seems to only test that we throw exceptions in the `WorkerInfo` constructor when invalid names are passed in, so I don't think we need to complicate by initializing RPC, and exposing ourselves to potential flakiness.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30620\n\nDifferential Revision: D18766955\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 11643de4d57431e5f46e096c7766de3ab0b9b05a", "pr_number": "30620", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp"], "labels": []}, "d5c136097a": {"title": "improve .view() performance (#30554)", "body": "Summary:\nImprove .view() performance by not calling set_ and instead restriding returned alias. This improves performance of .view() operation from ~500ns to ~360 ns\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30554\n\nTest Plan: covered by existing tests\n\nDifferential Revision: D18759896\n\nPulled By: ngimel\n\nfbshipit-source-id: 9757c93158bc55e9c87dc30ac3415ba8f8b849e5", "pr_number": "30554", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "aten/src/TH/THTensor.cpp", "aten/src/TH/THTensor.hpp"], "labels": []}, "89be1a22d4": {"title": "split getInvokedMethods (#30546)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30546\n\nfactor out this function for later support of quantizing shared types\n\nTest Plan:\ntest_jit.py, test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18776304\n\nfbshipit-source-id: f5a736b0f69019cefe17ec4517da1ae5462f78e1", "pr_number": "30546", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "4d30415f12": {"title": "Add ONNX Scripting Conv Support (#30618)", "body": "Summary:\nConvolution nodes are traced as aten:_convolution and are currently supported in ONNX.\nScripting convolution uses aten:conv<1,2,3>d which are currently not supported in ONNX.\nThis PR adds the symbolics for aten:conv<1,2,3>d and aten:conv_transpose<1,2,3>d\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30618\n\nReviewed By: hl475\n\nDifferential Revision: D18778145\n\nPulled By: houseroad\n\nfbshipit-source-id: 4af0379f29974a1ce8443024d1d87b3eb8d2dd36", "pr_number": "30618", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": []}, "a997f224ac": {"title": "Add torch.multiprocessing.create_processes", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28493\n\nDifferential Revision: D18766066\n\nPulled By: ailzhang\n\nfbshipit-source-id: 7f424c8fae3012be2416cf9bc72ee2dde40c1f89", "pr_number": "28493", "files_changed": ["test/test_multiprocessing_spawn.py", "torch/multiprocessing/__init__.py", "torch/multiprocessing/spawn.py"], "labels": ["module: multiprocessing"]}, "9740011f10": {"title": "Use normal dispatch to get to CUDA threshold kernels, instead of DispatchStub. (#30307)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30307\n\nDispatchStub will stop working when I split CPU/CUDA libraries, because\nthere are some symbols from the templates in DispatchStub stubs which aren't\nproperly exported and I couldn't figure out how to make them dispatch properly.\n\nThis is the only case where DispatchStub is being used to dispatch to CUDA,\nanyway.\n\nThis partially addresses #29844 but I need to also just completely delete\nthe CUDA registration logic from DispatchStub entirely.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762362\n\nPulled By: ezyang\n\nfbshipit-source-id: bdfa8739c0daf23badf3c5af61890a934af00813", "pr_number": "30307", "files_changed": ["aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": []}, "08394cede3": {"title": "DEFINE_DISPATCH in the correct namespace. (#30308)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30308\n\nDispatch is declared in non-anonymous namespace, so it definitely\nshouldn't be defined in an anonymous namespace.  This doesn't seem\nto matter today, but it matters when we split libtorch into two\nlibraries.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762361\n\nPulled By: ezyang\n\nfbshipit-source-id: 484f0fab183c385dd889db9dad3e48e92e0a3900", "pr_number": "30308", "files_changed": ["aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp", "aten/src/ATen/native/quantized/cpu/q_avgpool.cpp", "aten/src/ATen/native/quantized/cpu/qconcat.cpp", "aten/src/ATen/native/quantized/cpu/qpool.cpp"], "labels": []}, "a5b1f6e7d7": {"title": "Add missing _API definitions. (#30310)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30310\n\n- Annotate CUDAGenerator.h with correct TORCH_CUDA_API.\n  This is actually CUDA related functionality with its implementation living\n  in the cuda/ folder.  For some reason it lives at the top level; it\n  should be moved (but that should be handled in another PR.)\n- Add missing TORCH/CAFFE_API annotations to.  All of\n  these functions are used from CUDA code, which means that\n  we need to correctly annotate them if we split CPU/CUDA code\n  into separate libraries.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762357\n\nPulled By: ezyang\n\nfbshipit-source-id: c975a8e4f082fe9f4196c2cca40977623caf4148", "pr_number": "30310", "files_changed": ["aten/src/ATen/CUDAGenerator.h", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/native/sparse/SparseTensorMath.h", "caffe2/operators/load_save_op_util.h"], "labels": []}, "d43e205026": {"title": "Properly include declaration of dispatch in file that registers it. (#30311)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30311\n\nmultinomial_stub must be in scope to register against it.  Somehow,\nthis works today, but when I split torch_cpu and torch_cuda it\ndoesn't.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762358\n\nPulled By: ezyang\n\nfbshipit-source-id: ef9c111292cd02d816af1c94c8bbaadabffaabe5", "pr_number": "30311", "files_changed": ["aten/src/ATen/native/cpu/MultinomialKernel.cpp"], "labels": []}, "8269f7b652": {"title": "Delete redundant THC_API on THCStorage_new (#30312)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30312\n\nIt's not necessary because it's already defined in the header.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762363\n\nPulled By: ezyang\n\nfbshipit-source-id: 418bf355d460dd171ac449559f20bf55415e54ae", "pr_number": "30312", "files_changed": ["aten/src/THC/THCStorage.cpp"], "labels": []}, "a009fc14be": {"title": "Workaround hcc bug regarding extern \"C\" definitions (#30313)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30313\n\nSee comments in code about the bug.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762360\n\nPulled By: ezyang\n\nfbshipit-source-id: 406a01f2f0c3722b381428c89afd67b3c3c19142", "pr_number": "30313", "files_changed": ["aten/src/THC/THCTensorRandom.cu"], "labels": []}, "1b12fd33ed": {"title": "Add missing trigramma_stub definition. (#30314)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30314\n\nSomehow we forgot to define it!\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762356\n\nPulled By: ezyang\n\nfbshipit-source-id: 28afc605ad986266071e3831049ec8a7f71fd695", "pr_number": "30314", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp"], "labels": []}, "f114c33e69": {"title": "Fix iOS CI (#30327)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30327\n\n### Summary\n\nSeems like starting from macOS 10.15, we can no longer get access to the `Downloads` folder in our macOS machines.\n\n```\npermissionError: [Errno 1] Operation not permitted: '/Users/distiller/Downloads'\n```\n\nThe fix is to change the conda download directory to ${HOME}\n\n### Test Plan\n\n- iOS jobs are back to normal\n- Don't break other jobs\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18717380\n\nPulled By: xta0\n\nfbshipit-source-id: cad754076bf4ae5035741aa57a310ad87c76726e", "pr_number": "30327", "files_changed": [".circleci/config.yml", ".circleci/scripts/binary_ios_build.sh", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-nightly-ios-binary-builds.yml", ".circleci/verbatim-sources/workflows-pytorch-ios-builds.yml", "ios/TestApp/README.md", "ios/TestApp/TestApp.xcodeproj/project.pbxproj", "ios/TestApp/TestAppTests/TestAppTests.mm", "ios/TestApp/benchmark/setup.rb"], "labels": []}, "7023e13fbb": {"title": "Fix mapping white list (#30636)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30636\n\nCurrently DeQuantStub is still in whitelist because set union has\nlower precedence than set difference\nfix issue: https://github.com/pytorch/pytorch/issues/29646\n\nTest Plan:\nverified locally that we don't attach qconfig for DeQuantStub\n\nImported from OSS\n\nDifferential Revision: D18775275\n\nfbshipit-source-id: 8da07e40963555671b3d4326c9291706103f858e", "pr_number": "30636", "files_changed": ["torch/quantization/default_mappings.py"], "labels": []}, "1b5ce05924": {"title": "don't use size()/stride() functions in TensorImpl, use size_[d]/stride_[d] instead (#30452)", "body": "Summary:\nThis improved multi-d microbenchmark by ~100 ns, empty_tensor_restride used to be 13% of iteration time, now about 5%\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30452\n\nTest Plan: Covered by existing tests\n\nDifferential Revision: D18704233\n\nPulled By: ngimel\n\nfbshipit-source-id: be527f09183bc31e9d1f63fd49bfbe0998fe167f", "pr_number": "30452", "files_changed": ["c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h"], "labels": ["fb-exported"]}, "19cd90d303": {"title": "Globally record observer nodes (#30547)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30547\n\natt\n\nTest Plan:\ntest_jit.py test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18784752\n\nfbshipit-source-id: 000e140aa86ff12a240d98da71871a5a5053401f", "pr_number": "30547", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "40146eb48e": {"title": "Skip ProcessGroupGlooAyncTest if there is no CUDA available (#30345)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30345\n\nSkip ProcessGroupGlooAyncTest if there is no CUDA available, otherwise in sandcastle non GPU host the test will abort with failing to load CUDA library\nghstack-source-id: 94771241\n\nTest Plan: test skipped on non GPU host\n\nDifferential Revision: D18665322\n\nfbshipit-source-id: 8c7b89aeecc6ec007bee12d864a6058384254e61", "pr_number": "30345", "files_changed": ["torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp"], "labels": []}, "aff693ab1c": {"title": "Ensure MIOpen is called on same stream as operator for RNN (#30672)", "body": "Summary:\nTo ensure synchronization between copying of weights in RNN wei buf, and the operation, both the pyTorch operator as well as underlying MIOpen call must be on the same HIP stream. This is also consistent with MIOpen calls in other pyTorch operators\n\nezyang iotamudelta\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30672\n\nDifferential Revision: D18785683\n\nPulled By: bddppq\n\nfbshipit-source-id: 144611046cb70cfe450680295734203f253ac6e2", "pr_number": "30672", "files_changed": ["aten/src/ATen/native/miopen/RNN_miopen.cpp"], "labels": ["module: rocm", "open source"]}, "980aead1f8": {"title": "Add support for quantized slice conversion (#30498)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30498\n\nUpdated Int8SliceOp to accept dim, start and end index similar to Pytorch.\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_slice\n\nImported from OSS\n\nDifferential Revision: D18740519\n\nfbshipit-source-id: 2313f37a4936edb150ce04911b241e591e191801", "pr_number": "30498", "files_changed": ["caffe2/operators/quantized/int8_slice_op.cc", "caffe2/operators/quantized/int8_slice_op.h", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/onnx/symbolic_caffe2.py"], "labels": []}, "4e6379379c": {"title": "fetch before checking out PR tip", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30680\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18796189\n\nPulled By: suo\n\nfbshipit-source-id: 99da48e5fd510ffdf4e606c2393eb55d4f6ca8d5", "pr_number": "30680", "files_changed": [".github/workflows/lint.yml"], "labels": []}, "4d4d8e0dce": {"title": "Update persons_of_interest.rst (#30647)", "body": "Summary:\nAdding back the 3 names for the MSFT team - re: ONNX Governance.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30647\n\nDifferential Revision: D18781163\n\nPulled By: jlin27\n\nfbshipit-source-id: 7284ba29841ab41b9807c9d92694630b50de7b6a", "pr_number": "30647", "files_changed": ["docs/source/community/persons_of_interest.rst"], "labels": []}, "604a27361f": {"title": "remove tuple_parser (#30659)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30659\n\nI could only find one usage of TupleParser and it doesn't seem worth maintaining just for that one usage.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18795979\n\nPulled By: nairbv\n\nfbshipit-source-id: 6e50d65fc8fade0944f36ab20d00f1539a3d4cb8", "pr_number": "30659", "files_changed": ["tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/autograd/functions/init.cpp", "torch/csrc/utils/tuple_parser.cpp", "torch/csrc/utils/tuple_parser.h"], "labels": []}, "03a73cb9ac": {"title": "Remove namespace F = torch::nn::functional from torch/nn/modules/batchhnorm.h (#30684)", "body": "Summary:\nThis PR removes `namespace F = torch::nn::functional` from `torch/nn/modules/batchhnorm.h`, so that people don't have to define `torch::nn::functional` as `F` if they don't want to.\n\nFixes https://github.com/pytorch/pytorch/issues/30682.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30684\n\nDifferential Revision: D18795717\n\nPulled By: yf225\n\nfbshipit-source-id: c9feffbeb632cc6b4ce3e6c22c0a78533bab69ad", "pr_number": "30684", "files_changed": ["torch/csrc/api/include/torch/nn/modules/batchnorm.h", "torch/csrc/api/include/torch/nn/modules/instancenorm.h"], "labels": []}, "d4c25add45": {"title": "make sure the counter stays correct in between bailout transitions (#30186)", "body": "Summary:\nThis fixes the second issue reported in https://github.com/pytorch/pytorch/issues/29909 namely, a loop counter is assigned the wrong values after transitioning to a bailout graph.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30186\n\nDifferential Revision: D18646845\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 1f7c601dd9f35892979385ffa132fb0886a4f203", "pr_number": "30186", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/bailout_graph.cpp"], "labels": ["jit"]}, "2ba03e0287": {"title": "Enable test_trainer_ps in dist_autograd_test.py", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30341\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18769574\n\nPulled By: mrshenli\n\nfbshipit-source-id: caf25742fa1fc9dbf6486f5ec981fae3f29784bc", "pr_number": "30341", "files_changed": ["test/dist_autograd_test.py"], "labels": []}, "56dd2836ec": {"title": "Make zeros argument of torch.where same dtype as other argument (#30661)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30661\n\nCherry-picked from https://github.com/pytorch/pytorch/pull/29080\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18781870\n\nPulled By: nairbv\n\nfbshipit-source-id: 9de85aa91bf7e0856f35c7c6238a8923315ed27f\n\nCo-authored-by: ifedan", "pr_number": "30661", "files_changed": ["aten/src/ATen/native/Loss.cpp"], "labels": []}, "a376dd344c": {"title": "Added check for torch.where on CPU that both arguments have same dtype (#30662)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30662\n\nCherry picked from: https://github.com/pytorch/pytorch/pull/29081\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18782295\n\nPulled By: nairbv\n\nfbshipit-source-id: 897ab25ddf8819ca34f5e86c5d3f41debb56cb04\n\nCo-authored-by: ifedan", "pr_number": "30662", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "test/test_nn.py", "test/test_torch.py"], "labels": []}, "dcd1216efe": {"title": "Force early initialization of OpenMP in forked children (#29006)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/28389\n\nIntel's OpenMP implementation sets the thread affinity on the first call to an OpenMP function after a fork. By adding an atfork handler we can force this to happen before a user tries to set the affinity in their own DataLoader `worker_init_fn`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29006\n\nDifferential Revision: D18782456\n\nPulled By: ezyang\n\nfbshipit-source-id: ce0b515256da0cf18ceb125e0cdec99a3311bbd3", "pr_number": "29006", "files_changed": ["test/test_dataloader.py", "torch/__init__.py", "torch/multiprocessing/_atfork.py"], "labels": ["open source", "triaged"]}, "59151d3e43": {"title": "autograd/profiler: support merging FunctionEventAvg (#30677)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30677\n\nCurrently you can only add FunctionEvents to FunctionEventAvg. This makes it so you can add multiple FunctionEventAvg objects together. This is useful for merging multiple profiles together such as when dealing with distributed training.\n\nTest Plan:\nadded unit test\n\n  buck test //caffe2/test:autograd -- test_profiler\n\nReviewed By: bddppq\n\nDifferential Revision: D18785578\n\nfbshipit-source-id: 567a441dec885db7b0bd8f6e0ac9a60b18092278", "pr_number": "30677", "files_changed": ["test/test_autograd.py", "torch/autograd/profiler.py"], "labels": ["fb-exported"]}, "fef4360536": {"title": "remove default constructor in futureInfo (#30197)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30197\n\nThis default constructor was added because std::map's operator[]\nrequires a default constructor. However, instead of using operator[], we can\nuse emplace and remove the constructor, to ensure that the FutureInfo struct\ndoesnt get constructed with garbage values.\nghstack-source-id: 94802453\n\nTest Plan: Unit tests pass.\n\nDifferential Revision: D18627675\n\nfbshipit-source-id: c4cb000e60081478c0fd7308e17103ebbc4dc554", "pr_number": "30197", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": []}, "3cf8382984": {"title": "detect_anomaly() for SparseTensors (#29803)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/28649\n\n1. Modified detect_anomaly() to use isnan()\n2. isnan() for SparseTensors returns a bool Tensor of _values.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29803\n\nDifferential Revision: D18594299\n\nPulled By: ezyang\n\nfbshipit-source-id: 3f4190c569f53219be330584fc604ca43c4a6c7a", "pr_number": "29803", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "test/test_sparse.py", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/engine.cpp"], "labels": []}, "ea3697db69": {"title": "inline to prevent duplicate obj when linking (#30363)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30363\n\ngetting duplicate definition errors when linking test.\nghstack-source-id: 94472892\n\nTest Plan: CI passes\n\nDifferential Revision: D18669686\n\nfbshipit-source-id: 3d3bfc38e4247cf8bea655537824b891b84f67bc", "pr_number": "30363", "files_changed": ["test/cpp/jit/test_misc.cpp"], "labels": ["jit"]}, "4ac614191a": {"title": "Remove exp10 in TH (unused)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30422\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18764186\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 9343a5a7e4edf61ba3b85eaf846b2e149ed6529a", "pr_number": "30422", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cuda/CUDAUnaryOps.cpp", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/TH/generic/THVector.h", "aten/src/TH/generic/THVectorDefault.cpp", "aten/src/THC/THCNumerics.cuh", "aten/src/THC/THCTensorMathReduce.cuh", "aten/src/THC/generic/THCTensorMathPointwise.cu", "aten/src/THC/generic/THCTensorMathPointwise.h", "aten/src/THC/generic/THCTensorMathReduce.cu", "test/test_torch.py"], "labels": ["merge-this-please"]}, "76acf5b553": {"title": "Remove many unused bfloat16 functions in TH", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30329\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18764281\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: bc3f91c6d09d4f73c77fe1492a358128744aee76", "pr_number": "30329", "files_changed": ["aten/src/THC/THCNumerics.cuh"], "labels": []}, "ab834d5093": {"title": "Remove exp10 in TH (unused)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30422\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18764280\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 626b88a115f2efce4a53c6784f0a6660b36c97f9", "pr_number": "30422", "files_changed": ["aten/src/THC/THCNumerics.cuh"], "labels": ["merge-this-please"]}, "bb5dcaf24f": {"title": "Add logical_and and logical_or (#30521)", "body": "Summary:\nWith the CI failure caused in 8bbafa0b32d2899ef6101172d62c6049427c977b fixed (incorrect return type of the lambdas in CUDA kernels)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30521\n\nDifferential Revision: D18770151\n\nPulled By: ailzhang\n\nfbshipit-source-id: 02f0fe1d5718c34d24da6dbb5884ee8b247ce39a", "pr_number": "30521", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/README.md", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_namedtensor.py", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": []}, "6dda241ab8": {"title": "Add RRef.__str__() API", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30609\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18763593\n\nPulled By: mrshenli\n\nfbshipit-source-id: 20f1eea2d6cfe9ab2a27a9677d97dde07c1dca9b", "pr_number": "30609", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/distributed/rpc/rref_context.cpp"], "labels": []}, "63a1542ed2": {"title": "Adding Debug Info for RRef Context", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30610\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18763592\n\nPulled By: mrshenli\n\nfbshipit-source-id: ad8854bdb6250c29eaa0f582d66cfd31394312e5", "pr_number": "30610", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h"], "labels": []}, "b26401f965": {"title": "Dump operator names of a script module (#30467)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30467\n\nIntroduce function jit.export_opnames(module), which returns a list of all operator names used in the module and its submodules. One usage is to have mobile custom build to link only operators in the returned list to save the mobile size.\n\nExample:\nimport torch\nm = torch.jit.load(\"example.pt\")\nprint(torch.jit.export_opnames(m))\n\nThe outputs are in alphabetical order:\n['aten::_convolution', 'aten::add.Tensor', 'aten::add_.Tensor', 'aten::addmm', 'aten::append.Tensor', 'aten::cat', 'aten::dropout', 'aten::embedding', 'aten::matmul', 'aten::max.dim', 'aten::mul.Tensor', 'aten::permute', 'aten::relu', 'aten::t', 'aten::tanh', 'prim::ListConstruct', 'prim::TupleConstruct', 'prim::TupleUnpack']\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18801619\n\nPulled By: iseeyuan\n\nfbshipit-source-id: f9b198d3e82b095daf704ee595d8026ad889bb13", "pr_number": "30467", "files_changed": ["docs/source/torch.rst", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export.h", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py"], "labels": ["jit"]}, "7e472679ff": {"title": "pin actions/checkout version", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30703\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18805447\n\nPulled By: suo\n\nfbshipit-source-id: d58ebe0e90b81c9282d3977f36c53c54cac750d9", "pr_number": "30703", "files_changed": [".github/workflows/lint.yml"], "labels": []}, "f5c9452beb": {"title": "Fix toObject() r-value version (#30713)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30713\n\nIt should use moveToIntrusivePtr.\nThis function is a very hot one and used a lot in interpreter loop. e.g.\nGET_ATTR, SET_ATTR. Making a copy and doing incref/decref caused big overhead.\n\nReviewed By: yinghai\n\nDifferential Revision: D18805212\n\nfbshipit-source-id: 3a9368604f71638a21300ad086739c4b50f0644e", "pr_number": "30713", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["fb-exported"]}, "d12786b24f": {"title": "add __torch_function__ API override mechanism (#27064)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/24015 (see description of that issue for more details).\n\nFor a toy example, see the `DiagonalTensor` and `SubDiagonalTensor` class in test/test_overrides.py.\n\nThis PR currently contains:\n\n* tests for `__torch_function__` behavior\n* modification to `gen_python_functions` and `parse` function signatures and dispatched to correct overloaded argument.\n\nThis feature is inspired by and analogous to NumPy's `__array_function__` protocol ([see NumPy Enhancement Proposal 18](https://numpy.org/neps/nep-0018-array-function-protocol.html#trying-array-function-methods-until-the-right-one-works)).\n\n### Benchmarks:\nSee Nathan's comment below: https://github.com/pytorch/pytorch/pull/27064#issuecomment-554601189\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27064\n\nDifferential Revision: D18645954\n\nPulled By: ezyang\n\nfbshipit-source-id: 54b5e4344d7afdbcf996bb57191b0bdadc7b1767", "pr_number": "27064", "files_changed": ["docs/source/notes/extending.rst", "test/onnx/expect/TestOperators.test_frobenius_norm.expect", "test/onnx/expect/TestOperators.test_meshgrid.expect", "test/onnx/expect/TestOperators.test_unique.expect", "test/run_test.py", "test/test_overrides.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_torch_functions.cpp", "torch/_overrides.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/functional.py"], "labels": ["caffe2", "module: autograd", "module: docs", "module: internals", "module: operators", "module: pybind", "open source", "triaged"]}, "d6ca93b353": {"title": "add doc for F.softplus", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30055\n\nDifferential Revision: D18762624\n\nPulled By: zou3519\n\nfbshipit-source-id: 61da88cbb8cd0f37ac26b0fb8aaacdbe85c724ba", "pr_number": "30055", "files_changed": ["torch/nn/functional.py", "torch/nn/modules/activation.py"], "labels": ["merge-this-please"]}, "ec7bb9de1c": {"title": "format tri[lu]_indices doc better", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30377\n\nDifferential Revision: D18689152\n\nPulled By: zou3519\n\nfbshipit-source-id: 7fab1e39ecd39ef6a3869befcbe217f8d3b6a87e", "pr_number": "30377", "files_changed": ["torch/_torch_docs.py"], "labels": ["merge-this-please"]}, "a68b790293": {"title": "fix ref to nonexistent torch.repeat", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30614\n\nDifferential Revision: D18808517\n\nPulled By: ezyang\n\nfbshipit-source-id: 27f9bda6fbbd1c3c751a0e96fdc336bf724c0b31", "pr_number": "30614", "files_changed": ["torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merge-this-please"]}, "1189595875": {"title": "Fix Tensor.argsort -> torch.argsort documentation link", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30464\n\nDifferential Revision: D18717657\n\nPulled By: zou3519\n\nfbshipit-source-id: 9894f63c6cb1b5311117441e78805230d1bc09f3", "pr_number": "30464", "files_changed": ["torch/_tensor_docs.py"], "labels": []}, "38986e1dea": {"title": "Split libtorch.so back into libtorch_{cpu,cuda,hip} (#30315)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30315\n\nThe new structure is that libtorch_cpu contains the bulk of our\ncode, and libtorch depends on libtorch_cpu and libtorch_cuda.\nThis is a reland of https://github.com/pytorch/pytorch/pull/29731 but\nI've extracted all of the prep work into separate PRs which can be\nlanded before this one.\n\nSome things of note:\n\n* torch/csrc/cuda/nccl.cpp was added to the wrong list of SRCS, now fixed (this didn't matter before because previously they were all in the same library)\n* The dummy file for libtorch was brought back from the dead; it was previously deleted in #20774\nIn an initial version of the patch, I forgot to make torch_cuda explicitly depend on torch_cpu. This lead to some very odd errors, most notably \"bin/blob_test: hidden symbol `_ZNK6google8protobuf5Arena17OnArenaAllocationEPKSt9type_infom' in lib/libprotobuf.a(arena.cc.o) is referenced by DSO\"\n* A number of places in Android/iOS builds have to add torch_cuda explicitly as a library, as they do not have transitive dependency calculation working correctly\n* I had to torch_cpu/torch_cuda caffe2_interface_library so that they get whole-archived linked into torch when you statically link. And I had to do this in an *exported* fashion because torch needs to depend on torch_cpu_library. In the end I exported everything and removed the redefinition in the Caffe2Config.cmake. However, I am not too sure why the old code did it in this way in the first place; however, it doesn't seem to have broken anything to switch it this way.\n* There's some uses of `__HIP_PLATFORM_HCC__` still in `torch_cpu` code, so I had to apply it to that library too (UGH). This manifests as a failer when trying to run the CUDA fuser. This doesn't really matter substantively right now because we still in-place HIPify, but it would be good to fix eventually. This was a bit difficult to debug because of an unrelated HIP bug, see https://github.com/ROCm-Developer-Tools/HIP/issues/1706\n\nFixes #27215 (as our libraries are smaller), and executes on\npart of the plan in #29235.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18790941\n\nPulled By: ezyang\n\nfbshipit-source-id: 01296f6089d3de5e8365251b490c51e694f2d6c7", "pr_number": "30315", "files_changed": [".circleci/scripts/binary_ios_upload.sh", "android/pytorch_android/CMakeLists.txt", "c10/macros/Export.h", "caffe2/CMakeLists.txt", "caffe2/core/common_gpu.h", "cmake/Caffe2Config.cmake.in", "ios/LibTorch.podspec", "ios/TestApp/benchmark/setup.rb", "scripts/xcode_build.rb", "torch/utils/cpp_extension.py"], "labels": ["module: cpp"]}, "d0af07ca4c": {"title": "Fix capitalization inconsistency in optim.rst", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30608\n\nDifferential Revision: D18808516\n\nPulled By: ezyang\n\nfbshipit-source-id: 4be68be9a8c8c3da7a0b98162bc1050b588fab43", "pr_number": "30608", "files_changed": ["docs/source/optim.rst"], "labels": ["merge-this-please"]}, "ca072951d5": {"title": "move MaskedAdagrad to caffe2/operators/experimental/optimizers (#30714)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30714\n\nMove Masked*Adagrad operators so caffe2/python/optimizer.py can use them.\n\nTest Plan: buck test caffe2/caffe2/operators/experimental/optimizers:masked_adagrad_test\n\nReviewed By: chocjy\n\nDifferential Revision: D18805532\n\nfbshipit-source-id: 49b1f755b31296c62e7a6a8134313b962ad9690c", "pr_number": "30714", "files_changed": ["caffe2/operators/experimental/optimizers/masked_adagrad.cpp", "caffe2/operators/experimental/optimizers/masked_adagrad_test.py"], "labels": ["fb-exported"]}, "a55f125e3b": {"title": "Check the error return of nvrtcGetProgramLogSize and nvrtcGetProgramLog (#30663)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30663\n\nYes they can fail.  See https://github.com/ROCm-Developer-Tools/HIP/issues/1706\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18810088\n\nPulled By: ezyang\n\nfbshipit-source-id: 96186e71c9a195bdbbed811e7ba8dc40bec09eae", "pr_number": "30663", "files_changed": ["torch/csrc/jit/fuser/cuda/fused_kernel.cpp"], "labels": ["jit", "module: rocm"]}, "6e145b4614": {"title": "add irregular c10 op registration/invocation cases to test project (#30558)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30558\n\nMost c10 op registration/invocation cases are generated by aten codegen\nfollowing some fixed pattern, but a handful of them were written\nmanually, mainly for quantized ops. Added these \"irregular\" cases to the\ntest project to verify static code analyzer can handle them as well.\n\nTest:\n- build and run the test project;\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18811098\n\nPulled By: ljk53\n\nfbshipit-source-id: 7bdf17175dfec41c56c0d70f124cc96478135bc4", "pr_number": "30558", "files_changed": ["test/mobile/op_deps/CMakeLists.txt", "test/mobile/op_deps/expected_deps.yaml", "test/mobile/op_deps/main.cc", "test/mobile/op_deps/quantized_ops.cpp", "test/mobile/op_deps/quantized_ops.h", "test/mobile/op_deps/simple_ops.h", "test/mobile/op_deps/utils.cpp"], "labels": []}, "f73cd28082": {"title": "InsertObservers for shared class types (#30548)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30548\n\nClassTypes can be shared among different module instances, but previously we assumed\nthey would be unique, this PR enables the insert_observers pass to work with shared class types\n\nTest Plan:\npython test/test_jit.py\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18802465\n\nfbshipit-source-id: b782e71e44a043af45577ac2b5c83e695155bb8b", "pr_number": "30548", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "756f279d95": {"title": "Rename QuantizeHelper to InsertQuantDeQuantHelper (#30549)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30549\n\nPreparing for later refactoring\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18802464\n\nfbshipit-source-id: 0b5afb143549d93eed4c429125d3d5fd253093a9", "pr_number": "30549", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "6918f0ce86": {"title": "Move scalar_check for total_weight in NLLLoss functions to code from codegen. (#30665)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30665\n\ntotal_weight is a \"hidden\" output just for autograd, so it's not user visible.  The existing test_nn tests cover this (I verified that the new code is executed) and this matches the CPU behavior.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18782709\n\nPulled By: gchanan\n\nfbshipit-source-id: 6d1c20eeaeffa14d06f375b37f11e866587f5fa0", "pr_number": "30665", "files_changed": ["aten/src/ATen/nn.yaml", "aten/src/THCUNN/generic/ClassNLLCriterion.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu"], "labels": []}, "fa2aa245cf": {"title": "Simplify scalar_check of nll_loss. (#30669)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30669\n\nThe inputs can't be 0-d, so we don't need that check in the scalar_check.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18784524\n\nPulled By: gchanan\n\nfbshipit-source-id: d44222dffc91880a6e8c7be69e6e146e60040d43", "pr_number": "30669", "files_changed": ["aten/src/ATen/nn.yaml", "test/test_torch.py"], "labels": []}, "786de33832": {"title": "Move scalar_check logic from codegen to code in NLLLoss. (#30670)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30670\n\nAlso turn off scalar_check for grad_input: it isn't necessary because the input can't be 0-dimensional.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18784523\n\nPulled By: gchanan\n\nfbshipit-source-id: 246d30970457075a0403dd0089317659a2cd2dd4", "pr_number": "30670", "files_changed": ["aten/src/ATen/nn.yaml", "aten/src/THCUNN/generic/ClassNLLCriterion.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu", "test/test_torch.py"], "labels": []}, "d38f9117fd": {"title": "Cache compilation of free functions (#30503)", "body": "Summary:\nWe don't have to recompile free functions if we've already compiled them.\n\nImproved compilation of resnet18 by 27%.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30503\n\nDifferential Revision: D18796501\n\nPulled By: eellison\n\nfbshipit-source-id: 2dee0fc5fcf9adc5b92213f8cb813730d71b376f", "pr_number": "30503", "files_changed": ["test/test_jit.py", "torch/jit/__init__.py"], "labels": ["jit"]}, "289e9a07fd": {"title": "Move Tanh backward to Aten(CPU+CUDA) (#30224)", "body": "Summary:\nVitalyFedyunin, This PR is about port Tanh backward to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.Tanh()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    bwd_t = 0\n    for i in range(10000):\n        output = m(input)\n        t1 = _time()\n        output.backward(grad_output)\n        t2 = _time()\n        bwd_t = bwd_t + (t2 - t1)\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d)  backwad avg time is %.2f (ms).\" % (n, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) backwad avg time is 0.12 (ms).\ninput size(128, 10000) backwad avg time is 0.17 (ms).\nCPU\ninput size(128, 100) backwad avg time is 0.05 (ms).\ninput size(128, 10000) backwad avg time is 0.35 (ms).\n```\nAfter:\n```\nGPU:\ninput size(128, 100) backwad avg time is 0.12 (ms).\ninput size(128, 10000) backwad avg time is 0.17 (ms).\nCPU\ninput size(128, 100) backwad avg time is 0.04 (ms).\ninput size(128, 10000) backwad avg time is 0.25 (ms).\n```\n`OMP_NUM_THREADS=1:`\n```\nBefore:\ninput size(128, 100) backwad avg time is 0.03 (ms).\ninput size(128, 10000) backwad avg time is 1.85 (ms).\nAfter:\ninput size(128, 100) backwad avg time is 0.02 (ms).\ninput size(128, 10000) backwad avg time is 1.16 (ms).\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30224\n\nDifferential Revision: D18810045\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: ab37948ab8f76bdaf9f3d1388562eaf29dacc0ea", "pr_number": "30224", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/Tanh.cu", "aten/src/THCUNN/generic/Tanh.cu", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/generic/Tanh.c", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "9d3402e4cb": {"title": "Add the __torch_function__ API override mechanism (#30730)", "body": "Summary:\nThis is a re-do of https://github.com/pytorch/pytorch/issues/27064, which was reverted (https://github.com/pytorch/pytorch/commit/b8792c0438f4292aa813c36207f75eebcbb77a45). This was landed at the same time as other work that added new operators to the `torch` namespace so the check for whether the `torch` namespace is exhaustively checked for overridability was triggering test failures.\n\nI've temporarily disabled that check and added an explanatory comment that the check will be re-enabled in a future PR that will be merged during a time when the commit velocity on PyTorch is lower.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30730\n\nDifferential Revision: D18813270\n\nPulled By: ezyang\n\nfbshipit-source-id: 70477c4656dca8fea6e7bc59259555041fcfbf68", "pr_number": "30730", "files_changed": ["docs/source/notes/extending.rst", "test/onnx/expect/TestOperators.test_frobenius_norm.expect", "test/onnx/expect/TestOperators.test_meshgrid.expect", "test/onnx/expect/TestOperators.test_unique.expect", "test/run_test.py", "test/test_overrides.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_torch_functions.cpp", "torch/_overrides.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/functional.py"], "labels": []}, "42e79d7e8a": {"title": "Kill THNN version of MultiMarginCriterion; it's not used anymore.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30725\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18808767\n\nPulled By: gchanan\n\nfbshipit-source-id: bcc4a6e272036f3d167fc158a53fe7aa1dec51f9", "pr_number": "30725", "files_changed": ["aten/src/THNN/generic/MultiMarginCriterion.c"], "labels": []}, "2308a0ec1b": {"title": "Improve documentation around builtin functions (#30347)", "body": "Summary:\nThis breaks the builtins page into some more sections and adds details about Python built-in functions\n](https://our.intern.facebook.com/intern/diff/18718166/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30347\n\nPulled By: driazati\n\nReviewed By: wanchaol\n\nDifferential Revision: D18718166\n\nfbshipit-source-id: bf43260ab7bcf92cccef684a5ce68cb16020771d", "pr_number": "30347", "files_changed": ["docs/source/_static/css/jit.css", "docs/source/conf.py", "docs/source/jit.rst", "torch/jit/supported_ops.py"], "labels": ["jit"]}, "1707774417": {"title": "AddConstant and findConstant for ClassType (#29217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29217\n\nWe want to preserve constant information in ClassType so that\nusers can access the constants in the module by name.\nThis is also used later for freezing some attribute(converting\nattributes to constant)\n\nTest Plan:\ntbd\n\nImported from OSS\n\nDifferential Revision: D18799955\n\nfbshipit-source-id: fbfbcd5d3f7f560368b96e2a87e270c822a3d03a", "pr_number": "29217", "files_changed": ["aten/src/ATen/core/jit_type.h", "test/cpp/jit/test_class_type.cpp", "torch/csrc/jit/import_source.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/script/class_type.cpp", "torch/csrc/jit/script/script_type_parser.cpp", "torch/csrc/jit/script/script_type_parser.h"], "labels": ["jit"]}, "569ea63f3b": {"title": "fix anynonzero op", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29423\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18820523\n\nfbshipit-source-id: 55c7a1911121f0aed008bd684b448151bbbf0a8a", "pr_number": "29423", "files_changed": ["torch/csrc/jit/register_prim_ops.cpp"], "labels": ["jit"]}, "1f1ce53e8e": {"title": "Don't install pybind11 header directory for system pybind11 installs (#30758)", "body": "Summary:\nFor system pybind11 installs this is a system header location that should not get installed since it might include other unrelated headers. Since the header is already installed for a system install there's no need to install the headers, so only do the install when we use the bundled pybind11 version.\n\nCloses https://github.com/pytorch/pytorch/issues/29823. Closes https://github.com/pytorch/pytorch/issues/30627.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30758\n\nDifferential Revision: D18820189\n\nPulled By: bddppq\n\nfbshipit-source-id: fcc9fa657897e18c07da090752c912e3be513b17", "pr_number": "30758", "files_changed": ["cmake/Dependencies.cmake"], "labels": []}, "e09c415387": {"title": "Back out \"make the order btw div and mul in adagrad update consistent\" (#30737)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30737\n\nOriginal commit changeset: 2a8b2a3f5401\n\nReverting this to be safe until we address test failures in T58528495\n\nTest Plan: CI\n\nReviewed By: wx1988\n\nDifferential Revision: D18812384\n\nfbshipit-source-id: 2a3ac554024773022ec827f259127e4c8cffe6e2", "pr_number": "30737", "files_changed": ["caffe2/perfkernels/adagrad.h", "caffe2/perfkernels/adagrad_avx.cc", "caffe2/sgd/adagrad_op.h"], "labels": ["fb-exported"]}, "3c1bb21cf5": {"title": "Invoke more passes in `insertObservers` (#30473)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30473\n\nInvoked `ConstantPooling` and `FuseLinear` pass before\n`insertObservers`.\n`ConstantPooling` is for cleanning up traced graph, e.g. when we\nhave to constant node that has the same value, this pass will merge them,\nthis allows us to have less quantization patterns\n`FuseLinear` is to merge the exploded linear function into `aten::linear` so\nthat we can quantize this function properly. We need to fuse it because right now\nthe way we recognize weight and bias is by matching the argument position in certain function\ncalls, e.g. 1st argument of aten::conv2d is weight. Therefore we have to preserve\nthe bounary of the linear function to recognize the weight of linear. Since in the exploded\nlinear code, input of addmm is transposed weight rather than the original weight of linear.\nghstack-source-id: 94887831\n\nTest Plan:\nThis is needed for quantizing traced model tests to pass\n\nImported from OSS\n\nDifferential Revision: D18795722\n\nfbshipit-source-id: 192d9d1e56307e2e1d90e30dce0502e31cb4f829", "pr_number": "30473", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "7a2889b014": {"title": "Stop producing op_version_set version numbers.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28122\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D17959565\n\nPulled By: zdevito\n\nfbshipit-source-id: 701101bd870700eb0c9882c69e2cfdd2524b555e", "pr_number": "28122", "files_changed": ["caffe2/serialize/inline_container.h", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export_module.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/passes/python_print.h"], "labels": ["jit"]}, "c4c2e23385": {"title": "Supporting making submodules unique (#30037)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30037\n\nSupport quantization for modules with reused submodules, e.g. relu (automatically make unique)\nWe first do a pass on the graph to find all duplicate uses of the same module, and record the `Value`s of the\nmodule instance, for each of these values we create a new module and change the access to that module.\n\nTest Plan:\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D18821483\n\nfbshipit-source-id: 1698b981e9e9f0c728d9f03fcbcfbd260151f679", "pr_number": "30037", "files_changed": ["test/test_jit.py", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/passes/quantization.h"], "labels": ["jit"]}, "1d20c32bf1": {"title": "Make `InsertQuantDeQuantHelper` global (#30550)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30550\n\nRight now we have a `InsertQuantDeQuantHelper` for each module, but we need\nit to be global because we need to know what graphs have been quantized before\nand based on this information we can decide how to handle the module instance.\n\nTest Plan:\ntest_jit.py, test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18818651\n\nfbshipit-source-id: bfcaf37094ce20a257171a0c99b05b9348ebc13d", "pr_number": "30550", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit"]}, "a939b52ddb": {"title": "fix AvgPool2d for 2^31-1 sized inputs, and get test_cuda_kernel_loop_\u2026 (#30771)", "body": "Summary:\n\u2026overflow_large to working state\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30771\n\nDifferential Revision: D18821529\n\nPulled By: ngimel\n\nfbshipit-source-id: c5cbf56e686a2a3cfc7274dd96db37289dac7588", "pr_number": "30771", "files_changed": ["aten/src/ATen/native/cuda/AveragePool2d.cu"], "labels": []}, "139aa51962": {"title": "Clean up non-C++14 code (#28443)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28443\n\nWe're now on C++14, so we don't need the else branch of these ifdef's anymore\nghstack-source-id: 94904074\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18069136\n\nfbshipit-source-id: f1613cab9a99ee30f99775e4a60a1b06fd0a03ff", "pr_number": "28443", "files_changed": ["aten/src/ATen/cpu/vec256/vec256.h"], "labels": []}, "f2a2fec47c": {"title": "CUDA-strided-complex Binary and Unary Op support (#30295)", "body": "Summary:\nIn-tree changes to pytorch to support complex numbers are being submitted here.\nOut-of-tree support for CUDA complex numbers is here: [pytorch-cuda-strided-complex extension](https://gitlab.com/pytorch-complex/pytorch-cuda-strided-complex)\n\nChanges so far:\n\n- [x]  Added complex support of torch.empty and torch.fill()\n- [x]  Added complex support of CopyKernels\n    - The 'static_cast_with_inter_type' template function is specialized for the following cases\n        - `dest_t = thrust::complex<dest_value_t>`, `src_t = std::complex<src_value_t>`\n        - `dest_t = std::complex<dest_value_t>`, `src_t = thrust::complex<src_value_t>`\n     - This handles the compile-time case where `dest_value_t=double` and `src_value_t=float`.\n- [x]  Added complex support of BinaryOp kernels\n    - `using thrust_t = typename ztype_cuda<scalar_t>::thrust_t;` converts std::complex<T> ScalarTypes to thrust types and is a no-op of other Scalar Types.\n    - The operator is performed using complex number support defined in `thrust/complex.h`\n    - This could be extended to work with ROCm by using `rocm/complex.h`\n- [x]  Added complex support of UnaryOp kernels\n    - Added CUDA support for `angle()`, `real()`, `imag()`, `conj()`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30295\n\nDifferential Revision: D18781954\n\nPulled By: ezyang\n\nfbshipit-source-id: 25d204c0b8143ee27fda345a5d6a82f095da92a7", "pr_number": "30295", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/detail/ScalarTypeConversions.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cpu/CopyKernel.cpp", "aten/src/ATen/native/cuda/BinaryArithmeticKernel.cu", "aten/src/ATen/native/cuda/BinaryCompareKernel.cu", "aten/src/ATen/native/cuda/FillKernel.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/cuda/zmath.cuh", "aten/src/ATen/native/native_functions.yaml", "c10/util/TypeCast.h"], "labels": ["merged"]}, "c4e9748bc6": {"title": "Provide full path for buck hipification (#30746)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30746\n\nThis diff should be safe as long as open source build succeeds and should have no impact to cuda.\n\nDifferential Revision: D18811302\n\nfbshipit-source-id: a7adab993816cba51842701898fac5019438b664", "pr_number": "30746", "files_changed": ["aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h", "aten/src/ATen/miopen/Utils.h", "aten/src/ATen/native/SharedReduceOps.h", "aten/src/ATen/native/miopen/Conv_miopen.cpp", "aten/src/ATen/native/miopen/RNN_miopen.cpp"], "labels": ["fb-exported", "merged", "module: rocm"]}, "35a6997863": {"title": "Support 0-d tensors in CUDA MultiLabelMarginCriterion. (#30765)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30765\n\nIt is already supported in CPU and is pretty easy to add for consistency.\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30727\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18821557\n\nPulled By: gchanan\n\nfbshipit-source-id: e6aa3e91000ff3fd63941defc7d30aef58ae2f82", "pr_number": "30765", "files_changed": ["aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu", "test/common_nn.py"], "labels": ["merged"]}, "ba1a9871cb": {"title": "Turn off scalar_check for is_target for MultiLabelMarginCriterion, which is handled correctly in code. (#30766)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30766\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30728\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18821555\n\nPulled By: gchanan\n\nfbshipit-source-id: 27acc72f82e94eddeea675ae66e010cfb2fc7421", "pr_number": "30766", "files_changed": ["aten/src/ATen/nn.yaml"], "labels": ["merged"]}, "473a044835": {"title": "Fix a CUDA memory leak in MultiLabelMarginCriterion error checking. (#30767)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30767\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30733\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18821553\n\nPulled By: gchanan\n\nfbshipit-source-id: 8bf0365ce54dd2f07a5d6d0937332d0baf75b350", "pr_number": "30767", "files_changed": ["aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu", "test/test_torch.py"], "labels": ["merged"]}, "50625798df": {"title": "Fix scalar check of MultiLabelMarginLoss. (#30768)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30768\n\nThe behavior didn't match the documentation, because the documentation (for 'none' reduction) reads:\ninput X target -> output\n(N, C) X (N, C) -> (N,)\n(C,) X (C,) -> ()\n\nbut the later case would output (1,).  This also changes the case to:\n() X (C,) -> ()\nfrom:\n() X (C,) -> (C,)\nwhich makes more sense with the above formulas.\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30748\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18821554\n\nPulled By: gchanan\n\nfbshipit-source-id: 3df77c51cf25648cb5fab62a68b09f49c91dab4e", "pr_number": "30768", "files_changed": ["aten/src/ATen/native/LossMultiLabelMargin.cpp", "aten/src/ATen/nn.yaml", "test/common_nn.py", "test/test_torch.py"], "labels": ["merged", "topic: bc-breaking"]}, "f12332eb51": {"title": "Move scalar_check from codegen to code in MultiLabelMarginCriterion. (#30770)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30770\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30753\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18821556\n\nPulled By: gchanan\n\nfbshipit-source-id: 64b7311b1eb3855c4f1981d060accc918b99088d", "pr_number": "30770", "files_changed": ["aten/src/ATen/nn.yaml", "aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu"], "labels": ["merged"]}, "2607772959": {"title": "Turn off scalar_checks for SpatialDepthwiseConvolution and SpatialConvolutionMM. (#30789)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30789\n\nThe input(s) can't be 0-dimensional, so its irrelevant.\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30438\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18825716\n\nPulled By: gchanan\n\nfbshipit-source-id: a4883b795163efcb9d8dba6166d0f2102b6728a2", "pr_number": "30789", "files_changed": ["aten/src/ATen/nn.yaml", "test/test_torch.py"], "labels": ["merged"]}, "fa251cfd97": {"title": "Fully deprecate variadic inputs of checkpoint_sequential (#25985)", "body": "Summary:\nTo support variadic inputs of `checkpoint_sequential` was deprecated at https://github.com/pytorch/pytorch/issues/21006. This case should be warned with `DeprecationWarning` for PyTorch 1.2, but it should be simply failed with `TypeError` since PyTorch 1.3. This patch removes the `DeprecationWarning` for PyTorch 1.2.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25985\n\nDifferential Revision: D18809875\n\nPulled By: albanD\n\nfbshipit-source-id: e84dd8629c04979c4b2dc63e8ada94292e8cedd0", "pr_number": "25985", "files_changed": ["test/test_utils.py", "torch/utils/checkpoint.py"], "labels": ["merged", "module: checkpoint", "module: tests", "open source", "triaged"]}, "1578a28692": {"title": "Migrate max and min (binary) from TH to ATen. (#27185)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27185\n\nTH implementation will be removed after the unary max and min are migrated.\n\nBenchmark: (Debian 10, Release build, gcc 7.4, no turbo)\n\n```python\nimport timeit\nfor device in ('cpu', 'cuda'):\n    print(f'device: {device}')\n    for op in ('max', 'min'):\n        for dtype in ('torch.double', 'torch.float', 'torch.int16', 'torch.int32', 'torch.int64'):\n            for n, t in [(10_000, 200000),\n                        (100_000, 20000)]:\n                print(f'torch.{op}(a, b), numel() == {n} for {t} times, dtype={dtype}')\n                print(timeit.timeit(f'torch.{op}(a)' + (';torch.cuda.synchronize()' if device == 'cuda' else ''),\n                                    setup=f'import torch; a = torch.arange({n}, dtype={dtype}); b = torch.ones({n}, 0, dtype={dtype}) * ({n} / 2)', number=t))\n    print()\n```\n\nBefore:\n\n```\ndevice: cpu\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n2.241763713000182\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.7138833169992722\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.2183356810000987\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.7031846980007685\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.7704679510006827\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.289198366999699\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.7937613740014058\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.2930124340000475\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8032857640009752\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.2908709189996443\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n1.8829010000008566\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.2994690759987861\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n1.8037853410005482\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.2929310759991495\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.8075240359994496\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.2932477679987642\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.7868400779989315\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.2885970789993735\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8389664830010588\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.29402057399966\n\ndevice: cuda\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n4.787109836999662\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.842438002999188\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n3.429616614999759\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.835390076999829\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.940423873000327\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.4108991760003846\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.9318018840003788\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4168134739993548\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n2.9610764919998473\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4189234130008117\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n2.960172712999338\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.4162539499993727\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.8985912560001452\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.4113489299998037\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.9160250799995993\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.4128787690005993\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.8806865219994506\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4086357010000938\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n2.9362181240012433\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4151225870009512\n\n```\n\nAfter:\n\n```\ndevice: cpu\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n2.2685823729998447\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.72004808300062\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.212242640000113\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.7089235590001408\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.7767087259999244\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.2916517639996528\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.8265984959998605\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.3002885240002797\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8084679720004715\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.3012119999993956\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n1.8800218449996464\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.3060645710002063\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n2.4905043950002437\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.9126290209997023\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n1.7972335520007618\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.2918074379995232\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n1.8047651860006226\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.2992197730000044\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n1.8526509560006161\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.3030709570002728\n\ndevice: cuda\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n4.700986622000528\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.8415469050005413\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n3.3051693249999516\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n1.8321999460004008\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.8086475109994353\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.405110773999695\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.913458047999484\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4236377289998927\ntorch.max(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n2.9386842409994642\ntorch.max(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4230227469997772\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.double\n3.0341797270002644\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.double\n1.4289592409995748\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.float\n3.6091147850002017\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.float\n2.036691903999781\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int16\n2.8256167649997224\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int16\n1.4078955400000268\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int32\n2.8631781489993955\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int32\n1.4210130069996012\ntorch.min(a, b), numel() == 10000 for 200000 times, dtype=torch.int64\n3.0112479260005784\ntorch.min(a, b), numel() == 100000 for 20000 times, dtype=torch.int64\n1.4297719679998409\n\n```\n\nSolve partly #24594 #24595\n\nClose #25016\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18117070\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: e06d37a8a1405848ba0b9e398870a77eb52bae8b", "pr_number": "27185", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryCompareKernel.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py"], "labels": ["merged", "module: cpu", "module: cuda", "module: operators", "open source", "triaged"]}, "2171f91053": {"title": "reenable cuda_kernel_loop_overflow_large test (#30797)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/30771 has landed, original issue https://github.com/pytorch/pytorch/issues/26838 is now closed\n\ncc peterjc123\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30797\n\nDifferential Revision: D18827307\n\nPulled By: ngimel\n\nfbshipit-source-id: 41b3db5fc9db85daeaa1b53c55b468976c996285", "pr_number": "30797", "files_changed": ["test/test_cuda.py"], "labels": ["merge-this-please", "merged"]}, "f531815526": {"title": "Deprecate tensor.type() (#30281)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/29161.\n\nI looked a bit at the code changes related to this and think I have all of the use cases of `DeprecatedTypeProperties` covered in the message, but suggestions from someone with more context on this would be very much appreciated :)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30281\n\nDifferential Revision: D18830818\n\nPulled By: ezyang\n\nfbshipit-source-id: 1a7fcee15354ae09e6644577e7fa33bd26acfe20", "pr_number": "30281", "files_changed": ["aten/src/ATen/SparseTensorImpl.cpp", "aten/src/ATen/TensorUtils.cpp", "aten/src/ATen/core/Tensor.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/Cross.cpp", "aten/src/ATen/native/Distance.cpp", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/Memory.cpp", "aten/src/ATen/native/PackedSequence.cpp", "aten/src/ATen/native/PointwiseOps.cpp", "aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/SortingUtils.h", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorFactories.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/test/scalar_test.cpp", "c10/core/TensorOptions.h", "test/cpp/api/tensor_options.cpp", "test/cpp/api/tensor_options_cuda.cpp", "test/cpp/jit/test_argument_spec.cpp", "test/cpp_extensions/cuda_extension.cpp", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/Functions.h", "tools/autograd/templates/python_variable_methods.cpp", "torch/csrc/Generator.cpp", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/custom_function.cpp", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/functions/comm.cpp", "torch/csrc/autograd/input_metadata.h", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/autograd/saved_variable.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/node_hashing.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/utils/tensor_apply.cpp", "torch/csrc/utils/tensor_list.cpp", "torch/csrc/utils/tensor_numpy.cpp", "torch/csrc/utils/tensor_types.cpp", "torch/csrc/utils/tensor_types.h", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/Utils.hpp"], "labels": ["merged", "open source", "topic: deprecation"]}, "bf1b4b6fef": {"title": "add torch_cpu to the static library list in TorchConfig.cmake.in (#30769)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30769\n\nThe TorchConfig.cmake is the public cmake we produce in install folder for\n3rd party client code to get all libtorch dependencies easily.\n\nApparently this build flow is not well covered by our CI (which is focused\non 1st party build / shared libraries?) as the little dummy project for\ncode analysis testing purpose was broken by #30315 without fail any CI.\n\nFixed the problem for mobile build and add the dummy project build to mobile\nCI as well.\n\nTest Plan: - make sure new CI pass;\n\nDifferential Revision: D18825054\n\nPulled By: ljk53\n\nfbshipit-source-id: 80506f3875ffbc1a191154bb9e3621c621e08b12", "pr_number": "30769", "files_changed": [".jenkins/pytorch/build-mobile.sh", ".jenkins/pytorch/build.sh", "cmake/TorchConfig.cmake.in", "test/mobile/op_deps/CMakeLists.txt", "test/mobile/op_deps/quantized_ops.cpp"], "labels": ["merged"]}, "9617d07bd5": {"title": "Wrap warning handler in a function to avoid siof (#30800)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30800\n\nSparseNN benchmark crashed due to this.\nWrap warning handler in a function to avoid siof.\n\nTest Plan: Tested locally, SparseNN benchmark no longer crashes.\n\nReviewed By: yinghai\n\nDifferential Revision: D18826731\n\nfbshipit-source-id: 8fcab8a3f38cc20f775409c0686363af3c27d0a6", "pr_number": "30800", "files_changed": ["c10/util/Exception.cpp"], "labels": ["fb-exported", "merged"]}, "be55874f2c": {"title": "style fixes to code analyzer (#30808)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30808\n\nAddressed some comments on #29550 after it's landed.\n\nTest Plan:\n```\nLLVM_DIR=... ANALYZE_TEST=1 CHECK_RESULT=1 tools/code_analyzer/build.sh\nLLVM_DIR=... ANALYZE_TORCH=1 tools/code_analyzer/build.sh -closure=false -debug_path=true\n```\n\nDifferential Revision: D18835100\n\nPulled By: ljk53\n\nfbshipit-source-id: 991d292ddc0211a88b04d0bdc24719f471c7786e", "pr_number": "30808", "files_changed": ["tools/code_analyzer/CMakeLists.txt", "tools/code_analyzer/op_dependency.cpp"], "labels": ["merged"]}, "244b0bd1a5": {"title": "Add docs for how we expose declarations in at:: to torch:: (#30760)", "body": "Summary:\nThis PR adds docs for how we expose declarations in `at::` to `torch::`, to make the semantics more clear.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30760\n\nDifferential Revision: D18833081\n\nPulled By: yf225\n\nfbshipit-source-id: eff4d8815c67f681ce3a930ce99771cf2e55dbd9", "pr_number": "30760", "files_changed": ["torch/csrc/api/include/torch/types.h"], "labels": ["merged", "module: cpp"]}, "c564d794ed": {"title": "Add ATen/native/ headers to torch target (#30835)", "body": "Summary:\nWe dont have ATen/native/*.h in torch target before, and we would like it to be exposed for external use.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30835\n\nDifferential Revision: D18836160\n\nPulled By: zrphercule\n\nfbshipit-source-id: 7330a9c9d8b65f173cc332b1cfeeb18c7dca20a8", "pr_number": "30835", "files_changed": ["aten/src/ATen/CMakeLists.txt", "setup.py"], "labels": ["merged"]}, "6486bdfb90": {"title": "Fix `os.register_at_fork` not defined on Windows (#30809)", "body": "Summary:\nAccording to https://docs.python.org/3.8/library/os.html#os.register_at_fork, this function is only available in Unix platforms.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30809\n\nDifferential Revision: D18828777\n\nPulled By: bddppq\n\nfbshipit-source-id: 3325a984da488bb0a80a5c27131553fbcf78921f", "pr_number": "30809", "files_changed": ["torch/multiprocessing/_atfork.py"], "labels": ["merge-this-please", "merged"]}, "f874230d33": {"title": "Vectorize smooth L1 loss backward function on CPU. (#30046)", "body": "Summary:\nBenchmark (Intel i7-8850H, turbo off, release build, RHEL 7.7):\n\n```\nimport timeit\n\nfor dtype in ('torch.float', 'torch.double'):\n    print(f'dtype={dtype}')\n    for n, t in [(10_000, 100000),\n                (100_000, 20000)]:\n        print(f'numel() == {n} for {t} times')\n        print(timeit.timeit('output.backward(retain_graph=True)', number=t, setup=f\"\"\"\nimport torch\nloss = torch.nn.SmoothL1Loss()\ninput = torch.randn({n}, requires_grad=True)\ntarget = torch.randn({n})\noutput = loss(input, target)\n\"\"\"))\n```\n\nBefore:\n\n```\ndtype=torch.float\nnumel() == 10000 for 100000 times\n6.154701935998673\nnumel() == 100000 for 20000 times\n5.157296671999575\ndtype=torch.double\nnumel() == 10000 for 100000 times\n6.195317157000318\nnumel() == 100000 for 20000 times\n5.099748799999361\n```\n\nAfter:\n\n```\ndtype=torch.float\nnumel() == 10000 for 100000 times\n4.968745516000126\nnumel() == 100000 for 20000 times\n2.4029395039997326\ndtype=torch.double\nnumel() == 10000 for 100000 times\n4.9910852479988534\nnumel() == 100000 for 20000 times\n2.4867371629989066\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30046\n\nDifferential Revision: D18602399\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 4c6c7b7b69ad6bce759786ddd7d6bc1e88ecf6ab", "pr_number": "30046", "files_changed": ["aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp"], "labels": ["merged"]}, "2ced81f289": {"title": "Revert \"Default to not build Caffe2 operators on Windows. (#29061)\" (#30740)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30740\n\nThis reverts commit 7102aceaf88ab71781c6019458bd7a07e86a532f.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18834315\n\nPulled By: ezyang\n\nfbshipit-source-id: 2dbd1cf686864b9840365083182cd6188a285399", "pr_number": "30740", "files_changed": ["CMakeLists.txt", "test/test_jit.py", "test/test_torch.py"], "labels": ["merged"]}, "0974dcc244": {"title": "Fix error checking of CUDA multi_margin_loss. (#30825)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30825\n\nIt didn't verify in the 1-d case that the targets were size 1..\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18833659\n\nPulled By: gchanan\n\nfbshipit-source-id: 9b0276e7b0423fdaf2ba7cfa34bde541558c61f9", "pr_number": "30825", "files_changed": ["aten/src/THCUNN/generic/MultiMarginCriterion.cu", "test/test_nn.py"], "labels": ["merged"]}, "e5bd7a7942": {"title": "we should have a config-based way to skip flaky tests (#29944)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29944\n\nThis particular approach queries our issue tracker for test titles that\nmatch the following format:\n\n```\nDISABLED test_async_grad_guard_with_grad (jit.test_async.TestAsync)\n```\n\nAnd then skips the python test for them. There is 1 second timeout so\nif the internet flakes we still run the test suite, without disabling any\ntests.\n\nThis is intended as a quick fix, similar to ninja unland, to get to a green\nmaster. Long term test disables should go into the code.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18621773\n\nPulled By: zdevito\n\nfbshipit-source-id: 5532f1d5fa3f83f77fc3597126cbb7dba09a3c33", "pr_number": "29944", "files_changed": ["test/common_utils.py", "tools/update_disabled_tests.sh"], "labels": ["merged"]}, "82c3f4861f": {"title": "Move hardtanh activation to Aten(CPU, CUDA) (#30152)", "body": "Summary:\nVitalyFedyunin, This PR is about port Hardtanh activation to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.Hardtanh()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    fwd_t = 0\n    bwd_t = 0\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU\ninput size(128, 100) forward time is 0.02 (ms); backwad avg time is 0.06 (ms).\ninput size(128, 10000) forward time is 0.84 (ms); backwad avg time is 0.44 (ms).\n```\nAfter:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU\ninput size(128, 100) forward time is 0.02 (ms); backwad avg time is 0.05 (ms).\ninput size(128, 10000) forward time is 0.61 (ms); backwad avg time is 0.10 (ms).\n```\n`OMP_NUM_THREADS=1:`\n```\nBefore:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.07 (ms).\ninput size(128, 10000) forward time is 5.21 (ms); backwad avg time is 5.25 (ms).\nAfter:\ninput size(128, 100) forward time is 0.01 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10000) forward time is 1.09 (ms); backwad avg time is 1.09 (ms).\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30152\n\nDifferential Revision: D18815545\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: d23b6b340a7276457f22dce826bcbe3b341d755f", "pr_number": "30152", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/HardTanh.cu", "aten/src/THCUNN/generic/HardTanh.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/HardTanh.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp", "test/test_nn.py"], "labels": ["merged"]}, "4034aa7621": {"title": "make sure windows tests get triggered (#30836)", "body": "Summary:\nwe prefer \"_\" over \"-\" in build names, so change checks in test script\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30836\n\nDifferential Revision: D18840736\n\nPulled By: mingbowan\n\nfbshipit-source-id: 6fdf736496225c5f8ab44906d8f4681b7bf894a7", "pr_number": "30836", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/header-section.yml", ".circleci/verbatim-sources/windows-build-test.yml"], "labels": ["merged"]}, "a51c5f5cbf": {"title": "Add JIT pass to insert permutes for conv ops (#30679)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30679\n\nCaffe2 expects quantized ops to be in NHWC format while pytorch inputs are in NCHW.\nAdd a jit pass to insert permutes to convert from nchw2nhwc before each conv op and add nhwc2nchw permute after the conv op.\nUsing graph rewriter to find consecutive redundant permutes and remove them from the graph\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps\n\nImported from OSS\n\nDifferential Revision: D18790518\n\nfbshipit-source-id: 4dd39cf0b31b21f5586c0edfdce2260d4e245112", "pr_number": "30679", "files_changed": ["caffe2/python/onnx/backend_rep.py", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.h", "torch/onnx/utils.py"], "labels": ["jit", "merged"]}, "a7406516d1": {"title": "Refactor bias and weight check and add aten::linear pattern (#30474)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30474\n\nThere are some common parts in `isBiasOfConvOrLinear` and `isWeightOfConvOrLinear`, we can factor\nthem out, the refactor will allow for easier extension of new patterns\n\nTest Plan:\npython test/test_jit.py\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18795725\n\nfbshipit-source-id: 446463da5e3fa8464db441ed0d9651930487b3b7", "pr_number": "30474", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "ef95a72690": {"title": "modify test_local_shutdown_with_rpc to not be flaky (#30837)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30837\n\nThis test would get very occasional flakes, with an error saying the\nRPC timed out. This happened because one worker would still be waiting for the\nreturn value of an RPC, but another worker had already performed its local\nshutdown, so it would not have sent the response. This didn't show up in\ninitial testing since the flakiness is very rare (< 1/100 test runs). This diff\nfixes the issue by not erroring if these RPCs timeout. The reason this is okay\nis because with a local shutdown, we should not expect for all outstanding RPCs\nto be completed, since workers are free to shut down without completing/waiting\non outstanding work.\nghstack-source-id: 95021672\nghstack-source-id: 95021672\n\nTest Plan: Ran the test 1000 times to ensure that it is not flaky.\n\nDifferential Revision: D18775731\n\nfbshipit-source-id: 21074e8b4b4bbab2be7b0a59e80cb31bb471ea46", "pr_number": "30837", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "1fa4908ac0": {"title": "Refactor test_quantization.py and enable `test_nested` (#30475)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30475\n\natt\n\nTest Plan:\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18795727\n\nfbshipit-source-id: c9942c5361e0a34e91a08b8fc27405799db7ff4f", "pr_number": "30475", "files_changed": ["test/test_quantization.py"], "labels": ["merged"]}, "f1755d9aea": {"title": "Insert GetAttr for quantization parameters instead of Constant (#30551)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30551\n\nTo enable quantizing with shared types, we need to insert GetAttr nodes for\nquantization parameters since the code might be shared by multiple module instances\nand we'd like to make quantized module instance also share the same code but with\ndifferent values of attributes.\n\nTest Plan:\ntest_jit.py, test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18818652\n\nfbshipit-source-id: fc95623cac59dcedd9e3f95397524eae515e7a11", "pr_number": "30551", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "58cdf1429c": {"title": "Add tests for quantizing traced models (#30476)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30476\n\natt\n\nTest Plan:\npython test/test_quantization.py\n\nImported from OSS\n\nDifferential Revision: D18795724\n\nfbshipit-source-id: 9253e102bf458d9185f68848071a4e4eff9f9b08", "pr_number": "30476", "files_changed": ["test/test_quantization.py"], "labels": ["merged"]}, "d32aec5ad6": {"title": "Add get_metrics and get_debug_info to rpc agent (#30833)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30833\n\n[rpc] Add get_metrics and get_debug_info to rpc agent\n\nTest Plan: UT and builds\n\nReviewed By: mrshenli\n\nDifferential Revision: D18835068\n\nfbshipit-source-id: f552cf196bb6d54ccd38a44ba981e7d5b15513f0", "pr_number": "30833", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h"], "labels": ["merged"]}, "2011cc1e91": {"title": "Fix half->float case of softmax backward when inner_size is not 1 (#30838)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30572\n\nThat unit test is tested to fail with master and success with this PR.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30838\n\nDifferential Revision: D18841066\n\nPulled By: ngimel\n\nfbshipit-source-id: 86a7ccdb3016c98d62dd0946daff101704cd1f68", "pr_number": "30838", "files_changed": ["aten/src/ATen/native/cuda/SoftMax.cu", "test/test_nn.py"], "labels": ["merged"]}, "b0cba8ceae": {"title": "Replace deprecated AT_ERROR with TORCH_CHECK to reduce warnings in rpc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30794\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18826311\n\nPulled By: mrshenli\n\nfbshipit-source-id: bfd58d30f386bbe9535264b2afce4acbe7ac5b0e", "pr_number": "30794", "files_changed": ["torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/script_call.cpp"], "labels": ["merged"]}, "619e2ffe23": {"title": "Replace deprecated AT_* with TORCH_* to reduce warnings in c10d", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30795\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18826310\n\nPulled By: mrshenli\n\nfbshipit-source-id: 0041ac2e5788e874e0a566abd57a8a90e658da9b", "pr_number": "30795", "files_changed": ["torch/csrc/distributed/c10d/comm.cpp", "torch/csrc/distributed/c10d/ddp.cpp", "torch/csrc/distributed/c10d/reducer.cpp"], "labels": ["merged"]}, "9a858aba5f": {"title": "Moving checks related to options.aliasAnalysis and schema.hasAliasInfo to read callsite (#30671)", "body": "Summary:\n**Context:**\nIn D18530964, we allow not set aliasAnalysis at previous registration call, and then update it to the correct one in following registration call.\n\nBut its not working E2E due to those existing checks.\n\nSo we want to remove or delay those TORCH_CHECKs.\n\nHere is the existing three callsites for operator.aliasAnalysisKind():\nhttps://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/caffe2/torch/csrc/jit/ir.cpp?lines=994%2C995%2C996%2C1001%2C1004\n\nhttps://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/caffe2/torch/csrc/jit/operator.cpp?lines=147%2C155\n\nhttps://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/caffe2/torch/csrc/jit/passes/alias_analysis.cpp?lines=260%2C277%2C380\n\n**Things to check**\n1. Those two checks are different. But since in original op_registration code, if options.schemaOrName_->is_right() is FALSE, we kind of convert it to FunctionSchema type, so in the read callsites, we only need to check the following: options.aliasAnalysisKind_ == AliasAnalysisKind::FROM_SCHEMA ||  !schema.hasAnyAliasInfo()\n\n2. If the three callsites above are indeed needed for those checks.\n\n3. Here we made assumptions that for reads from jit or other places, its always being called after all registrations calls are done. Trying to make sure its a valid assumption\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30671\n\nTest Plan: Will update and refactor the tests soon.\n\nDifferential Revision: D18784623\n\nPulled By: charliechen0401\n\nfbshipit-source-id: 75edea140d0ae3e54820e1aeef010c81fe26416a", "pr_number": "30671", "files_changed": ["aten/src/ATen/core/op_registration/op_registration.cpp", "test/cpp/jit/test_alias_analysis.cpp", "torch/csrc/jit/operator.h"], "labels": ["fb-exported", "jit", "merged"]}, "11b3065323": {"title": "Run method_tests on CUDA. (#30821)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30821\n\nWhile investigating while our tests didn't catch #30704 I noticed that none\nof our tests in method_tests() were being run on CUDA.  This diff moves\nthose tests into the new device-generic test framework so that we also get\nCUDA coverage.  For expediency, I blacklisted all tests which didn't work\non CUDA (rather than fix them); that's something we can leave for future PRs.\nThis is done by way of a new expectedFailure gadget.\n\nNote that all occurences of skipIfNoLapack needed to be replaced with\nskipCPUIfNoLapack.\n\nI punted for test_jit; it's possible those tests should also run CUDA but a JIT\nexpert should take a look here.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18840089\n\nPulled By: ezyang\n\nfbshipit-source-id: 66b613b5024c91d3e391c456bb642be7e73d4785", "pr_number": "30821", "files_changed": ["test/common_device_type.py", "test/common_methods_invocations.py", "test/test_autograd.py", "test/test_jit.py"], "labels": ["merged"]}, "1d7b40f1c4": {"title": "Fix reading `__cuda_array_interface__` without strides (#24947)", "body": "Summary:\nWhen converting a contiguous CuPy ndarray to Tensor via `__cuda_array_interface__`, an error occurs due to incorrect handling of default strides. This PR fixes this problem. It makes `torch.tensor(cupy_ndarray)` works for contiguous inputs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/24947\n\nDifferential Revision: D18838986\n\nPulled By: ezyang\n\nfbshipit-source-id: 2d827578f54ea22836037fe9ea8735b99f2efb42", "pr_number": "24947", "files_changed": ["test/test_numba_integration.py", "torch/csrc/utils/tensor_numpy.cpp", "torch/tensor.py"], "labels": ["merged", "module: internals", "module: numba", "module: numpy", "module: operators", "open source", "triaged"]}, "60714dfb64": {"title": "change index_select scalar_check to retain dimensionality of input. (#30790)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30790\n\nThe index_select documentaiton reads:\n\"The returned tensor has the same number of dimensions as the original tensor (input).\"\n\nBut the implementation would return a 0-dimensional tensor iff both the input and index were 0-dimensional.\nThis change makes it so we retuan a 0-dimensional tensor iff the input is 0-dimensional.\n\nRestacked version of: https://github.com/pytorch/pytorch/pull/30502\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18825717\n\nPulled By: gchanan\n\nfbshipit-source-id: aeb10c5107e748af3e264fbdc81fff5dd4833cc4", "pr_number": "30790", "files_changed": ["aten/src/ATen/Declarations.cwrap", "tools/autograd/derivatives.yaml"], "labels": ["merged", "topic: bc-breaking"]}, "e5d571ae25": {"title": "Remove scalar_check from topk, move it to the THC implementation.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30852\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18842662\n\nPulled By: gchanan\n\nfbshipit-source-id: b5e8a4367fce9441be2ddbd026495f1911038221", "pr_number": "30852", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/THC/generic/THCTensorTopK.cu", "test/test_torch.py"], "labels": ["merged"]}, "5687ee1d85": {"title": "added a serialize function in SGD class to utilize the existing macro for serialization/deserialization calls", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30739\n\nDifferential Revision: D18842908\n\nPulled By: anjali411\n\nfbshipit-source-id: 7dc13ff9c4fc126790b88b1b4b5d03425c349d38", "pr_number": "30739", "files_changed": ["torch/csrc/api/include/torch/optim/sgd.h", "torch/csrc/api/src/optim/sgd.cpp"], "labels": ["merged"]}, "377131b0eb": {"title": "MultiMarginCriterion: fix scalar_check in the case where reduction == None. (#30826)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30826\n\nPreviously the scalar_check for the reduction None case was:\ninput.dim() <= 1, but it should be target based, i.e.:\ntarget.dim() == 0.  This follows from the \"correct cases\", i.e.\n(N, C) X (N,) -> (N,)\n(C,) X () -> ()\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18833660\n\nPulled By: gchanan\n\nfbshipit-source-id: 26338b842a8311718c4b89da3e2f1b726d5409b8", "pr_number": "30826", "files_changed": ["aten/src/ATen/native/LossMultiMargin.cpp", "aten/src/ATen/nn.yaml", "test/common_nn.py", "test/test_torch.py"], "labels": ["merged", "topic: bc-breaking"]}, "0051467118": {"title": "Update CITATION from Workshop paper to Conference paper (#30872)", "body": "Summary:\nThe conference paper is finally published at NeurIPS 2019: http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30872\n\nDifferential Revision: D18854253\n\nPulled By: soumith\n\nfbshipit-source-id: 4f91838b1953e976542997959d5571884f739872", "pr_number": "30872", "files_changed": ["CITATION"], "labels": ["merged"]}, "4ed2eae2d0": {"title": "Add registerQParams function (#30552)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30552\n\nFor upcoming changes to support quantizing shared class type\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18818653\n\nfbshipit-source-id: 393a55db69b20a1c00ffa0157ab568cb097915b2", "pr_number": "30552", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "37435d36ed": {"title": "Refactor VariableTypeManual (#30649)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30649\n\nOperators in VariableTypeManual are now no longer registered against the VariableTypeId key, but they are registered as compound ops. See https://github.com/pytorch/pytorch/issues/30102 for background.\n\nThis also requires the non-variable codegen to ignore them and requires removal of VariableMethodStubs.cpp.\n\nSo, because function_wrapper.py now also needs to know which ops are manual, instead of having a hard-coded list in gen_variable_type.cpp for ops with manual implementation, we now have a `manual_kernel_registration` flag in native_functions.yaml that disables the registration of operator kernels for this operator (the schema is still registered). Then, we manually register the right kernels for the operator.\nghstack-source-id: 95082204\n\nTest Plan: unit tests\n\nDifferential Revision: D18778191\n\nfbshipit-source-id: 0af6f9e43ff4fb9800ce19b286dfccd0fd22cc41", "pr_number": "30649", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/README.md", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/TensorProperties.cpp", "aten/src/ATen/native/VariableMethodStubs.cpp", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp", "aten/src/ATen/native_parse.py", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/VariableType.h", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["merged"]}, "e123d90a93": {"title": "Back out \"Back out \"Back out \"Revert D18542342: Boxed variable dispatch\"\"\" (#30650)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30650\n\nOriginal commit changeset: 51bb7aac7cb7\nghstack-source-id: 95082205\n\nTest Plan: CI\n\nDifferential Revision: D18778190\n\nfbshipit-source-id: 7e9577e88fd0492006b6ea836ec081aea9da6b0c", "pr_number": "30650", "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "caffe2/c2_aten_srcs.bzl", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["merged"]}, "44ff7b08d8": {"title": "Reduce intrusive_ptr incref/decref costs (#30709)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30709\n\nIntrusive_ptr doesn't provide a explicit incref method. When a users want to\nincref the target, they creates a intrusive_ptr to wrap the target, then makes\na copy which does the actual incref, then release both the first intrusive_ptr\nand the copy to prevent decref at deconstruction time. This is very\ninefficient. Instead, do the incref/decref directly.\n\nDifferential Revision: D18798505\n\nfbshipit-source-id: 524d4f30d07d733df09d54423b044d80e4651454", "pr_number": "30709", "files_changed": ["c10/core/TensorImpl.h", "c10/test/util/intrusive_ptr_test.cpp", "c10/util/intrusive_ptr.h"], "labels": ["fb-exported", "merged"]}, "d6ddfab11f": {"title": "save linux build binary size to Scuba (#30832)", "body": "Summary:\nexample: https://fburl.com/scuba/mjheume7\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30832\n\nDifferential Revision: D18857146\n\nPulled By: mingbowan\n\nfbshipit-source-id: 66bcd352922944c227f337a66e8a75e2d7393fd3", "pr_number": "30832", "files_changed": [".circleci/config.yml", ".circleci/scripts/upload_binary_size_to_scuba.py", ".circleci/verbatim-sources/binary-job-specs.yml"], "labels": ["merged"]}, "81e4739141": {"title": "Move QScheme ops to c10 (#30134)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30134\n\nghstack-source-id: 95055387\n\nTest Plan: buck build mode/dev caffe2:generate-code\n\nDifferential Revision: D18609716\n\nfbshipit-source-id: fec39359e0b97387a9b13f8179d72a731cc61808", "pr_number": "30134", "files_changed": ["aten/src/ATen/core/boxing/kernel_functor.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "aten/src/ATen/native/native_functions.yaml", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/script/schema_type_parser.cpp", "torch/csrc/jit/unpickler.cpp"], "labels": ["merged"]}, "6d06b925ba": {"title": "Remove `values_to_quantize_` (#30858)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30858\n\nThis is not needed since we have `values_to_qparams_`\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18848992\n\nfbshipit-source-id: dc81f59967a93abdd5562f1010f02de4f4e60db0", "pr_number": "30858", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "5e6c3fb23b": {"title": "Add more details to explain rpc_backend_options arg in init_rpc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30855\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18847529\n\nPulled By: mrshenli\n\nfbshipit-source-id: b4f0d5797f3b41cce155b7821d6bd34b268bd24e", "pr_number": "30855", "files_changed": ["torch/distributed/rpc/__init__.py"], "labels": ["merged"]}, "642469b706": {"title": "Fix examples in API doc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30856\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18847528\n\nPulled By: mrshenli\n\nfbshipit-source-id: 57f666d9d4b634fb77b1b65debd2b07e2bebd57a", "pr_number": "30856", "files_changed": ["torch/distributed/rpc/api.py"], "labels": ["merged"]}, "26c51468c5": {"title": "Fix examples in RRef API doc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30857\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18847527\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7dc9d28277597f8fc3ef97fa9ac98a312e76e6fb", "pr_number": "30857", "files_changed": ["torch/csrc/distributed/rpc/init.cpp"], "labels": ["merged"]}, "4fd20c0816": {"title": "Kill hypothesis deadline testing (#30890)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30890\n\nWe've received way too many complaints about this functionality making tests flaky, and it's not providing value to us anyway. Let's cut the shit and kill deadline testing\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18857597\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 67e3412795ef2fb7b7ee896169651084e434d2f6", "pr_number": "30890", "files_changed": ["test/hypothesis_utils.py", "test/test_fake_quant.py", "test/test_qat.py", "test/test_quantization.py", "test/test_quantized.py", "test/test_quantized_nn_mods.py"], "labels": ["merged"]}, "223f46f5fa": {"title": "Fix flake8 warning (#30905)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30905\n\n-\nghstack-source-id: 95117983\n\nTest Plan: -\n\nDifferential Revision: D18861981\n\nfbshipit-source-id: b794a7fbe05af29471286c7f665cf3f86541eb5a", "pr_number": "30905", "files_changed": ["aten/src/ATen/function_wrapper.py"], "labels": ["merged"]}, "baccd26df7": {"title": "update code analyzer script to handle splitted torch libraries (#30864)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30864\n\nChange it to handle all archive files under install folder.\n\nTest Plan:\n```\nANALYZE_TEST=1 CHECK_RESULT=1 tools/code_analyzer/build.sh\nANALYZE_TORCH=1 tools/code_analyzer/build.sh\n```\n\nDifferential Revision: D18850317\n\nPulled By: ljk53\n\nfbshipit-source-id: 7c57ae16c82b6ded53aa7df385f3b6074190fc04", "pr_number": "30864", "files_changed": ["tools/code_analyzer/build.sh"], "labels": ["merged"]}, "a77eafa1d8": {"title": "Fix 'initialized after field' error (#30908)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30908\n\nSame as title.\n\nTest Plan: Wait for CI to clear.\n\nReviewed By: bddppq, xw285cornell\n\nDifferential Revision: D18862837\n\nfbshipit-source-id: bc34356b85774fc20ba46d321c8a2bb5d5c727f6", "pr_number": "30908", "files_changed": ["aten/src/ATen/native/cuda/Reduce.cuh"], "labels": ["fb-exported", "merged", "module: rocm"]}, "c37de32b23": {"title": "Enable len(dataloader) for iterable dataset (#23587)", "body": "Summary:\nCopy-paste comment from code for reasoning:\n\n```\n            # NOTE [ IterableDataset and __len__ ]\n            #\n            # For `IterableDataset`, `__len__` could be inaccurate when one naively\n            # does multi-processing data loading, since the samples will be duplicated.\n            # However, no real use case should be actually using that behavior, so\n            # it should count as a user error. We should generally trust user\n            # code to do the proper thing (e.g., configure each replica differently\n            # in `__iter__`), and give us the correct `__len__` if they choose to\n            # implement it (this will still throw if the dataset does not implement\n            # a `__len__`).\n            #\n            # To provide a further warning, we track if `__len__` was called on the\n            # `DataLoader`, save the returned value in `self._len_called`, and warn\n            # if the iterator ends up yielding more than this number of samples.\n```\n\nFixes https://github.com/pytorch/pytorch/issues/30184\nPull Request resolved: https://github.com/pytorch/pytorch/pull/23587\n\nDifferential Revision: D18852625\n\nPulled By: ailzhang\n\nfbshipit-source-id: aea8d4d70c7f21aaa69b35908a6f43026493d826", "pr_number": "23587", "files_changed": ["test/common_utils.py", "test/test_dataloader.py", "torch/utils/data/dataloader.py"], "labels": ["merged", "module: dataloader", "open source", "triaged"]}, "118f1c633b": {"title": "refactor the way we are handling bailout counts", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30410\n\nDifferential Revision: D18733370\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 0ea9dc0f3dd1a47bcc09f1d54745460f9bd71886", "pr_number": "30410", "files_changed": ["torch/csrc/jit/passes/bailout_graph.cpp", "torch/csrc/jit/passes/liveness.cpp"], "labels": ["jit", "merged"]}, "7b97eaeba5": {"title": "Add module level qpl logging. (#30906)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30906\n\nAdd mobile module observer to measure performance of each method run.\nghstack-source-id: 95120194\n\nTest Plan:\nRun pytext model through BI cloaking flow on lite-interpreter and verify logs are sent:\n1. buck install -r fb4a\n2. Go to internal setting and find MobileConfig, search for android_bi_infra_cloaking_iab_models and set the following params:\na. sample_rate: 1.0\nb. enabled: true\nc. use_bytedoc_pytorch_model: true\nd. use_bytedoc_caffe2_model: false\ne. use_full_jit: false\n3. Go back to new feed and scroll down until find an ads which will direct you to offsite webpage;\n4. Click on the ads, wait for the offsite ads loads;\n5. Click back to news feed;\n6. Go to scuba table: https://fburl.com/scuba/4fghwp0b and see all the operator runs have been logged:\n\n{F223456981}\n\nReviewed By: ljk53\n\nDifferential Revision: D18702116\n\nfbshipit-source-id: a9f07eee684e3022cef5ba3c5934f30f20192a85", "pr_number": "30906", "files_changed": ["torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/observer.cpp", "torch/csrc/jit/mobile/observer.h"], "labels": ["jit", "merged"]}, "cd6167ff63": {"title": "Upgrade bazel to 1.2.0. (#30885)", "body": "Summary:\nCompanion diff for https://github.com/pytorch/xla/pull/1464. Should land only after the pytorch/xla PR is in.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30885\n\nDifferential Revision: D18866835\n\nPulled By: ailzhang\n\nfbshipit-source-id: 51f4d2770f8ef873a659579ddd81a42957ffb885", "pr_number": "30885", "files_changed": [".jenkins/pytorch/build.sh"], "labels": ["merged"]}, "8d35b6cec7": {"title": "embedding_bag make_bag_size optimization (#30701)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30701\n\nFrom James' PR https://github.com/pytorch/pytorch/pull/19715\n\nembedding_bag microbenchmarks:\nBaseline: P123020983\nRefactor make_bag_size, no changing at::zeros to at::empty (this diff): P123021393\nInference benchmark on T6_SKL - _embedding_bag self time only:\nbs=40, baseline: .302 ms/iter\nbs=40, with diff: .244 ms/iter\nbs=1 baseline: .148 ms/iter\nbs=1 with diff: .124 ms/iter\nThe bigger gap comes from fb::embedding_bag_byte_rowwise_offsets, I'm looking into that one too.\n\nTest Plan:\nMKL_NUM_THREADS=1 OMP_NUM_THREADS=1 numactl -m 0 -C 3 ./inference_benchmark_nolr_emb.par --pt-scripted-model=traced_model.pt --pt-inputs=\"batch_size_40/pt_inputs.pth\" --iters=3000 --warmup-iters=100\nbuck run mode/opt //caffe2/benchmarks/operator_benchmark:benchmark_all_other_test -- --tag_filter all --iterations 3000 --operators embeddingbag\n\nReviewed By: yinghai, qizzzh\n\nDifferential Revision: D18800166\n\nfbshipit-source-id: 820e6ece0b6ade72ee42409661f92c548f43a4cb", "pr_number": "30701", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp"], "labels": ["fb-exported", "merged"]}, "62b10721fb": {"title": "Actually make flake8 do something (#30892)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30892\n\nFixes all outstanding lints and actually installs a properly configured\nflake8\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18862825\n\nPulled By: suo\n\nfbshipit-source-id: 08e9083338a7309272e17bb803feaa42e348aa85", "pr_number": "30892", "files_changed": [".flake8", ".github/workflows/lint.yml", "aten/src/ATen/native/quantized/cpu/qnnpack/generate-wrapper.py", "test/common_methods_invocations.py", "test/dist_autograd_test.py", "test/jit/_imported_class_test/bar.py", "test/jit/_imported_class_test/very/very/nested.py", "test/jit/test_class_type.py", "test/onnx/test_pytorch_onnx_caffe2.py", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "test/rpc_agent_test_fixture.py", "test/test_cuda.py", "test/test_jit.py", "test/test_quantization.py", "test/test_sparse.py", "test/test_torch.py", "test/test_type_promotion.py", "torch/_torch_docs.py", "torch/autograd/gradcheck.py", "torch/functional.py", "torch/jit/__init__.py", "torch/onnx/symbolic_caffe2.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py", "torch/onnx/utils.py", "torch/quantization/_quantize_script.py", "torch/tensor.py"], "labels": ["jit", "merged"]}, "5c56986738": {"title": "Attach autograd edges only for tensors requiring grad. (#30904)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30904\n\nWhen we sent tensors over RPC, on the server side we would call\naddRecvRpcBackward which would call `set_history` on all tensors. This was\nincorrect and set the `requires_grad` flag on tensors that didn't actually need\ngrad.\n\nTo fix this, we only attach autograd edges to tensors that need grads.\nghstack-source-id: 95113672\nghstack-source-id: 95113999\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18828561\n\nfbshipit-source-id: d8942b76e9e4c567f8f1821f125c00d275ea0f90", "pr_number": "30904", "files_changed": ["test/dist_autograd_test.py", "torch/csrc/distributed/autograd/utils.cpp"], "labels": ["merged"]}, "a26238da57": {"title": "Enable using `torch.autograd.profiler.record_function` as decorator (#30861)", "body": "Summary:\n```python\nrecord_function('my_func')\ndef f(x, y):\n    return x + y\n\nwith profile() as p:\n    f(1, 2)\nprint(prof.key_averages().table())\n```\n\n```\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                                  Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\nmy_func                               85.42%           86.796us         87.27%           88.670us         88.670us         1\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\nSelf CPU time total: 101.606us\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30861\n\nDifferential Revision: D18857993\n\nPulled By: bddppq\n\nfbshipit-source-id: eb6b8e2a8d4f3a7f8e5b4cb3da1ee3320acb1ae7", "pr_number": "30861", "files_changed": ["test/test_autograd.py", "torch/autograd/profiler.py"], "labels": ["merged"]}, "63f1b780ba": {"title": "Support exporting aten::copy_ and aten::index_put to ONNX opset 11 (#26941)", "body": "Summary:\n- [x] Add more comments and refactor the logic of `ReshapeToAdvancedIndexingFormat`\n- [x] Add more description here. Cases that are/aren't supported, and how they are supported.\n- [x] Need to merge this PR https://github.com/pytorch/pytorch/issues/27186 to enable testing inplace operators.\n\nWe are now supporting exporting aten::copy_ and aten::index_put to ONNX.\nHere's a breakdown of the different cases in PyTorch code.\n\n```\n# Case 1: Scalar Indices\nx[0, 1, 2] = data\n\n# Case 2: Slice Indices\nx[1:3, :, ::2] = data\n\n# Case 3: Ellipsis Indices\nx[..., 0] = data\n\n# Case 4: Tensor Indices\nind1 = torch.tensor([0, 2])\nind2 = torch.tensor([1, 1])\nx[ind1, ind2] = data\n\n# Case 5: Mixing all the above cases\nind1 = torch.tensor([0, 2])\nind2 = torch.tensor([1, 1])\nx[1:3, ind1, ind2, ..., 3] = data\n```\n\nLimitations:\n\nTensor indices must be consecutive, and 1-d tensors.\n\n```\n# Supported\nind1 = torch.tensor([0, 2])\nind2 = torch.tensor([1, 1])\nx[ind1, ind2] = data\n\n# Not supported\nind1 = torch.tensor([0, 2])\nind2 = torch.tensor([1, 1])\nind3 = torch.tensor([[0], [1]])\nx[ind1, :, ind2] = data\nx[ind3] = data\n```\n\nNegative indices are not supported.\n```\n# Not supported\nx[-1] = data\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26941\n\nDifferential Revision: D17951030\n\nPulled By: houseroad\n\nfbshipit-source-id: 4357777072f53aa0bc4b297aa1ee53457a7f8dec", "pr_number": "26941", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.cpp", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.h", "torch/onnx/symbolic_opset11.py", "torch/onnx/utils.py"], "labels": ["jit", "merged", "module: autograd", "module: build", "module: ci", "module: onnx", "open source", "triaged"]}, "f1bd8cc286": {"title": "Fix lint issues in dist_autograd_test.py (#30928)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30928\n\nghstack-source-id: 95152373\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18872870\n\nfbshipit-source-id: 2cd1ef228da4bd90c13e2f067a0c89b975fa3179", "pr_number": "30928", "files_changed": ["test/dist_autograd_test.py"], "labels": ["merged"]}, "4bb497b38e": {"title": "MultiheadAttention fixes", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30666\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18864094\n\nPulled By: pbelevich\n\nfbshipit-source-id: f7a634b2c7f526282bf918d47b9cc82aa0c0af1d", "pr_number": "30666", "files_changed": ["test/test_nn.py"], "labels": ["merged"]}, "776fdda753": {"title": "Add debug info API for distributed autograd. (#30642)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30642\n\nAdding a couple of basic metrics for distributed autograd which would\nhelp in determining stuckness.\nghstack-source-id: 95156189\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18776478\n\nfbshipit-source-id: a0556ad6fe2b7c3cd0082ee2350c1c78cafaaec5", "pr_number": "30642", "files_changed": ["test/dist_autograd_test.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/distributed/autograd/context/container.cpp", "torch/csrc/distributed/autograd/context/container.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h", "torch/csrc/distributed/autograd/init.cpp"], "labels": ["merged"]}, "6848f9abb8": {"title": "call fp16<->fp32 routines in fbgemm from Half2Float and Float2Half operators (#30715)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30715\n\nChanged caffe2/caffe2/TARGETS file to define USE_FBGEMM for x86 and USE_SSE_ONLY is not defined.\n\nTest Plan: buck test caffe2/caffe2:caffe2_test_cpu -- Float16\n\nReviewed By: jianyuh\n\nDifferential Revision: D18806067\n\nfbshipit-source-id: 1b44b90a9f6dc3c27f81a46038c0f7542ed2bab3", "pr_number": "30715", "files_changed": ["caffe2/operators/half_float_ops.cc", "caffe2/operators/half_float_ops.h"], "labels": ["fb-exported", "merged"]}, "190dac13e3": {"title": "Use universal references and perfect forwarding in Loops.h. (#30466)", "body": "Summary:\nThis simplifies the generated code a bit, saving about 40K off of libtorch.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30466\n\nDifferential Revision: D18836215\n\nPulled By: resistor\n\nfbshipit-source-id: ad75c9e04783bb29cc06afd2022f73f9625dd52b", "pr_number": "30466", "files_changed": ["aten/src/ATen/native/cpu/Loops.h"], "labels": ["merged"]}, "c75bc9067c": {"title": "MultiMarginCriterion: move scalar_check from codegen to code.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30827\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18833658\n\nPulled By: gchanan\n\nfbshipit-source-id: decd42789d92d4fbfeea9b470b3d7333e3862263", "pr_number": "30827", "files_changed": ["aten/src/ATen/nn.yaml", "aten/src/THCUNN/generic/MultiMarginCriterion.cu"], "labels": ["merged"]}, "4f342a61c1": {"title": "add the worker IDs outside of addSendRpcBackward to ensure they are (#30914)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30914\n\nWhen tensors don't require grad, we don't call `addSendRpcBackward`, where we record known workerIDs to clean up the dist autograd context later. But since  https://github.com/pytorch/pytorch/pull/29781, we always include the autograd context ID in RPCs, even if tensors do not require grad. So, it could be possible that we don't release the contexts on some nodes.\n\nThis can contribute to OOMs since the contexts will not be cleaned up in this case, which can be checking by running the unit test without this patch. We can fix this issue by moving the `addKnownWorkerIds`  call to the `getMessageWithAutograd` function.\nghstack-source-id: 95178561\n\nTest Plan: Added a unit test: `test_context_cleanup_tensor_no_grad`\n\nDifferential Revision: D18869191\n\nfbshipit-source-id: b80f66bfd0dd7d01960abe1691d3f44095bb1b2b", "pr_number": "30914", "files_changed": ["test/cpp/dist_autograd/test_dist_autograd.cpp", "test/dist_autograd_test.py", "torch/csrc/distributed/autograd/utils.cpp"], "labels": ["merged"]}, "daef363b15": {"title": "Move Softshrink activation to Aten(CPU+CUDA) (#30229)", "body": "Summary:\nVitalyFedyunin, This PR is about port Softshrink activation to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.Softshrink()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    fwd_t = 0\n    bwd_t = 0\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) forward time is 0.06 (ms); backwad avg time is 0.12 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.18 (ms).\nCPU:\ninput size(128, 100) forward time is 0.19 (ms); backwad avg time is 0.23 (ms).\ninput size(128, 10000) forward time is 17.23 (ms); backwad avg time is 16.83 (ms).\n```\nAfter:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU:\ninput size(128, 100) forward time is 0.08 (ms); backwad avg time is 0.05 (ms).\ninput size(128, 10000) forward time is 0.32 (ms); backwad avg time is 0.08 (ms).\n```\n`OMP_NUM_THREADS=1:`\n```\nBefore:\ninput size(128, 100) forward time is 0.08 (ms); backwad avg time is 0.10 (ms).\ninput size(128, 10000) forward time is 7.58 (ms); backwad avg time is 7.91 (ms).\nAfter:\ninput size(128, 100) forward time is 0.08 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10000) forward time is 7.30 (ms); backwad avg time is 1.02 (ms).\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30229\n\nDifferential Revision: D18810054\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: e19074824396570db45ba488ae4f9fe1b07a5839", "pr_number": "30229", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/SoftShrink.cu", "aten/src/THCUNN/generic/SoftShrink.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/SoftShrink.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "528fa737ba": {"title": "Custom op autograd tests (#30519)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30519\n\nRe-enable them and write a few additional ones\nghstack-source-id: 95143051\n\nTest Plan: unit tests\n\nDifferential Revision: D18729561\n\nfbshipit-source-id: 8cefd8320913d72a450a3324bfd7c88faed072d7", "pr_number": "30519", "files_changed": ["aten/src/ATen/core/op_registration/op_registration_test.cpp"], "labels": ["merged"]}, "536481d9de": {"title": "Fix missing virtual destructor (#30927)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30927\n\nClasses that are used virtually (e.g. have virtual methods) must have a virtual destructor or bad things happen\nghstack-source-id: 95144736\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18870351\n\nfbshipit-source-id: 333af4e95469fdd9103aa9ef17b40cbc4a343f82", "pr_number": "30927", "files_changed": ["torch/csrc/api/include/torch/data/datasets/chunk.h"], "labels": ["merged"]}, "5bf58274cc": {"title": "getQParams return a dictionary of qparams (#30859)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30859\n\nWe can dictionary of quantization parameters to simplify the code\nhandling these things a bit\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18849023\n\nfbshipit-source-id: 09e9860b2656a1affa8776016e16794529bcee3b", "pr_number": "30859", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "45f0556ba0": {"title": "Proper print for one element tuple (#30853)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30853\n\nRight now we print one element tuple as `(val)`, and it will\nbe interpreted as `val` in parsing, this PR changes it\nto `(val,)` so we can recognize the one element tuple in parsing\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18846849\n\nfbshipit-source-id: 42959b9190c2567ef021a861497077c550324b7c", "pr_number": "30853", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "test/cpp/jit/test_ivalue.cpp"], "labels": ["merged"]}, "648bb501a1": {"title": "rename shouldAnnotate api (#30543)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30543\n\n`shouldAnnotate` doesn't make make a ton of sense as a public api\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18833608\n\nPulled By: eellison\n\nfbshipit-source-id: 460ee05d0fa91b1edc640c037be2a6ee8eaf50a6", "pr_number": "30543", "files_changed": ["torch/csrc/jit/passes/alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.h"], "labels": ["jit", "merged"]}, "3eefc06feb": {"title": "add constant prop for immutable types (#30544)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30544\n\nRun Constant Propagation upon compilation only on ops with non-aliasing inputs and outputs. This speeds up the first run of `torchvision.models.resnet18` by over 50% and speeds up compilation by about 25% (although the effects didn't seem additive with with https://github.com/pytorch/pytorch/pull/30503, so I'm going to land this PR first and then see if caching still has a sizable impact).\n\nRunning constant prop only with non-aliasing types does a lot of graph cleanup by removing constant ifs and a bunch of other smaller ops. It also avoids all the jitter problems we had when we tried running full constant prop previously. Bc it is idempotent it doesn't jitter, and it doesn't jitter graphs constructed from tracing because tracing doesn't emit any ops that only involve non-aliasing inputs.\n\nFull constant prop isn't idempotent because what ops are run depends on the state of mutation in alias db, which will often change upon successive iterations of constant propagation, and bc it affects graphs constructed from tracing.\n\nEdit: if we were okay with running constant propagation on graphs constructed from tracing (potentially making them hard to debug), an alternative would be to run constant propagation until the graph reaches a fixed point.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18833607\n\nPulled By: eellison\n\nfbshipit-source-id: 92a0adb4882d67ed5a0db5c279f5e122aeeba54a", "pr_number": "30544", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/passes/constant_propagation.h", "torch/csrc/jit/script/compiler.cpp"], "labels": ["jit", "merged"]}, "82268bf300": {"title": "handle reassignment to inf and nan (#30877)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30877\n\nPreviously, when the environment tried to reassign variables which had been assigned to \"inf\" or \"nan\" it would fail because they are not simple values. Constant prop exposed this, a test was failing internally because of it.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D18861016\n\nPulled By: eellison\n\nfbshipit-source-id: b9b72978a26a0b00b13bf8ea7685825551f5a541", "pr_number": "30877", "files_changed": ["test/test_jit.py", "torch/csrc/jit/import_source.cpp"], "labels": ["jit", "merged"]}, "a38c9b1ade": {"title": "Adding debugging metrics to process group agent", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30884\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18857140\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4ec61d13778dd49467159d0db4b6dd51feaf282b", "pr_number": "30884", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/process_group_agent.cpp"], "labels": ["merged"]}, "8a57362000": {"title": "Fix index out of bound error in Engine::ready_queue_size when called before start_threads", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30967\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18887178\n\nPulled By: mrshenli\n\nfbshipit-source-id: 67baeac9214a4749ce7e9b4d89862c93620b2d5e", "pr_number": "30967", "files_changed": ["torch/csrc/autograd/engine.cpp"], "labels": ["merged"]}, "e9ca13d7f5": {"title": "Add glue code to collect debug info from all components", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30888\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18857139\n\nPulled By: mrshenli\n\nfbshipit-source-id: 5c1bfb83a21a4a57c4297bb94f14baa09520b791", "pr_number": "30888", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/init.cpp", "torch/distributed/rpc/__init__.py"], "labels": ["merged"]}, "a03581b927": {"title": "add tests that schemas are valid (#30749)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30749\n\nAdd check to schemas that the schema is sane.\n\nI removed the defaults from symbolic_script because they were in some cases wrong and don't actually do anything. At the point they're invoked the forward should already have matched all arguments.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18864775\n\nPulled By: eellison\n\nfbshipit-source-id: 273d7e96d65b8a3d3de72e2d7bfcdf2417046c6b", "pr_number": "30749", "files_changed": ["aten/src/ATen/core/function_schema.cpp", "aten/src/ATen/core/function_schema.h", "test/test_function_schema.py", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/symbolic_script.cpp"], "labels": ["jit", "merged"]}, "446488960a": {"title": "polish up overloads on free functions (#30356)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30356\n\nThis finishes up the `torch.jit.overload` api for free-functions.\n- defaults now required on the implementation function itself\n- fully follows [overload spec](https://mypy.readthedocs.io/en/latest/more_types.html#function-overloading) such that the following is supported\n\n```\noverload\ndef mouse_event(x1: int, y1: int) -> ClickEvent: ...\ndef mouse_event(x1: int,\n                y1: int,\n                x2: Optional[int] = None,\n                y2: Optional[int] = None): ...\n```\n\nNote: `jit.overload` isn't supported yet for UDT, but is support for modules. This PR doesn't make the same changes for modules, if reviewers think I should include them then I could do so in a follow up PR or wait to land this. Since that's still an internal api I think it's fine, and the changes here would allow us to expose `torch.jit.overload` on free functions.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18864774\n\nPulled By: eellison\n\nfbshipit-source-id: 6c566738bd6f0551a000a9ea8d56e403636b7856", "pr_number": "30356", "files_changed": ["aten/src/ATen/core/function_schema.h", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "fa6661422f": {"title": "Disable flaky test_rref_context_debug_info", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30990\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18893023\n\nPulled By: mrshenli\n\nfbshipit-source-id: 80b36927f243fa53c4d64f7e7c51097290ffdeee", "pr_number": "30990", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "04b9324476": {"title": "Factor out getInvokedMethod in `InsertQuantDeQuantHelper` (#30860)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30860\n\natt\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18849021\n\nfbshipit-source-id: e5ff260f2f4e88075b0c6b32ccfd8272053ccc41", "pr_number": "30860", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "5205556782": {"title": "Export custom ops (#29752)", "body": "Summary:\nUpdated to export API:\nWhen calling this API, a dict containing the custom opsets (domain and version) used to export the model could be provided.\nWe allow registering one custom opset (domain, version) per ONNX opset. So, when exporting an operator from a custom domain, users need to pass this pair. Default custom opset version is 1.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29752\n\nReviewed By: hl475\n\nDifferential Revision: D18703662\n\nPulled By: houseroad\n\nfbshipit-source-id: 84d22557d132b526169051193d730761798fce60", "pr_number": "29752", "files_changed": ["test/onnx/test_operators.py", "torch/csrc/jit/export.cpp", "torch/csrc/jit/export.h", "torch/csrc/jit/python_ir.cpp", "torch/onnx/__init__.py", "torch/onnx/symbolic_registry.py", "torch/onnx/utils.py"], "labels": ["jit", "merged"]}, "42324cb6e8": {"title": "Change interface from map of TensorShape to shapeInfoMap (#30802)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30802\n\nChange shape_hints from map<string, TensorShape> to ShapeInfoMap to catch dimType info from model file.\n\nReviewed By: ipiszy\n\nDifferential Revision: D18821486\n\nfbshipit-source-id: c5d9ed72e158d3698aba38900aeda00f776745b4", "pr_number": "30802", "files_changed": ["caffe2/opt/backend_transformer_base.cc", "caffe2/opt/backend_transformer_base.h", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h", "caffe2/opt/custom/glow_net_transform.cc", "caffe2/opt/custom/glow_net_transform.h", "caffe2/opt/onnxifi_transformer.cc", "caffe2/opt/onnxifi_transformer.h", "caffe2/opt/shape_info.cc", "caffe2/opt/shape_info.h", "caffe2/opt/tvm_transformer.cc", "caffe2/opt/tvm_transformer.h", "caffe2/python/pybind_state.cc"], "labels": ["fb-exported", "merged"]}, "f48a8901c5": {"title": "Add floor_divide function (#30493)", "body": "Summary:\nAdds `torch.floor_divide` following the numpy's `floor_divide` api. I only implemented the out-of-place version, I can add the inplace version if requested.\n\nAlso fixes  https://github.com/pytorch/pytorch/issues/27512\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30493\n\nDifferential Revision: D18896211\n\nPulled By: eellison\n\nfbshipit-source-id: ee401c96ab23a62fc114ed3bb9791b8ec150ecbd", "pr_number": "30493", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/torch.rst", "test/test_jit.py", "test/test_namedtensor.py", "torch/_torch_docs.py", "torch/csrc/jit/script/builtin_functions.cpp", "torch/tensor.py"], "labels": ["jit", "merged"]}, "e05ee4c421": {"title": "Remove BUILD_NAMEDTENSOR macros (#30894)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30894\n\nThis PR begins the process of removing BUILD_NAMEDTENSOR macros. There\nwill be followups.\n\nReasons for removing the macros:\n- BUILD_NAMEDTENSOR is always on and has been on since pytorch 1.3.0.\n- Since we don't test building without it, it is useless to keep around.\n- Code becomes nicer to read without the macros\n\nReasons for not removing the macros:\n- potential for feature flagging\n\nNow, I argue against needing to feature flag. The main reason why we\nmight want to feature flag is if we need to disable the feature.\nWe'd need a fast switch to disable the feature if someone discovers\nin the future that named tensors caused some regression in some existing workflows.\n\nIn https://github.com/pytorch/pytorch/pull/25798, I did a variety of\nmacro- and micro- benchmarks to determine the performance impact of named\ntensors on regular tensors.\n\n[The\nmicrobenchmarks](https://github.com/pytorch/pytorch/pull/25798#issuecomment-529014810)\nwere not very stable, and running the\nmicrobenchmarks for more iterations doesn't actually help because the\nnoise is not distributed in a nice way. Instead of microbenchmarks I ran\na [profiler\n(perf)](https://github.com/pytorch/pytorch/pull/25798#issuecomment-555707645)\nto estimate how much overhead named tensors add to unnamed code. I\nestimated the overhead to be less than 100ns for `add` and even smaller\nfor `mm`; there are ways to optimize even futher if we find this to be a\nproblem.\n\n[Initial\nmacrobenchmarks](https://github.com/pytorch/pytorch/pull/25798#issuecomment-530539104)\nwere also not very stable. I ran imagenet for some number of epochs. To\nmake them more stable, I got rid of the data loading (which seemed to\nvary between runs). [In some benchmarkers without data\nloading](https://github.com/pytorch/pytorch/pull/25798#issuecomment-562214053),\nwe can see that the results are less noisy now. These results support\nno noticeable regressions in speed.\n\nTest Plan: - wait for CI\n\nDifferential Revision: D18858543\n\nPulled By: zou3519\n\nfbshipit-source-id: 08bf3853a9f506c6b084808dc9ddd1e835f48c13", "pr_number": "30894", "files_changed": ["aten/src/ATen/NamedTensorUtils.cpp", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/TensorNames.cpp", "aten/src/ATen/TensorNames.h", "aten/src/ATen/core/Dimname.cpp", "aten/src/ATen/core/Dimname.h", "aten/src/ATen/core/NamedTensor.cpp", "aten/src/ATen/core/NamedTensor.h", "aten/src/ATen/core/boxing/boxing.h", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/Distance.cpp", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/Dropout.cpp", "aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/LegacyDefinitions.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/NamedTensor.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/ResizeCommon.h", "aten/src/ATen/native/SoftMax.cpp", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/TensorProperties.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/cuda/CUDAUnaryOps.cpp", "aten/src/ATen/native/cuda/LegacyDefinitions.cpp", "aten/src/ATen/native/quantized/cpu/qreduction.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/templates/TensorMethods.h", "aten/src/ATen/test/Dimname_test.cpp", "aten/src/ATen/test/NamedTensor_test.cpp", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMoreMath.cpp", "tools/autograd/templates/VariableType.h", "tools/autograd/templates/python_variable_methods.cpp", "tools/autograd/templates/variable_factories.h", "torch/csrc/Module.cpp", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h", "torch/csrc/python_dimname.cpp", "torch/csrc/python_dimname.h", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/csrc/utils/tensor_new.cpp"], "labels": ["jit", "merged"]}, "af4040d808": {"title": "resubmit polish up overloads on free functions (#31014)", "body": "Summary:\nResubmitting https://github.com/pytorch/pytorch/pull/30356\n\nSecond commit has reintroduces deleted function which caused revert previously.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31014\n\nDifferential Revision: D18899127\n\nPulled By: eellison\n\nfbshipit-source-id: 9049b8718926c329d9cb46bb96eac6c278e9b866", "pr_number": "31014", "files_changed": ["aten/src/ATen/core/function_schema.h", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "06c7420fa2": {"title": "Raise error if a block can not be found from a CUDA tensor (#30870)", "body": "Summary:\nAfter several discussions, we agreed not to put any extra safety check for recordStream as either the check will cause failures in certain scenarios or there is no need to throw for user errors.\n\nAs a summary, it simply does what is described in https://github.com/pytorch/pytorch/issues/27405, check if a tensor is indeed allocated by a CUDACachingAllocator instance, if it is, then throw internal error if a block can not be retrieved.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30870\n\nDifferential Revision: D18851669\n\nPulled By: yxia11\n\nfbshipit-source-id: c2f01798cd24f1fd0f35db8764057d5d333dab95", "pr_number": "30870", "files_changed": ["aten/src/ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.cpp", "aten/src/ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.h", "c10/cuda/CUDACachingAllocator.cpp", "c10/cuda/CUDACachingAllocator.h", "test/test_cuda.py", "tools/autograd/templates/python_variable_methods.cpp", "torch/csrc/distributed/c10d/ddp.cpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["merged"]}, "c34ef1aa2e": {"title": "Automatic update of fbcode/onnx to c08a7b76cf7c1555ae37186f12be4d62b2c39b3b (#30619)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30619\n\nPrevious import was fea8568cac61a482ed208748fdc0e1a8e47f62f5\n\nIncluded changes:\n- **[c08a7b76](https://github.com/onnx/onnx/commit/c08a7b76)**: doc: fix some typos at ONNXIFI (#2473) <Yorkie Liu>\n- **[4be12d46](https://github.com/onnx/onnx/commit/4be12d46)**: remove workshop update since it is done (#2460) <Prasanth Pulavarthi>\n- **[86107d1b](https://github.com/onnx/onnx/commit/86107d1b)**: Updated with correct URL to LICENSE (#2468) <Ryan Loney>\n- **[9bf6fbb6](https://github.com/onnx/onnx/commit/9bf6fbb6)**: Update Argmin/Argmax (#2461) <Lara Haidar>\n- **[748d81b8](https://github.com/onnx/onnx/commit/748d81b8)**: Fix windows conda build (#2452) <Ashwini Khade>\n- **[a32db1c5](https://github.com/onnx/onnx/commit/a32db1c5)**: Delete duplicate word in comment (#2439) <Haibo Hao>\n- **[e108da9a](https://github.com/onnx/onnx/commit/e108da9a)**: Fix bug in function body verifier (#2390) <G. Ramalingam>\n- **[c3d3ef82](https://github.com/onnx/onnx/commit/c3d3ef82)**: docs: fix typo in IR.md (#2441) <Elliot Waite>\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D18766132\n\nfbshipit-source-id: 13c04f21399579acb87a8f9fac2e4c329b0720b8", "pr_number": "30619", "files_changed": ["caffe2/python/onnx/tests/onnx_backend_test.py", "third_party/onnx"], "labels": ["fb-exported", "merged"]}, "a42d093db2": {"title": "FCTransposed to FbFCPacked (#29766)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29766\n\nAdd FbgemmPackTranspose op to support the packing on FCTransposed weights\n\nAdd FCTransposed to FbFCPacked transformation to Dper fp16 exporter\n\nTest Plan:\n```\nbuck test mode/opt caffe2/caffe2/fb/fbgemm:fb_fc_packed_op_test\n```\n\n```\nbuck test mode/opt caffe2/caffe2/python:layers_test\n```\n\nDifferential Revision: D18482306\n\nfbshipit-source-id: e8f1947b3d0d04892293509ebf88742f5f0f5997", "pr_number": "29766", "files_changed": ["caffe2/python/layers/fc.py", "caffe2/python/layers_test.py"], "labels": ["fb-exported", "merged"]}, "bb7befb12c": {"title": "Support loading by blob in predictor", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30805\n\nReviewed By: ipiszy\n\nDifferential Revision: D18827383\n\nfbshipit-source-id: b97f958768618ca29a02b057667a9b4ee313ad3c", "pr_number": "30805", "files_changed": ["caffe2/opt/custom/glow_net_transform.cc", "caffe2/opt/custom/glow_net_transform.h", "caffe2/opt/onnxifi_transformer.h", "caffe2/proto/predictor_consts.proto"], "labels": ["fb-exported", "merged"]}, "313c211f3f": {"title": "Calling JITed 8 Bit Fused SLS in FBGEMM from C2 (#30926)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30926\n\nCalling the JITed FBGEMM kernel for Fused 8 Bit Sparse Length Sum (Fused8BitRowwiseEmbeddingLookup)\n\nTest Plan:\nbuck test  mode/dbg //caffe2/caffe2/python:lengths_reducer_fused_8bit_rowwise_ops_test\n\nAll tests pass.\n\nReviewed By: jspark1105\n\nDifferential Revision: D18058128\n\nfbshipit-source-id: 0dfa936eb503712c39e53748e015fc156afde86f", "pr_number": "30926", "files_changed": ["caffe2/operators/lengths_reducer_fused_8bit_rowwise_ops.h"], "labels": ["fb-exported", "merged"]}, "394d2f7037": {"title": "Fix the rendering of the doc of max. (#30779)", "body": "Summary:\nClose https://github.com/pytorch/pytorch/issues/30731\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30779\n\nDifferential Revision: D18837317\n\nPulled By: zou3519\n\nfbshipit-source-id: b9b5ba414756a68d4b39a7a7c2d89fee1e3c040f", "pr_number": "30779", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged"]}, "47033b49f3": {"title": "Suppress XCode build warnings (#31000)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31000\n\n## Summary\n\nAdd Fastlane configurations to suppress the build warnings from XCode.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18912489\n\nPulled By: xta0\n\nfbshipit-source-id: f2c54d54a12ad2415695d1fcb1800684c7a9e560", "pr_number": "31000", "files_changed": ["ios/TestApp/.gitignore", "ios/TestApp/fastlane/Scanfile"], "labels": ["merged"]}, "27d7dba9ab": {"title": "Remove scalar_check specification and codegen. (#30874)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30874\n\nThese have all been disabled at this point, so there is no difference in the generated code.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18855990\n\nPulled By: gchanan\n\nfbshipit-source-id: 03796b2978e23ef9060063f33241a1cbb39f1cf3", "pr_number": "30874", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/OpaqueTensorImpl.h", "aten/src/ATen/SparseTensorImpl.cpp", "aten/src/ATen/SparseTensorImpl.h", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp", "aten/src/ATen/nn.yaml", "aten/src/ATen/nn_parse.py", "aten/src/ATen/test/broadcast_test.cpp", "aten/src/ATen/test/wrapdim_test.cpp", "aten/src/TH/THTensor.hpp", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/THC/generic/THCTensorIndex.cu", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h"], "labels": ["merged"]}, "57f29a44c7": {"title": "Bug fix of the histogram observers (#30970)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30970\n\nCheck null tensors in the histogram observers\n\nTest Plan: f154576636 vs f154820243\n\nReviewed By: hx89\n\nDifferential Revision: D18865771\n\nfbshipit-source-id: 669c014d914525deee36142e12f013afaf3caf1d", "pr_number": "30970", "files_changed": ["caffe2/quantization/server/activation_distribution_observer.cc"], "labels": ["fb-exported", "merged"]}, "b01b05790e": {"title": "Fix memory leak due to circular dependency. (#31030)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31030\n\nDistAutogradContext held a shared_ptr reference to RecvRpcBackward and\nRecvRpcBackward held a shared_ptr reference to the context. This circular\ndependency caused significant memory leaks. As a result, I'm changing the\nreference in RecvRpcBackward to be a weak_ptr.\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18896389\n\nfbshipit-source-id: e5bc588b6f998885854e3a67de1e82452e8475ce", "pr_number": "31030", "files_changed": ["torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp", "torch/csrc/distributed/autograd/functions/recvrpc_backward.h"], "labels": ["merged"]}, "cc319659e3": {"title": "qnnpack TanH", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31013\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18898903\n\nPulled By: z-a-f\n\nfbshipit-source-id: aa126a98627b808678f629f39853c3b9c70eb2bf", "pr_number": "31013", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt", "aten/src/ATen/native/quantized/cpu/qnnpack/README.md", "aten/src/ATen/native/quantized/cpu/qnnpack/bench/tanh.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/configure.py", "aten/src/ATen/native/quantized/cpu/qnnpack/include/pytorch_qnnpack.h", "aten/src/ATen/native/quantized/cpu/qnnpack/src/tanh.c", "aten/src/ATen/native/quantized/cpu/qnnpack/test/tanh-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/tanh.cc"], "labels": ["merged"]}, "ed20937231": {"title": "Remove TensorImpl::maybe_zero_dim.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30878\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18855989\n\nPulled By: gchanan\n\nfbshipit-source-id: 44087b6136ec40d0a3de5b5a9f03c60d002a1107", "pr_number": "30878", "files_changed": ["c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h"], "labels": ["merged"]}, "e3d40f857b": {"title": "Make nn.Module `forward()` type annotation more permissive (#31057)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31057\n\nThe current signature basically will always fail to type check, because\nmypy enforces that the subclass method's input types must be \"wider\"\nthan their superclass method's input types (i.e. they can vary\ncontravariantly). And nothing is wider than `Any`.\n\nThis change makes it so that any input params are allowed in\n`forward()`. Fixes #29099\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18918034\n\nPulled By: suo\n\nfbshipit-source-id: 9940e9f769b55d580d9d7f23abf6f88edb92627f", "pr_number": "31057", "files_changed": ["torch/nn/modules/module.pyi.in"], "labels": ["merged"]}, "5edfe9cb80": {"title": "add torch.square (#30719)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/30524\nThis adds an new operator `torch.square` to PyTorch\n\nI think it is ready for the first-time review now albanD\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30719\n\nDifferential Revision: D18909268\n\nPulled By: albanD\n\nfbshipit-source-id: 5626c445d8db20471a56fc1d7a3490e77812662b", "pr_number": "30719", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "benchmarks/operator_benchmark/pt/unary_test.py", "docs/source/tensors.rst", "docs/source/torch.rst", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged"]}, "d113b22571": {"title": "kill PyTorch py2 circle jobs (#29353)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29353\n\nFirst step to killing Python 2 everywhere. I don't really know that much\nabout the caffe2 circle jobs so I left them alone for now.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18920563\n\nPulled By: suo\n\nfbshipit-source-id: b37d8427a6ecd4b8a7e16c1ff948e0ce13b5798f", "pr_number": "29353", "files_changed": [".circleci/cimodel/data/binary_build_data.py", ".circleci/cimodel/data/dimensions.py", ".circleci/cimodel/data/pytorch_build_data.py", ".circleci/config.yml", ".circleci/verbatim-sources/workflows-binary-builds-smoke-subset.yml"], "labels": ["merged"]}, "b7652a2f81": {"title": "remove py2 flake8 lint (#29357)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29357\n\nAs title\n\nTest Plan: Imported from OSS\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D18920562\n\nPulled By: suo\n\nfbshipit-source-id: b5dd559cfb0ba6c64b9ccf3655417afb56a7b472", "pr_number": "29357", "files_changed": [".github/workflows/lint.yml"], "labels": ["merged"]}, "3de8584de8": {"title": "Correct definition of nodes that work with Autograd (#30683)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30683\n\nAssume that a node can work with autograd only if it is not a fusion\ngroup and in prim or aten namespaces.\n\nTest Plan: CI\n\nReviewed By: lly-zero-one\n\nDifferential Revision: D18795171\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 301090557e330b58be70e956784f7f0dc343c684", "pr_number": "30683", "files_changed": ["test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests.h", "torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp", "torch/csrc/jit/passes/inline_autodiff_subgraphs.h"], "labels": ["jit", "merged"]}, "e42af97349": {"title": "Add quantized concat conversion (#30887)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30887\n\nSupport to convert quantized concat from pytorch to caffe2\n\nTest Plan:\npython test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_cat\n\nImported from OSS\n\nDifferential Revision: D18855676\n\nfbshipit-source-id: 5d0cf3f03c61819e168b080afa368b1255d0419c", "pr_number": "30887", "files_changed": ["caffe2/onnx/backend.cc", "test/onnx/test_pytorch_onnx_caffe2_quantized.py", "torch/onnx/symbolic_caffe2.py"], "labels": ["merged"]}, "e7e6d56b77": {"title": "Allow async work in rpc RequestCallback processing. (#30637)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30637\n\nRequestCallback api currently forces work to be always synchronous, which,\nas we scale, means we're going to need to throw large number of (mostly\nblocked) threads at the rpc problem. For some activities like dependent\nautograd rpcs, there's not a necessary reason to block in these threads.\n\nIn this change, the RequestCallback api is updated to return a\nshared_ptr<FutureMessage> rather than a Message:\n\n   std::shared_ptr<FutureMessage> operator()(Message& request) const;\n\nWith a futures-style api, RPC ops that wish to be async can then be async,\nwhile short-lived blocking functions (or Python UDFs) can just block.\n\nIn this change, we keep all of the current ops as synchronous (i.e. we block\nand then return a completed FutureMessage). We also update the rpc_agents in\na manner compatible with this sort of parallelism.\n\nHere, we only want to incur overhead when we use the async behavior.\nSome modest extra cost seems unavoidable here (e.g. the allocation for the\nstd::make_shared<>), but we can trivially detect the synchronous/completed\ncase in the rpc_agent and avoid the extra thread-switches/etc. in that case.\nghstack-source-id: 95287026\n\nTest Plan:\n- Basic: buck test mode/dev-nosan caffe2/test/...\n  - Additional testcase in ThriftRpcAgentTest for deferred work.\n\nDifferential Revision: D18774322\n\nfbshipit-source-id: cf49922a71707cfb1726de16f93af23b160385d8", "pr_number": "30637", "files_changed": ["test/cpp/dist_autograd/test_dist_autograd.cpp", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/context/context.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h", "torch/csrc/distributed/rpc/future_message.cpp", "torch/csrc/distributed/rpc/future_message.h", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/request_callback.cpp", "torch/csrc/distributed/rpc/request_callback.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/request_callback_impl.h"], "labels": ["merged", "module: rpc"]}, "d02280b432": {"title": "move migration guide to appendix (#31068)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31068\n\nLet's get it out of the early parts now that the recursive API has been\naround for a while\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18920498\n\nPulled By: suo\n\nfbshipit-source-id: 6f4389739dd9e7e5f3014811b452249cc21d88e7", "pr_number": "31068", "files_changed": ["docs/source/jit.rst"], "labels": ["merged"]}, "9f3fe78239": {"title": "peephole optimize type refinements (#31024)", "body": "Summary:\nPeephole optimize out type refinements when they are no longer refining the type.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31024\n\nDifferential Revision: D18920958\n\nPulled By: eellison\n\nfbshipit-source-id: 6d05d9812b9f9dcf001de760a78a2042fb832773", "pr_number": "31024", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/peephole.cpp"], "labels": ["jit", "merged"]}, "c72dd526a7": {"title": "kill py2 onnx builds", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31082\n\nDifferential Revision: D18922689\n\nPulled By: suo\n\nfbshipit-source-id: 98c91b90ee3b1dd13c6020597a0ace741a1597da", "pr_number": "31082", "files_changed": [".circleci/cimodel/data/caffe2_build_data.py", ".circleci/config.yml"], "labels": ["merged"]}, "7f5f2e8871": {"title": "add ZERO_COLLISION_HASH to caffe2 data type (#30912)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30912\n\nAdd a new data type ZERO_COLLISION_HASH .\n\nTest Plan: ci\n\nReviewed By: boryiingsu\n\nDifferential Revision: D18843626\n\nfbshipit-source-id: b2d8280f13c78b4a656cf95822198df59de7b64c", "pr_number": "30912", "files_changed": ["caffe2/core/blob_serialization.cc", "caffe2/proto/caffe2.proto"], "labels": ["fb-exported", "merged"]}, "9a5fd2eb07": {"title": "Fix conflicts in CMAKE_GENERATOR and generator (#30971)", "body": "Summary:\n...specified in -G\n\nhttps://cmake.org/cmake/help/latest/variable/CMAKE_GENERATOR.html\nAccording to the document, the generator could be determined through two methods:\n1. Specify in `-G`\n2. Read from `CMAKE_GENERATOR`\n\nWe should avoid conflicts in these two methods. This fixes https://github.com/pytorch/pytorch/issues/30910.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30971\n\nDifferential Revision: D18927529\n\nPulled By: mingbowan\n\nfbshipit-source-id: e9a179ceb32d6fbabfaeac6cfe9e6170ca170b20", "pr_number": "30971", "files_changed": ["tools/setup_helpers/cmake.py"], "labels": ["merged"]}, "8013ffd400": {"title": "Fix weight_norm export for dim=0 (#31015)", "body": "Summary:\nExported weight_norm is incorrectly reducing over axis 0 as well when dim is set to 0.\nPrevious test case only covers weight with size(0) == 1, which yields the same result whether reduced over or not.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31015\n\nReviewed By: hl475\n\nDifferential Revision: D18900894\n\nPulled By: houseroad\n\nfbshipit-source-id: 19004f51933b37f848dbe4138e617a7a8e35a9ec", "pr_number": "31015", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged"]}, "28ee309c9a": {"title": "disable onnx py3 gcc5 build (#31100)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31100\n\nThis appears to not work right now. Disabling pending an investigation.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18928777\n\nPulled By: suo\n\nfbshipit-source-id: 63089131bad98902979e5cf4373732c85badef9d", "pr_number": "31100", "files_changed": [".circleci/cimodel/data/caffe2_build_data.py", ".circleci/config.yml", ".github/workflows/lint.yml"], "labels": ["merged"]}, "d6d6075573": {"title": "Optimize LayerNorm with explicit vectorization using Vec256 (#29104)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29104\n\nWe would like to provide the vectorized implementation for layer norm. This PR reuses https://github.com/pytorch/pytorch/pull/23349.\n\nTest Plan:\nbuck test mode/dev-nosan //caffe2/test:nn -- \"LayerNorm\"\n\nbuck test mode/dev-nosan //caffe2/test:nn -- \"test_LayerNorm_1d_no_elementwise_affine_eval\"\n\n python run_test.py -i nn -- TestNN.test_LayerNorm_1d_no_elementwise_affine_eval\n\nDifferential Revision: D18293522\n\nfbshipit-source-id: f4cfed6e62bac1b43ee00c32b495ecc836bd9ec5", "pr_number": "29104", "files_changed": ["aten/src/ATen/native/cpu/layer_norm_kernel.cpp"], "labels": ["merged"]}, "9305f44854": {"title": "Remove BUILD_NAMEDTENSOR from codegen and .cu files (#31047)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31047\n\nChangelist:\n- remove BUILD_NAMEDTENSOR from .cu files\n- remove BUILD_NAMEDTENSOR special handling in function_wrapper.py\n- remove BUILD_NAMEDTENSOR from cpp_extension.py. This code actually\ndid nothing because we always compile with BUILD_NAMEDTENSOR.\n\nTest Plan: - run tests\n\nDifferential Revision: D18908442\n\nPulled By: zou3519\n\nfbshipit-source-id: b239e24de58580adaf3cef573350773a38b1e4f0", "pr_number": "31047", "files_changed": ["aten/src/ATen/function_wrapper.py", "aten/src/ATen/gen.py", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/cuda/SortingKthValue.cu", "aten/src/THC/generic/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorMasked.cu", "aten/src/THC/generic/THCTensorMathBlas.cu", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPointwise.cu", "torch/utils/cpp_extension.py"], "labels": ["merged"]}, "3301794855": {"title": "Port ELU activation to Aten (#29275)", "body": "Summary:\nVitalyFedyunin, This PR is about port  ELU activation to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.ELU()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    fwd_t = 0\n    bwd_t = 0\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) forward time is 0.04 (ms); backwad avg time is 0.09 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU:\nOMP_NUM_THREADS=56\ninput size(128, 100) forward time is 0.28 (ms); backwad avg time is 0.18 (ms).\ninput size(128, 10000) forward time is 23.53 (ms); backwad avg time is 14.46 (ms).\nOMP_NUM_THREADS=1\ninput size(128, 100) forward time is 0.16 (ms); backwad avg time is 0.08 (ms).\ninput size(128, 10000) forward time is 15.53 (ms); backwad avg time is 6.60 (ms).\n```\nAfter:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU:\nOMP_NUM_THREADS=56\ninput size(128, 100) forward time is 0.24 (ms); backwad avg time is 0.17 (ms).\ninput size(128, 10000) forward time is 0.73 (ms); backwad avg time is 1.11 (ms).\nOMP_NUM_THREADS=1\ninput size(128, 100) forward time is 0.15 (ms); backwad avg time is 0.07 (ms).\ninput size(128, 10000) forward time is 14.40 (ms); backwad avg time is 6.00 (ms).\n```\nHow to set the numbers of thread? using following script:\n```\nnum_threads=$1\nscript=$2\nlast_core=`expr $num_threads - 1`\necho \"using $num_threads OMP threads\"\necho \"bind cores to 0~$last_core\"\nexport OMP_NUM_THREADS=$num_threads\nexport KMP_AFFINITY=granularity=fine,compact,1,0\nnumactl --physcpubind=0-$last_core --membind=0 python $script\n```\nand run .**/run.sh num_threads test.py**.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29275\n\nDifferential Revision: D18587389\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: bea8f3f006c6893090f863d047c01886d195437a", "pr_number": "29275", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/ELU.cu", "aten/src/THCUNN/generic/ELU.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/ELU.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "717274c001": {"title": "Add useful warnings for t.grad when it won't be populated for known reasons (#30531)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/2362 and https://github.com/pytorch/pytorch/issues/19778\n\nTo avoid issues with frozen model, we only consider warning for Tensors that require gradients and are neither leafs nor retain gradients.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30531\n\nDifferential Revision: D18832767\n\nPulled By: albanD\n\nfbshipit-source-id: 743e863dc14ab57713e66da78b2e4d759dfba0ff", "pr_number": "30531", "files_changed": ["test/test_jit.py", "test/test_torch.py", "torch/_tensor_docs.py", "torch/autograd/gradcheck.py", "torch/csrc/autograd/python_variable.cpp", "torch/tensor.py"], "labels": ["merged"]}, "a929d312ac": {"title": "Add dill>=0.3.1 as testing dependency (#31121)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31121\n\nFor https://github.com/pytorch/pytorch/pull/30985 .\n\nTest Plan:\n- run `pip install \"dill>=0.3.1\"` locally, check that it actually\ninstalls dill>=0.3.1.\n\nDifferential Revision: D18934871\n\nPulled By: zou3519\n\nfbshipit-source-id: 688a489b9e81134ccb5ab4b099116e3fe6b6b7ae", "pr_number": "31121", "files_changed": [".circleci/docker/common/install_travis_python.sh"], "labels": ["merged"]}, "1f87e823b8": {"title": "Make `nn.Transformer` TorchScript compatible (#28561)", "body": "Summary:\nThis makes `nn.Transformer` usable from TorchScript. It preserves backwards compatibility via `__setstate__` on the encoder/decoder.\n\nFixes https://github.com/pytorch/pytorch/issues/24173\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28561\n\nDifferential Revision: D18124753\n\nPulled By: driazati\n\nfbshipit-source-id: 7314843e5aa9c9bf974c4672e4edb24ed8ef4a6f", "pr_number": "28561", "files_changed": ["test/test_jit.py", "torch/jit/_recursive.py", "torch/nn/modules/activation.py", "torch/nn/modules/transformer.py"], "labels": ["merged"]}, "672f4cfad9": {"title": "Added C++ API test (#30980)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30980\n\nThis stack is a first step toward an effort to fix, clean up and simplify code generation logic. \ufffdPlease see the master [task](https://github.com/pytorch/pytorch/issues/30405) to see related discussions and all the known issues.\n\nMain focus of these changes is TensorOptions in code generation.\nGoals:\n- Remove TensorOptions from generated code wherever it's possible. Leave it only in python/C++ API layers.\n- Refactor TensorOptions logic to a single place.\n- Log all discovered issues.\n\nNon goals:\n- Fix Everything!\n- Remove all the hacks in code generation scripts.\n- Clean up and defector all code generation scripts.\n\n--------------\nIn this PR:\nAdd a test to check that C++ API behavior stays the same after all the changes.\nWhile working on it a bug related to `requires_grad` was found and logged in the master task.\n\n--------------\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18912681\n\nPulled By: izdeby\n\nfbshipit-source-id: 19772a37c92dde820839b79055f348689b99fa77", "pr_number": "30980", "files_changed": ["aten/src/ATen/test/basic.cpp"], "labels": ["merged"]}, "44ecc3a70b": {"title": "Add tracing support for optional Device and Layout (#30979)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30979\n\nThis stack is a first step toward an effort to fix, clean up and simplify code generation logic. \ufffdPlease see the master [task](https://github.com/pytorch/pytorch/issues/30405) to see related discussions and all the known issues.\n\nMain focus of these changes is TensorOptions in code generation.\nGoals:\n- Remove TensorOptions from generated code wherever it's possible. Leave it only in python/C++ API layers.\n- Refactor TensorOptions logic to a single place.\n- Log all discovered issues.\n\nNon goals:\n- Fix Everything!\n- Remove all the hacks in code generation scripts.\n- Clean up and defector all code generation scripts.\n\n--------------\nIn this PR:\nAdd tracing support for optional Device and Layout types.\n\n--------------\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18912685\n\nPulled By: izdeby\n\nfbshipit-source-id: 4a9514ce2eee0041f9bc96636d3ddb4f077675e1", "pr_number": "30979", "files_changed": ["torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h"], "labels": ["jit", "merged"]}, "a53b39f09d": {"title": "Disable flaky test_process_group_debug_info", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31113\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18932365\n\nPulled By: mrshenli\n\nfbshipit-source-id: a2996b6a8d3881be4ffc174b85509aeee8c51c96", "pr_number": "31113", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "4a751dfc20": {"title": "optimize MulGradient for common shapes (#19705)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/19705\n\nOptimizing for a case when there's a consecutive dims that are not broadcasted followed by another consecutive dims that are broadcasted.\nFor example, MulGradient([\"dC\", \"A\", \"B\"], [\"dA\", \"dB\"], broadcast=True, axis=0) where A.shape == dC.shape == [9508, 80] and B.shape == [80] .\n\nTest Plan:\nIn SKL T6,\n\nRunning mul_gradient_benchmark without this optimization\nOperator #0 (dA, MulGradient) 11.9119 ms/iter\n\nAfter this optimization,\nOperator #0 (dA, MulGradient) 0.672759 ms/iter\n\nNeed to land D15291800 before to fix the unit test error\n\nReviewed By: dmudiger\n\nDifferential Revision: D15075415\n\nfbshipit-source-id: 0f97be17cf8f1dacbafa34cd637fb8bc1c5e5387", "pr_number": "19705", "files_changed": ["caffe2/operators/elementwise_mul_gradient_op.cc", "caffe2/operators/elementwise_mul_op.h", "caffe2/python/operator_test/mul_gradient_benchmark.py"], "labels": ["caffe2", "merged", "module: internals", "module: pybind"]}, "dbc8b00816": {"title": "Document WorkerInfo and RpcBackendOptions structures in RPC docs. (#31077)", "body": "Summary:\nWe mention `WorkerInfo` and `RpcBackendOptions` in a couple of different locations in our docs, and these are public classes that the user may use, so we should add the class to the documentation.\n<img width=\"978\" alt=\"Screen Shot 2019-12-10 at 1 42 22 PM\" src=\"https://user-images.githubusercontent.com/8039770/70571759-47db2080-1b53-11ea-9d61-c83985a29dd9.png\">\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31077\n\nDifferential Revision: D18928162\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 67f11eedd87523c469377b791a0ba23704ec3723", "pr_number": "31077", "files_changed": ["docs/source/rpc.rst", "torch/csrc/distributed/rpc/init.cpp"], "labels": ["merged"]}, "5b03ff0a09": {"title": "Update embedding renorm comment to reference fixed issue (#29140)", "body": "Summary:\nAddress last comment in https://github.com/pytorch/pytorch/issues/28546\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29140\n\nDifferential Revision: D18915091\n\nPulled By: albanD\n\nfbshipit-source-id: 756ff5bb6a92d47c80aa9f96ff6f0edea5fd24de", "pr_number": "29140", "files_changed": ["aten/src/ATen/native/Embedding.cpp"], "labels": ["merged"]}, "4b2d356ac1": {"title": "Re-enable test_rref_context_debug_info after enforcing proper synchronization (#30994)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30994\n\nThe flakiness we saw was due to missing barriers(), which caused\nstates leaked into previous or subsequent checks. This commit\nattempts fix this problem by adding barriers before and after each\ncheck.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18893457\n\nPulled By: mrshenli\n\nfbshipit-source-id: 42bcc12efa7e6e43e2841ef23e4bc2543b0236c6", "pr_number": "30994", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "06d874f95b": {"title": "Change startTime_ to endTime_ in FutureInfo (#30342)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30342\n\nThis can eliminate the unnecessary calls to getRPCEndTime(). Reduce lines of code for simplicity.\n\nghstack-source-id: 95377162\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rpc_timeouts\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_rpc_timeouts\n```\n\nDifferential Revision: D5705624\n\nfbshipit-source-id: aca4c4917718124022c09ee0d13cf5ca483402af", "pr_number": "30342", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": ["merged", "module: rpc"]}, "6225443009": {"title": "Expose setNumThreads to android api (#31033)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31033\n\nIntention:\nThere are requests from users to control number of threads from android side:\nhttps://discuss.pytorch.org/t/android-pytorch-forward-method-running-in-a-separate-thread-slow-down-ui-thread/63516/2\nhttps://discuss.pytorch.org/t/threading-of-model-pytorch-android/62490/2\n\nAt the moment `setNumThreads` is placed in `org.pytorch.Module`, but this method changes global threadPool size, in future we will move it to some separate class to repeat python binding structure, which has torch.set_num_threads()\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18923167\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 8d98c2edbff42e9b673509672dce3f2dd03a923e", "pr_number": "31033", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/src/main/java/org/pytorch/INativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/LiteNativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/Module.java", "android/pytorch_android/src/main/java/org/pytorch/NativePeer.java", "caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/ThreadPool.h"], "labels": ["merged"]}, "293a139d79": {"title": "add a warning for script classes (#31069)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31069\n\nJust to clarify that they are still experimental.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18920496\n\nPulled By: suo\n\nfbshipit-source-id: d2f3014592a01a21f7fc60a4ce46dd0bfe5e19e9", "pr_number": "31069", "files_changed": ["docs/source/jit.rst"], "labels": ["merged"]}, "945ce71b18": {"title": "Correctly handle scalar types, fix parse of numpy ints (#30486)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30486\n\nFixes: https://github.com/pytorch/pytorch/issues/29252\n\nThere is some incorrect code in the handling of parsing python numbers that led to issue #29252:\n\nWhen we allow interpretation of a zero-dim numpy integer value\nas a scalar in pytorch, we incorrectly parse the int as a float.\n\nThis PR also fixes the issue described in the \"FIXME\" here:\nhttps://github.com/pytorch/pytorch/pull/27628/files#diff-f539198dd366265fb8dc2d661bc5d5bcR1487\n\nTest Plan: Added a unit test based on the example given in the issue.\n\nDifferential Revision: D18932520\n\nPulled By: nairbv\n\nfbshipit-source-id: f6416f28dfd73ac72c1042042851d76beb5fcf65", "pr_number": "30486", "files_changed": ["test/test_dataloader.py", "test/test_torch.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_numbers.h", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_numpy.cpp", "torch/csrc/utils/tensor_numpy.h"], "labels": ["merged"]}, "e5a550cd1d": {"title": "Fix Test CI by pinning hypothesis and correcting the import (#31137)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31137\n\nOur Test CI is broken because:\n- hypothesis recently did a new release that reorganized their internal\nmodules\n- we were importing something from their internal module structure.\n\nThis PR fixes the CI by doing the following:\n- import SearchStrategy from the correct (public) location\n- Pin the hypothesis version to avoid future surprises.\n\nIn the long term, we should stop install hypothesis every time the CI\nruns and instead install it as a part of our docker build process. See\nhttps://github.com/pytorch/pytorch/issues/31136 for details.\n\nTest Plan:\n- I tested this locally; before this PR test/test_nn.py fails to run but\nafter it does run.\n- Wait for CI\n\nDifferential Revision: D18940817\n\nPulled By: zou3519\n\nfbshipit-source-id: c1ef78faa5a33ddf4d923f947c03cf075a590bb8", "pr_number": "31137", "files_changed": [".jenkins/pytorch/test.sh", "test/hypothesis_utils.py"], "labels": ["merged"]}, "0414463007": {"title": "doc fix for max method: a warning about different behaviour on CPU and GPU (#31115)", "body": "Summary:\nFixes [30708](https://github.com/pytorch/pytorch/issues/30708),\nAdds warning regarding different behaviour of the method depending on device type.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31115\n\nDifferential Revision: D18937365\n\nPulled By: zou3519\n\nfbshipit-source-id: 7c731dd80f8b371de08d7fdfcc2196be15a593e1", "pr_number": "31115", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "679b20b1e4": {"title": "Unify list elements for all list types (#30777)", "body": "Summary:\nPreviously list elements were only unified for tensor lists.\nThis improves error messages and expands the unification logic\nto include all types.\n](https://our.intern.facebook.com/intern/diff/18837726/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30777\n\nPulled By: driazati\n\nDifferential Revision: D18837726\n\nfbshipit-source-id: c4d275562a8429700987569426d694faa8f6002e", "pr_number": "30777", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/test_jit.py", "test/test_jit_py3.py", "torch/csrc/jit/script/compiler.cpp"], "labels": ["jit", "merged"]}, "73f9e81660": {"title": "Make rref fetch calls async. (#31086)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31086\n\nThis change leverages the new future response framework so that server\nthreads don't block until setValue is called. Particulurly, we add a\ngetFuture() method to OwnerRRef so that we get a future that is satisfied\nonce setValue is called.\nghstack-source-id: 95402273\n\nTest Plan: buck test mode/dev-nosan caffe2/test/...\n\nDifferential Revision: D18925272\n\nfbshipit-source-id: 2caf51019e5b5fd7ec45539544780067deb28610", "pr_number": "31086", "files_changed": ["torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref.cpp", "torch/csrc/distributed/rpc/rref.h"], "labels": ["merged"]}, "efe683fb2a": {"title": "dynamicly quantized linear benchmarking", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30148\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18613006\n\nPulled By: z-a-f\n\nfbshipit-source-id: 3851189a2822fd09a5dd97c9d54774727822d2bf", "pr_number": "30148", "files_changed": ["benchmarks/operator_benchmark/pt/qlinear_test.py"], "labels": ["merged"]}, "79c27ba4ef": {"title": "Add ONNX Export Support to floor_divide (#31081)", "body": "Summary:\nAdding support for the new ATen op floor_divide which was introduced in https://github.com/pytorch/pytorch/pull/30493/files.\n\nThis operation is used in Torchvision/FasterRCNN-MaskRCNN, which are now failing after the new op was introduced.\nThis PR fixes the failure.\n\ncc: neginraoof\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31081\n\nReviewed By: houseroad\n\nDifferential Revision: D18945316\n\nPulled By: eellison\n\nfbshipit-source-id: 09919c237d618ce7db293c7770f48f7304949dcf", "pr_number": "31081", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged"]}, "97c1e90f46": {"title": "ONNX Interpolate Add Scales Params (#28324)", "body": "Summary:\nFix for : https://github.com/pytorch/pytorch/issues/27176\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28324\n\nReviewed By: hl475\n\nDifferential Revision: D18309133\n\nPulled By: houseroad\n\nfbshipit-source-id: 348bb41393442c6b107d88fc2cd3224e0afa3ccf", "pr_number": "28324", "files_changed": [".jenkins/caffe2/test.sh", "aten/src/ATen/native/UpSample.h", "aten/src/ATen/native/UpSampleBicubic2d.cpp", "aten/src/ATen/native/UpSampleBilinear2d.cpp", "aten/src/ATen/native/UpSampleLinear1d.cpp", "aten/src/ATen/native/UpSampleNearest1d.cpp", "aten/src/ATen/native/UpSampleNearest2d.cpp", "aten/src/ATen/native/UpSampleNearest3d.cpp", "aten/src/ATen/native/UpSampleTrilinear3d.cpp", "aten/src/ATen/native/cuda/UpSample.cuh", "aten/src/ATen/native/cuda/UpSampleBicubic2d.cu", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleNearest1d.cu", "aten/src/ATen/native/cuda/UpSampleNearest2d.cu", "aten/src/ATen/native/cuda/UpSampleNearest3d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_nearest2d.cpp", "test/onnx/expect/TestOperators.test_upsample_nearest.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_scale.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_scale_default_scale_factor.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_size.expect", "test/onnx/test_operators.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_jit.py", "test/test_nn.py", "tools/autograd/derivatives.yaml", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/symbolic_script.cpp", "torch/nn/functional.py", "torch/onnx/symbolic_caffe2.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset8.py", "torch/onnx/symbolic_opset9.py"], "labels": ["jit", "topic: bc-breaking"]}, "85107e72b4": {"title": "Fix type unification With Specialized Tensor Shapes (#31076)", "body": "Summary:\nFix for https://github.com/pytorch/pytorch/issues/30015\n\nWe had a model that failed in shape propagation because we could not unify `Tensor` and `Optional[BoolTensor]`. Tensor not subtyping Optional[BoolTensor] was correct, but we should have unified those two types to `Optional[Tensor]`.\n The fix here is that for immutable types containers (Optional, Tuple Type), we should be attempting to unify with complete shape information, and if that fails, then try to unify those types with unshaped types.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31076\n\nDifferential Revision: D18921802\n\nPulled By: eellison\n\nfbshipit-source-id: aa6890277470c60b349ed1da4d81cc5d71d377f6", "pr_number": "31076", "files_changed": ["aten/src/ATen/core/type.cpp", "test/cpp/jit/test_jit_type.cpp", "test/cpp/jit/tests.h"], "labels": ["jit", "merged"]}, "49a5841a9f": {"title": "Make Conv{1,2,3}dOptions and ConvTranspose{1,2,3}dOptions different classes (#31005)", "body": "Summary:\nCurrently, both `Conv{1,2,3}dOptions` and `ConvTranspose{1,2,3}dOptions` are aliases of the `ConvOptions<{1,2,3}>` class, which causes confusion because the `ConvOptions` class has parameters such as `transposed` that shouldn't be exposed to the end user. (This has caused issues such as https://github.com/pytorch/pytorch/issues/30931.) This PR makes the following improvements:\n1. Rename the original `torch::nn::ConvOptions<N>` class to `torch::nn::detail::ConvNdOptions<N>` class, to signify that it's an implementation detail and should not be used publicly.\n2. Create new classes `torch::nn::ConvOptions<N>` and `torch::nn::ConvTransposeOptions<N>`, which have parameters that exactly match the constructor of `torch.nn.Conv{1,2,3}d` and `torch.nn.ConvTranspose{1,2,3}d` in Python API.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31005\n\nDifferential Revision: D18898048\n\nPulled By: yf225\n\nfbshipit-source-id: 7663d646304c8cb004ca7f4aa4e70d3612c7bc75", "pr_number": "31005", "files_changed": ["torch/csrc/api/include/torch/nn/modules/conv.h", "torch/csrc/api/include/torch/nn/options/conv.h", "torch/csrc/api/src/nn/modules/conv.cpp"], "labels": ["merged", "module: cpp", "topic: bc-breaking"]}, "a3ed350eb2": {"title": "Change type of timeoutFutures_ key to time_point instead of duration (#31078)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31078\n\nMake `ProcessGroupAgent::pollTimedOutRPCs` code more conventional.\n\n- Use `std::chrono::time_point` to represent `endTime` instead of `std::chrono::duration`.\n- Replace `std::condition_variable::wait_for(lock, endTime)` with `std::condition_variable::wait_until(lock, endTime)`.\n- Remove the unnecessary `::getRPCRemainingTime()`.\nghstack-source-id: 95408482\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rpc_timeouts\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_rpc_timeouts\n```\n\nDifferential Revision: D5705442\n\nfbshipit-source-id: ba54b7bdb84bc02d05c22360b01290d044bbfcf5", "pr_number": "31078", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": ["merged"]}, "6ab2d1b1a4": {"title": "Partially support tensor lists in loop/concat/stack (#30126)", "body": "Summary:\nThis is a follow-up PR after https://github.com/pytorch/pytorch/pull/29136 ~~and https://github.com/pytorch/pytorch/pull/29171~~\n\nONNX::Loop does not support Sequence type as loop-carried dependencies. Only tensors are supported.\nThis PR adds a pass that converts Sequence loop-carried dependencies to scan_outputs.\nIn opset 11, only the below pattern is supported.\n```\nPTIR graph:\n ...\n %res.1 : Tensor[] = prim::ListConstruct()\n %res : Tensor[] = prim::Loop(%11, %22, %res.1)\n   block0(%i.1 : Tensor, %res.6 : Tensor[]):\n     ...\n     %res.3 : Tensor[] = aten::append(%res.6, %17)\n     -> (%22, %res.3)\n return (%res.3)\n\nONNX graph:\n ...\n %res : Tensor = onnx::Loop(%11, %22)\n   block0(%i.1 : Tensor):\n     ...\n     -> (%22, %17)\n %res_seq : Tensor[] = onnx::SplitToSequence[keepdims=0](%res)\n return (%res_seq)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30126\n\nReviewed By: hl475\n\nDifferential Revision: D18946880\n\nPulled By: houseroad\n\nfbshipit-source-id: 67ee65700513e8a942344a3d647e2e73c19ee3d2", "pr_number": "30126", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/dead_code_elimination.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.cpp", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_registry.py"], "labels": ["jit", "merged"]}, "4f5a4be45f": {"title": "Add native/quantized to the list of header rewrites (#31151)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31151\n\nsame as title. I am not sure why this was not added in the first place.\n\nTest Plan: wait for build to succeed.\n\nReviewed By: bddppq, xw285cornell\n\nDifferential Revision: D18880216\n\nfbshipit-source-id: 8b17d4fbd5dd08c28c52df8b1da77b69d56d65dc", "pr_number": "31151", "files_changed": ["torch/utils/hipify/hipify_python.py"], "labels": ["fb-exported", "merged"]}, "0db6c01301": {"title": "Re-enable python 2 builds (#31164)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31164\n\nWe have a small number of internal projects that still are on Python 2.\nUntil we can figure out how to get rid of them, we need to continue\nsupporting Python 2 for PyTorch.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18949698\n\nPulled By: suo\n\nfbshipit-source-id: 4a9d7e4306ed81576e05f243de472937a2bb1176", "pr_number": "31164", "files_changed": [".circleci/cimodel/data/binary_build_data.py", ".circleci/cimodel/data/caffe2_build_data.py", ".circleci/cimodel/data/dimensions.py", ".circleci/cimodel/data/pytorch_build_data.py", ".circleci/config.yml", ".circleci/verbatim-sources/workflows-binary-builds-smoke-subset.yml", ".github/workflows/lint.yml"], "labels": ["merged"]}, "2488231fe3": {"title": "Tweak pollTimedOutRPCs thread synchronization (#30355)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30355\n\n- Make processTimedOutFutures hold lock.\n- Reduce unnecessary scan on future and future timeout maps.\n- Reduce the scope of lock at a spot.\n- Avoid repeatedly wake up if user set timeout = 0.\n\nghstack-source-id: 95409528\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rpc_timeouts\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_rpc_timeouts\n\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_rpc_timeouts\n```\n\nDifferential Revision: D5516149\n\nfbshipit-source-id: 4bb0bd59fa31d9bfaef9f07ac0126782da17f762", "pr_number": "30355", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp"], "labels": ["merged", "module: rpc"]}, "159835e666": {"title": "Add types for the remaining optimizers. (#31130)", "body": "Summary:\n**Patch Description**\nRound out the rest of the optimizer types in torch.optim by creating the stubs for the rest of them.\n\n**Testing**:\nI ran mypy looking for just errors in that optim folder. There's no *new* mypy errors created.\n```\n$ mypy torch/optim | grep optim\n$ git checkout master; mypy torch/optim | wc -l\n968\n$ git checkout typeoptims; mypy torch/optim | wc -l\n968\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31130\n\nReviewed By: stephenroller\n\nDifferential Revision: D18947145\n\nPulled By: vincentqb\n\nfbshipit-source-id: 5b8582223833b1d9123d829acc1ed8243df87561", "pr_number": "31130", "files_changed": ["torch/optim/__init__.pyi", "torch/optim/adadelta.pyi", "torch/optim/adagrad.pyi", "torch/optim/adamax.pyi", "torch/optim/adamw.pyi", "torch/optim/asgd.pyi", "torch/optim/lbfgs.pyi", "torch/optim/rmsprop.pyi", "torch/optim/rprop.pyi", "torch/optim/sparse_adam.pyi"], "labels": ["merged", "module: optimizer", "module: typing"]}, "3a02ed822b": {"title": "Remove `insert_prepack_unpack` and `fold_prepack` for now (#30909)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30909\n\n`fold_prepack` doesn't work anymore after we change `scale`, `zero_point`\nto be attributes, but since the freeze API is coming up, I don't want to\nspend time to make this work since this will be thrown away later.\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18864537\n\nfbshipit-source-id: 649e6b91f2b04b8babacc0afb6bc1530ed7259d3", "pr_number": "30909", "files_changed": ["torch/quantization/_quantize_script.py"], "labels": ["merged", "quantization"]}, "56de8853da": {"title": "Resubmit overload v2 (#31123)", "body": "Summary:\nResubmit of https://github.com/pytorch/pytorch/pull/30356 and https://github.com/pytorch/pytorch/pull/31014 :'(\n\nThe last commit contains the fix. There was an internal FBcode error not able to compile the previous `impl_default->second.equal(default_val.second))` line. I tried various fixes in C++ internally but couldn't figure anything out. This is a good example of the programming costs of going from python -> c++ for different types of objects, because the conceptual overhead has expanded in scope from (python) -> (python, c++, pybind).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31123\n\nDifferential Revision: D18936128\n\nPulled By: eellison\n\nfbshipit-source-id: 7d8fd66a6dd4a3e9838f3a0b68c219b6565a9462", "pr_number": "31123", "files_changed": ["aten/src/ATen/core/function_schema.h", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "9047d4df45": {"title": "Remove all remaining usages of BUILD_NAMEDTENSOR (#31116)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31116\n\nChangelist:\n- remove BUILD_NAMEDTENSOR macro\n- remove torch._C._BUILD_NAMEDTENSOR\n- remove all python behavior that relies on torch._C._BUILD_NAMEDTENSOR\n\nFuture:\n- In the next diff, I will remove all usages of\nATen/core/EnableNamedTensor.h since that header doesn't do anything\nanymore\n- After that, we'll be done with the BUILD_NAMEDTENSOR removal.\n\nTest Plan: - run CI\n\nDifferential Revision: D18934951\n\nPulled By: zou3519\n\nfbshipit-source-id: 0a0df0f1f0470d0a01c495579333a2835aac9f5d", "pr_number": "31116", "files_changed": ["CMakeLists.txt", "aten/src/ATen/core/EnableNamedTensor.h", "aten/src/ATen/env.py", "aten/src/ATen/native/README.md", "caffe2/core/macros.h.in", "cmake/Summary.cmake", "test/test_namedtensor.py", "tools/pyi/gen_pyi.py", "torch/_namedtensor_internals.py", "torch/_tensor_str.py", "torch/csrc/Module.cpp", "torch/functional.py"], "labels": ["merged"]}, "bcb0bb7e0e": {"title": "Remove unnecessary ATen/core/EnableNamedTensor.h (#31117)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31117\n\nAfter this diff, we will have completely removed the named tensor\nfeature flagging. This means that named tensors are always on and that\nthere is no mechanism to turn them off. There should be no more follow-up\ndiffs.\n\nI performed the deletion of the header with\n```\nfind . -type f -print0 | xargs -0 sed -i '/#include\n<ATen\\/core\\/EnableNamedTensor.h>/d'\n```\n\nTest Plan: - wait for CI\n\nDifferential Revision: D18934952\n\nPulled By: zou3519\n\nfbshipit-source-id: 253d059074b910fef15bdf885ebf71e0edf5bea5", "pr_number": "31117", "files_changed": ["aten/src/ATen/NamedTensorUtils.cpp", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/TensorNames.cpp", "aten/src/ATen/TensorNames.h", "aten/src/ATen/core/Dimname.cpp", "aten/src/ATen/core/Dimname.h", "aten/src/ATen/core/EnableNamedTensor.h", "aten/src/ATen/core/NamedTensor.cpp", "aten/src/ATen/core/NamedTensor.h", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/LegacyDefinitions.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/NamedTensor.cpp", "aten/src/ATen/native/PointwiseOps.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/SoftMax.cpp", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorIterator.cpp", "aten/src/ATen/native/TensorIterator.h", "aten/src/ATen/native/TensorProperties.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cuda/CUDAUnaryOps.cpp", "aten/src/ATen/native/cuda/LegacyDefinitions.cpp", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/LegacyTHFunctions.cpp", "aten/src/ATen/templates/NativeFunctions.h", "aten/src/ATen/templates/OpsAlreadyMovedToC10.cpp", "aten/src/ATen/templates/SparseTypeDerived.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/templates/TensorMethods.h", "aten/src/ATen/templates/TypeDefault.cpp", "aten/src/ATen/templates/TypeDefault.h", "aten/src/ATen/templates/TypeDerived.cpp", "aten/src/ATen/templates/TypeDerived.h", "aten/src/ATen/test/Dimname_test.cpp", "aten/src/ATen/test/NamedTensor_test.cpp", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/THC/generic/THCTensorMasked.cu", "aten/src/THC/generic/THCTensorMathBlas.cu", "aten/src/THC/generic/THCTensorMathPointwise.cu", "tools/autograd/templates/VariableType.h", "tools/autograd/templates/python_variable_methods.cpp", "tools/autograd/templates/variable_factories.h", "torch/csrc/Module.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h", "torch/csrc/python_dimname.cpp", "torch/csrc/python_dimname.h", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/csrc/utils/tensor_new.cpp"], "labels": ["jit", "merged"]}, "4ead2e8996": {"title": "Fix CircleCI behavior for non-leaf stack PRs (#31088)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31088\n\nOriginal issue:\nhttps://github.com/pytorch/pytorch/issues/31027\n\nThe problem is that for the stacks of PRs for non-leaf PRs circleCI does not set environment variable `CIRCLE_PULL_REQUEST` which is used to filter out some jobs that should run only on `master`.\n\n(Android job for master includes alll 4 abis (x86, x86_64, armeabi-v7a, arm64-v8a)  and gradle build tries to get results from all 4 abis, for PRs we run only x86 build for resources economy. Thats why not filtered master android job fails as abis apart x86 were not scheduled)\n\nenv variable `CIRCLE_BRANCH ` is set fine and can be used as a workaround to distinguish that this is PR (published with ghstack).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18966385\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 644c5ef07fcf2d718b72695da2cc303da8b94ef4", "pr_number": "31088", "files_changed": [".circleci/scripts/should_run_job.sh"], "labels": ["merged"]}, "66f2bba852": {"title": "Adding function to convert Module to channels last", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28991\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18430810\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 0693d4e31fc6f9831722c29fc83517f16ddfc028", "pr_number": "28991", "files_changed": ["test/test_nn.py", "tools/autograd/templates/python_nn_functions.cpp", "tools/autograd/templates/python_variable_methods.cpp", "torch/nn/modules/module.py"], "labels": ["merged"]}, "066e3ed953": {"title": "Re-apply \"[bert/RoBERTa] Optimize LayerNorm with explicit vectorization using Vec256\" (#31127)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31127\n\nOriginal commit changeset: d22448b90843\n\nOn Skylake T6:\n\nSingle Core:\n(Note that our benchmark generates batch_size=47 for first case and batch_size=56 for the second case. In spite of that, the vectorized version is still faster than the original reference C version without vectorization.)\n- Before the PR:\n```\nnative_layer_norm        0.81%            5.884ms          0.81%            5.884ms          122.580us        NaN              0.000us          0.000us          48               [[47, 1, 1024], [1024], [1024]]\n```\n\n- After the PR:\n```\nnative_layer_norm        0.68%            5.053ms          0.68%            5.053ms          105.272us        NaN              0.000us          0.000us          48               [[56, 1, 1024], [1024], [1024]]\n```\n\n20 Cores:\n- Before the PR:\n```\nnative_layer_norm        1.65%            41.682ms         1.65%            41.682ms         868.365us        NaN              0.000us          0.000us          48               [[61, 64, 1024], [1024], [1024]]\n```\n\n- After the PR:\n```\nnative_layer_norm        1.34%            33.829ms         1.34%            33.829ms         704.771us        NaN              0.000us          0.000us          48               [[61, 64, 1024], [1024], [1024]]\n```\nghstack-source-id: 95420889\n\nTest Plan:\nbuck test mode/dev-nosan //caffe2/test:nn -- \"LayerNorm\"\n\nbuck test mode/dev-nosan //caffe2/test:nn -- \"test_LayerNorm_1d_no_elementwise_affine_eval\"\n\n python run_test.py -i nn -- TestNN.test_LayerNorm_1d_no_elementwise_affine_eval\n\nDifferential Revision: D18936428\n\nfbshipit-source-id: 8cae33d35fb338b5ac49b1597c2709152612d6e5", "pr_number": "31127", "files_changed": ["aten/src/ATen/native/cpu/layer_norm_kernel.cpp"], "labels": ["merged"]}, "bee6344d4e": {"title": "remove / rewrite weak module tests (#31193)", "body": "Summary:\nRemove most of the testing for `weak_script`, since we removed it. Refactor a few of the existing tests to use recursive scripting api.\n\nFix for https://github.com/pytorch/pytorch/issues/23965\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31193\n\nDifferential Revision: D18966291\n\nPulled By: eellison\n\nfbshipit-source-id: 6b1e18c293f55017868a14610d87b69be42bde12", "pr_number": "31193", "files_changed": ["test/test_jit.py"], "labels": ["merged"]}, "f6c31f61c5": {"title": "Enabled roll for bool tensor (#31194)", "body": "Summary:\nFixed this [issue](https://github.com/pytorch/pytorch/issues/31079).\nTested via unit test\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31194\n\nDifferential Revision: D18958141\n\nPulled By: izdeby\n\nfbshipit-source-id: 119bf4d31df10ee02c277f5a4663038470cf7780", "pr_number": "31194", "files_changed": ["aten/src/ATen/native/cuda/TensorTransformations.cu", "test/test_torch.py"], "labels": ["merged"]}, "a38184dbab": {"title": "Only create OwnerRRefs when processing remote calls (#31163)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31163\n\nThe purpose is to unblock integration with TorchScript. Currently,\nan OwnerRRef will be created by either a remote call or a to_here\ncall, whichever arrives first. However, when making RRef an IValue,\nwe need to know the type of value held by the RRef, which is\nretrived by checking the return type of the TorchScript function.\nThe TorchScript function is only avaible during the remote call\nbut not in the to_here() call. Hence, an OwnerRRef can only be\ncreated when processing a remote call. This commit implements this\nbehavior by introducing a conditional variable for every OwnerRRef\nin the RRefContext, and let the to_here() call and PyRRef::unpickle\nblock on the CV until the value is ready.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18949591\n\nPulled By: mrshenli\n\nfbshipit-source-id: 17513c6f1fd766885ea8e1cd38f672a403fa4222", "pr_number": "31163", "files_changed": ["torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h"], "labels": ["merged"]}, "5c936845cf": {"title": "fix torch_train build (#30497)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30497\n\nfix torch_train build\n\nTest Plan: buck build //xplat/caffe2:torch_trainAndroid\n\nReviewed By: dreiss\n\nDifferential Revision: D18719662\n\nfbshipit-source-id: a3d06b4068d502dbe29681d9f26906f2b8c7b622", "pr_number": "30497", "files_changed": ["torch/csrc/jit/export_module.cpp"], "labels": ["fb-exported", "jit", "merged"]}, "84d6796658": {"title": "move AWS ECR gc jobs to circleci (#30996)", "body": "Summary:\nall jobs are currently running with \"--dry-run\", so you can verify if the jobs are doing the right thing.  i'll remove the flag and make it runs every hour same as on Jenkins once this PR is approved.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30996\n\nDifferential Revision: D18971001\n\nPulled By: mingbowan\n\nfbshipit-source-id: 2384bdb50ebdf47aad265395f26be3843f0ce05e", "pr_number": "30996", "files_changed": [".circleci/config.yml", ".circleci/ecr_gc_docker/Dockerfile", ".circleci/ecr_gc_docker/gc.py", ".circleci/ecr_gc_docker/requirements.txt", ".circleci/generate_config_yml.py", ".circleci/validate-docker-version.py", ".circleci/verbatim-sources/docker_build_job.yml", ".circleci/verbatim-sources/docker_jobs.yml", ".circleci/verbatim-sources/workflows-docker-builder.yml", ".circleci/verbatim-sources/workflows-ecr-gc.yml", ".github/workflows/lint.yml"], "labels": ["merged"]}, "1d5af9599d": {"title": "Update ONNX Flatten to accept negative indices in opset 11 (#30751)", "body": "Summary:\nUpdate ONNX Flatten to accept negative indices in opset 11.\nWith this change, some cases of flatten do not rely on the input rank being available.\nFixes : https://github.com/pytorch/pytorch/issues/30512 .\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30751\n\nReviewed By: hl475\n\nDifferential Revision: D18946904\n\nPulled By: houseroad\n\nfbshipit-source-id: a6fa30a9182fff92211e505a19325525c6112f19", "pr_number": "30751", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py"], "labels": ["merged"]}, "a2463cbc38": {"title": "Adding quantized clamp kernel (#30541)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30541\n\nghstack-source-id: 95450749\n\nAdding quantized clamp kernel\n\nTest Plan:\nAdded test.\n\nbuck test mode/dev //caffe2/test:quantized -- 'test_qclamp \\(test_quantized\\.TestQuantizedOps\\)' --print-passing-details\n\nDifferential Revision: D18739628\n\nfbshipit-source-id: 38a029ab96c5b0689bb15c67dc4f274883e74975", "pr_number": "30541", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_qint.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/test_quantized.py", "torch/nn/quantized/functional.py"], "labels": ["merged"]}, "5ef0d6f854": {"title": "Remove subgraphNode kind assert in unmergeSubgraph (#31212)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31212\n\nTo be able to use this function more broadly.\n\nTest Plan: unit tests\n\nReviewed By: jackm321\n\nDifferential Revision: D18978913\n\nfbshipit-source-id: d998dc7c7f9540f491a8a4bc5d6d25d9c3bf8764", "pr_number": "31212", "files_changed": ["torch/csrc/jit/passes/utils/subgraph_utils.cpp"], "labels": ["fb-exported", "jit", "merged"]}, "20a2e526ef": {"title": "build a generic future<T> (#29579)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29579\n\nPer #28923, this diff is to move Future<Message> to torch::utils and extend it to be Future<T>, most of implementations are copied from FutureMessage and ivalue::Future. merge ivalue::Future with Future<T> will be done separately.\n\nThe main difference between Future<T>  and FutureMessage is the error handling, instead of checking message type inside Future to handle error, this future<T> owns has_error_ and error_ states.\n\nalso this future passes value_, has_error_ and error_ states to callbacks for easily read future states.\n\nIn next diff, a torch script rpc async API will be created, before the API returns, it will create an ivalue::Future and passes it to Future<T>'s call back where state of ivalue::Future will be set.  In this way, the torch script rpc async API  can still return a ivalue::Future and call wait() to get its state appropriately afterwards.\nghstack-source-id: 95479525\n\nTest Plan: unit tests\n\nDifferential Revision: D18263023\n\nfbshipit-source-id: 48a65712656a72c2feb0bb3ec8b308c0528986a6", "pr_number": "29579", "files_changed": ["caffe2/CMakeLists.txt", "tools/build_variables.py", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/context/context.h", "torch/csrc/distributed/rpc/future_message.cpp", "torch/csrc/distributed/rpc/future_message.h", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/message.h", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_functions.h", "torch/csrc/distributed/rpc/request_callback.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/rref.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/utils/future.h", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "f30b14dead": {"title": "Fix handling of type comments in body (#30590)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30477. Any type comment after `# type: (...) -> ` is ignored.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30590\n\nDifferential Revision: D18887351\n\nPulled By: driazati\n\nfbshipit-source-id: 162c652f6d7610d14609bbcb25aaa27cdd947a76", "pr_number": "30590", "files_changed": ["test/test_jit.py", "torch/jit/annotations.py"], "labels": ["jit", "merged"]}, "b7c148013f": {"title": "fix torch square_ benchmark runtime error (#31221)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31221\n\nThis is fixing the runtime error introduced in https://github.com/pytorch/pytorch/pull/30719 that added torch square_ operator to the benchmark suite.\n\nTest Plan:\n```\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: square_\n# Mode: Eager\n# Name: square__M512_N512_cpu\n# Input: M: 512, N: 512, device: cpu\nForward Execution Time (us) : 66.291\n\nReviewed By: hl475\n\nDifferential Revision: D18987889\n\nfbshipit-source-id: 09c56e3a73aab5ab661aac2b06429063b3a82fac", "pr_number": "31221", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": ["fb-exported", "merged"]}, "ca8cb3241a": {"title": "Expose setNumThreads to android api (#31205)", "body": "Summary:\nPR https://github.com/pytorch/pytorch/pull/31033 was unlanded due to macos build failure:\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/3916388\n\nThis PR has changes that `setNumThreads` is only for android and moved to separate class `org.pytorch.PytorchAndroid` as a static function which is better as it has global effect\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31205\n\nReviewed By: dreiss\n\nDifferential Revision: D18977250\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 4995859808af498c82933c4db52bd7c7dfae90e5", "pr_number": "31205", "files_changed": ["android/pytorch_android/host/build.gradle", "android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_common.h", "android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/src/main/java/org/pytorch/PytorchAndroid.java", "caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/ThreadPool.h"], "labels": ["merged"]}, "199e1fb348": {"title": "Use AVX2 to increase frequency for FP16<->FP32 Caffe2 ops (#31203)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31203\n\nFor multi-instance environment, AVX2 should help increase the clock frequency.\nghstack-source-id: 95502576\n\nTest Plan: buck test //caffe2/caffe2:caffe2_test_cpu -- \"Float16\"\n\nReviewed By: jspark1105\n\nDifferential Revision: D18962649\n\nfbshipit-source-id: 6532d929a99f41f2f6ad1a1a1962e38ae3ddaecb", "pr_number": "31203", "files_changed": ["caffe2/operators/half_float_ops.cc"], "labels": ["merged"]}, "db90a5b992": {"title": "Switch to open sourced fbjni (#30175)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30175\n\nfbjni was opensourced and java part is published as 'com.facebook.fbjni:fbjni-java-only:0.0.3'\nswitching to it.\nWe still need submodule fbjni inside the repo (which is already pointing to  https://github.com/facebookincubator/fbjni) for so linking.\n\n**Packaging changes**:\nbefore that `libfbjni.so` came from pytorch_android_fbjni dependency, as we also linked fbjni in `pytorch_android/CMakeLists.txt` - it was built in pytorch_android, but excluded for publishing. As we had 2 libfbjni.so there was a hack to exclude it for publishing and resolve duplication locally.\n```\n        if (rootProject.isPublishing()) {\n            exclude '**/libfbjni.so'\n        } else {\n            pickFirst '**/libfbjni.so'\n        }\n```\n\nAfter this change fbjni.so will be packaged inside pytorch_android.aar artefact and we do not need this gradle logic.\n\nI will update README in separate PR after landing previous PR to readme(https://github.com/pytorch/pytorch/pull/30128) to avoid conflicts\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18982235\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 5097df2557858e623fa480625819a24a7e8ad840", "pr_number": "30175", "files_changed": ["android/build.gradle", "android/pytorch_android/build.gradle", "android/test_app/app/build.gradle"], "labels": ["merged"]}, "f7c92f60ba": {"title": "Typo in filename align with classname", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31235\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19001793\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: ae7f410be6b3c291f1feb3027b5b4a6b7ce15ab3", "pr_number": "31235", "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java", "android/pytorch_android/src/main/java/org/pytorch/PytorchAndroid.java"], "labels": ["merged"]}, "1ef99cf0ab": {"title": "Intrusive_ptr implementation slower than shared_ptr (#30810)", "body": "Summary:\nIt was a random coding exercise so I wasn't putting much effort into it; but, I was like \"hey is the current intrusive_ptr implementation optimized enough?\" so I compared it with shared_ptr (using std::shared_from_this).\n\nMy benchmark result shows that intrusive_ptr is actually slower. On my macbook the speed is:\n\n```\n---------------------------------------------------------------\nBenchmark                        Time           CPU Iterations\n---------------------------------------------------------------\nBM_IntrusivePtrCtorDtor         14 ns         14 ns   52541902\nBM_SharedPtrCtorDtor            10 ns         10 ns   71898849\nBM_IntrusivePtrArray         14285 ns      14112 ns      49775\nBM_SharedPtrArray            13821 ns      13384 ns      51602\n```\n\nWanted to share the results so someone could probably take a look if interested.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30810\n\nReviewed By: yinghai\n\nDifferential Revision: D18828785\n\nPulled By: bddppq\n\nfbshipit-source-id: 202e9849c9d8a3da17edbe568572a74bb70cb6c5", "pr_number": "30810", "files_changed": [".jenkins/caffe2/test.sh", "c10/CMakeLists.txt", "c10/benchmark/CMakeLists.txt", "c10/benchmark/intrusive_ptr_benchmark.cpp"], "labels": ["merged"]}, "36d17f4105": {"title": "abort nccl communicators before throwing operation timed out (#31128)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31128\n\nWhen operation times out due to some errors that are not detected by nccl communicators, ncclCommWatchdog can not check this time out error and thus can not abort ncclComms accordingly. So explicitly abort ncclComms here before throwing this timed out exception to users, after this, ncclCommWatchdog can detect nccl communicators are aborted and clean up devNCCLCommMap_ accordingly. if throwing timed out excepiton without aborting nccl communicators here, it was observed that CUDA GPU will have 100% utilization and can not run new events successfully.\nghstack-source-id: 95528488\n\nTest Plan: newly revised test _test_nccl_errors_blocking passed with the changes in this diff; the reviesed test failed withtout the changes in this diff\n\nReviewed By: isunjin\n\nDifferential Revision: D18928607\n\nfbshipit-source-id: be65a05ce4ff005f0c7fed36ae8e28903e8ffe2b", "pr_number": "31128", "files_changed": ["test/test_c10d.py", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp"], "labels": ["merged"]}, "b64baa963f": {"title": "Robustify rpc_agent handlers with generic Future<T> (#31224)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31224\n\nIf a future coming back to a rpc_agent server is satisfied with an\nexception, ensure this information is propagated back over the wire.\nghstack-source-id: 95522418\n\nTest Plan: buck test mode/dev-nosan caffe2/torch/fb/distributed/thriftRpcBackend/...\n\nDifferential Revision: D18979185\n\nfbshipit-source-id: 99848ae805cc2d48948809a238f61a2e0ef234c9", "pr_number": "31224", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/utils/future.h"], "labels": ["merged"]}, "8fea7a49d6": {"title": "pinning hypothesis for windows", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31169\n\nDifferential Revision: D19036734\n\nPulled By: mingbowan\n\nfbshipit-source-id: 2205a40720329cb53e741c9827c9049142759588", "pr_number": "31169", "files_changed": [".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat"], "labels": ["merged"]}, "a9ad98fb25": {"title": "Remove unused argument \"destId\" in addSendRpcBackward (#31207)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31207\n\nCleanup after #30914.\n\nIn #30914, `autogradContext->addKnownWorkerId(dst);` was moved out of `addSendRpcBackward()`.\n\nSo `addSendRpcBackward()` does not need `dstId` as it's argument anymore.\nghstack-source-id: 95509218\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:dist_autograd_fork -- test_context_cleanup_tensor_no_grad\n```\n\nDifferential Revision: D5742365\n\nfbshipit-source-id: accd041a594ec18d369231f5590289828d87baa7", "pr_number": "31207", "files_changed": ["test/cpp/dist_autograd/test_dist_autograd.cpp", "torch/csrc/distributed/autograd/utils.cpp", "torch/csrc/distributed/autograd/utils.h"], "labels": ["merged"]}, "3587f769dc": {"title": "use propagate_names instead of propagate_names_for_reduction for cumsum and cumprod", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31134\n\nDifferential Revision: D18964172\n\nPulled By: anjali411\n\nfbshipit-source-id: 3050c6d283a469a858378c44ac2ab9102baefce5", "pr_number": "31134", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp"], "labels": ["merged"]}, "9954739956": {"title": "Refactor test for unique and unique_consecutive and fix some bugs (#31211)", "body": "Summary:\nTests for unique_dim will be refactored in a separate PR.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31211\n\nDifferential Revision: D19034968\n\nPulled By: ngimel\n\nfbshipit-source-id: 855d326b37638b5944f11fbbce03394cf000daf9", "pr_number": "31211", "files_changed": ["aten/src/ATen/native/Unique.cpp", "aten/src/ATen/native/cuda/Unique.cu", "test/test_torch.py"], "labels": ["merged"]}, "cd3f05b44d": {"title": "Small fixes for hipification (#31200)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31200\n\nWe do not hipify these files when doing out of place.\n\nTest Plan: wait for CI to clear.\n\nDifferential Revision: D18963683\n\nfbshipit-source-id: eeba8597143f26417d0a8181a4c746139afefa24", "pr_number": "31200", "files_changed": ["aten/src/ATen/native/Distributions.h", "aten/src/ATen/native/SharedReduceOps.h"], "labels": ["fb-exported", "merged"]}, "d7d07e7caf": {"title": "thrust is included in SortingKthValue.cu but never used", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31263\n\nDifferential Revision: D19042793\n\nPulled By: ngimel\n\nfbshipit-source-id: 28f06c46a53e15f106ebee6c36e2ad25a3676bd2", "pr_number": "31263", "files_changed": ["aten/src/ATen/native/cuda/SortingKthValue.cu"], "labels": ["merged"]}, "1ec989404c": {"title": "Kill some unnecessary function declarations.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31216\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18986640\n\nPulled By: gchanan\n\nfbshipit-source-id: 30630d9ea025bb510f85e9627cbb4ba46de5e93d", "pr_number": "31216", "files_changed": ["aten/src/TH/generic/THTensor.hpp"], "labels": ["merged"]}, "927588df8e": {"title": "Switch default memory format of _like operators to Preserve", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30087\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18624986\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 8e434966f872ffaddf1249248ea445cbbab300ce", "pr_number": "30087", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "test/test_torch.py"], "labels": ["merged"]}, "fde3d707ad": {"title": "Switch default memory format of to (and similar) operators to Preserve", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30088\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18624984\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 54901786d7496c7dce785140b0585ac9093b1d86", "pr_number": "30088", "files_changed": ["aten/src/ATen/native/TensorConversions.cpp", "test/test_cuda.py", "test/test_torch.py"], "labels": ["merged"]}, "c35cddb306": {"title": "Switch default memory format of clone operator to Preserve", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30089\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18624985\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 8d315b08b7b5858fd0a81d3375b44ccb94787ad4", "pr_number": "30089", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "test/test_torch.py"], "labels": ["merged", "topic: bc-breaking"]}, "6e1e09fd10": {"title": "Compile time type names (#26618)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26618\n\nImplement a mechanism to get type names at compile time\nIn a future diff, I'm planning to introduce this to caffe2::TypeMeta and a few other places.\nghstack-source-id: 95337871\n\nTest Plan: unit tests\n\nDifferential Revision: D17519253\n\nfbshipit-source-id: e14017f962fd181d147accb3f53fa8d6ee42a3f8", "pr_number": "26618", "files_changed": ["c10/test/util/ConstexprCrc_test.cpp", "c10/test/util/TypeIndex_test.cpp", "c10/util/ConstexprCrc.h", "c10/util/TypeIndex.h", "c10/util/string_view.h"], "labels": ["merged", "module: internals"]}, "2950530031": {"title": "caffe2::TypeMeta uses compile time type names (#26619)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26619\n\nghstack-source-id: 95348564\n\nTest Plan: unit tests\n\nDifferential Revision: D17519252\n\nfbshipit-source-id: 337ec76d17172dd1af60a1676d69964a41dcb7a1", "pr_number": "26619", "files_changed": ["aten/src/ATen/core/blob.h", "c10/test/util/typeid_test.cpp", "c10/util/TypeIndex.h", "c10/util/typeid.cpp", "c10/util/typeid.h"], "labels": ["merged", "module: internals"]}, "7c1b5084a7": {"title": "Enable equality operator for bfloat16 CPU scalar types. (#30817)", "body": "Summary:\nSee https://github.com/pytorch/xla/issues/1330 for reference.\n\nmruberry ailzhang FYI\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30817\n\nDifferential Revision: D18847375\n\nPulled By: mruberry\n\nfbshipit-source-id: d1efedf8b975b8d9b55cf0ddf141818eaa7c91f0", "pr_number": "30817", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp"], "labels": ["merged"]}, "7cb83bea3b": {"title": "Fix static cuda builds on older cmake versions (#30935)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/pull/28378#issuecomment-562597033\n\nTo reproduce the failure I had to downgrade to `cmake 3.9` (Ubuntu 18 uses 3.10 apparently). These older `cmake` versions unfortunately don't seem to allow `target_link_libraries(INTERFACE)` to be used with imported libraries. Switching back to `set_property(TARGET)` fixes the issue.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30935\n\nDifferential Revision: D18956912\n\nPulled By: albanD\n\nfbshipit-source-id: a2b728ee3268599a428b7878c988e1edef5d9dda", "pr_number": "30935", "files_changed": ["cmake/public/cuda.cmake"], "labels": ["merged", "open source"]}, "70013415c7": {"title": "DDP should not set grad for globally unused params (#28883)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/28294 DDP should not set grad for globally unused parameters\n\nDDP currently computes the param to bucket mapping upfront, and allreduce grads for all params in every iteration. Even if params are unused, it will just set grad to zero. With such behavior, optimizer cannot tell if a param indeed has a zero grad or it is not used in the current iteration. This could trigger convergence problems for optimizers with weight decay and momentum such as SGD. However, DDP cannot simply set grad to None for local unused parameters, as local unused parameters might be used in other processes, and hence we still need to allreduce its grad. Instead DDP should figure out the globally unused parameters and skip touching their grad in the end of backward.\n\nImplementation summary:\n* Add locally used parameter map for each model replica.\n* Mark the locally unused parameters in the end of forward and then reduce to get the globally unused parameters.\n* In the end of backward skip touching grad for those globally unused parameters.\n* Add a unit test test_global_local_unused_params_grad\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28883\n\nDifferential Revision: D18491530\n\nPulled By: mrshenli\n\nfbshipit-source-id: 24e9b5f20df86c34ddbf9c7106250fd6ce186699", "pr_number": "28883", "files_changed": ["test/test_c10d.py", "torch/csrc/distributed/c10d/reducer.cpp", "torch/csrc/distributed/c10d/reducer.h"], "labels": []}, "065685180d": {"title": "Loading module from android asset (#30378)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30378\n\nLoading module directly from android assets. Iteration on https://github.com/pytorch/pytorch/pull/30109\nLoading Module:\n```\nmModule = AndroidUtils.loadModuleFromAsset(assetName, getAssets());\n```\n\n`org.pytorch.AndroidUtils` is excluded from pytorch_jni host build\n\nTesting:\ntest_app module load switched to this approach and works fine\n```\ngradle test_app:installMobNet2QuantDebug -PABI_FILTERS=x86 && adb shell am start -n org.pytorch.testapp.mobNet2Quant/org.pytorch.testapp.MainActivity\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18893269\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: a7c73776f40e9c67bef233da05db56cc6efbe76a", "pr_number": "30378", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_common.h", "android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/src/main/java/org/pytorch/NativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java", "android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java"], "labels": ["merged"]}, "58eb15f41c": {"title": "JIT Type parser for mobile (#30391)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30391\n\nA Type parser to parse the python string of a Type. For example,\n\"Tuple[str, Optional[float], Dict[str, List[Tensor]], int]\".\nPlease refer to test_type_parser.cpp for the usage.\n\nOne of the use cases is in lite interpreter, types needs to be serialized (directly calling the python_str() of the Type) and deserialized (calling parseType(str)).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18924268\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 830d411563abfbeec023f01e7f8f4a1796f9a59a", "pr_number": "30391", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_mobile_type_parser.cpp", "test/cpp/jit/tests.h", "tools/build_variables.py", "torch/csrc/jit/mobile/type_parser.cpp", "torch/csrc/jit/script/init.h", "torch/csrc/jit/script/lexer.h", "torch/csrc/jit/script/parser_constants.h", "torch/csrc/jit/script/script_type_parser.cpp", "torch/csrc/jit/script/string_to_type.cpp"], "labels": ["jit", "merged"]}, "57ee7dab87": {"title": "Wraps assert statements in cuda kernels (#31276)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31276\n\nChange assert --> CUDA_ASSERT_KERNEL to avoid hip undefined __assert_fail()\n\nThis is similar to https://github.com/pytorch/pytorch/pull/13902 in caffe2 land.\n\nTest Plan: wait for CI to clear\n\nReviewed By: bddppq\n\nDifferential Revision: D19047582\n\nfbshipit-source-id: 34703b03786c8eee9c78d2459eb54bde8dc21a57", "pr_number": "31276", "files_changed": ["aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/THC/THCTensorScatterGather.cu", "aten/src/THC/THCTensorTopK.cuh", "aten/src/THCUNN/BCECriterion.cu", "aten/src/THCUNN/ClassNLLCriterion.cu", "aten/src/THCUNN/SpatialClassNLLCriterion.cu", "c10/macros/Macros.h", "c10/util/TypeCast.h"], "labels": ["fb-exported", "merged", "module: rocm"]}, "ffe0c1ae4d": {"title": "Make test_torch.py pass cuda-memcheck (#29243)", "body": "Summary:\nMake the following changes:\n- When there are more than 10k errors, cuda-memcheck only shows 10k errors, in this case we shouldn't raise an Exception\n- Add UNDER_CUDA_MEMCHECK environment to allow disabling `pin_memory` tests when running cuda-memcheck.\n- Add a `--ci` command option, when turned on, then this script would run output to stdout instead of writing a file, and exit with an error if cuda-memcheck fails\n- Add a `--nohang` command option. When turned on, then hang would be treated as pass instead of error\n- Do simple filtering on the test to run: if `'cpu'` in the test name but not `'cuda'` is not in the test name\n- Add `--split` and `--rank` to allowing splitting the work (NVIDIA CI has a limitation of 3 hours, we have to split the work to satisfy this limitation)\n- The error summary could be `ERROR SUMMARY: 1 error`, or `ERROR SUMMARY: 2 errors`, the tail could be `error` or `errors`, it is not of the same length. The script is fixed to handle this case.\n- Ignore errors from `cufft`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29243\n\nDifferential Revision: D18941701\n\nPulled By: mruberry\n\nfbshipit-source-id: 2048428f32b66ef50c67444c03ce4dd9491179d2", "pr_number": "29243", "files_changed": ["test/common_device_type.py", "test/scripts/cuda_memcheck_common.py", "test/scripts/run_cuda_memcheck.py", "test/test_torch.py"], "labels": ["merged"]}, "ec92711aac": {"title": "Fix error message in incorrect rref.localValue() call (#31199)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/31198, see the issue for more details. We throw an error when `local_value()` is called on a non-owned rref, but the incorrect node name is printed in the error message. This PR fixes that and adds a relevant unit test.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31199\n\nDifferential Revision: D19072014\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 760c20bfd2fbf286eaaca19500469509a575cfec", "pr_number": "31199", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/py_rref.cpp"], "labels": ["merged"]}, "0e50c1b0d9": {"title": "Replace assert with cuda assert macro (#31297)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31297\n\nFollow-up to https://github.com/pytorch/pytorch/pull/31276\n\nThis is final replacement needed for aten out of place hipification.\n\nTest Plan: wait for CI to clear.\n\nReviewed By: bddppq\n\nDifferential Revision: D19070209\n\nfbshipit-source-id: 1428cd0ddfb5a8f4e234fabce822285e898047ea", "pr_number": "31297", "files_changed": ["aten/src/THC/THCTensorIndex.cu"], "labels": ["fb-exported", "merged", "module: rocm"]}, "9dc3d8738c": {"title": "fix view call on discontiguous tensor in to_sparse_backward (#31223)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30820\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31223\n\nDifferential Revision: D19044172\n\nPulled By: ngimel\n\nfbshipit-source-id: ac9fa71197d4f6c5b90a26e8d23360250745a2e2", "pr_number": "31223", "files_changed": ["aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp", "test/common_methods_invocations.py"], "labels": ["merged"]}, "60ec53c7fd": {"title": "Fix copy kernel speed regression introduced in #29631 (#31279)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31271\n\nThis fixes copy kernel speed regression introduced in https://github.com/pytorch/pytorch/issues/29631.\n\nThe previous implementation forces the compiler to instantiate `static_cast_with_inter_type` because it is passed as an argument of a function. This behavior makes it impossible for compilers to do optimizations like automatic vectorization, and, function call itself is expensive compared to a single casting instruction.\n\nTo check the change, run\n```\nreadelf -Ws /home/xgao/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so | grep static_cast_with_inter_type\n```\n\nOn nightly build, we have output\n```\n168217: 0000000001852bf0     5 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIsdE5applyEd\n168816: 0000000001852d30    33 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIfEaE5applyEa\n168843: 00000000018531f0     7 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIblE5applyEl\n168930: 0000000001852c20     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIslE5applyEl\n168935: 00000000018528d0   124 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIfNS_4HalfEE5applyES1_\n169023: 0000000001852f30    17 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIdEhE5applyEh\n169713: 00000000018525c0     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIahE5applyEh\n170033: 0000000001852c10     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIsiE5applyEi\n170105: 0000000001852bd0     5 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIshE5applyEh\n170980: 0000000001852fc0    27 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIdES1_IfEE5applyES3_\n171398: 0000000001852810    13 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIdbE5applyEb\n171574: 00000000018532e0    35 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIbNS_8BFloat16EE5applyES1_\n171734: 0000000001852b20     6 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIlSt7complexIdEE5applyES2_\n172422: 0000000001853350    54 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeINS_8BFloat16EaE5applyEa\n172704: 00000000018533c0    38 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeINS_8BFloat16EfE5applyEf\n172976: 0000000001852890    10 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIflE5applyEl\n173038: 0000000001852f80     9 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIdEfE5applyEf\n173329: 00000000018531c0    20 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIbfE5applyEf\n173779: 00000000018524d0     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIhiE5applyEi\n174032: 0000000001852960    14 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIfNS_8BFloat16EE5applyES1_\n174334: 0000000001852d60    29 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIfEdE5applyEd\n174470: 0000000001852c60   124 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIsNS_4HalfEE5applyES1_\n174770: 0000000001852bc0    15 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIlNS_8BFloat16EE5applyES1_\n176408: 0000000001853980   144 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeINS_4HalfEbE5applyEb\n176475: 0000000001852790   128 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIdNS_4HalfEE5applyES1_\n....\n```\n\nAnd after this PR, we get empty output\n```\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31279\n\nDifferential Revision: D19075587\n\nPulled By: ngimel\n\nfbshipit-source-id: c20088241f39fa40c1d055f0a46eb5b9ece52e71", "pr_number": "31279", "files_changed": ["aten/src/ATen/native/cpu/CopyKernel.cpp"], "labels": ["merged"]}, "930d0751e6": {"title": "Java Tensor hybrid, owns at::Tensor, no memcopy for java outputs. (#30501)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30501\n\n**Motivation**:\nIn current state output of libtorch Module forward,runMethod is mem copied to java ByteBuffer, which is allocated, at least in some versions of android, on java heap. That could lead to intensive garbage collection.\n\n**Change**:\nOutput java tensor becomes owner of output at::Tensor and holds it (as `pytorch_jni::TensorHybrid::tensor_` field) alive until java part is not destroyed by GC. For that org.pytorch.Tensor becomes 'Hybrid' class in fbjni naming and starts holding member field `HybridData mHybridData;`\n\nIf construction of it starts from java side - java constructors of subclasses (we need all the fields initialized, due to this `mHybridData` is not declared final, but works as final) call `this.mHybridData = super.initHybrid();` to initialize cpp part (`at::Tensor tensor_`).\n\nIf construction starts from cpp side - cpp side is initialiaed using provided at::Tensor with `makeCxxInstance(std::move(tensor))` and is passed to java method `org.pytorch.Tensor#nativeNewTensor` as parameter `HybridData hybridData`, which holds native pointer to cpp side.\n\nIn that case `initHybrid()` method is not called, but parallel set of ctors of subclasses are used, which stores `hybridData` in `mHybridData`.\n\nRenaming:\n`JTensor` -> `TensorHybrid`\n\nRemoved method:\n`JTensor::newAtTensorFromJTensor(JTensor)` becomes trivial `TensorHybrid->cthis()->tensor()`\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18893320\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: df94775d2a010a1ad945b339101c89e2b79e0f83", "pr_number": "30501", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_lite.cpp", "android/pytorch_android/src/main/java/org/pytorch/Tensor.java"], "labels": ["merged"]}, "0d7391f8b2": {"title": "Test cases for custom ops with autograd (#31003)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31003\n\n-\nghstack-source-id: 95663728\n\nTest Plan: unit tests\n\nDifferential Revision: D18896189\n\nfbshipit-source-id: d71f7678fff644536fe30452ee21a5a7df1f1f0b", "pr_number": "31003", "files_changed": ["test/custom_operator/op.cpp", "test/custom_operator/op.h", "test/custom_operator/test_custom_ops.cpp", "test/custom_operator/test_custom_ops.py", "torch/script.h"], "labels": ["merged"]}, "c95d46abbd": {"title": "Remove C++11 compatibility from c10::util::crc64_t (#30920)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30920\n\ndeletecode\nghstack-source-id: 95255641\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869640\n\nfbshipit-source-id: c3d7f4e1a29caff9fd8a8141c258f6f1c3fd830c", "pr_number": "30920", "files_changed": ["c10/util/ConstexprCrc.h"], "labels": ["merged"]}, "409151e1bb": {"title": "Use [[noreturn]] instead of C10_NORETURN or CAFFE_NORETURN (#30917)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30917\n\nThis is a C++14 feature, we can use this now.\nghstack-source-id: 95255753\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869637\n\nfbshipit-source-id: dd02036b9faeaffa64b2d2d305725443054da31b", "pr_number": "30917", "files_changed": ["c10/macros/Macros.h", "c10/util/Logging.h", "caffe2/core/common.h"], "labels": ["merged"]}, "9ca61aec0f": {"title": "Kill THLogAdd (#31217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31217\n\nIt doesn't seem to be used.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18986642\n\nPulled By: gchanan\n\nfbshipit-source-id: 96d615df82731d2224d403ab6e2cad6d4c6674fd", "pr_number": "31217", "files_changed": ["aten/src/TH/CMakeLists.txt", "aten/src/TH/TH.h", "aten/src/TH/THLogAdd.cpp", "aten/src/TH/THLogAdd.h"], "labels": ["merged"]}, "455e85a2f1": {"title": "Fix unflatten when dim is a negative integer (#31208)", "body": "Summary:\nChangelog:\n- Wrap dim to be a positive integer when dim is negative\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31208\n\nTest Plan:\n- Updated tests in test_namedtensor.py\n\nFixes https://github.com/pytorch/pytorch/issues/31184\n\nDifferential Revision: D19036569\n\nPulled By: zou3519\n\nfbshipit-source-id: 86e01e20988dee7c4b6c73232f66282d687f9a2c", "pr_number": "31208", "files_changed": ["aten/src/ATen/native/NamedTensor.cpp", "test/test_namedtensor.py"], "labels": ["merged", "open source"]}, "d401ba1417": {"title": "benchmark binary ops in binary_test (#31326)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31326\n\nas title\n\nTest Plan:\n```\nbuck run caffe2/benchmarks/operator_benchmark/pt:binary_test -- --iterations 1\n\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: add\n# Mode: Eager\n# Name: add_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.float32\n# Input: in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.float32\nForward Execution Time (us) : 28080.802\n\nReviewed By: hl475\n\nDifferential Revision: D19120113\n\nfbshipit-source-id: 1105de208f7609cc6d74f0b5bc6fe75f19146b28", "pr_number": "31326", "files_changed": ["benchmarks/operator_benchmark/benchmark_all_other_test.py", "benchmarks/operator_benchmark/pt/binary_test.py"], "labels": ["fb-exported", "merged"]}, "c6a8f884d8": {"title": "add copy_ operator the op bench (#31327)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31327\n\nAdds copy_ operator to the benchmark suite\n\nTest Plan:\n```\nbuck run caffe2/benchmarks/operator_benchmark/pt:binary_test -- --iterations 1 --operators copy_\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: copy_\n# Mode: Eager\n# Name: copy__M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32\n# Input: M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32\nForward Execution Time (us) : 60.645\n\nReviewed By: hl475\n\nDifferential Revision: D19122910\n\nfbshipit-source-id: e5f0b0e2612daae0201b1b4a87f52b971e0cc4a8", "pr_number": "31327", "files_changed": ["benchmarks/operator_benchmark/pt/binary_test.py"], "labels": ["fb-exported", "merged"]}, "643ca5def2": {"title": "Replace c10::guts::stuff with std::stuff (#30915)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30915\n\nSince we now have C++14, we don't need these c10::guts helpers anymore\nghstack-source-id: 95777609\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869639\n\nfbshipit-source-id: 97716f932297c64c6e814410ac47b444c33d4e2e", "pr_number": "30915", "files_changed": ["aten/src/ATen/core/DeprecatedTypePropertiesRegistry.cpp", "aten/src/ATen/core/List.h", "aten/src/ATen/core/NamedTensor.cpp", "aten/src/ATen/core/NamedTensor.h", "aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/boxing.h", "aten/src/ATen/core/boxing/kernel_function.h", "aten/src/ATen/core/boxing/kernel_function_legacy_test.cpp", "aten/src/ATen/core/boxing/kernel_function_test.cpp", "aten/src/ATen/core/boxing/kernel_functor.h", "aten/src/ATen/core/boxing/kernel_functor_test.cpp", "aten/src/ATen/core/boxing/kernel_lambda_legacy_test.cpp", "aten/src/ATen/core/boxing/kernel_lambda_test.cpp", "aten/src/ATen/core/boxing/kernel_stackbased_test.cpp", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/op_registration/infer_schema.h", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/cpu/vec256/vec256.h", "aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/native/QuantizedLinear.cpp", "aten/src/ATen/native/cpu/Loops.h", "aten/src/ATen/native/cpu/Reduce.h", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/test/NamedTensor_test.cpp", "aten/src/TH/generic/THTensorRandom.cpp", "binaries/benchmark_helper.cc", "binaries/convert_and_benchmark.cc", "binaries/predictor_verifier.cc", "c10/core/TensorOptions.h", "c10/test/util/Metaprogramming_test.cpp", "c10/test/util/TypeList_test.cpp", "c10/util/Array.h", "c10/util/C++17.h", "c10/util/Half.h", "c10/util/Metaprogramming.h", "c10/util/TypeIndex.h", "c10/util/TypeList.h", "c10/util/TypeTraits.h", "c10/util/either.h", "c10/util/intrusive_ptr.h", "c10/util/order_preserving_flat_hash_map.h", "c10/util/string_view.h", "c10/util/typeid.cpp", "c10/util/typeid.h", "caffe2/core/common.h", "caffe2/core/export_c10_op_to_caffe2.h", "caffe2/core/net_async_base.cc", "caffe2/core/net_async_task_future.cc", "caffe2/core/net_async_task_graph.cc", "caffe2/core/net_parallel.cc", "caffe2/core/net_test.cc", "caffe2/core/observer_test.cc", "caffe2/core/operator.cc", "caffe2/core/operator_schema.cc", "caffe2/core/plan_executor.cc", "caffe2/mobile/contrib/ulp2/ulp.cc", "caffe2/mobile/contrib/ulp2/ulp_neon.cc", "caffe2/mobile/contrib/ulp2/ulp_test.cc", "caffe2/observers/operator_attaching_net_observer.h", "caffe2/observers/time_observer_test.cc", "caffe2/onnx/backend_rep.cc", "caffe2/operators/counter_ops.cc", "caffe2/operators/filler_op.h", "caffe2/operators/quantized/int8_test_utils.h", "caffe2/operators/string_ops_test.cc", "caffe2/opt/backend_cutting.cc", "caffe2/opt/nql/tests/GraphMatcherTest.cc", "caffe2/opt/optimize_ideep.cc", "caffe2/predictor/predictor_test.cc", "caffe2/predictor/predictor_utils.cc", "caffe2/python/mpi_python.cc", "caffe2/quantization/server/dynamic_histogram.cc", "caffe2/queue/blobs_queue_db.cc", "caffe2/serialize/file_adapter.cc", "caffe2/serialize/inline_container.cc", "caffe2/sgd/iter_op.cc", "caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/WorkersPool.h", "caffe2/video/video_decoder.cc", "modules/observers/perf_observer.cc", "torch/csrc/autograd/custom_function.h", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h", "torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_req.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp", "torch/csrc/distributed/autograd/utils.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/python_call.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_remote_call.cpp", "torch/csrc/distributed/rpc/python_resp.cpp", "torch/csrc/distributed/rpc/rref_proto.cpp", "torch/csrc/distributed/rpc/script_call.cpp", "torch/csrc/distributed/rpc/script_remote_call.cpp", "torch/csrc/distributed/rpc/script_resp.cpp", "torch/csrc/jit/import.cpp", "torch/csrc/jit/init.cpp", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/passes/guard_elimination.cpp", "torch/csrc/jit/passes/quantization.cpp", "torch/csrc/jit/pickle.cpp", "torch/csrc/jit/register_c10_ops.cpp", "torch/csrc/jit/script/lexer.h", "torch/custom_class.h", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": ["jit", "merged"]}, "c5d3be1102": {"title": "Remove the second copy on calling dist_autograd_context._known_worker_ids() (#31206)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31206\n\nImprovement on #25525.\n\n- DistAutogradContext::getKnownWorkerIds() returns a unordered_map as temp value. No need to copy this temp value A into another temp value B.\nghstack-source-id: 95736296\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:dist_autograd_fork --  test_worker_ids_recorded\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:dist_autograd_fork_thrift -- test_context_cleanup_tensor_with_grad\n```\n\nDifferential Revision: D5707771\n\nfbshipit-source-id: 9fea83dc69b02047aef8b02a73028a260ac0be40", "pr_number": "31206", "files_changed": ["test/dist_autograd_test.py", "torch/csrc/distributed/autograd/init.cpp"], "labels": ["merged"]}, "229ce89b92": {"title": "Fix coverage and hypothesis conflict (#31320)", "body": "Summary:\nTemporarily enforcing versions for all envs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31320\n\nDifferential Revision: D19122781\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: fe6473b177367371387d4b3b873131e7ecfbc0f8", "pr_number": "31320", "files_changed": [".jenkins/caffe2/test.sh"], "labels": ["merged"]}, "f9010d7648": {"title": "remove wipe cache from op bench (#31334)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31334\n\nThe wipe cache logic was introduced hoping to reduce the variations in the benchmark results. Based on our experiments result, it didn't actually help with that. In addition, several engineers had encountered the issue of missing cpuinfo.h which was used in the wipe cache logic. So this diff removes that feature to ensure smooth installation and running of the op bench.\n\nTest Plan:\n```\nbuck run caffe2/benchmarks/operator_benchmark/pt:add_test -- --iterations 1\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: add\n# Mode: Eager\n# Name: add_M1_N1_K1_cpu\n# Input: M: 1, N: 1, K: 1, device: cpu\nForward Execution Time (us) : 111.192\n\nA/B test also pass Benchmark Run #2476535015\n\nReviewed By: hl475\n\nDifferential Revision: D19126970\n\nfbshipit-source-id: 9b1ab48c121838836ba6e0ae664a48fe2d18efdd", "pr_number": "31334", "files_changed": ["benchmarks/operator_benchmark/benchmark_core.py", "benchmarks/operator_benchmark/benchmark_runner.py", "benchmarks/operator_benchmark/pt_extension/extension.cpp"], "labels": ["fb-exported", "merged"]}, "10ce1765be": {"title": "Introducing ScalarTypeType and LayoutType (#31074)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31074\n\nAs the title,\nIt's step 1 in https://github.com/pytorch/pytorch/pull/30694#issuecomment-564205276.\n\nNot using those types in any other place.\n\nTest Plan: Making sure all unit tests and build pass successfully.\n\nDifferential Revision: D18916246\n\nfbshipit-source-id: c8213307ed196e1b51ce1a2a7c10869dcd45b79e", "pr_number": "31074", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/unpickler.cpp"], "labels": ["fb-exported", "jit", "merged"]}, "0e548a76eb": {"title": "Upgrade exported ONNX IR version to 6 (#31025)", "body": "Summary:\nUpgrade IR version from 4 to 6, below is change doc from ONNX. The upgrade should be backward compatible.\n\n```\n  // IR VERSION 5 published on March 18, 2019\n  // - Add message TensorAnnotation.\n  // - Add quantization annotation in GraphProto to map tensor with its scale and zero point quantization parameters.\n  IR_VERSION_2019_3_18 = 0x0000000000000005;\n\n  // IR VERSION 6 published on Sep 19, 2019\n  // - Add support for sparse tensor constants stored in model.\n  //   - Add message SparseTensorProto\n  //   - Add sparse initializers\n  IR_VERSION = 0x0000000000000006;\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31025\n\nReviewed By: hl475\n\nDifferential Revision: D18935444\n\nPulled By: houseroad\n\nfbshipit-source-id: 9ba47f9657fa1a668db291cf04af07d5e8d73c21", "pr_number": "31025", "files_changed": ["test/onnx/expect/TestOperators.test_acos.expect", "test/onnx/expect/TestOperators.test_add_broadcast.expect", "test/onnx/expect/TestOperators.test_add_left_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_right_broadcast.expect", "test/onnx/expect/TestOperators.test_add_size1_singleton_broadcast.expect", "test/onnx/expect/TestOperators.test_addconstant.expect", "test/onnx/expect/TestOperators.test_addmm.expect", "test/onnx/expect/TestOperators.test_arange_dynamic.expect", "test/onnx/expect/TestOperators.test_argmax.expect", "test/onnx/expect/TestOperators.test_asin.expect", "test/onnx/expect/TestOperators.test_at_op.expect", "test/onnx/expect/TestOperators.test_atan.expect", "test/onnx/expect/TestOperators.test_avg_pool2d.expect", "test/onnx/expect/TestOperators.test_baddbmm.expect", "test/onnx/expect/TestOperators.test_basic.expect", "test/onnx/expect/TestOperators.test_batchnorm.expect", "test/onnx/expect/TestOperators.test_batchnorm_1d.expect", "test/onnx/expect/TestOperators.test_batchnorm_noaffine.expect", "test/onnx/expect/TestOperators.test_batchnorm_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_batchnorm_training.expect", "test/onnx/expect/TestOperators.test_bitshift.expect", "test/onnx/expect/TestOperators.test_c2_op.expect", "test/onnx/expect/TestOperators.test_chunk.expect", "test/onnx/expect/TestOperators.test_clip.expect", "test/onnx/expect/TestOperators.test_clip_max.expect", "test/onnx/expect/TestOperators.test_clip_min.expect", "test/onnx/expect/TestOperators.test_concat2.expect", "test/onnx/expect/TestOperators.test_conv.expect", "test/onnx/expect/TestOperators.test_conv_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_conv_onnx_irv4_opset8.expect", "test/onnx/expect/TestOperators.test_convtranspose.expect", "test/onnx/expect/TestOperators.test_cos.expect", "test/onnx/expect/TestOperators.test_cumsum.expect", "test/onnx/expect/TestOperators.test_det.expect", "test/onnx/expect/TestOperators.test_dict.expect", "test/onnx/expect/TestOperators.test_dict_str.expect", "test/onnx/expect/TestOperators.test_dropout.expect", "test/onnx/expect/TestOperators.test_elu.expect", "test/onnx/expect/TestOperators.test_embedding_bags.expect", "test/onnx/expect/TestOperators.test_empty_like.expect", "test/onnx/expect/TestOperators.test_empty_like_opset7.expect", "test/onnx/expect/TestOperators.test_equal.expect", "test/onnx/expect/TestOperators.test_erf.expect", "test/onnx/expect/TestOperators.test_exp.expect", "test/onnx/expect/TestOperators.test_expand.expect", "test/onnx/expect/TestOperators.test_flatten.expect", "test/onnx/expect/TestOperators.test_flatten2D.expect", "test/onnx/expect/TestOperators.test_fmod.expect", "test/onnx/expect/TestOperators.test_frobenius_norm.expect", "test/onnx/expect/TestOperators.test_full.expect", "test/onnx/expect/TestOperators.test_full_like.expect", "test/onnx/expect/TestOperators.test_gather.expect", "test/onnx/expect/TestOperators.test_gather_opset11.expect", "test/onnx/expect/TestOperators.test_ge.expect", "test/onnx/expect/TestOperators.test_gelu.expect", "test/onnx/expect/TestOperators.test_gt.expect", "test/onnx/expect/TestOperators.test_hardtanh.expect", "test/onnx/expect/TestOperators.test_implicit_expand.expect", "test/onnx/expect/TestOperators.test_index.expect", "test/onnx/expect/TestOperators.test_isnan.expect", "test/onnx/expect/TestOperators.test_layer_norm_aten.expect", "test/onnx/expect/TestOperators.test_le.expect", "test/onnx/expect/TestOperators.test_linear.expect", "test/onnx/expect/TestOperators.test_log_sigmoid.expect", "test/onnx/expect/TestOperators.test_logsoftmax.expect", "test/onnx/expect/TestOperators.test_lt.expect", "test/onnx/expect/TestOperators.test_master_opset.expect", "test/onnx/expect/TestOperators.test_max.expect", "test/onnx/expect/TestOperators.test_maxpool.expect", "test/onnx/expect/TestOperators.test_maxpool_dilations.expect", "test/onnx/expect/TestOperators.test_maxpool_indices.expect", "test/onnx/expect/TestOperators.test_mean.expect", "test/onnx/expect/TestOperators.test_meshgrid.expect", "test/onnx/expect/TestOperators.test_min.expect", "test/onnx/expect/TestOperators.test_mm.expect", "test/onnx/expect/TestOperators.test_narrow.expect", "test/onnx/expect/TestOperators.test_ne.expect", "test/onnx/expect/TestOperators.test_nonzero.expect", "test/onnx/expect/TestOperators.test_norm_p1.expect", "test/onnx/expect/TestOperators.test_norm_p2.expect", "test/onnx/expect/TestOperators.test_ones_like.expect", "test/onnx/expect/TestOperators.test_pad.expect", "test/onnx/expect/TestOperators.test_params.expect", "test/onnx/expect/TestOperators.test_params_onnx_irv4.expect", "test/onnx/expect/TestOperators.test_permute2.expect", "test/onnx/expect/TestOperators.test_pixel_shuffle.expect", "test/onnx/expect/TestOperators.test_pow.expect", "test/onnx/expect/TestOperators.test_prelu.expect", "test/onnx/expect/TestOperators.test_prod.expect", "test/onnx/expect/TestOperators.test_rand.expect", "test/onnx/expect/TestOperators.test_randn.expect", "test/onnx/expect/TestOperators.test_reduce_sum_negative_indices.expect", "test/onnx/expect/TestOperators.test_reduced_mean.expect", "test/onnx/expect/TestOperators.test_reduced_mean_keepdim.expect", "test/onnx/expect/TestOperators.test_reduced_prod.expect", "test/onnx/expect/TestOperators.test_reduced_prod_keepdim.expect", "test/onnx/expect/TestOperators.test_reduced_sum.expect", "test/onnx/expect/TestOperators.test_reduced_sum_keepdim.expect", "test/onnx/expect/TestOperators.test_reducemax.expect", "test/onnx/expect/TestOperators.test_reducemin.expect", "test/onnx/expect/TestOperators.test_remainder.expect", "test/onnx/expect/TestOperators.test_repeat.expect", "test/onnx/expect/TestOperators.test_repeat_dim_overflow.expect", "test/onnx/expect/TestOperators.test_retain_param_name_disabled.expect", "test/onnx/expect/TestOperators.test_round.expect", "test/onnx/expect/TestOperators.test_rrelu.expect", "test/onnx/expect/TestOperators.test_rsqrt.expect", "test/onnx/expect/TestOperators.test_rsub.expect", "test/onnx/expect/TestOperators.test_scatter_add.expect", "test/onnx/expect/TestOperators.test_scatter_add_opset11.expect", "test/onnx/expect/TestOperators.test_selu.expect", "test/onnx/expect/TestOperators.test_sign.expect", "test/onnx/expect/TestOperators.test_sin.expect", "test/onnx/expect/TestOperators.test_slice.expect", "test/onnx/expect/TestOperators.test_slice_dynamic.expect", "test/onnx/expect/TestOperators.test_split.expect", "test/onnx/expect/TestOperators.test_split_with_sizes.expect", "test/onnx/expect/TestOperators.test_sqrt.expect", "test/onnx/expect/TestOperators.test_std.expect", "test/onnx/expect/TestOperators.test_sum.expect", "test/onnx/expect/TestOperators.test_tan.expect", "test/onnx/expect/TestOperators.test_topk.expect", "test/onnx/expect/TestOperators.test_topk_smallest_unsorted.expect", "test/onnx/expect/TestOperators.test_transpose.expect", "test/onnx/expect/TestOperators.test_type_as.expect", "test/onnx/expect/TestOperators.test_unfold.expect", "test/onnx/expect/TestOperators.test_unique.expect", "test/onnx/expect/TestOperators.test_unsqueeze.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_scale.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_scale_default_scale_factor.expect", "test/onnx/expect/TestOperators.test_upsample_nearest_size.expect", "test/onnx/expect/TestOperators.test_view.expect", "test/onnx/expect/TestOperators.test_view_flatten.expect", "test/onnx/expect/TestOperators.test_zeros_like.expect", "torch/csrc/jit/export.cpp", "torch/csrc/onnx/init.cpp", "torch/csrc/onnx/onnx.h", "torch/onnx/__init__.py"], "labels": ["jit", "merged"]}, "52b8a52e4d": {"title": "move AliasWithNameOp to caffe2/operators", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31281\n\nReviewed By: houseroad\n\nDifferential Revision: D19053453\n\nfbshipit-source-id: 350bfd5c001db9c17916dcae7ade8f56db1e9841", "pr_number": "31281", "files_changed": ["caffe2/operators/alias_with_name.cc", "caffe2/operators/alias_with_name.cu", "caffe2/operators/alias_with_name.h", "caffe2/python/operator_test/alias_with_name_test.py", "caffe2/python/operator_test/torch_integration_test.py"], "labels": ["fb-exported", "merged"]}, "49eff2f43c": {"title": "Kill THSize. (#31218)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31218\n\nIt isn't used.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18986641\n\nPulled By: gchanan\n\nfbshipit-source-id: 0a434941d12193941f097232c18ffe4268bf5f82", "pr_number": "31218", "files_changed": ["aten/src/TH/CMakeLists.txt", "aten/src/TH/TH.h", "aten/src/TH/THSize.cpp", "aten/src/TH/THSize.h"], "labels": ["merged"]}, "d2067569e7": {"title": "Kill THTensor_(bhistc). (#31254)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31254\n\nIt's not used.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19022923\n\nPulled By: gchanan\n\nfbshipit-source-id: caa5e6b7a133f24f8f3349fd1e53147f8dd3fd97", "pr_number": "31254", "files_changed": ["aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp"], "labels": ["merged"]}, "dab5f72543": {"title": "we should have a config-based way to skip flaky tests (#30978)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30978\n\nThis particular approach queries our issue tracker for test titles that\nmatch the following format:\n\n```\nDISABLED test_async_grad_guard_with_grad (jit.test_async.TestAsync)\n```\n\nAnd then skips the python test for them. There is 1 second timeout so\nif the internet flakes we still run the test suite, without disabling any\ntests.\n\nThis is intended as a quick fix, similar to ninja unland, to get to a green\nmaster. Long term test disables should go into the code.\n\nTest Plan: Imported from OSS\n\nPulled By: zdevito\n\nDifferential Revision: D18890532\n\nfbshipit-source-id: fe9447e59a6d5c9ad345f7c3ff15d63b6d2a09e2", "pr_number": "30978", "files_changed": ["test/common_utils.py", "tools/update_disabled_tests.sh"], "labels": ["merged"]}, "cc8d6342fc": {"title": "make profiling take no_grad flags into account (#31071)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31071\n\nPreviously the profiler would think Tensors would require grad, even\nwhen the no_grad flag is enabled during execution. This makes the profiling\nand guards respect the no_grad flag, which eliminates extra differentiable\ngraphs that appear in the backward graph (where no_grad is typically enabled).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18915468\n\nPulled By: zdevito\n\nfbshipit-source-id: 1ae816a16ab78ae5352825cc6b4a68ed7681a089", "pr_number": "31071", "files_changed": ["test/test_jit_fuser.py", "torch/csrc/jit/interpreter.cpp", "torch/csrc/jit/interpreter.h", "torch/csrc/jit/profiling_record.cpp", "torch/csrc/jit/profiling_record.h"], "labels": ["jit", "merged"]}, "5554e5b793": {"title": "Docs: c++11 -> c++14 (#30530)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30530\n\nSwitch some mentions of \"C++11\" in the docs to \"C++14\"\nghstack-source-id: 95812049\n\nTest Plan: testinprod\n\nDifferential Revision: D18733733\n\nfbshipit-source-id: b9d0490eb3f72bad974d134bbe9eb563f6bc8775", "pr_number": "30530", "files_changed": ["CONTRIBUTING.md", "aten/conda/meta.yaml", "caffe2/contrib/aten/README.md", "cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake", "docs/cpp/source/frontend.rst", "docs/cpp/source/notes/tensor_basics.rst"], "labels": ["merged"]}, "0b8332efb4": {"title": "Remove c++11 examples from doc comments (#30925)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30925\n\n-\nghstack-source-id: 95810835\n\nTest Plan: it's just comments\n\nDifferential Revision: D18869634\n\nfbshipit-source-id: 346498ae2472dbfe23ef40533bff891fde9922c4", "pr_number": "30925", "files_changed": ["c10/util/Metaprogramming.h", "c10/util/TypeList.h"], "labels": ["merged"]}, "d9c3913dfc": {"title": "move BatchPermutationOp to caffe2/operators", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31350\n\nReviewed By: houseroad\n\nDifferential Revision: D19053527\n\nfbshipit-source-id: 50d11f137d0f5c07e8ad899a3a84d56a042bbc32", "pr_number": "31350", "files_changed": ["caffe2/operators/batch_permutation_op.cc", "caffe2/operators/batch_permutation_op.cu", "caffe2/operators/batch_permutation_op.h", "caffe2/operators/batch_permutation_op_gpu_test.cc", "modules/detectron/batch_permutation_op.cc", "modules/detectron/batch_permutation_op.cu", "modules/detectron/batch_permutation_op.h"], "labels": ["fb-exported", "merged"]}, "f0243ea712": {"title": "Use [[deprecated]] instead of C10_DEPRECATED (#30918)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30918\n\nThis is a C++14 feature we can use now\nghstack-source-id: 95811482\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869636\n\nfbshipit-source-id: b5b3d78b61b6ceb2deda509131f8502e95b1d057", "pr_number": "30918", "files_changed": ["aten/src/ATen/Dispatch.h", "aten/src/ATen/core/TensorAccessor.h", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/TensorBody.h", "c10/core/MemoryFormat.h", "c10/core/Scalar.h", "c10/core/ScalarType.h", "c10/util/ArrayRef.h", "c10/util/Deprecated.h", "c10/util/Exception.h", "torch/csrc/api/include/torch/nn/modules/container/named_any.h", "torch/csrc/utils/auto_gil.h", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["merged"]}, "e33dea6e4e": {"title": "dynamicly quantized lstm benchmarking", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30149\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18613005\n\nPulled By: z-a-f\n\nfbshipit-source-id: 966bfe2c862b1b4006b228bd9115c5c1cd3ad8cf", "pr_number": "30149", "files_changed": ["benchmarks/operator_benchmark/pt/qrnn_test.py"], "labels": ["merged"]}, "e3fecabdcb": {"title": "Setup operator registration for distributed package (#31214)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31214\n\nThis set up the basic infrastructure for distributed autograd and rpc to\nbind their operators to TorchScript, since the whole distributed package\nis builtin behind the `USE_DISTRIBUTED` flag, we separate the\nregistration and build it only when the flag is on.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19137160\n\nfbshipit-source-id: ff47dc4c380ebe273fe0eea9e5e3fccfbd6466d7", "pr_number": "31214", "files_changed": ["caffe2/CMakeLists.txt", "test/dist_autograd_test.py", "test/test_dist_autograd_spawn.py", "tools/build_variables.py", "torch/csrc/jit/register_distributed_ops.cpp", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "c5d2758c35": {"title": "Disable flaky TestMomentumSGD.test_fp16momentum_sgd (#31369)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/31368\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31369\n\nDifferential Revision: D19147072\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 6fad13be7b35f992d84a20f23877cad05ff18616", "pr_number": "31369", "files_changed": ["caffe2/python/operator_test/momentum_sgd_test.py"], "labels": ["merged"]}, "e169e02836": {"title": "Refactor custom op tests (#31282)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31282\n\nIntroduce a helper to easily call stack ops\nghstack-source-id: 95855728\n\nTest Plan: unit tests\n\nDifferential Revision: D19061515\n\nfbshipit-source-id: a7d6329e26cd3d94730d88c8a6393e10bfbd8e9b", "pr_number": "31282", "files_changed": ["test/custom_operator/test_custom_ops.cpp"], "labels": ["merged"]}, "e0ab255a51": {"title": "Updates to serialization.md (#31372)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31372\n\nKeeping it current with the latest changes.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19145986\n\nPulled By: suo\n\nfbshipit-source-id: 88122e66fa87a354ef8e87faffe58551074e3f03", "pr_number": "31372", "files_changed": ["torch/csrc/jit/docs/serialization.md"], "labels": ["jit", "merged"]}, "4ec2448580": {"title": "Update OVERVIEW.md (#31373)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31373\n\nJust some housekeeping\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19145987\n\nPulled By: suo\n\nfbshipit-source-id: ae8142dab2bddcf0b628c27c426ca26334c48238", "pr_number": "31373", "files_changed": ["torch/csrc/jit/docs/OVERVIEW.md"], "labels": ["jit", "merged"]}, "e5631119f6": {"title": "use expect instead of casting in register_c10_ops (#31401)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31401\n\nAs title, just a mechanical change\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19152965\n\nPulled By: suo\n\nfbshipit-source-id: 6bb27df7c8f542c55110286c156358ba0936269f", "pr_number": "31401", "files_changed": ["torch/csrc/jit/register_c10_ops.cpp"], "labels": ["jit", "merged"]}, "7e81d72d12": {"title": "remove unnecessary arg from create_script_module (#31017)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31017\n\nThis arg is now derivable from another one. So we don't need to pass\nboth\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18904111\n\nPulled By: suo\n\nfbshipit-source-id: ea74ea9c2ae83d9e0e6977b0eb6629f53545e2e4", "pr_number": "31017", "files_changed": ["torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "82d52bc718": {"title": "remove remnants of properties hack (#31018)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31018\n\nProperties are now disallowed so this hack is no longer necessary\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18904112\n\nPulled By: suo\n\nfbshipit-source-id: 83448da677082d59355729bb72d9f9f4c31ea756", "pr_number": "31018", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "878b0e35f7": {"title": "Simplify recursive script compilation flow. (#31019)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31019\n\nNo more `recurisve_script`, just direct calls to `create_script_module`.\nThis reduces the number of pathways through the frontend, and the\nuniformity is useful for a future PR.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18904113\n\nPulled By: suo\n\nfbshipit-source-id: 7de061dfef0cbdfc9376408fc6c1167b81803f01", "pr_number": "31019", "files_changed": ["torch/jit/__init__.py", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "3c8892aa0c": {"title": "avoid doing quadratic work in concrete type inference (#31020)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31020\n\nBefore, the recursive scripting process re-did the concrete type\ninference process for every submodule call. This changes things so that\nthe concrete type inference process only occurs once (at the top level),\nand we re-use all the inferred concrete types while recursively\ncompiling submodules.\n\nThis is both more efficient (we don't do n^2 work inferring concrete\ntypes) and less bug-prone (since we infer the concrete type only once,\nthere is no possibility of a mismatch).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18904110\n\nPulled By: suo\n\nfbshipit-source-id: 6560b85ae29fe5e9db1ee982dbf8bc222614b8d8", "pr_number": "31020", "files_changed": ["torch/csrc/jit/script/concrete_module_type.cpp", "torch/csrc/jit/script/concrete_module_type.h", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "c05538b831": {"title": "Move TorchScript language reference to its own page (#31138)", "body": "Summary:\nPreview: https://driazati.github.io/pytorch_doc_previews/jit.html#torchscript-language\n](https://our.intern.facebook.com/intern/diff/18941024/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31138\n\nPulled By: driazati\n\nDifferential Revision: D18941024\n\nfbshipit-source-id: d0ff600870a14c4a7c6ce54867d152072a12c48c", "pr_number": "31138", "files_changed": ["docs/source/jit.rst", "docs/source/jit_language_reference.rst"], "labels": ["merged"]}, "74e59c6fed": {"title": "caffe2::TypeInfo fix when using clang-cl on Windows (#31364)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31364\n\nclang-cl defines both `_MSC_VER` and `__clang__`. Names are mangled clang style though. calling `extract` with the wrong name mangling pattern will throw `std::logic_error`. This crashes on Windows when `get_fully_qualified_type_name` is called because it is marked with `noexcept`.\n\nTest Plan: Windows builds no longer crash on startup.\n\nReviewed By: mattjgalloway\n\nDifferential Revision: D19142064\n\nfbshipit-source-id: 516b9b63daeff30f5c097d192b0971c7a42db57e", "pr_number": "31364", "files_changed": ["c10/util/TypeIndex.h"], "labels": ["fb-exported", "merged"]}, "3694749cd1": {"title": "Detect dill version in torch.save/load (#30985)", "body": "Summary:\nFix for issue https://github.com/pytorch/pytorch/issues/28313\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30985\n\nDifferential Revision: D19142947\n\nPulled By: zou3519\n\nfbshipit-source-id: 10e3a182a99e80ca8c9c8328b6f8764b27d78eb3", "pr_number": "30985", "files_changed": ["test/common_utils.py", "test/test_torch.py", "torch/serialization.py"], "labels": ["merged", "open source", "triaged"]}, "58d2dd5b73": {"title": "Enabled flip for bool tensors (#31267)", "body": "Summary:\nFix this [issue](https://github.com/pytorch/pytorch/issues/31213)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31267\n\nDifferential Revision: D19047249\n\nPulled By: izdeby\n\nfbshipit-source-id: f58ca3ac88aab28742b8d345400270f7d31c3856", "pr_number": "31267", "files_changed": ["aten/src/ATen/native/TensorTransformations.cpp", "aten/src/ATen/native/cuda/TensorTransformations.cu", "test/test_torch.py"], "labels": ["merged"]}, "386cd59d44": {"title": "Remove redundant queries of qconfig in `insertObservers` (#31292)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31292\n\natt\nAlso we need to do this check after we call `insertObservers` on invoked modules\nas well since qconfig can be None for parent module while being valid for invoked modules\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19146668\n\nfbshipit-source-id: be6811353d359ed3edd5415ced29a4999d86650b", "pr_number": "31292", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "359c39b3c2": {"title": "Use global lock instead of per instance lock. (#31404)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31404\n\nMultiple \"trainers\" could each create different instances of DistributedOptimizer, which means we can still have a race condition unless we do a trully global per worker lock.\nghstack-source-id: 95874624\n\nTest Plan: run unit tests -- unfortunatelly due to the non-deterministic behavior it's not clear how to unit test this properly.\n\nDifferential Revision: D19154248\n\nfbshipit-source-id: fab6286c17212f534f1bd1cbdf9f0de002d48c74", "pr_number": "31404", "files_changed": ["torch/distributed/optim/optimizer.py"], "labels": ["merged"]}, "913323750d": {"title": "CODEOWNERS for distributed optimizer. (#31403)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31403\n\nghstack-source-id: 95874532\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19154217\n\nfbshipit-source-id: a18ebe646b97c83cc0eb0821b10b4c76d5ce2878", "pr_number": "31403", "files_changed": ["CODEOWNERS"], "labels": ["merged"]}, "285cc13435": {"title": "check devices for all input tensors in index_put (#31280)", "body": "Summary:\nFix for https://github.com/pytorch/pytorch/issues/30960\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31280\n\nDifferential Revision: D19149114\n\nPulled By: ngimel\n\nfbshipit-source-id: af185a98ac6ea614f43bbf865de02ea113d4ed56", "pr_number": "31280", "files_changed": ["aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/TensorIterator.cpp", "test/test_indexing.py"], "labels": ["merged"]}, "c63f8e5ebe": {"title": "Fix typo in data.rst docs", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31395\n\nDifferential Revision: D19160010\n\nPulled By: zou3519\n\nfbshipit-source-id: cbc4e719e69117e8747617729d240c72e7a4e3dd", "pr_number": "31395", "files_changed": ["docs/source/data.rst"], "labels": ["merged"]}, "47766e648f": {"title": "C++ API parity: MultiheadAttention", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/27309\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D17766736\n\nPulled By: pbelevich\n\nfbshipit-source-id: 7a5f2399f081945d31d4c13d7a8d248c387fc1a6", "pr_number": "27309", "files_changed": ["test/cpp/api/modules.cpp", "torch/csrc/api/include/torch/nn/functional/activation.h", "torch/csrc/api/include/torch/nn/modules/activation.h", "torch/csrc/api/include/torch/nn/options/activation.h", "torch/csrc/api/src/nn/modules/activation.cpp", "torch/csrc/api/src/nn/options/activation.cpp"], "labels": ["merged", "module: cpp"]}, "4d22c3ba01": {"title": "fix docker login, add docker image tag list after purge as html (#31328)", "body": "Summary:\nexample of the generated html: http://ossci-docker.s3-website.us-east-1.amazonaws.com/pytorch.html\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31328\n\nDifferential Revision: D19147113\n\nPulled By: mingbowan\n\nfbshipit-source-id: 5104e92d4490f047a6474e2b12aed3293b52a9df", "pr_number": "31328", "files_changed": [".circleci/config.yml", ".circleci/ecr_gc_docker/gc.py", ".circleci/verbatim-sources/docker_jobs.yml"], "labels": ["merged"]}, "b0bd35ff13": {"title": "caffe2/event: allow multiple errors such as when cancelled (#31335)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31335\n\nWhen an error occurs in a net we end up cancelling all the async ops. If one error occurs it's highly likely other errors will occur as well.\n\nTypically we see:\n1. SendOp failed due to a network error\n2. async scheduling cancels all other ops via `SetFinished(\"Cancelled\");`\n3. Another SendOp fails due to a network error and crashes the process when the exception is thrown.\n\nThis changes caffe2 ops to allow failing twice.\n\nTest Plan: buck test //caffe2/caffe2:caffe2_test_cpu\n\nReviewed By: andrewwdye\n\nDifferential Revision: D19106548\n\nfbshipit-source-id: 4b7882258a240894cc16d061a563c83a3214d3d9", "pr_number": "31335", "files_changed": ["caffe2/core/event.cc", "caffe2/core/event_test.cc"], "labels": ["fb-exported", "merged"]}, "7cf8b9bada": {"title": "Move leaky_relu to Aten(CPU, CUDA) (#29899)", "body": "Summary:\nVitalyFedyunin, This PR is about port LeakyReLU activation to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.LeakyReLU()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    fwd_t = 0\n    bwd_t = 0\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\n\nCPU:\nOMP_NUM_THREADS=56\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.14 (ms).\ninput size(128, 10000) forward time is 4.21 (ms); backwad avg time is 8.02 (ms).\nOMP_NUM_THREADS=1\ninput size(128, 100) forward time is 0.02 (ms); backwad avg time is 0.07 (ms).\ninput size(128, 10000) forward time is 1.98 (ms); backwad avg time is 6.21 (ms)\n```\nAfter:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\n\nCPU:\nOMP_NUM_THREADS=56\ninput size(128, 100) forward time is 0.02 (ms); backwad avg time is 0.04 (ms).\ninput size(128, 10000) forward time is 0.03 (ms); backwad avg time is 0.09 (ms).\nOMP_NUM_THREADS=1\ninput size(128, 100) forward time is 0.01 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10000) forward time is 0.47 (ms); backwad avg time is 1.02 (ms).\n```\nHow to set the numbers of thread? using following script:\n```\nnum_threads=$1\nscript=$2\nlast_core=`expr $num_threads - 1`\necho \"using $num_threads OMP threads\"\necho \"bind cores to 0~$last_core\"\nexport OMP_NUM_THREADS=$num_threads\nexport KMP_AFFINITY=granularity=fine,compact,1,0\nnumactl --physcpubind=0-$last_core --membind=0 python $script\n```\nand run .**/run.sh num_threads test.py**.\n\nFixes https://github.com/pytorch/pytorch/issues/24583 #24584 https://github.com/pytorch/pytorch/issues/24720 #24721\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29899\n\nDifferential Revision: D18816231\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: afb1e43a99317d17f50cff1b593cd8f7a0a83da2", "pr_number": "29899", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/LeakyReLU.cu", "aten/src/THCUNN/generic/LeakyReLU.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/LeakyReLU.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "5e8bac24b4": {"title": "Migrate soft_margin_loss from the TH to Aten (CUDA+CPU) (#28135)", "body": "Summary:\nFix: https://github.com/pytorch/pytorch/issues/24631, https://github.com/pytorch/pytorch/issues/24632, https://github.com/pytorch/pytorch/issues/24764, https://github.com/pytorch/pytorch/issues/24765\n\nPort of TH SoftMarginCriterion to ATen using un-fused tensor operators but with custom backward code. This is a follow-up/fixc of reverted PR https://github.com/pytorch/pytorch/issues/27673.\n\nBenchmark results:\n\nCPU became faster, GPU slower. To reach previous TH perf probably manual fusion is necessary.\n\n### WITH patch\n```\nCPU warmup 1000 took 7.997200009413064e-05\nCPU warmup 10000 took 0.0008116499957395718\nCPU warmup 100000 took 0.0012691459996858612\nCPU warmup TOTAL time 0.0021982479956932366\nCPU forward 1000 took 7.320100849028677e-05\nCPU forward 10000 took 0.00015837099635973573\nCPU forward 100000 took 0.0010471990099176764\nCPU forward 1000000 took 0.01238470000680536\nCPU forward 10000000 took 0.12747182900784537\nCPU forward 100000000 took 1.2076255190040683\nCPU forward TOTAL time 1.3488940890092636\nCPU for- & backward 1000 took 0.00032587299938313663\nCPU for- & backward 10000 took 0.0006926299975020811\nCPU for- & backward 100000 took 0.002146183993318118\nCPU for- & backward 1000000 took 0.019158899012836628\nCPU for- & backward 10000000 took 0.2957490350090666\nCPU for- & backward 100000000 took 1.7630806300003314\nCPU for- & backward TOTAL time 2.081367089995183\n\nGPU warmup 1000 took 0.0004558280052151531\nGPU warmup 10000 took 0.0002567449992056936\nGPU warmup 100000 took 0.0001593509950907901\nGPU warmup TOTAL time 0.0009442300070077181\nGPU forward 1000 took 0.00015061900194268674\nGPU forward 10000 took 0.00015258099301718175\nGPU forward 100000 took 0.00015409699699375778\nGPU forward 1000000 took 0.0008183339959941804\nGPU forward 10000000 took 0.004424853003001772\nGPU forward 100000000 took 0.04356115800328553\nGPU forward TOTAL time 0.04938192600093316\nGPU for- & backward 1000 took 0.0008062430133577436\nGPU for- & backward 10000 took 0.0006074949924368411\nGPU for- & backward 100000 took 0.0007091690058587119\nGPU for- & backward 1000000 took 0.001022183001623489\nGPU for- & backward 10000000 took 0.009945805999450386\nGPU for- & backward 100000000 took 0.0944173600000795\nGPU for- & backward TOTAL time 0.28060428200114984\n```\n\n### WITHOUT patch\n```\nCPU warmup 1000 took 6.394000956788659e-05\nCPU warmup 10000 took 0.00038220599526539445\nCPU warmup 100000 took 0.0034939230099553242\nCPU warmup TOTAL time 0.003981974994530901\nCPU forward 1000 took 4.7855006414465606e-05\nCPU forward 10000 took 0.000347569992300123\nCPU forward 100000 took 0.003367935001733713\nCPU forward 1000000 took 0.03605044000141788\nCPU forward 10000000 took 0.35935167300340254\nCPU forward 100000000 took 3.630371332008508\nCPU forward TOTAL time 4.029640004009707\nCPU for- & backward 1000 took 0.00028494100843090564\nCPU for- & backward 10000 took 0.0006738200027029961\nCPU for- & backward 100000 took 0.0051178760040784255\nCPU for- & backward 1000000 took 0.04925115800870117\nCPU for- & backward 10000000 took 0.7172313440096332\nCPU for- & backward 100000000 took 5.441953932997421\nCPU for- & backward TOTAL time 6.21466830400459\n\nGPU warmup 1000 took 0.001803738996386528\nGPU warmup 10000 took 0.00041877900366671383\nGPU warmup 100000 took 0.0003870719956466928\nGPU warmup TOTAL time 0.0026561370032140985\nGPU forward 1000 took 0.00037833399255760014\nGPU forward 10000 took 0.00038825398951303214\nGPU forward 100000 took 0.0003841099969577044\nGPU forward 1000000 took 0.0007090550061548129\nGPU forward 10000000 took 0.0016171559982467443\nGPU forward 100000000 took 0.013463679002597928\nGPU forward TOTAL time 0.017010531009873375\nGPU for- & backward 1000 took 0.0007374050037469715\nGPU for- & backward 10000 took 0.0006343529967125505\nGPU for- & backward 100000 took 0.0006375070079229772\nGPU for- & backward 1000000 took 0.0007550300069851801\nGPU for- & backward 10000000 took 0.002672752001672052\nGPU for- & backward 100000000 took 0.023170708998804912\nGPU for- & backward TOTAL time 0.20251446698966902\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28135\n\nDifferential Revision: D18001447\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: ad90dc1cca42dcaf3ea9e17e4f8fd79cee0a293e", "pr_number": "28135", "files_changed": ["aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/SoftMarginCriterion.cu", "aten/src/THCUNN/generic/SoftMarginCriterion.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/SoftMarginCriterion.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "fb30a48b4e": {"title": "add unsupported section (#31329)", "body": "Summary:\nAdd a section for unsupported ops, and modules. Automatically generate the properties and attributes that aren't bound, and for ops that have semantic mismatches set up tests so the docs stay up to date.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31329\n\nDifferential Revision: D19164472\n\nPulled By: eellison\n\nfbshipit-source-id: 46290bb8a64d9de928cfb1eda5ff4558c3799c88", "pr_number": "31329", "files_changed": ["docs/source/jit.rst", "docs/source/jit_unsupported.rst", "test/jit/unsupported_ops.py", "test/test_jit.py", "torch/jit/unsupported_tensor_ops.py"], "labels": ["jit", "merged"]}, "1f50cfc24d": {"title": "Throw a better error for int too big for int64_t", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29931\n\nPulled By: driazati\n\nDifferential Revision: D19124934\n\nfbshipit-source-id: 91841d7ba4f2f6142c51fba07b7faa14bb817e3a", "pr_number": "29931", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/tree_views.h"], "labels": ["jit", "merged"]}, "7692494c67": {"title": "Fix hex literal parsing (#29935)", "body": "Summary:\nStacked PRs\n * #29940 - [jit] Fix parsing of big float literals\n * **#29935 - [jit] Fix hex literal parsing**\n * #29931 - [jit] Throw a better error for int too big for int64_t\n\nPreviously these were all parsed as `0`\n](https://our.intern.facebook.com/intern/diff/19124944/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29935\n\nPulled By: driazati\n\nDifferential Revision: D19124944\n\nfbshipit-source-id: 1ee0c1dee589933363a5efba069a2cfaf94373c5", "pr_number": "29935", "files_changed": ["c10/util/string_utils.h", "test/test_jit.py", "torch/csrc/jit/script/tree_views.h"], "labels": ["jit", "merged"]}, "d08250c223": {"title": "fix zero-batch handling in convtranspose (#24341)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/24341\n\nConvTransposeOp doesn't crash for zero-batch, but it doesn't modify the output blob. This leads to buggy behaviour especially when running the same network twice using different input, or backprop during training.\n\nSeems `ConvTransposeUnpoolBase<Context>::GetOutputSize` works for zero-batch, so I remove the check for `input.numel() > 0`, and reshape the output blob before returning.\n\nFor CudnnConvTransposeGradientOp, it's a bit verbose to set `dfilter` and `dbias`, it's a  seems the Cudnn can handle it, so simply remove the `X.numel() == 0` branch.\n\nTest Plan: buck test mode/dev-nosan caffe2/caffe2/python/operator_test:conv_transpose_test -- --run-disabled\n\nReviewed By: BIT-silence\n\nDifferential Revision: D16807606\n\nfbshipit-source-id: 0d72c5bd8f2e03c34465e7b530cca548d9bdd5e1", "pr_number": "24341", "files_changed": ["caffe2/operators/conv_transpose_op_cudnn.cc", "caffe2/operators/conv_transpose_op_impl.h", "caffe2/operators/conv_transpose_op_mobile_impl.h", "caffe2/operators/conv_transpose_unpool_op_base.h", "caffe2/python/operator_test/conv_transpose_test.py"], "labels": ["caffe2", "merged", "module: pybind"]}, "ae2487bf4d": {"title": "Move TorchScript language reference to its own page (#31138)", "body": "Summary:\nStacked PRs\n * #31146 - [jit] Cleanup after moving language reference\n * **#31138 - [jit] Move TorchScript language reference to its own page**\n\nPreview: https://driazati.github.io/pytorch_doc_previews/jit.html#torchscript-language\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31138\n\nPulled By: driazati\n\nDifferential Revision: D19167375\n\nfbshipit-source-id: d37110d85fc8b8d2c741be49846e873de1357c2a", "pr_number": "31138", "files_changed": ["docs/source/jit.rst", "docs/source/jit_language_reference.rst"], "labels": ["merged"]}, "503a4e9019": {"title": "Cleanup after moving language reference (#31146)", "body": "Summary:\nStacked PRs\n * **#31146 - [jit] Cleanup after moving language reference**\n * #31138 - [jit] Move TorchScript language reference to its own page\n\nPreview: https://driazati.github.io/pytorch_doc_previews/jit.html#torchscript-language\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31146\n\nPulled By: driazati\n\nDifferential Revision: D19167390\n\nfbshipit-source-id: f28daed36754a553264fc8ac142ed22c3e26d63e", "pr_number": "31146", "files_changed": ["docs/source/jit.rst", "docs/source/jit_language_reference.rst", "torch/_jit_internal.py", "torch/jit/__init__.py", "torch/jit/supported_ops.py"], "labels": ["jit", "merged"]}, "148bcd3ee5": {"title": "Add support for builtins as attributes (#31269)", "body": "Summary:\nFixes #27495\n\nThis adds builtins as another piece of a concrete type. They're separate from normal functions since they represent the `BuiltinFunction` sugared value (which is a direct call to a builtin op). It also moves the builtins related logic from `jit/__init__.py` to `jit/_builtins.py` so it can be used from `jit/_recursive.py` to look up functions in the builtins table.\n](https://our.intern.facebook.com/intern/diff/19149779/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31269\n\nPulled By: driazati\n\nDifferential Revision: D19149779\n\nfbshipit-source-id: d4e5e5d7d7d528b75a2f503e6004394251a4e82d", "pr_number": "31269", "files_changed": ["test/jit/test_type_sharing.py", "test/test_jit.py", "torch/csrc/jit/script/concrete_module_type.cpp", "torch/csrc/jit/script/concrete_module_type.h", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/jit/__init__.py", "torch/jit/_builtins.py", "torch/jit/_recursive.py"], "labels": ["jit", "merged"]}, "1e80ff7a67": {"title": "autograd/profiler: make record_function more threadsafe (#31346)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31346\n\nThis makes it so that if profiling is enabled/disabled from a different thread while a RecordFunction span is active via an op it doesn't crash the process.\n\nWe currently see when using torch.distributed.rpc to enable/disable profiling on other nodes while other things are running.\n\nTest Plan: buck test //caffe2/test:autograd -- test_record_function\n\nReviewed By: albanD\n\nDifferential Revision: D19133258\n\nfbshipit-source-id: 30712b06c6aa051789948de2918dcfb9b78967ba", "pr_number": "31346", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/record_function.h", "torch/csrc/autograd/record_function_ops.cpp"], "labels": ["fb-exported", "merged"]}, "a3cdb7eca3": {"title": "Fix default instantation of dynamic quantized LSTM", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31433\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19164539\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 7045817ab3dfb530c4480a10523c4c6bcdbfc7eb", "pr_number": "31433", "files_changed": ["test/test_jit.py", "test/test_quantization.py", "torch/nn/quantized/dynamic/modules/rnn.py"], "labels": ["merged"]}, "d2e66b44cc": {"title": "Temporary fix to support building pytorch from fbsource (for xplat dependencies) (#31393)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31393\n\npytorch build was set up with the include paths (-I) relative to fbcode/. This works well for fbcode builds, but doesn't work for the new fbcode_deps args for xplat build targets that work across xplat and fbcode. When these targets are built, the include paths need to be relative to fbsource, so fbcode/ suffix needs to be added to those paths.\n\nLonger term, to properly fix this, we need to use raw_headers with public_include_directories specified for all of these targets.\n\nTest Plan: buck test mode/dev //papaya/integration/service/local/test:mnist_federated_system_test -- 'MnistFederatedSystemTest\\.test' --run-disabled\n\nReviewed By: mzlee\n\nDifferential Revision: D19148465\n\nfbshipit-source-id: a610e84bf4cad5838e54e94bae71b957c4b6d4b5", "pr_number": "31393", "files_changed": ["tools/build_variables.py"], "labels": ["fb-exported", "merged"]}, "e7d25a3e4d": {"title": "add a suggested alternative to _get_trace_graph", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31441\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19165646\n\nPulled By: suo\n\nfbshipit-source-id: 96a264bc55ceafd798d92b986d319cddbb0d9c69", "pr_number": "31441", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "e1509cb468": {"title": "Add support for `del` (#31273)", "body": "Summary:\nAdds the `del` keyword to the parser and corresponding `aten::Delete` op for lists and dicts\n\nFixes #20615\n](https://our.intern.facebook.com/intern/diff/19054937/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31273\n\nPulled By: driazati\n\nDifferential Revision: D19054937\n\nfbshipit-source-id: c535ea16a9e62d176f8ad45947670fc3535af77c", "pr_number": "31273", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_list_dict.py", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/script/compiler.cpp", "torch/csrc/jit/script/lexer.h", "torch/csrc/jit/script/parser.cpp", "torch/csrc/jit/script/python_tree_views.cpp", "torch/csrc/jit/script/tree_views.h", "torch/jit/frontend.py"], "labels": ["jit", "merged"]}, "fe707c7849": {"title": "Use `default_observer` and `default_weight_observer` in tests (#31424)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31424\n\natt\n\nTest Plan:\ntest_jit.py\n\nImported from OSS\n\nDifferential Revision: D19162368\n\nfbshipit-source-id: 33b95ba643eeeae942283bbc33f7ceda8d14c431", "pr_number": "31424", "files_changed": ["test/test_jit.py"], "labels": ["merged"]}, "dff7b945bf": {"title": "Avoid sending large unneeded data over wire in process_group_agent. (#31357)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31357\n\nIf a user selects a subset of a Tensor and sends it in an RPC, we were sending\nthe whole original Tensor Storage over the network.\n\nWhile this sounds reasonable, in practice, we observed view-like Tensors being sent\nover rpc, where only 1% of the data in the provided Tensor's Storage was\nactually used/needed.\n\nThe simple solution here is to just force a clone in the serializer code if we see that\nless than (arbitrary) half the bits are used, and the tensor is more than a nominal few KB.\nAdd related tests to ensure this doesn't break.\n\nAn alternate approach would be to modify the Pickler. That said, since Pickler is shared by more\ncomponents, the logic might be harder to tailor appropriately at that layer (particularly\ngiven that the Pickler has explicit logic to share a single Storage* among several Tensors\nthat commonly point to the same Storage*).\n\nIt's possible that we might want to further refine the basic thresholds in this change.\nIn practice, we've seen a mostly bimodal distribution thus far for the percent of Tensor\nStorage referred by a Tensor in observed rpcs (i.e. either 90%+ or sub-10% of the Storage\nreferenced), hence the existing 50% threshold here is probably not an unreasonable\nstarting point.\nghstack-source-id: 95925474\n\nTest Plan: buck test mode/dev caffe2/test/cpp/rpc/...\n\nDifferential Revision: D19137056\n\nfbshipit-source-id: e2b3a4dd0cc6e1de820fd0740aa1d59883dbf8d4", "pr_number": "31357", "files_changed": ["test/cpp/rpc/test_wire_serialization.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h"], "labels": ["merged"]}, "1bb6c51421": {"title": "Fix getAttribute (#31011)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31011\n\n`getAttribute` is supposed to throw when there the attribute is not\nfound rather than return a `nullptr`.\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D18898417\n\nfbshipit-source-id: 0fe7d824b978ad19bb5ef094d3aa560e9fc57f87", "pr_number": "31011", "files_changed": ["aten/src/ATen/core/jit_type.h", "test/cpp/api/serialize.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/script/object.h", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/csrc/jit/script/sugared_value.cpp"], "labels": ["merged"]}, "fb24f7c4ad": {"title": "catch all exceptions in converting default values to ivalues (#31398)", "body": "Summary:\nPreviously we would only catch `py::cast_error` which led to incomprehensible error messages like: `TypeError: 'NoneType' object is not iterable`. We are running arbitrary pybind code here, and not doing anything with the error message, so we should be less restrictive with the types of errors we catch.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31398\n\nDifferential Revision: D19166655\n\nPulled By: eellison\n\nfbshipit-source-id: 84db8b3714c718b475913f2f4bb6f19e62f2d9ec", "pr_number": "31398", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/init.cpp"], "labels": ["jit", "merged"]}, "489dd6cb90": {"title": "Add TORCH_DCHECK macro that checks only in debug builds (#31240)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31240\n\nFollow up on discoveries/discussions in https://github.com/pytorch/pytorch/pull/30810\n\nMimic the `DCHECK` macro from https://github.com/pytorch/pytorch/blob/e5eb871/c10/util/logging_is_not_google_glog.h#L117-L125\n\nWith this change the perf gap is eliminated:\n\n```\n================================================================================\nProgram Output:\n================================================================================\nRun on (36 X 1601 MHz CPU s)\n2019-12-12 20:12:13\n-----------------------------------------------------------------\nBenchmark                          Time           CPU Iterations\n-----------------------------------------------------------------\nBM_IntrusivePtrCtorDtor           23 ns         23 ns   30914703\nBM_SharedPtrCtorDtor              27 ns         27 ns   25895944\nBM_IntrusivePtrArray/16          503 ns        503 ns    1392139\nBM_IntrusivePtrArray/32         1006 ns       1006 ns     695749\nBM_IntrusivePtrArray/64         2013 ns       2013 ns     347714\nBM_IntrusivePtrArray/128        4024 ns       4024 ns     173964\nBM_IntrusivePtrArray/256        8047 ns       8047 ns      86994\nBM_IntrusivePtrArray/512       16106 ns      16106 ns      43461\nBM_IntrusivePtrArray/1024      32208 ns      32207 ns      21731\nBM_IntrusivePtrArray/2048      64431 ns      64430 ns      10865\nBM_IntrusivePtrArray/4096     128940 ns     128938 ns       5429\nBM_SharedPtrArray/16             503 ns        503 ns    1392128\nBM_SharedPtrArray/32            1006 ns       1006 ns     695940\nBM_SharedPtrArray/64            2012 ns       2012 ns     347817\nBM_SharedPtrArray/128           4024 ns       4023 ns     173927\nBM_SharedPtrArray/256           8069 ns       8069 ns      86741\nBM_SharedPtrArray/512          16143 ns      16142 ns      43357\nBM_SharedPtrArray/1024         32283 ns      32283 ns      21685\nBM_SharedPtrArray/2048         64718 ns      64717 ns      10817\nBM_SharedPtrArray/4096        129469 ns     129466 ns       5407\n================================================================================\n```\n```\n================================================================================\nProgram Output:\n================================================================================\nRun on (80 X 2001 MHz CPU s)\n2019-12-12 20:12:23\n-----------------------------------------------------------------\nBenchmark                          Time           CPU Iterations\n-----------------------------------------------------------------\nBM_IntrusivePtrCtorDtor           18 ns         18 ns   38630411\nBM_SharedPtrCtorDtor              22 ns         22 ns   32356114\nBM_IntrusivePtrArray/16          402 ns        402 ns    1739637\nBM_IntrusivePtrArray/32          805 ns        805 ns     869818\nBM_IntrusivePtrArray/64         1610 ns       1609 ns     434881\nBM_IntrusivePtrArray/128        3218 ns       3218 ns     217437\nBM_IntrusivePtrArray/256        6436 ns       6436 ns     108739\nBM_IntrusivePtrArray/512       12882 ns      12882 ns      54356\nBM_IntrusivePtrArray/1024      25763 ns      25763 ns      27177\nBM_IntrusivePtrArray/2048      51532 ns      51531 ns      13590\nBM_IntrusivePtrArray/4096     103091 ns     103091 ns       6778\nBM_SharedPtrArray/16             402 ns        402 ns    1740165\nBM_SharedPtrArray/32             804 ns        804 ns     869035\nBM_SharedPtrArray/64            1610 ns       1610 ns     434975\nBM_SharedPtrArray/128           3218 ns       3218 ns     217505\nBM_SharedPtrArray/256           6457 ns       6457 ns     108510\nBM_SharedPtrArray/512          12909 ns      12909 ns      54249\nBM_SharedPtrArray/1024         25810 ns      25810 ns      27127\nBM_SharedPtrArray/2048         51763 ns      51763 ns      13531\nBM_SharedPtrArray/4096        103506 ns     103505 ns       6759\n================================================================================\n```\n\nTest Plan:\nbuck test caffe2/c10/...\nbuck test mode/opt caffe2/c10/...\n\nDifferential Revision: D18998243\n\nfbshipit-source-id: ddf0a118a80efe032b52d403867c1f416c721590", "pr_number": "31240", "files_changed": ["c10/test/util/exception_test.cpp", "c10/test/util/intrusive_ptr_test.cpp", "c10/util/Exception.h", "c10/util/intrusive_ptr.h"], "labels": ["fb-exported", "merged"]}, "fc3103b116": {"title": "fixing a naming issue in creating a residual loop node in a bailout graph (#31400)", "body": "Summary:\nThis addresses the issue of differentiating between `%4` in\n`%12 : int, %y.1 : Tensor = prim::Loop(%9, %6, %4, %3)` and `%y.5 : Double(3) = aten::cat(%22, %4) # test_jit.py:3772:24` in `%4` loop's body in a residual continuation loop, because these should be different values.\n\n```\n[DUMP profiling_graph_executor_impl.cpp:124] with prim::BailoutTemplate_0 = graph(%z.1 : int,\n[DUMP profiling_graph_executor_impl.cpp:124]       %size.1 : int):\n[DUMP profiling_graph_executor_impl.cpp:124]   %2 : Tensor = prim::Constant[value= 1  1 [ CPUDoubleType{2} ]]()\n[DUMP profiling_graph_executor_impl.cpp:124]   %3 : Double(2) = prim::BailOut[index=0](%2, %z.1, %size.1)\n[DUMP profiling_graph_executor_impl.cpp:124]   %4 : int = prim::Constant[value=0]() # test_jit.py:3772:54\n[DUMP profiling_graph_executor_impl.cpp:124]   %5 : None = prim::Constant()\n[DUMP profiling_graph_executor_impl.cpp:124]   %6 : bool = prim::Constant[value=1]() # test_jit.py:3770:16\n[DUMP profiling_graph_executor_impl.cpp:124]   %counters.1 : int[] = prim::ListConstruct()\n[DUMP profiling_graph_executor_impl.cpp:124]   %8 : int = prim::Constant[value=8]()\n[DUMP profiling_graph_executor_impl.cpp:124]   %9 : int = aten::__round_to_zero_floordiv(%size.1, %8)\n[DUMP profiling_graph_executor_impl.cpp:124]   %10 : int = aten::mul(%9, %8)\n[DUMP profiling_graph_executor_impl.cpp:124]   %11 : int = aten::sub(%size.1, %10)\n[DUMP profiling_graph_executor_impl.cpp:124]   %12 : int, %y.1 : Tensor = prim::Loop(%9, %6, %4, %3) # test_jit.py:3770:16\n[DUMP profiling_graph_executor_impl.cpp:124]     block0(%i.2 : int, %15 : int, %y.7 : Tensor):\n[DUMP profiling_graph_executor_impl.cpp:124]       %17 : Double(2) = prim::BailOut[index=1](%y.7, %z.1, %counters.1, %9, %11, %i.2, %15)\n[DUMP profiling_graph_executor_impl.cpp:124]       %18 : int[] = aten::append(%counters.1, %15) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %19 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %20 : Tensor = aten::ones(%19, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %21 : Double(1) = prim::BailOut[index=2](%20, %z.1, %counters.1, %9, %11, %i.2, %15, %17)\n[DUMP profiling_graph_executor_impl.cpp:124]       %22 : Tensor[] = prim::ListConstruct(%17, %21)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.5 : Double(3) = aten::cat(%22, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %24 : int = prim::Constant[value=1]()\n[DUMP profiling_graph_executor_impl.cpp:124]       %25 : int = aten::add(%15, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %26 : int[] = aten::append(%counters.1, %25) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %27 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %28 : Tensor = aten::ones(%27, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %29 : Double(1) = prim::BailOut[index=3](%28, %z.1, %counters.1, %9, %11, %i.2, %y.5, %25)\n[DUMP profiling_graph_executor_impl.cpp:124]       %30 : Tensor[] = prim::ListConstruct(%y.5, %29)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.9 : Double(4) = aten::cat(%30, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %32 : int = aten::add(%25, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %33 : int[] = aten::append(%counters.1, %32) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %34 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %35 : Tensor = aten::ones(%34, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %36 : Double(1) = prim::BailOut[index=4](%35, %z.1, %counters.1, %9, %11, %i.2, %y.9, %32)\n[DUMP profiling_graph_executor_impl.cpp:124]       %37 : Tensor[] = prim::ListConstruct(%y.9, %36)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.10 : Double(5) = aten::cat(%37, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %39 : int = aten::add(%32, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %40 : int[] = aten::append(%counters.1, %39) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %41 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %42 : Tensor = aten::ones(%41, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %43 : Double(1) = prim::BailOut[index=5](%42, %z.1, %counters.1, %9, %11, %i.2, %y.10, %39)\n[DUMP profiling_graph_executor_impl.cpp:124]       %44 : Tensor[] = prim::ListConstruct(%y.10, %43)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.11 : Double(6) = aten::cat(%44, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %46 : int = aten::add(%39, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %47 : int[] = aten::append(%counters.1, %46) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %48 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %49 : Tensor = aten::ones(%48, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %50 : Double(1) = prim::BailOut[index=6](%49, %z.1, %counters.1, %9, %11, %i.2, %y.11, %46)\n[DUMP profiling_graph_executor_impl.cpp:124]       %51 : Tensor[] = prim::ListConstruct(%y.11, %50)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.12 : Double(7) = aten::cat(%51, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %53 : int = aten::add(%46, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %54 : int[] = aten::append(%counters.1, %53) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %55 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %56 : Tensor = aten::ones(%55, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %57 : Double(1) = prim::BailOut[index=7](%56, %z.1, %counters.1, %9, %11, %i.2, %y.12, %53)\n[DUMP profiling_graph_executor_impl.cpp:124]       %58 : Tensor[] = prim::ListConstruct(%y.12, %57)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.13 : Double(8) = aten::cat(%58, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %60 : int = aten::add(%53, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %61 : int[] = aten::append(%counters.1, %60) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %62 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %63 : Tensor = aten::ones(%62, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %64 : Double(1) = prim::BailOut[index=8](%63, %z.1, %counters.1, %9, %11, %i.2, %y.13, %60)\n[DUMP profiling_graph_executor_impl.cpp:124]       %65 : Tensor[] = prim::ListConstruct(%y.13, %64)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.14 : Double(9) = aten::cat(%65, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %67 : int = aten::add(%60, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       %68 : int[] = aten::append(%counters.1, %67) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %69 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %70 : Tensor = aten::ones(%69, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %71 : Double(1) = prim::BailOut[index=9](%70, %z.1, %counters.1, %9, %11, %i.2, %y.14, %67)\n[DUMP profiling_graph_executor_impl.cpp:124]       %72 : Tensor[] = prim::ListConstruct(%y.14, %71)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.15 : Tensor = aten::cat(%72, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %74 : int = aten::add(%67, %24)\n[DUMP profiling_graph_executor_impl.cpp:124]       -> (%6, %74, %y.15)\n[DUMP profiling_graph_executor_impl.cpp:124]   %75 : Double(10) = prim::BailOut[index=10](%y.1, %z.1, %counters.1, %11, %12)\n[DUMP profiling_graph_executor_impl.cpp:124]   %76 : int, %y : Tensor = prim::Loop(%11, %6, %12, %75) # test_jit.py:3770:16\n[DUMP profiling_graph_executor_impl.cpp:124]     block0(%i.1 : int, %79 : int, %y.6 : Tensor):\n[DUMP profiling_graph_executor_impl.cpp:124]       %81 : Double(*) = prim::BailOut[index=11](%y.6, %z.1, %counters.1, %11, %i.1, %79)\n[DUMP profiling_graph_executor_impl.cpp:124]       %82 : int[] = aten::append(%counters.1, %79) # test_jit.py:3771:20\n[DUMP profiling_graph_executor_impl.cpp:124]       %83 : int[] = prim::ListConstruct(%z.1)\n[DUMP profiling_graph_executor_impl.cpp:124]       %84 : Tensor = aten::ones(%83, %5, %5, %5, %5) # test_jit.py:3772:38\n[DUMP profiling_graph_executor_impl.cpp:124]       %85 : Double(1) = prim::BailOut[index=12](%84, %counters.1, %11, %i.1, %79, %81)\n[DUMP profiling_graph_executor_impl.cpp:124]       %86 : Tensor[] = prim::ListConstruct(%81, %85)\n[DUMP profiling_graph_executor_impl.cpp:124]       %y.4 : Tensor = aten::cat(%86, %4) # test_jit.py:3772:24\n[DUMP profiling_graph_executor_impl.cpp:124]       %88 : int = prim::Constant[value=1]()\n[DUMP profiling_graph_executor_impl.cpp:124]       %89 : int = aten::add(%79, %88)\n[DUMP profiling_graph_executor_impl.cpp:124]       -> (%6, %89, %y.4)\n[DUMP profiling_graph_executor_impl.cpp:124]   %90 : Double(12) = prim::BailOut[index=13](%y, %counters.1)\n[DUMP profiling_graph_executor_impl.cpp:124]   %91 : (Tensor, int[]) = prim::TupleConstruct(%90, %counters.1)\n[DUMP profiling_graph_executor_impl.cpp:124]   return (%91)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31400\n\nDifferential Revision: D19172750\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 85d3aac4e80b65b83b6be3c0bca8075a731a2b7e", "pr_number": "31400", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/bailout_graph.cpp"], "labels": ["jit", "merged"]}, "540b9da41e": {"title": "Bump numba version in circleCI config to 0.46.0. (#31435)", "body": "Summary:\nThe current numba version doesn't appear to actually work with our numba-cuda tests (numba.cuda.is_available()) fails.\n\nPrevious attempts to upgrade were blocked by https://github.com/numba/numba/issues/4368.\n\nIt's a bit unclear to me, but I believe 0.46.0 fixes the above version.  I'm verify that we catch that issue in CI via https://github.com/pytorch/pytorch/pull/31434.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31435\n\nDifferential Revision: D19166865\n\nPulled By: gchanan\n\nfbshipit-source-id: e01fa48c577e35de178423db7a7f79ac3dd3894d", "pr_number": "31435", "files_changed": [".circleci/docker/common/install_conda.sh"], "labels": ["merged"]}, "28376e826d": {"title": "Fix lint", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31463\n\nPulled By: driazati\n\nDifferential Revision: D19173580\n\nfbshipit-source-id: 6e5bb24949ec357c4d5b29a16d1733b664f21e05", "pr_number": "31463", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "c4121ed8db": {"title": "Fix is_fundamental template for MSVC (#30959)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30932\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30959\n\nDifferential Revision: D18891797\n\nPulled By: mingbowan\n\nfbshipit-source-id: e6c36ee80065e66117873e768f86f507c48aaef1", "pr_number": "30959", "files_changed": ["c10/util/TypeTraits.h", "c10/util/typeid.h", "caffe2/core/context.h", "caffe2/core/context_base.h", "caffe2/ideep/utils/ideep_context.h"], "labels": ["merged"]}, "d6acc87c93": {"title": "Guard against copying from quantized Tensor to non-quantized Tensor (#29660)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29660\n\natt\n\nTest Plan:\npython test/test_quantized_tensor.py\n\nImported from OSS\n\nDifferential Revision: D18799897\n\nfbshipit-source-id: 5d1b4ef84f5ae8eba830784b74485d78fa1e6fcf", "pr_number": "29660", "files_changed": ["aten/src/ATen/native/Copy.cpp", "test/test_quantized_tensor.py"], "labels": ["merged"]}, "49fe7a7401": {"title": "Updated documentation for NLLLoss to explain what x, y and w refer to (#31488)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/31385\n\nIn the current documentation for NLLLoss, it's unclear what `y` refers to in the math section of the loss description. There was an issue(https://github.com/pytorch/pytorch/issues/31295) filed earlier where there was a confusion if the loss returned for reduction=mean is right or not, perhaps because of lack in clarity of formula symbol description in the current documentation.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31488\n\nDifferential Revision: D19181391\n\nPulled By: anjali411\n\nfbshipit-source-id: 8b75f97aef93c92c26ecbce55b3faf2cd01d3e74", "pr_number": "31488", "files_changed": ["torch/nn/modules/loss.py"], "labels": ["merged"]}, "779b128872": {"title": "add back in reference to jit_unsupported section (#31486)", "body": "Summary:\nIt was added in https://github.com/pytorch/pytorch/pull/31329 and removed in a bad merge in https://github.com/pytorch/pytorch/pull/31138/\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31486\n\nDifferential Revision: D19181967\n\nPulled By: eellison\n\nfbshipit-source-id: 7e4b4a9b2042c30ec18f7f737bc4a9a56fac7d92", "pr_number": "31486", "files_changed": ["docs/source/jit.rst"], "labels": ["merged"]}, "9d9bc93bfb": {"title": "Added error message to indicate that reduction operations are not supported for dim>=64 (#31476)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/23159\nCurrently we don't support reduction operations for dim>=64 and we should give a descriptive RuntimeError indicating the same\nDiff: D19179039\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31476\n\nDifferential Revision: D19179039\n\nPulled By: anjali411\n\nfbshipit-source-id: 58568f64627bf3df6b3e00a1498544c030e74a0e", "pr_number": "31476", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "test/test_torch.py"], "labels": ["merged"]}, "8f3c0d541e": {"title": "Speed up `Tensor::has_names` for unnamed tensors (#31436)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31436\n\nTensor::has_names is slower than it should be for unnamed tensors\nbecause of the following:\n- it always tries to access the TLS for NamesMode. Unnamed tensors don't\nneed to peek at NamesMode to determine if they have names or not.\n- There is some virtual function being called because TensorImpl is in\nc10 and NamedTensorMeta is in libtorch.\n\nThis PR short-circuits Tensor::has_names for unnamed tensors by\nchecking if the underlying TensorImpl hold a pointer to NamedTensorMeta\nor not. If the NamedTensorMeta is nullptr; then the tensor is definitely\nunnamed.\n\nBenchmarks:\n- I have a dedicated benchmarking machine where I isolate a single CPU\nand make sure it runs at a fixed frequency.\n- I benchmarked torch.add, which calls `tensor::has_names` three times.\n- The TL;DR is that torch.add between size-1 unnamed tensors gets sped up\n~200ns after this change which is a 9% improvement.\n- Before, on my machine:\nhttps://gist.github.com/zou3519/dfd648a1941d584711d850754e0694bc\n- After on my machine:\nhttps://gist.github.com/zou3519/e78f0d8980b43d0d9c3e3e78ecd0d4d5\n\nTest Plan: - run tests\n\nDifferential Revision: D19166510\n\nPulled By: zou3519\n\nfbshipit-source-id: 1888a4e92d29152a5e3b778a95e531087e532f53", "pr_number": "31436", "files_changed": ["aten/src/ATen/templates/TensorMethods.h", "c10/core/TensorImpl.h"], "labels": ["merged"]}, "e67064a96f": {"title": "Exclude generated source docs from Google (#31484)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31484\n\nSee https://github.com/pytorch/pytorch/issues/26123 for context.\n\nPreviously, when someone googles for `pytorch \"adaptive_max_pool2d\"`,\nhttps://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html\nis the first result. This PR changes the docs build script to exclude\nall such generated source docs under `_modules/` from Google.\n\nIt does this by doing a search for `<head>` and then appending\n`<meta name=\"robots\" content=\"noindex\">`.\nThe [google developer\ndocs](https://support.google.com/webmasters/answer/93710?hl=en) suggest\nthat this is the right way to prevent google from indexing the page.\n\nIn the future, when the CI\nbuilds documentation (both master and stable docs), the newly created\ndocs under _modules will have the meta noindex tag.\n\nTest Plan:\n- I ran `find \"$install_path/_modules\" -name \"*.html\" -print0 | xargs -0\nsed -i '/<head>/a \\ \\ <meta name=\"robots\" content=\"noindex\">'` on a docs\nbuild locally and checked that it does indeed append the meta noindex\ntag after `<head>`.\n- In a few days we should rerun the search to see if these pages are\nstill being indexed.\n\nDifferential Revision: D19180300\n\nPulled By: zou3519\n\nfbshipit-source-id: 5f5aa95a85dd9f065607c2a16f4cdd24ed699a83", "pr_number": "31484", "files_changed": [".circleci/scripts/python_doc_push_script.sh"], "labels": ["merged"]}, "dbe2f265d0": {"title": "Better error msg for autograd profiler + multi-worker dataloader crash (#31473)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31473\n\nMitigates #6313\n\nA common use case for the autograd profiler is to use it to run over an\nentire model, including dataloading. The following will crash:\n- run autograd profiler in CUDA mode\n- Use a multi-worker DataLoader (presumably with the 'fork' spawn\nmethod)\n- because the autograd profiler initializes CUDA and forking after CUDA is\ninitialized is bad.\n\nThis PR puts in a nice error message when this happens so that users\naren't too confused. The new error message looks like:\nhttps://gist.github.com/zou3519/903f15c3e86bad4585b7e5ce14cc1b70\n\nTest Plan:\n- Tested locally.\n- I didn't add a test case for this because it's hard to write a test\ncase that doesn't completely stop the rest of our test suite from\nrunning.\n\nDifferential Revision: D19178080\n\nPulled By: zou3519\n\nfbshipit-source-id: c632525ba1f7b168324f1aa55416e5250f56a086", "pr_number": "31473", "files_changed": ["torch/autograd/profiler.py", "torch/csrc/autograd/profiler_cuda.cpp"], "labels": ["merged"]}, "226c2d79ce": {"title": "Get QScheme from observer module (#31293)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31293\n\nPreviously we check the number of elements in scale to determine if we are using per channel quantization,\nbut we should get qscheme information from observer module directly and we'll expose this information\nto caller as well\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19146669\n\nfbshipit-source-id: ea430eeae0ef8f441be39aa6dcc1bb530b065554", "pr_number": "31293", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "57caeb3fc1": {"title": "Fix builtins table (#31492)", "body": "Summary:\nFixes a bad merge that is breaking distributed tests on master\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31492\n\nPulled By: driazati\n\nDifferential Revision: D19180978\n\nfbshipit-source-id: f69f525e2c7f61194686f07cf75db00eb642882f", "pr_number": "31492", "files_changed": ["torch/jit/_builtins.py"], "labels": ["jit", "merged"]}, "348d42114e": {"title": "Kill MessageType::SHUTDOWN related logic in pg agent (#31270)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/pull/30330 got rid of the need to send a `MessageType::SHUTDOWN` message, so we can now remove the logic/utils for this type of message.\n\nI think we can also delete the enum entry in the `enum MessageType`, but we may want to keep it in case the logic in https://github.com/pytorch/pytorch/pull/30710 is ever moved to C++.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31270\n\nTest Plan: All existing unit tests pass\n\nDifferential Revision: D19146983\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 35b185411f9446d7d4dfc37a6cb5477cf041e647", "pr_number": "31270", "files_changed": ["torch/csrc/distributed/rpc/message.cpp", "torch/csrc/distributed/rpc/message.h", "torch/csrc/distributed/rpc/process_group_agent.cpp"], "labels": ["merged"]}, "457286a383": {"title": "fix missing type check in dictionary literal", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31375\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19145440\n\nPulled By: zdevito\n\nfbshipit-source-id: 69909089586149ef766b4858d3420864a81b2493", "pr_number": "31375", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/compiler.cpp"], "labels": ["jit", "merged"]}, "624088e444": {"title": "Don't dispatch to cudnn if it is not possible to make it 32bit by splitting batch dim (#31383)", "body": "Summary:\nAlso a step towards supporting 64bit indexing in convolution.\n\nSee also: https://github.com/pytorch/pytorch/pull/31379\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31383\n\nDifferential Revision: D19183443\n\nPulled By: ngimel\n\nfbshipit-source-id: 0c2030fac147e629d7be0c29f0683ec2b3f28c71", "pr_number": "31383", "files_changed": ["aten/src/ATen/native/ConvUtils.h", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/NNPACK.cpp", "aten/src/ATen/native/cudnn/Conv.cpp", "aten/src/ATen/native/miopen/Conv_miopen.cpp", "aten/src/ATen/native/mkldnn/Conv.cpp", "aten/src/ATen/native/mkldnn/Utils.cpp", "aten/src/ATen/native/mkldnn/Utils.h", "test/test_nn.py"], "labels": ["merged"]}, "06dbef663d": {"title": "Add support for `del` (#31273)", "body": "Summary:\nAdds the `del` keyword to the parser and corresponding `aten::Delete` op for lists and dicts\n\nFixes #20615\n](https://our.intern.facebook.com/intern/diff/19181473/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31273\n\nPulled By: driazati\n\nDifferential Revision: D19181473\n\nfbshipit-source-id: c42a2d43ec361a98e0c425232981edc9c39388c4", "pr_number": "31273", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_list_dict.py", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/script/compiler.cpp", "torch/csrc/jit/script/lexer.h", "torch/csrc/jit/script/parser.cpp", "torch/csrc/jit/script/python_tree_views.cpp", "torch/csrc/jit/script/tree_views.h", "torch/jit/frontend.py"], "labels": ["jit", "merged"]}, "4c341582ea": {"title": "modify model to enable loading by blob (#31507)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31507\n\nThis script is used to generate a model with bound shape inference and\nblob reorder, which are requirements for big model loading on T17.\n1. Load existing model.\n2. Do bound shape inference and blob reorder (put embedding blobs at the end).\n3. Save the modified model.\n\nTest Plan:\nGenerated a new moel and tested on NNPI.\nP124181047 (mismatch is AA variance)\n\nReviewed By: ipiszy\n\nDifferential Revision: D19165467\n\nfbshipit-source-id: c3522fc5dc53b7ec652420558e9e8bf65a1ccfae", "pr_number": "31507", "files_changed": ["caffe2/opt/bound_shape_inferencer.cc"], "labels": ["fb-exported", "merged"]}, "d0d6e0b5e3": {"title": "add type promotion support for sparse tensors (#30429)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30429\n\nalso fix a bug in uncoalesced division\n\nGeneral approach here is that we:\n* compute the common dtype based on input tensors\n* error if the output tensor is specified and the common type can't be cast back to the output type (e.g. for inplace ops)\n* convert input tensor (values) to the common dtype\n* perform the op as normal (computing at the common dtype instead of the result type).\n* convert/copy the result values back to that of the result tensor (for in-place ops).\n\nFor uncoalesced division we need to coalesce, because an integral tensor with values=[1,1] at the same index divided by 2 would give 1/2 + 1/2 =0 instead of 2/2=1.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19143223\n\nPulled By: nairbv\n\nfbshipit-source-id: 480fa334c0b2b3df046818f2342cfd4e2d9d892a", "pr_number": "30429", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "test/test_type_promotion.py"], "labels": ["merged"]}, "b38901aa15": {"title": "Test reading `__cuda_array_interface__` inferred strides. (#31451)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31451\n\nThe PR that fixed this, https://github.com/pytorch/pytorch/pull/24947, didn't add a test.\n\nFixes: https://github.com/pytorch/pytorch/issues/31443\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19170020\n\nPulled By: gchanan\n\nfbshipit-source-id: bdbf09989ac8a61b1b70bb1ddee103caa8ef435b", "pr_number": "31451", "files_changed": ["test/test_numba_integration.py"], "labels": ["merged"]}, "2099cfa13d": {"title": "Fix input_channels divisibility check in concat_split_op (#31448)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31448\n\nReplace `(!x%y)` with `(x%y != 0)`\n\nTest Plan: CI\n\nReviewed By: orionr\n\nDifferential Revision: D19165492\n\nfbshipit-source-id: 246635fb8ddd5823196bcef9d0e6cdf1c349015e", "pr_number": "31448", "files_changed": ["caffe2/operators/concat_split_op.cc"], "labels": ["fb-exported", "merged"]}, "6cd987e7c0": {"title": "Make fully_qualified_type_name_impl() compatible with VS2017 15.9 (#31455)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31455\n\nIn 15.9, __FUNCSIG__ unwraps using definitions as well as preserves noexcept qualifiers\n\nTest Plan: Build caffe2 on Windows using VS2017\n\nDifferential Revision: D19166204\n\nfbshipit-source-id: b6c5f70e5262d13adf585f77b92223cf5f1e78dd", "pr_number": "31455", "files_changed": ["c10/util/TypeIndex.h"], "labels": ["fb-exported", "merge-this-please", "merged"]}, "0b57b383b1": {"title": "Im2col export (#30972)", "body": "Summary:\nAdded im2col to opset 11.\nThis symbolic is used to export torch.nn.Unfold\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30972\n\nReviewed By: hl475\n\nDifferential Revision: D18946921\n\nPulled By: houseroad\n\nfbshipit-source-id: 13dd0cbae899700df32fd74d6dff1f29033a2b4c", "pr_number": "30972", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py"], "labels": ["merged"]}, "7a12ccd003": {"title": "optimize FloatToFused8BitRowwiseQuantized and Fused8BitRowwiseQuantizedToFloat (#31470)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31470\n\nOptimize performance of these two operators.\nAdditionally use nearbyint instead of round to be consistent with 4-bit embedding table quantization.\n\nReviewed By: hyuen\n\nDifferential Revision: D19072103\n\nfbshipit-source-id: efe96f14aeff7958cceb453ed625d3fd693891ff", "pr_number": "31470", "files_changed": ["caffe2/operators/fused_rowwise_8bit_conversion_ops.cc", "caffe2/operators/fused_rowwise_8bit_conversion_ops.h", "caffe2/perfkernels/fused_8bit_rowwise_conversion.cc", "caffe2/perfkernels/fused_8bit_rowwise_conversion.h", "caffe2/perfkernels/fused_8bit_rowwise_conversion_avx2.cc", "caffe2/python/fused_8bit_rowwise_conversion_ops_test.py"], "labels": ["fb-exported", "merged"]}, "256db1e61b": {"title": "Add fake parsing for torchbind classes in schema type parser", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31506\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19187722\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 4529409454d64393a821b8fa795db39bc82da8fc", "pr_number": "31506", "files_changed": ["torch/csrc/jit/script/schema_type_parser.cpp"], "labels": ["jit", "merged"]}, "5375ceae80": {"title": "run optimizations on pre-profiled graph (#31392)", "body": "Summary:\nThis is the first stab at running profile-insensitive optimizations on pre-profiled graphs. Running those optimizations has a potential to simplify graphs greatly before GuardElimination and GuardElimination should be able to remove more guards.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31392\n\nDifferential Revision: D19173639\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 2485a2a598c10f9b5445efb30b16439ad4551b3f", "pr_number": "31392", "files_changed": ["test/test_jit.py", "torch/csrc/jit/profiling_graph_executor_impl.cpp", "torch/csrc/jit/profiling_graph_executor_impl.h"], "labels": ["jit", "merged"]}, "df9d5b8a77": {"title": "Use macros instead of directly accessing Python object fields (#31388)", "body": "Summary:\nThe Python C API documentation states \"Access to the [PyObject]\nmembers must be done by using the macros Py_REFCNT and Py_TYPE.\"\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31388\n\nDifferential Revision: D19161790\n\nPulled By: colesbury\n\nfbshipit-source-id: ac9a3738c913ad290a6d3460d0d657ec5c13b711", "pr_number": "31388", "files_changed": ["torch/csrc/Exceptions.h", "torch/csrc/tensor/python_tensor.cpp"], "labels": ["merged"]}, "b4c48b7e29": {"title": "Call `getQSchemeAndQParamMap` later in `quantizeTensors` (#31406)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31406\n\nPreviously we record quantization parameters for a given value when we collect the observer nodes,\nbut actually the quantization parameter can vary depending on each module instance, to achieve\nthat, we need to delay the call to later stage and only record the `Value*` that's needed\nin `collectObserverNodesAndValueToQuantize` function\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19162369\n\nfbshipit-source-id: e0f97e322d18a281bf15b6c7bbb04c3dfacb512f", "pr_number": "31406", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "08de70cad1": {"title": "Remove observers in the end (#31407)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31407\n\nRemove observers in the end instead of before quantize tensor\nsince we still need them to find the quantization paramters for each module instance\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19162367\n\nfbshipit-source-id: f817af87183f6c42dc97becea85ddeb7e050e2b1", "pr_number": "31407", "files_changed": ["torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged"]}, "35b249769d": {"title": "Exclude lite interpreter Java files from OSS host build", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31204\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19200610\n\nPulled By: dreiss\n\nfbshipit-source-id: 0cf41c99b4c2604afc2dccfebbea213c0e1f9638", "pr_number": "31204", "files_changed": ["android/pytorch_android/host/build.gradle"], "labels": ["merged"]}, "11854bcd38": {"title": "Add test to torch.jit.export_opnames, make the _C function private", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31446\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19172851\n\nPulled By: iseeyuan\n\nfbshipit-source-id: f06d8766ed73c9abe4ebf41c402ee64880d745be", "pr_number": "31446", "files_changed": ["docs/source/torch.rst", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "3a19980b78": {"title": "Tensor class created from java does not call native methods", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31520\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D19199477\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: ba51454586a9385dba4ab73936f907346e0105d1", "pr_number": "31520", "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/src/main/java/org/pytorch/Tensor.java", "android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java"], "labels": ["merged"]}, "c808eed04a": {"title": "Nightly dimension, input shape in gradle (#30195)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30195\n\n1. Added flavorDimensions 'build' local/nightly\nto be able to test the latest nightlies\n\n```\ncls && gradle clean test_app:installMobNet2QuantNightlyDebug -PABI_FILTERS=x86 --refresh-dependencies && adb shell am start -n org.pytorch.testapp.mobNet2Quant/org.pytorch.testapp.MainActivity\n```\n\n 2. To be able to change all new model setup editing only `test_app/build.gradle`\n Inlined model asset file names to `build.gradle`\n\nExtracted input tensor shape to `build.gradle` (BuildConfig)\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18893394\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 1fae9989d6f4b02afb42f8e26d0f3261d7ca929b", "pr_number": "30195", "files_changed": ["android/gradle.properties", "android/test_app/app/build.gradle", "android/test_app/app/src/main/AndroidManifest.xml", "android/test_app/app/src/main/java/org/pytorch/testapp/CameraActivity.java", "android/test_app/app/src/main/java/org/pytorch/testapp/Constants.java", "android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java", "android/test_app/app/src/main/java/org/pytorch/testapp/Result.java", "android/test_app/app/src/main/java/org/pytorch/testapp/Utils.java", "android/test_app/app/src/main/res/layout/activity_camera.xml", "android/test_app/app/src/main/res/layout/texture_view.xml", "android/test_app/gradle.properties"], "labels": ["merged"]}, "3820d6f6b9": {"title": "make gc script python2 compatible (#31536)", "body": "Summary:\nget rid of f-string, somehow we still have python2\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31536\n\nDifferential Revision: D19204187\n\nPulled By: mingbowan\n\nfbshipit-source-id: da8e17e4dccdd6fd1b0e92eb4740f5a09a8a4209", "pr_number": "31536", "files_changed": [".circleci/ecr_gc_docker/gc.py"], "labels": ["merged"]}, "0b0f90f53c": {"title": "Split on batch dimension when 32bit indexing not enough for convolution forward (#31379)", "body": "Summary:\nPartially fixes https://github.com/pytorch/pytorch/issues/22496\n\nThis is just a first step towards the support of 64bit convolution on CUDA. In the forward of convolution, if the total tensor size is larger than 2^31, then we split it on the batch dimension. I want to get some review feedback before moving forward for the same splitting approach for backward.\n\nThere are real-world use cases that even when N=1 the input is still larger than 2^31. For this case, the splitting would be complicated, so I am planning to modify `use_cudnn` to just dispatch to the slow fallback kernel in PyTorch in a later PR.\n\nUpdate: `later PR` is https://github.com/pytorch/pytorch/pull/31383\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31379\n\nDifferential Revision: D19192018\n\nPulled By: ngimel\n\nfbshipit-source-id: c26ecc56319ac67c4d5302ffed246b8d9b5eb972", "pr_number": "31379", "files_changed": ["aten/src/ATen/native/cudnn/Conv.cpp", "test/test_nn.py"], "labels": ["merged"]}, "8d8e82883e": {"title": "set stream everytime when we get a cuBlas handle (#31537)", "body": "Summary:\nI don't see any reason for not doing so, because it is a common error that people forget to set the stream. And I don't think there is a reason for not running on the current stream.\n\nThis is just for cublas, cusparse and cudnn should be modified also.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31537\n\nDifferential Revision: D19206908\n\nPulled By: ngimel\n\nfbshipit-source-id: ba2b2b74e9847f0495c76dbc778751a9f23f8b36", "pr_number": "31537", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "aten/src/ATen/cuda/CublasHandlePool.cpp", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu", "aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu", "aten/src/THC/THCBlas.cu"], "labels": ["merged"]}, "b5bbec7bad": {"title": "set stream everytime when we get a cuSparse handle (#31538)", "body": "Summary:\ncuSparse version of https://github.com/pytorch/pytorch/pull/31537\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31538\n\nDifferential Revision: D19206895\n\nPulled By: ngimel\n\nfbshipit-source-id: a32c0bc310189a89a0098837438d62458b5c0a7c", "pr_number": "31538", "files_changed": ["aten/src/ATen/cuda/CuSparseHandlePool.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cu"], "labels": ["merged"]}, "700109eb63": {"title": "set stream everytime when we get a cuDNN handle (#31541)", "body": "Summary:\ncudnn version of https://github.com/pytorch/pytorch/pull/31537\n\nhttps://github.com/pytorch/pytorch/pull/31532 is a quick fix and this is a bigger change. This would deprecate https://github.com/pytorch/pytorch/pull/31532, but we could also merge https://github.com/pytorch/pytorch/pull/31532 first for a quick fix and then work on this later.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31541\n\nDifferential Revision: D19206753\n\nPulled By: ngimel\n\nfbshipit-source-id: 3352f923d13a9baf0971f64f8b7ce03e9a8b42b1", "pr_number": "31541", "files_changed": ["aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/cudnn/Handle.cpp", "aten/src/ATen/cudnn/Utils.h", "aten/src/ATen/native/cudnn/AffineGridGenerator.cpp", "aten/src/ATen/native/cudnn/BatchNorm.cpp", "aten/src/ATen/native/cudnn/Conv.cpp", "aten/src/ATen/native/cudnn/GridSampler.cpp", "aten/src/ATen/native/cudnn/LossCTC.cpp", "aten/src/ATen/native/cudnn/RNN.cpp"], "labels": ["merged"]}, "7d630278da": {"title": "Separate torchbind from Python (#30242)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30242\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29501\n\nCurrently blocked on schema serialization issue\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18463063\n\nPulled By: jamesr66a\n\nfbshipit-source-id: c12a1b644eb9bf04e68ff93cccf91d6cb3e75359", "pr_number": "30242", "files_changed": [".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/test.sh", "aten/src/ATen/core/custom_class.cpp", "aten/src/ATen/core/function_schema.h", "caffe2/CMakeLists.txt", "test/cpp/jit/test_custom_class.cpp", "test/custom_operator/CMakeLists.txt", "test/custom_operator/classes.cpp", "test/custom_operator/test_custom_classes.py", "test/test_jit.py", "tools/build_variables.py", "torch/CMakeLists.txt", "torch/_classes.py", "torch/csrc/jit/custom_class.cpp", "torch/csrc/jit/custom_class.h", "torch/csrc/jit/init.cpp", "torch/csrc/jit/operator.h", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/python_custom_class.cpp", "torch/csrc/jit/python_custom_class.h", "torch/csrc/jit/register_c10_ops.cpp", "torch/csrc/jit/script/schema_type_parser.cpp", "torch/custom_class.h"], "labels": ["fb-exported", "jit"]}, "cc2d5ca37f": {"title": "add enabled API to autograd profiler (#31380)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31380\n\nFor being able to profile async RPCs, we attach a `RecordFunction` object to the future that is created during the RPC to persist it across the lifetime of the RPC (this is implemented in the next PR: ). Since we'd only like to do this when profiling is enabled, this PR adds an enabled API to the autograd profiler.\nghstack-source-id: 96053933\n\nTest Plan: Modified unit test.\n\nDifferential Revision: D19050391\n\nfbshipit-source-id: aa382110e69d06b4a84c83b31d2bec2d8a81ba10", "pr_number": "31380", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler.cpp", "torch/csrc/autograd/profiler.h"], "labels": ["merged"]}, "fe76af96ed": {"title": "fix test_process_group_debug_info flaky test (#31533)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31533\n\nFixes this test that was flaky and has been disabled (see\nhttps://github.com/pytorch/pytorch/issues/31112)\nghstack-source-id: 96038999\n\nTest Plan: Run the test 1000 times and ensure that it passes.\n\nDifferential Revision: D19203366\n\nfbshipit-source-id: 7978cbb8ca0989a0a370a36349cdd4db3bb8345b", "pr_number": "31533", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "9459db86bf": {"title": "Raise warning for schedulers following chainable shedulers (#31125)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/29697.\n\nRaise warning for schedulers following chainable schedulers in https://github.com/pytorch/pytorch/issues/26423. See explanation for\n* [new warning when load/save](https://github.com/pytorch/pytorch/issues/29697#issuecomment-564655802)\n* [change from deprecation to user warning](https://github.com/pytorch/pytorch/issues/29697#issuecomment-564659775).\n\ngchanan -- This should go in the upcoming release following https://github.com/pytorch/pytorch/issues/26423.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31125\n\nDifferential Revision: D19143740\n\nPulled By: vincentqb\n\nfbshipit-source-id: 35b55fe6c5b39ca5a68b1a6e19f14eb95b9a784e", "pr_number": "31125", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["merged"]}, "68e5172382": {"title": "Support optional float parameters (float?, optional<double>). (#31517)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31517\n\nThis is going to be used by upsample (which currently uses magic values to represent optionals).\n\nFor now, we just introduce a fake function for testing (torch._test_optional_float(x)).\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19198721\n\nPulled By: gchanan\n\nfbshipit-source-id: 0a1382fde0927c5d277d02d62bfb31fb574b8c74", "pr_number": "31517", "files_changed": ["aten/src/ATen/native/Pooling.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native_parse.py", "test/test_jit.py", "test/test_torch.py", "tools/autograd/gen_python_functions.py", "tools/jit/gen_jit_dispatch.py", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h", "torch/csrc/utils/python_arg_parser.h"], "labels": ["jit", "merged"]}, "218cfd568d": {"title": "Conv transpose/backward split 32bit (#31510)", "body": "Summary:\nBasically the same as https://github.com/pytorch/pytorch/pull/31379 except for that I write a separate function `split_batch_dim_to_32bit_out` for the logic. This function could also be used for convolution forward, and I will rebase this PR after https://github.com/pytorch/pytorch/issues/31379 get merged and then change `raw_cudnn_convolution_forward_out` to use `split_batch_dim_to_32bit_out` here.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31510\n\nDifferential Revision: D19210563\n\nPulled By: ngimel\n\nfbshipit-source-id: e20bb82b6360aa2c0e449e127188c93f44e1e9b4", "pr_number": "31510", "files_changed": ["aten/src/ATen/native/cudnn/Conv.cpp", "test/test_nn.py"], "labels": ["merged"]}, "446e9af5b9": {"title": "Fix parsing of big float literals (#29940)", "body": "Summary:\nStacked PRs\n * **#29940 - [jit] Fix parsing of big float literals**\n * #29935 - [jit] Fix hex literal parsing\n * #29931 - [jit] Throw a better error for int too big for int64_t\n](https://our.intern.facebook.com/intern/diff/19186604/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29940\n\nPulled By: driazati\n\nDifferential Revision: D19186604\n\nfbshipit-source-id: 6ef66588a5cf956f281e7bd1e5584ef06f5296e9", "pr_number": "29940", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/tree_views.h"], "labels": ["jit", "merged"]}, "46ad80c839": {"title": "Fix null pointer dereference on Android for strtod_c (#31582)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31582\n\nD19124934 removed a dummy pointer passed to strtod_c() that's used only for Android (https://fburl.com/diffusion/zkv34jf1). Without it, jit parsing on Android start throwing SIGSEGV due to null pointer dereferencing. This diff adds the dummy pointer back.\n\nTest Plan: Tests\n\nReviewed By: driazati, shoumikhin\n\nDifferential Revision: D19221071\n\nfbshipit-source-id: 2e230c3fbfa873c3f7b92f73c87ee766ac182115", "pr_number": "31582", "files_changed": ["torch/csrc/jit/script/tree_views.h"], "labels": ["fb-exported", "jit", "merged"]}, "363d8be787": {"title": "Bypass _TorchScriptTesting_StackString::pop in BC check now (#31586)", "body": "Summary:\nFailed result: https://circleci.com/gh/pytorch/pytorch/4054919?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link/console\n\nOriginal PR: https://github.com/pytorch/pytorch/pull/30242\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31586\n\nReviewed By: hl475\n\nDifferential Revision: D19222086\n\nPulled By: houseroad\n\nfbshipit-source-id: 96db2bf18fa06eaebdd558e86615e26b95f34516", "pr_number": "31586", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "a54dc87e8e": {"title": "revert D18805532 and make numerics of masked adagrad consistent with unmasked adagrad (#30784)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30784\n\nInstead of putting experimental Masked*Adagrad to OSS, we decided to change D18805278 .\n\nTest Plan: CI\n\nReviewed By: chocjy\n\nDifferential Revision: D18824265\n\nfbshipit-source-id: 3d893fe6c441f2ff7af4c497cf81b9c49363e7a8", "pr_number": "30784", "files_changed": ["caffe2/operators/experimental/optimizers/masked_adagrad.cpp", "caffe2/operators/experimental/optimizers/masked_adagrad_test.py"], "labels": ["fb-exported", "merged"]}, "29f345831e": {"title": "Error out if legacy Tensor.new is called on alternate layouts / dtypes (#31485)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31485\n\nFixes: https://github.com/pytorch/pytorch/issues/22158\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19196499\n\nPulled By: gchanan\n\nfbshipit-source-id: a01ea7641b5fcd00a9d267243539ff64a5492e5f", "pr_number": "31485", "files_changed": ["test/test_mkldnn.py", "test/test_quantized_tensor.py", "torch/csrc/utils/tensor_new.cpp"], "labels": ["merged"]}, "866c1b1fcc": {"title": "Ensure legacy sparse constructor/new doesn't interpret python data as tensor data. (#31490)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31490\n\nWhen this happens, a dense tensor is constructed from a sparse constructor.\n\nFixes: https://github.com/pytorch/pytorch/issues/16154\n\nTest Plan: Imported from OSS\n\nReviewed By: cpuhrsch, mrshenli\n\nDifferential Revision: D19196498\n\nPulled By: gchanan\n\nfbshipit-source-id: 57a6324833e35f3e62318587ac74267077675b93", "pr_number": "31490", "files_changed": ["test/test_sparse.py", "torch/csrc/utils/tensor_new.cpp"], "labels": ["merged"]}, "5d95a9ca79": {"title": "Print all broken ops instead of the first one (#31628)", "body": "Summary:\nOriginally, we only print one broken schema. With this changeset, all the broken schemas are printed out.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31628\n\nReviewed By: hl475\n\nDifferential Revision: D19231444\n\nPulled By: houseroad\n\nfbshipit-source-id: 3dd5b4609a6a9a9046e95f2f30deb9beeb5dcd56", "pr_number": "31628", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "ec4e347744": {"title": "Add Python language reference docs (#30686)", "body": "Summary:\nThis exposes our audit of https://docs.python.org/3/reference/ with descriptions for each line item.\n\nTo generate the `.rst` from the Quip:\n\n```bash\npip install m2r\nm2r jit_language_reference.md\n```\n\nhttps://driazati.github.io/pytorch_doc_previews/30686/jit.html#python-functions-and-modules\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30686\n\nPulled By: driazati\n\nDifferential Revision: D19219587\n\nfbshipit-source-id: 249db9b5ee20e38804d4302bbfeca7d54f27d0bd", "pr_number": "30686", "files_changed": ["docs/source/jit.rst", "docs/source/jit_python_reference.rst"], "labels": ["merged"]}, "91eb7c26cd": {"title": "Fix Typos", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31630\n\nDifferential Revision: D19233162\n\nPulled By: zou3519\n\nfbshipit-source-id: c2716a2df2b2ccfeda7718b484e9605515ecdf01", "pr_number": "31630", "files_changed": ["aten/src/ATen/native/Normalization.cpp"], "labels": ["merged"]}, "39508501a4": {"title": "Create byte-aware word lstm benchmark (#31260)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31260\n\n1. Update the LiteLM dataset conversion script (fbcode/pytext/fb/tools/lite_lm_dataset_to_tensorproto.py)\n2. Created a benchmark json file for byte-aware lstm word model (xplat/aibench/specifications/models/caffe2/assistant/lite_lm_len5.json)\n3. In order to run the model -- created an int64 Tensor for the model, added batch gather ops to the BUCK file\n\nTest Plan:\n```\n1. Create tensorproto of the model input\nbuck run mode/opt //pytext/fb/tools:byte_lm_dataset_to_tensorproto -- --in-path /mnt/vol/pytext/smart_keyboard/aibench/test_5.txt --out-path /mnt/vol/pytext/smart_keyboard/aibench/byteAwareWordLM/ --hidden_dim 203 --layers_num 2 --max_seq_len 64 --max_byte_len 15\n\n2. Run the aibench command\nbuck run fbsource//xplat/aibench:run_bench -- -b aibench/specifications/models/caffe2/assistant/lm_byte_lstm_len5.json --remote --devices SM-G960U-8.0.0-26\n```\n\nReviewed By: gardenia22\n\nDifferential Revision: D17785682\n\nfbshipit-source-id: 351c3c8bae16449e72ac641522803b23a83349be", "pr_number": "31260", "files_changed": ["caffe2/python/utils.py"], "labels": ["fb-exported", "merged"]}, "909b8eba0d": {"title": "cudnn grouped convolution nhwc patch (#31444)", "body": "Summary:\nEarlier cudnn version doesn't support grouped convolution in NHWC well. Legit\nconfiguration in later cudnn version might return CUDNN_STATUS_NOT_SUPPORTED.\nWe are falling back to NCHW when runtime check of cudnn version is < 7.6.0 to\nkeep the logic simple.\n\nNote:\nWe might update the heuristics, 7.6.0 is very conservative.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31444\n\nDifferential Revision: D19232414\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 4c2d79ed347c49cd388bbe5b2684dbfa233eb2a3", "pr_number": "31444", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cudnn/Conv.cpp", "test/test_nn.py"], "labels": ["merged"]}, "4983ef8de1": {"title": "Integrating MaskedAdagrad", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31640\n\nTest Plan: unit test\n\nReviewed By: ellie-wen\n\nDifferential Revision: D18805278\n\nfbshipit-source-id: 1def4a89b7e4e04385c762bf127d95c5e513180e", "pr_number": "31640", "files_changed": ["caffe2/python/optimizer.py"], "labels": ["fb-exported", "merged"]}, "ffcac9ad37": {"title": "Clean White List for BC Checks (#31629)", "body": "Summary:\nDelete obsolete items\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31629\n\nReviewed By: hl475\n\nDifferential Revision: D19231522\n\nPulled By: houseroad\n\nfbshipit-source-id: 393ed630f7854b643c8fa8c5f3f576718934de96", "pr_number": "31629", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "204939b401": {"title": "Automatic update of fbcode/onnx to 57ebc587fcf3913b4be93653b0dd58c686447298 (#31642)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31642\n\nPrevious import was c08a7b76cf7c1555ae37186f12be4d62b2c39b3b\n\nIncluded changes:\n- **[57ebc587](https://github.com/onnx/onnx/commit/57ebc587)**: python_out does not recognize dllexport_decl. (#2482) <xkszltl>\n- **[477a9b87](https://github.com/onnx/onnx/commit/477a9b87)**: Edited PythonAPIOverview.md (#2491) <AlexMuresan>\n- **[59b9f908](https://github.com/onnx/onnx/commit/59b9f908)**: Minor correction type (#2411) <Jhuo IH>\n- **[cdc8b861](https://github.com/onnx/onnx/commit/cdc8b861)**: fix the optimize pass of fuse_consecutive_transposes (#2471) <XavierAtShanghai>\n- **[ad1f5567](https://github.com/onnx/onnx/commit/ad1f5567)**: Add clarification for bias quantization in QlinearConv Op spec (#2464) <Ashwini Khade>\n- **[d9a73ccc](https://github.com/onnx/onnx/commit/d9a73ccc)**: Add remove operator and function requirements to the add new op doc. (#2486) <Emad Barsoum>\n\nTest Plan: cont build\n\nReviewed By: hl475\n\nDifferential Revision: D19234753\n\nfbshipit-source-id: 4b7de1407d9b64e584f6e6d68cbe03fa1b4c854d", "pr_number": "31642", "files_changed": ["third_party/onnx"], "labels": ["fb-exported", "merged"]}, "b522a8e1ff": {"title": "Optimize zero length input (#31602)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31602\n\nPull Request resolved: https://github.com/pytorch/glow/pull/3943\n\nZero length input is something we hit fairly frequently in practice. Previous handling of global TensorPool involves two locks per input (acquire and reclaim). Here we use a specialized anchor tensor to host zero length input. Note that it is only padded to max sequence length. If necessary, an easy extension can be added to pad to max `InputPlaceholder.getType().size()`.\n\nReviewed By: jfix71\n\nDifferential Revision: D19192467\n\nfbshipit-source-id: cafdc1eb7bf9b9d6ead04a0243b0be838f6b71cd", "pr_number": "31602", "files_changed": ["caffe2/opt/onnxifi_op.h", "third_party/foxi"], "labels": ["fb-exported", "merged"]}, "e84e7ec556": {"title": "Kill aten_custom_call.", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/25613\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D17172503\n\nPulled By: gchanan\n\nfbshipit-source-id: 1456ecca8f459d008e335412cd7084bdfcb93439", "pr_number": "25613", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/function_wrapper.py", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/THC/generic/THCTensorMath.cu"], "labels": ["merged", "module: cpu", "module: cuda", "module: internals", "module: operators"]}, "35bee0c729": {"title": "separate op for rowwise counter (#31612)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31612\n\nCount the number recent update on rows. Exponential decay is applied on the counter with decay rate r, such that\n    r^{counter_halflife} = 0.5;\nIf counter_halflife is nonpositive, this operator is turned off.\n\nTest Plan: added unittest\n\nReviewed By: chocjy\n\nDifferential Revision: D19217921\n\nfbshipit-source-id: 96d850123e339212cc0e0ef352ea8a1b1bf61dfa", "pr_number": "31612", "files_changed": ["caffe2/python/operator_test/rowwise_counter_test.py", "caffe2/sgd/rowwise_counter.cc", "caffe2/sgd/rowwise_counter.h"], "labels": ["fb-exported", "merged"]}, "647569e546": {"title": "get rid of choco install (#30897)", "body": "Summary:\n7zip and cmake are part of base image, no need to re-install. Remove the install step can make build/test more stable.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30897\n\nDifferential Revision: D19232961\n\nPulled By: mingbowan\n\nfbshipit-source-id: fa3bbd1325839a2a977bf13fdbd97fda43793b8d", "pr_number": "30897", "files_changed": [".circleci/config.yml", ".circleci/scripts/vs_install.ps1", ".circleci/verbatim-sources/pytorch-job-specs.yml", ".circleci/verbatim-sources/windows-build-test.yml", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/installation-helpers/install_miniconda3.bat", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", "aten/src/ATen/Dispatch.h", "aten/src/ATen/core/TensorAccessor.h", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/function_wrapper.py", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/TensorBody.h", "c10/core/MemoryFormat.h", "c10/core/Scalar.h", "c10/core/ScalarType.h", "c10/util/ArrayRef.h", "c10/util/Deprecated.h", "c10/util/Exception.h", "test/test_jit.py", "test/test_nn.py", "torch/csrc/api/include/torch/nn/modules/container/named_any.h", "torch/csrc/utils/auto_gil.h", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["merged"]}, "ae214f67a5": {"title": "updated code to ensure error check for negative dims", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31636\n\nDifferential Revision: D19233031\n\nPulled By: anjali411\n\nfbshipit-source-id: c29265ddd1f887f1a0b98aca56a2691d7584353d", "pr_number": "31636", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "test/test_torch.py"], "labels": ["merged"]}, "90a187618e": {"title": "Integrate masked sparse Adagrad (#31641)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31641\n\nAssuming mask is provided as a tensor\n\nTest Plan: unit test\n\nReviewed By: ellie-wen\n\nDifferential Revision: D18928737\n\nfbshipit-source-id: a4f3dd51769c2b56e5890043e91c18e6128be082", "pr_number": "31641", "files_changed": ["caffe2/python/optimizer.py"], "labels": ["fb-exported", "merged"]}, "e8e47c0a1b": {"title": "Split RRef class into abstract RRef and RRefBase (#28942)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28942\n\nThe new abstract RRef class contains only user-facing RRef APIs.\nIt will be later moved to a common folder so that it can be shared\nby jit and distributed packages to provide TorchScript support.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18240590\n\nPulled By: mrshenli\n\nfbshipit-source-id: ac28cfc2c8039ab7131b537b2971ed4738710acb", "pr_number": "28942", "files_changed": ["tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref.cpp", "torch/csrc/distributed/rpc/rref.h", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/distributed/rpc/rref_interface.h"], "labels": ["merged"]}, "3b7916fccd": {"title": "Modify the order of arguments position of torch.std and torch.std_mean in doc (#31677)", "body": "Summary:\nChange log:\n\n- [x] Change the order of arguments position of torch.std and torch.std_mean in doc.\n- [x] Correct a spelling mistake of torch.std_mean in doc.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31677\n\nDifferential Revision: D19247372\n\nPulled By: ngimel\n\nfbshipit-source-id: 8685f5207c39be524cdc81250430beac9d75f330", "pr_number": "31677", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged"]}, "22d84204f7": {"title": "Expose torch.poisson in documentation (#31667)", "body": "Summary:\nChangelog:\n- Add doc string for torch.poisson briefing current behavior\n- Check for non-positive entries in the tensor passed as input to torch.poisson\n\nCloses https://github.com/pytorch/pytorch/issues/31646\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31667\n\nDifferential Revision: D19247371\n\nPulled By: ngimel\n\nfbshipit-source-id: b53d105e73bf59a45beeb566f47365c3eb74efca", "pr_number": "31667", "files_changed": ["aten/src/ATen/native/Distributions.cpp", "docs/source/torch.rst", "torch/_torch_docs.py"], "labels": ["merged"]}, "ee87b01f40": {"title": "add additional types to indexing operations dispatch (#31692)", "body": "Summary:\n- Fixes https://github.com/pytorch/pytorch/issues/31672\n- Adds Bfloat16 dispatch to the indexing operations that were missing it\n    - index_put on cuda does not have bfloat16 dispatch, because I'm not sure bfloat16 math ops work on cuda\n\nNote: `index_put_` with `accum=True` is enabled for `bool`, which does not make much sense, but I'm not the one who started it, so this behavior is preserved.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31692\n\nDifferential Revision: D19249561\n\nPulled By: ngimel\n\nfbshipit-source-id: 1269196194f7b9f611b32be198c001704731a78f", "pr_number": "31692", "files_changed": ["aten/src/ATen/AccumulateType.h", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/Indexing.cu", "test/test_indexing.py"], "labels": ["merged"]}, "6064223808": {"title": "`@slowTest` some slow tests (#31706)", "body": "Summary:\nThese are all the jit tests that take > 10 seconds according to `pytest test/test_jit.py --durations=15`\n\n```\n32.76s call     test/test_jit.py::TestModels::test_super_resolution\n32.20s call     test/test_jit.py::TestModels::test_neural_style\n30.90s call     test/test_jit.py::TestJit::test_export_batchnorm\n25.95s call     test/test_jit.py::TestJit::test_dropout_module_requires_grad\n22.24s call     test/test_jit.py::TestJitGeneratedModule::test_nn_Transformer\n12.38s call     test/test_jit.py::TestScript::test_fuser_double_float_codegen\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31706\n\nPulled By: driazati\n\nDifferential Revision: D19251567\n\nfbshipit-source-id: 8e76f717506b8bf28d1a63ce302feb0446dc9141", "pr_number": "31706", "files_changed": ["test/jit/test_models.py", "test/test_jit.py"], "labels": ["merged"]}, "dd0f2f0c19": {"title": "add float[] str[] constants (#31503)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31503\n\nAdd support for float lists and string lists constants, which enables better constant propagation + constant pooling + freezing.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19233558\n\nPulled By: eellison\n\nfbshipit-source-id: 4f7c6d9ddbe7623757a9a20606ce5f394e14e93d", "pr_number": "31503", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/cpp/jit/test_constant_propagation.cpp", "test/cpp/jit/test_irparser.cpp", "test/test_jit.py", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/passes/python_print.cpp"], "labels": ["jit", "merged"]}, "f4e955ff62": {"title": "Change PackSegments to ensure consistent behavior between CPU and GPU", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31673\n\nReviewed By: Wakeupbuddy, BIT-silence\n\nDifferential Revision: D18925762\n\nfbshipit-source-id: e0c318e97f69b14a54f43c176af57d98fbc16c9f", "pr_number": "31673", "files_changed": ["caffe2/operators/pack_segments.cc", "caffe2/python/operator_test/pack_ops_test.py"], "labels": ["fb-exported", "merged"]}, "39297bfe08": {"title": "Fix flaky test_debug_info. (#31675)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31675\n\nThis test could be flaky since there could be inflight RPC requests as\npart of startup which might not have finished. As a result, if they finish\nbetween the different calls to retrieve debug_info, there could be a problem\nsince we would report separate information. As a result, we wait to ensure\nthe metrics stabilize to avoid flakiness.\nghstack-source-id: 96188488\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19242588\n\nfbshipit-source-id: 8f3db7e7365acbd3742e6ec0c2ddcca68f27db9e", "pr_number": "31675", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "b102550d2c": {"title": "Allow to pass in masks through db (#31676)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31676\n\nFacebook\uff1a\n\nPreviously we assumed mask is passed in as a tensor which is not feasible for sparse parameter.\nHere we allow to pass in the mask through db path which requires the masks to be stored in some db first.\n\nTest Plan: unit tests\n\nReviewed By: ellie-wen\n\nDifferential Revision: D18928753\n\nfbshipit-source-id: 75ca894de0f0dcd64ce17b13652484b3550cbdac", "pr_number": "31676", "files_changed": ["caffe2/python/optimizer.py"], "labels": ["fb-exported", "merged"]}, "1499b894c4": {"title": "Apply clang-format to csrc/distributed/rpc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31681\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19247085\n\nPulled By: mrshenli\n\nfbshipit-source-id: ce6c1710663eecda3641d8dcf80ef16f9d21b93e", "pr_number": "31681", "files_changed": ["torch/csrc/distributed/rpc/python_call.cpp", "torch/csrc/distributed/rpc/python_resp.cpp", "torch/csrc/distributed/rpc/rref_proto.cpp"], "labels": ["merged"]}, "7a3ed36309": {"title": "Fix nvcc math functions for MSVC 2019 (#31704)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31108.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31704\n\nDifferential Revision: D19256110\n\nPulled By: mingbowan\n\nfbshipit-source-id: a4aba2830aba002497f70a75ef995e5e7de08393", "pr_number": "31704", "files_changed": ["aten/src/ATen/native/cuda/PowKernel.cu"], "labels": ["merged"]}, "236b0a318c": {"title": "Delete ATen/stub (#31763)", "body": "Summary:\nThis folder contained an empty CombinedStub file which isn't explicitly used anywhere.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31763\n\nDifferential Revision: D19262563\n\nPulled By: ezyang\n\nfbshipit-source-id: 5d095c93d6f7a1cc35f5919aa6006b31c2376b18", "pr_number": "31763", "files_changed": ["aten/src/ATen/stub/CombinedStub.cpp"], "labels": ["merged"]}, "c4f10e0fe7": {"title": "Renaming scales parameter for interpolate (#31526)", "body": "Summary:\nPR separated from https://github.com/pytorch/pytorch/pull/31274.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31526\n\nReviewed By: zou3519\n\nDifferential Revision: D19221931\n\nPulled By: gchanan\n\nfbshipit-source-id: 81958a9910867ac9d62f2b47abc49384526c4e51", "pr_number": "31526", "files_changed": ["aten/src/ATen/native/Pooling.cpp", "aten/src/ATen/native/UpSample.h", "aten/src/ATen/native/UpSampleBicubic2d.cpp", "aten/src/ATen/native/UpSampleBilinear2d.cpp", "aten/src/ATen/native/UpSampleLinear1d.cpp", "aten/src/ATen/native/UpSampleNearest1d.cpp", "aten/src/ATen/native/UpSampleNearest2d.cpp", "aten/src/ATen/native/UpSampleNearest3d.cpp", "aten/src/ATen/native/UpSampleTrilinear3d.cpp", "aten/src/ATen/native/cuda/UpSample.cuh", "aten/src/ATen/native/cuda/UpSampleBicubic2d.cu", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleNearest1d.cu", "aten/src/ATen/native/cuda/UpSampleNearest2d.cu", "aten/src/ATen/native/cuda/UpSampleNearest3d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_nearest2d.cpp", "test/backward_compatibility/check_backward_compatibility.py", "test/onnx/test_operators.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_jit.py", "test/test_nn.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/symbolic_script.cpp", "torch/nn/functional.py", "torch/onnx/symbolic_caffe2.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset8.py", "torch/onnx/symbolic_opset9.py"], "labels": ["jit", "merged"]}, "9e9bfbfd8d": {"title": "Update old scheduler example usage (#31358)", "body": "Summary:\nUpdate the old example usage in CosineAnnealingWarm, `scheduler.step()` should be called after `optimizer.step()`.\n\nhttps://github.com/pytorch/pytorch/issues/20028#issuecomment-566061580\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31358\n\nDifferential Revision: D19199311\n\nPulled By: vincentqb\n\nfbshipit-source-id: cb29b95f8277d2dfa75ec2a83c1af03a5c9c9a69", "pr_number": "31358", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["merged", "open source", "triaged"]}, "7078f4b27d": {"title": "skip _test_optional_float in BC check (#31786)", "body": "Summary:\nSkip _test_optional_float\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31786\n\nReviewed By: hl475\n\nDifferential Revision: D19265059\n\nPulled By: houseroad\n\nfbshipit-source-id: 6b95bd3b8cad83a4c459c0603befaaeeade6cdff", "pr_number": "31786", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "d770fbc1d2": {"title": "Some modifications to improve readability (#31352)", "body": "Summary:\nIn the long string, formalstring thinks it is good to have a name.\n\nWhen using dict, literal is better for readability and faster than dict constructor.\n\nI always appreciate your efforts in creating the world's best frameworks.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31352\n\nDifferential Revision: D19191967\n\nPulled By: ngimel\n\nfbshipit-source-id: 21f063b163b67de8cf9761a4db5991f74318e991", "pr_number": "31352", "files_changed": ["benchmarks/fastrnns/bench.py", "torch/serialization.py", "torch/utils/hooks.py"], "labels": ["merged", "open source", "triaged"]}, "ed5cd0d742": {"title": "Use numeric limits to define TensorTypeSet(FULL) representation (#31668)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31668\n\nThis also removes an annoying warning about change of sign conversion\n\nTest Plan: Run unit tests\n\nReviewed By: ezyang\n\nDifferential Revision: D19238631\n\nfbshipit-source-id: 29b50abac635e530d5b0453c3a0f36a4573fbf5b", "pr_number": "31668", "files_changed": ["c10/core/TensorTypeSet.h"], "labels": ["fb-exported", "merged"]}, "5f8308e32d": {"title": "Pin Pillow to v6 as PILLOW_VERSION is removed in v7", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31777\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19264247\n\nPulled By: mrshenli\n\nfbshipit-source-id: 52b0a3629e3a96ef2f9d3e289b9f7bb6a2745786", "pr_number": "31777", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml"], "labels": ["merged"]}, "155376721c": {"title": "Pin hypothesis package to 4.57.1 to avoid test failures", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31794\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19266039\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4b1839c4de2b4476c8173a79582c861bf4fa998f", "pr_number": "31794", "files_changed": [".jenkins/pytorch/macos-test.sh"], "labels": ["merged"]}, "dc43f9dc54": {"title": "fix test_backward_node_failure flakiness (#31588)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31588\n\nPer title. This test can sometimes fail with a different error regex\nthan the one that is currently tested, so add this error regex to make the test\npass consistently.\n\nDifferential Revision: D19222275\n\nfbshipit-source-id: 89c95276d4d9beccf9e0961f970493750d78a96b", "pr_number": "31588", "files_changed": ["test/dist_autograd_test.py"], "labels": ["merged"]}, "fa0424f224": {"title": "add LLVM-dev package to android docker image (#31215)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31215\n\nInstall LLVM-dev package for code analysis CI job: #30937\n\nLLVM-dev package is not related to android NDK but the whole code\nanalysis thing is for mobile custom build so choose this docker image.\n\nTest Plan: - wait docker image to build?\n\nDifferential Revision: D19193223\n\nPulled By: ljk53\n\nfbshipit-source-id: 54a79daf8d98fa7c8b9eed11f519e1c7b1614be8", "pr_number": "31215", "files_changed": [".circleci/docker/build.sh"], "labels": ["merged"]}, "5be8dac329": {"title": "Remove non-ascii character from torch/onnx/symbolic_opset11.py", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31814\n\nReviewed By: houseroad\n\nDifferential Revision: D19270742\n\nPulled By: bddppq\n\nfbshipit-source-id: 80800d588e63701d6e1b5838d7ada993f0246a81", "pr_number": "31814", "files_changed": ["torch/onnx/symbolic_opset11.py"], "labels": ["merged"]}, "f39105b68f": {"title": "add num_pending_users to debug info (#31539)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31539\n\nAdding this metric primarily because it is needed to unblock unit\ntests for https://github.com/pytorch/pytorch/pull/31381. It also may be useful\nto look at this metric to see the number of pending RRef forks that currently\nexist.\nghstack-source-id: 96230360\n\nTest Plan: Modified the relevant unit test.\n\nDifferential Revision: D19204158\n\nfbshipit-source-id: 016345e52cd02cc5f46837bffd8d589ba8575f29", "pr_number": "31539", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/rref_context.cpp"], "labels": ["merged"]}, "95cb66570a": {"title": "Erase array sizes from types in c10::str(). (#31683)", "body": "Summary:\nThis dramatically reduces the number of instantiations and eliminates\n~900KB of code from my local build of libtorch_cpu.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31683\n\nDifferential Revision: D19258364\n\nPulled By: resistor\n\nfbshipit-source-id: addb921a26289978ffd14c203325ca7e35a4515b", "pr_number": "31683", "files_changed": ["c10/util/StringUtil.h"], "labels": ["merged"]}, "f56c59ead6": {"title": "clarify when to use `as_tuple` in `torch.nonzero`", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31798\n\nDifferential Revision: D19272332\n\nPulled By: zou3519\n\nfbshipit-source-id: 954d086a7b9f1a719e0dac303a4253bf7ec8e9f4", "pr_number": "31798", "files_changed": ["torch/_torch_docs.py"], "labels": ["merge-this-please", "merged"]}, "8c425dd201": {"title": "Fix race condition when creating build dir (#30956)", "body": "Summary:\nThe original `check-and-act` style can raise `FileExistsError` when multiple processes are jit-compiling the extension on the same node.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30956\n\nDifferential Revision: D19262570\n\nPulled By: ezyang\n\nfbshipit-source-id: bb18c72e42648770b47f9378ac7c3929c3c03efc", "pr_number": "30956", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["merged"]}, "daf00beaba": {"title": "Remove duplicated Numa detection code. (#30628)", "body": "Summary:\ncmake/Dependencies.cmake (https://github.com/pytorch/pytorch/blob/1111a6b8100386af82f5612ef551004188cf396c/cmake/Dependencies.cmake#L595-L609) has already detected Numa. Duplicated detection and variables may lead to\nincorrect results.\n\nClose https://github.com/pytorch/pytorch/issues/29968\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30628\n\nDifferential Revision: D18782479\n\nPulled By: ezyang\n\nfbshipit-source-id: f74441f03367f11af8fa59b92d656c6fa070fbd0", "pr_number": "30628", "files_changed": ["c10/CMakeLists.txt", "c10/macros/cmake_macros.h.in", "c10/util/numa.cpp", "caffe2/core/macros.h.in", "cmake/MiscCheck.cmake"], "labels": ["merged", "module: build"]}, "0b9cd410a9": {"title": "Fix cumsum error for tensors with zero elements (#31694)", "body": "Summary:\nCurrently `cumsum` crashes for tensors with non-empty dimensions but with zero elements, which could happen when some dimension is zero. This commit fixes the error by checking both `dim()` and `numel()` in cumsum backward\n\nFixes https://github.com/pytorch/pytorch/issues/31515\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31694\n\nReviewed By: mrshenli\n\nDifferential Revision: D19266613\n\nPulled By: leedtan\n\nfbshipit-source-id: 9407e0aa55440fed911c01a3580bb6c5eab62a16", "pr_number": "31694", "files_changed": ["test/test_torch.py", "tools/autograd/templates/Functions.cpp"], "labels": ["merged"]}, "68f3782106": {"title": "remove std_single and var_single code in TH (#31608)", "body": "Summary:\nstd_single and var_single in TH never be used, remove them.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31608\n\nDifferential Revision: D19270920\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: e106a42383bf224f7e2c1c092b95484d23af4b0a", "pr_number": "31608", "files_changed": ["aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/THC/generic/THCTensorMathReduce.cu", "aten/src/THC/generic/THCTensorMathReduce.h"], "labels": ["merged"]}, "b47e9b97a2": {"title": "Add op bitwise_and (#31104)", "body": "Summary:\nRefer to https://github.com/pytorch/pytorch/pull/25665,  add `bitwise_and` operator.\nBenchmark script :\n```\nimport timeit\n#for __and__\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__and__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a & b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\")', number=t))\n#for __iand__\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__iand__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a & b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n```\nDevice: **Tesla P100, skx-8180**\nCuda verison: **9.0.176**\n\nBefore:\n```\n__and__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.1766007635742426\ndevice: cpu, dtype: torch.uint8, 100000 times           0.17322628945112228\ndevice: cpu, dtype: torch.int16, 100000 times           0.17650844901800156\ndevice: cpu, dtype: torch.int32, 100000 times           0.17711848113685846\ndevice: cpu, dtype: torch.int64, 100000 times           0.18240160401910543\ndevice: cuda, dtype: torch.int8, 100000 times           1.273967768996954\ndevice: cuda, dtype: torch.uint8, 100000 times          1.2778537990525365\ndevice: cuda, dtype: torch.int16, 100000 times          1.2753686187788844\ndevice: cuda, dtype: torch.int32, 100000 times          1.2797665279358625\ndevice: cuda, dtype: torch.int64, 100000 times          1.2933144550770521\n__and__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.031139614060521126\ndevice: cpu, dtype: torch.uint8, 10000 times            0.03091452084481716\ndevice: cpu, dtype: torch.int16, 10000 times            0.022756479680538177\ndevice: cpu, dtype: torch.int32, 10000 times            0.025045674294233322\ndevice: cpu, dtype: torch.int64, 10000 times            0.024164282716810703\ndevice: cuda, dtype: torch.int8, 10000 times            0.12820732593536377\ndevice: cuda, dtype: torch.uint8, 10000 times           0.12775669433176517\ndevice: cuda, dtype: torch.int16, 10000 times           0.12697868794202805\ndevice: cuda, dtype: torch.int32, 10000 times           0.12832533661276102\ndevice: cuda, dtype: torch.int64, 10000 times           0.1280576130375266\n__iand__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.3687064303085208\ndevice: cpu, dtype: torch.uint8, 100000 times           0.36253443732857704\ndevice: cpu, dtype: torch.int16, 100000 times           0.362891579978168\ndevice: cpu, dtype: torch.int32, 100000 times           0.37680106051266193\ndevice: cpu, dtype: torch.int64, 100000 times           0.3689364707097411\ndevice: cuda, dtype: torch.int8, 100000 times           1.419940729625523\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4247053815051913\ndevice: cuda, dtype: torch.int16, 100000 times          1.4191444097086787\ndevice: cuda, dtype: torch.int32, 100000 times          1.4305962566286325\ndevice: cuda, dtype: torch.int64, 100000 times          1.4567416654899716\n__iand__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.06224383972585201\ndevice: cpu, dtype: torch.uint8, 10000 times            0.06205617543309927\ndevice: cpu, dtype: torch.int16, 10000 times            0.05016433447599411\ndevice: cpu, dtype: torch.int32, 10000 times            0.05216377507895231\ndevice: cpu, dtype: torch.int64, 10000 times            0.06139362137764692\ndevice: cuda, dtype: torch.int8, 10000 times            0.14827249851077795\ndevice: cuda, dtype: torch.uint8, 10000 times           0.14801877550780773\ndevice: cuda, dtype: torch.int16, 10000 times           0.14952312968671322\ndevice: cuda, dtype: torch.int32, 10000 times           0.14999118447303772\ndevice: cuda, dtype: torch.int64, 10000 times           0.14951884001493454\n```\nAfter:\n```\n__and__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.23157884553074837\ndevice: cpu, dtype: torch.uint8, 100000 times           0.23063660878688097\ndevice: cpu, dtype: torch.int16, 100000 times           0.23005440644919872\ndevice: cpu, dtype: torch.int32, 100000 times           0.23748818412423134\ndevice: cpu, dtype: torch.int64, 100000 times           0.24106105230748653\ndevice: cuda, dtype: torch.int8, 100000 times           1.4394256137311459\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4436759827658534\ndevice: cuda, dtype: torch.int16, 100000 times          1.4631587155163288\ndevice: cuda, dtype: torch.int32, 100000 times          1.459101552143693\ndevice: cuda, dtype: torch.int64, 100000 times          1.4784048134461045\n__and__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.028442862443625927\ndevice: cpu, dtype: torch.uint8, 10000 times            0.028130197897553444\ndevice: cpu, dtype: torch.int16, 10000 times            0.025318274274468422\ndevice: cpu, dtype: torch.int32, 10000 times            0.02519288007169962\ndevice: cpu, dtype: torch.int64, 10000 times            0.028299466706812382\ndevice: cuda, dtype: torch.int8, 10000 times            0.14342594426125288\ndevice: cuda, dtype: torch.uint8, 10000 times           0.145280827768147\ndevice: cuda, dtype: torch.int16, 10000 times           0.14673697855323553\ndevice: cuda, dtype: torch.int32, 10000 times           0.14499565307050943\ndevice: cuda, dtype: torch.int64, 10000 times           0.14582364354282618\n__iand__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.25548241566866636\ndevice: cpu, dtype: torch.uint8, 100000 times           0.2552562616765499\ndevice: cpu, dtype: torch.int16, 100000 times           0.25905191246420145\ndevice: cpu, dtype: torch.int32, 100000 times           0.26635489892214537\ndevice: cpu, dtype: torch.int64, 100000 times           0.26269810926169157\ndevice: cuda, dtype: torch.int8, 100000 times           1.485458506271243\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4742380809038877\ndevice: cuda, dtype: torch.int16, 100000 times          1.507783885113895\ndevice: cuda, dtype: torch.int32, 100000 times          1.4926990242674947\ndevice: cuda, dtype: torch.int64, 100000 times          1.519851053133607\n__iand__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.03425929415971041\ndevice: cpu, dtype: torch.uint8, 10000 times            0.03293587639927864\ndevice: cpu, dtype: torch.int16, 10000 times            0.029559112153947353\ndevice: cpu, dtype: torch.int32, 10000 times            0.030915481969714165\ndevice: cpu, dtype: torch.int64, 10000 times            0.03292469773441553\ndevice: cuda, dtype: torch.int8, 10000 times            0.15792148280888796\ndevice: cuda, dtype: torch.uint8, 10000 times           0.16000914946198463\ndevice: cuda, dtype: torch.int16, 10000 times           0.1600684942677617\ndevice: cuda, dtype: torch.int32, 10000 times           0.16162546630948782\ndevice: cuda, dtype: torch.int64, 10000 times           0.1629159888252616\n```\nFix  https://github.com/pytorch/pytorch/issues/24508, https://github.com/pytorch/pytorch/issues/24509,  https://github.com/pytorch/pytorch/issues/24655, https://github.com/pytorch/pytorch/issues/24656.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31104\n\nDifferential Revision: D18938930\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: a77e805a0b84e8ace16c6e648c2f67dad44f2e44", "pr_number": "31104", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/THC/THCTensorMathPairwise.cu", "aten/src/THC/THCTensorMathPointwise.cuh", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPairwise.h", "aten/src/THC/generic/THCTensorMathPointwise.cu", "aten/src/THC/generic/THCTensorMathPointwise.h", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged"]}, "a02a5129a8": {"title": "Move rrelu to Aten(CPU) (#31094)", "body": "Summary:\nVitalyFedyunin, this PR is about port rrelu activation to Aten:\nTest script:\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\n\ndef _time():\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.RReLU(0.1, 0.3).train()\n# for inference\n#m = nn.RReLU(0.1, 0.3).eval()\n#warm up\nfor n in [1, 10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.randn(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [1, 10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.randn(128, n, device=device)\n    fwd_t = 0\n    bwd_t = 0\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\n**Before:**\n```\nTraining:\ninput size(128, 1) forward time is 0.01 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 10) forward time is 0.03 (ms); backwad avg time is 0.04 (ms).\ninput size(128, 100) forward time is 0.17 (ms); backwad avg time is 0.06 (ms).\ninput size(128, 1000) forward time is 1.45 (ms); backwad avg time is 0.07 (ms).\ninferecne:\ninput size(128, 1) forward time is 0.01 (ms).\ninput size(128, 10) forward time is 0.01 (ms).\ninput size(128, 100) forward time is 0.02 (ms).\ninput size(128, 1000) forward time is 0.15 (ms).\n```\n**After:**\n```\nTraining:\ninput size(128, 1) forward time is 0.01 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 10) forward time is 0.03 (ms); backwad avg time is 0.04 (ms).\ninput size(128, 100) forward time is 0.17 (ms); backwad avg time is 0.07 (ms).\ninput size(128, 1000) forward time is 1.43 (ms); backwad avg time is 0.08 (ms).\ninferecne:\ninput size(128, 1) forward time is 0.02 (ms).\ninput size(128, 10) forward time is 0.02 (ms).\ninput size(128, 100) forward time is 0.02 (ms).\ninput size(128, 1000) forward time is 0.03 (ms).\n```\n**OMP_NUM_THREADS=1:**\n```\nBefore:\nTraining:\ninput size(128, 1) forward time is 0.01 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 100) forward time is 0.15 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 1000) forward time is 1.45 (ms); backwad avg time is 0.14 (ms).\ninferecne:\ninput size(128, 1) forward time is 0.01 (ms).\ninput size(128, 10) forward time is 0.01 (ms).\ninput size(128, 100) forward time is 0.02 (ms).\ninput size(128, 1000) forward time is 0.20 (ms).\n\nAfter:\nTraining:\ninput size(128, 1) forward time is 0.01 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 100) forward time is 0.15 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 1000) forward time is 1.43 (ms); backwad avg time is 0.15 (ms).\ninferecne:\ninput size(128, 1) forward time is 0.01 (ms).\ninput size(128, 10) forward time is 0.02 (ms).\ninput size(128, 100) forward time is 0.02 (ms).\ninput size(128, 1000) forward time is 0.06 (ms).\n```\nFix https://github.com/pytorch/pytorch/issues/24755, https://github.com/pytorch/pytorch/issues/24756.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31094\n\nDifferential Revision: D19270936\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 11bb3236b1037a558022d3777d1f9a429af2bffe", "pr_number": "31094", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/THNN/generic/RReLU.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged"]}, "0ae063d5d9": {"title": "Fixed concatenation benchmark + added it to the microbenchmarking runs", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31587\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19221813\n\nPulled By: z-a-f\n\nfbshipit-source-id: ee0eb60da7899b23fdc63326302d1e2fd4b540ee", "pr_number": "31587", "files_changed": ["benchmarks/operator_benchmark/benchmark_all_quantized_test.py", "benchmarks/operator_benchmark/pt/qcat_test.py"], "labels": ["merged"]}, "79e30ff3f8": {"title": "optimize index_select performance on CPU with TensorIterator (#30598)", "body": "Summary:\nThis PR aims at improving `index_select` performance on CPU with `TensorIterator`.\nThe code has equally effective optimization for both contiguous tensor and non-contiguous tensor.\nThe code will try to parallel inner loop in case the slice of copy is large enough, otherwise it will parallel on outer loop.\nThus both the user scenarios from DLRM (from `Embedding`) and Fairseq transformer is covered.\n\n1. for contiguous input, single socket: **1.25x** performance speedup\n2. for non-contiguous input, single socket: **799x** performance speedup\n3. for contiguous input, single core: same performance\n4. for non-contiguous input, single core: **31x** performance speedup\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30598\n\nDifferential Revision: D19266892\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 7aaf8e2c861b4a96250c968c4dd95c8d2c5b92d7", "pr_number": "30598", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.h"], "labels": ["merged"]}, "b44c0f328e": {"title": "Skip same tests in ONNX Python3 CI as in Python2 (#31827)", "body": "Summary:\nresolve https://github.com/pytorch/pytorch/issues/31103\n\nvgg models were not tested in Python2 but are turned on in Python3\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31827\n\nReviewed By: houseroad\n\nDifferential Revision: D19274123\n\nPulled By: bddppq\n\nfbshipit-source-id: c48beb574e8b03b2adbd6c9d8ca3f600bee93024", "pr_number": "31827", "files_changed": ["scripts/onnx/test.sh"], "labels": ["merged"]}, "457c57d9f7": {"title": "use unordered_set instead of vector for futureTimeouts key in (#31813)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31813\n\nCloses https://github.com/pytorch/pytorch/issues/31804. We were using\nan `std::vector` for the key for a map that keeps track of futures to mark them\nif they timeout, but we can instead use an `unordered_set`. This results in a\nfaster lookup in the code block where we remove futureIDs from this set when\nthey complete successfully. Previously we were finding them via a linear\n`std::find`. Switching it to a constant time find will help performance in the\ncase where a large number of futures are scheduled to time out at the same\ntime, or if there is no timeout enforced.\n\nTo benchmark a rough perf improvement, I created 50k futures with the same\ntimeout. Before this PR, the lookup `std::find(futuresAtTime.begin(),\nfuturesAtTime.end(), id)` took ~200us, now it takes 1us.\nghstack-source-id: 96251355\n\nTest Plan: Unit tests pass.\n\nDifferential Revision: D19269798\n\nfbshipit-source-id: 1a0fa84a478ee27a16ab0b9fa6f5413b065a663e", "pr_number": "31813", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": ["merged"]}, "86a4e2135d": {"title": "Do not register `const float *` type on utiliy_ops.cu (#31583)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31583\n\nBut rather use `float *`, which is alredy registered\n\nTest Plan: CI\n\nReviewed By: xianjiec\n\nDifferential Revision: D19221405\n\nfbshipit-source-id: eb8eabcf828745022bc1e4185a0e65abd19a8f04", "pr_number": "31583", "files_changed": ["caffe2/operators/utility_ops.cu"], "labels": ["fb-exported", "merged"]}, "40e720282c": {"title": "Using _floats_wrapper in per_channel_tensor generation (#31780)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31780\n\nWe need to specify width to ensure the generated float is representable by `float32`\nfixes: https://github.com/pytorch/pytorch/issues/31774\n\nTest Plan:\nci\n\nImported from OSS\n\nDifferential Revision: D19275165\n\nfbshipit-source-id: 50560b4208c562b6bcd2abccadd234f29fbb4b0a", "pr_number": "31780", "files_changed": [".jenkins/pytorch/macos-test.sh", "test/hypothesis_utils.py", "test/test_quantized.py"], "labels": ["merged"]}, "9407137102": {"title": "Update the descriptive error message for enforce fail (#31575)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31575\n\nWe need a new exception class specifically for the enforce_finite operator, because we need to map it to a specific python exception ExitException, not the RuntimeError type that all c10::Errors get mapped to by default. This diff includes:\n- Define c10::EnforceFiniteNotMet\n- API CAFFE_ENFORCE_FINITE to throw c10::EnforceFiniteNotMet\n- Map from c10::EnforceFiniteNotMet to python ExitException\n- Apply CAFFE_ENFORCE_FINITE in caffe2 op\n\nTest Plan:\n- integration test pass: https://fburl.com/fblearner/xwkzbqyo\n- integration test with D19213617: https://fburl.com/fblearner/479y4jrj Generate error message as desired\n\n- Example:\n  - Original error message  f157597803\n{F225477055}\n\n  - Updated error message  (with D19213617 to generate the error): f158571327\n{F225477071}\n\nReviewed By: zheng-xq\n\nDifferential Revision: D19206240\n\nfbshipit-source-id: bd256862801d5957a26b76d738edf4e531f03827", "pr_number": "31575", "files_changed": ["c10/util/Exception.h", "c10/util/Logging.cpp", "c10/util/Logging.h", "caffe2/operators/enforce_finite_op.h"], "labels": ["fb-exported", "merged"]}, "6b1db202bc": {"title": "Add tanh to c10::cuda::compat (#31844)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31844\n\nAdd tanh to c10::cuda::compat\n\nTest Plan: unittest\n\nReviewed By: bddppq\n\nDifferential Revision: D19277230\n\nfbshipit-source-id: d2cceea58722393ecb90aacec05b692dbb92d467", "pr_number": "31844", "files_changed": ["c10/cuda/CUDAMathCompat.h"], "labels": ["fb-exported", "merged"]}, "c829c6f3d2": {"title": "Disable flaky test_debug_info", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31847\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19278009\n\nPulled By: mrshenli\n\nfbshipit-source-id: 652fa6741a48f35d9f8f54534e84d64fdd96b439", "pr_number": "31847", "files_changed": ["test/rpc_test.py"], "labels": ["merged"]}, "28c9dd4436": {"title": "fix ProcessGroupGlooTest (#31255)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31255\n\nThis test had 2 issues. A timeout would occasionally happen due to a timeout of 50ms, and CUDA could would get compiled and run on CPU, leading to errors. This PR fixes those issues.\n\nDifferential Revision: D19028231\n\nfbshipit-source-id: e50752228affe0021e7c0caa83bce78d76473759", "pr_number": "31255", "files_changed": ["torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["merged"]}, "e5b7231edc": {"title": "Adding version check for hypothesis deadline", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31262\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19036700\n\nPulled By: z-a-f\n\nfbshipit-source-id: 8e898a6f064dfb4876aa0d3cc299288b5af7b37d", "pr_number": "31262", "files_changed": ["test/hypothesis_utils.py"], "labels": ["merged"]}, "5fe3604987": {"title": "Preserve constant from ConcreteModuleType to ClassType (#29218)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29218\n\nWe need to be able to access constant in module.\n\nTest Plan:\ntbd\n\nImported from OSS\n\nDifferential Revision: D18846847\n\nfbshipit-source-id: 22d2c485c3c449bc14ad798f6e1a0c64fc8fb346", "pr_number": "29218", "files_changed": ["torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/script/concrete_module_type.cpp", "torch/csrc/jit/script/concrete_module_type.h", "torch/csrc/jit/script/python_sugared_value.cpp"], "labels": ["jit", "merged"]}, "b0a2765103": {"title": "move docker image html to correct bucket (#31832)", "body": "Summary:\nsave docker image version to docker.pytorch.org bucket to be served with http://docker.pytorch.org\n\ntest result: https://s3.amazonaws.com/docker.pytorch.org/pytorch.html\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31832\n\nDifferential Revision: D19281263\n\nPulled By: mingbowan\n\nfbshipit-source-id: d906a72d419876c81a570a2086b2d8d2c47d5d17", "pr_number": "31832", "files_changed": [".circleci/ecr_gc_docker/gc.py"], "labels": ["merged"]}, "8420f205ee": {"title": "Remove refs from ArrayRef arguments (#31845)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31845\n\nArrayRef is trivially copyable and should be passed by value. Removing\nunnecessary `&`s.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19278523\n\nPulled By: suo\n\nfbshipit-source-id: 026db693ea98d19246b02c48d49d1929ecb6478e", "pr_number": "31845", "files_changed": ["torch/csrc/jit/fuser/executor.cpp", "torch/csrc/jit/ir.cpp", "torch/csrc/jit/passes/alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.h", "torch/csrc/jit/passes/bailout_graph.cpp", "torch/csrc/jit/passes/utils/memory_dag.cpp", "torch/csrc/jit/passes/utils/memory_dag.h", "torch/lib/c10d/Utils.hpp"], "labels": ["jit", "merged"]}, "2bac76969c": {"title": "Fix getConstant (#31012)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31012\n\n- getConstant should throw when the item is not found\n- add another getConstant which takes slot index as argument\n\nTest Plan:\ntest_class_type.cpp\n\nImported from OSS\n\nDifferential Revision: D18898418\n\nfbshipit-source-id: d3a23a4896fdbf5fa98e1c55c9c4d6205840014b", "pr_number": "31012", "files_changed": ["aten/src/ATen/core/jit_type.h", "test/cpp/jit/test_class_type.cpp", "torch/csrc/jit/passes/python_print.cpp", "torch/csrc/jit/script/class_type.cpp", "torch/csrc/jit/script/python_sugared_value.cpp"], "labels": ["jit", "merged"]}, "6f62c311a1": {"title": "Add unsafeRemoveConstant for ClassType (#30787)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30787\n\nThis is needed when we fuse conv bn modules,\nwhere we need to rewrite a constant bias (None) of conv to an attribute\nbias of Tensor\n\nTest Plan:\nbuild/bin/test_jit\n\nImported from OSS\n\nDifferential Revision: D18846850\n\nfbshipit-source-id: 9fd5fe85d93d07226e180b75d2e068fe00ca25fe", "pr_number": "30787", "files_changed": ["aten/src/ATen/core/jit_type.h", "test/cpp/jit/test_class_type.cpp", "torch/csrc/jit/script/class_type.cpp"], "labels": ["jit", "merged"]}, "ebe69236d1": {"title": "Expose class constant through `attr` and `setattr` in object (#29219)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29219\n\nWe added class constant in previous PRs, this PR allows access to\nclass constant in the object API\n\nTest Plan:\nbuild/bin/test_jit\npython test/test_jit.py\n\nImported from OSS\n\nDifferential Revision: D18846851\n\nfbshipit-source-id: 888a6517d5f747d1f8ced283c0c2c30b2f6c72c6", "pr_number": "29219", "files_changed": ["test/cpp/api/serialize.cpp", "test/cpp/jit/test_module_api.cpp", "test/jit/test_class_type.py", "test/test_jit.py", "torch/csrc/jit/script/init.cpp", "torch/csrc/jit/script/object.h"], "labels": ["jit", "merged", "topic: quantize script"]}, "5579611544": {"title": "Enable foldbn tests (#29220)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29220\n\nSupport for accessing constant is added in previous\nPRs, this PR re-enables the foldbn tests\n\nTest Plan:\ntest_jit.py\n\nImported from OSS\n\nDifferential Revision: D18846848\n\nfbshipit-source-id: 90ceaf42539ffee80b984e0d8b2420da66c263c3", "pr_number": "29220", "files_changed": ["test/test_jit.py", "torch/csrc/jit/passes/quantization.cpp"], "labels": ["jit", "merged", "topic: quantize script"]}, "f362cd510d": {"title": "Move prim ops from JIT registration to C10 (#30612)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30612\n\nThe first version to move prim ops to c10 registration. After the reviewers are fine with the initial changes, more operators will be moved in the same style.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19237648\n\nPulled By: iseeyuan\n\nfbshipit-source-id: c5a519604efffb80564a556536f17d829f71d9f9", "pr_number": "30612", "files_changed": ["caffe2/CMakeLists.txt", "test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/tests.h", "tools/build_variables.py", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/register_mobile_ops.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/register_prim_ops_c10.cpp"], "labels": ["jit", "merged"]}, "502533cfe6": {"title": "Implement backend-agnostic rpc._wait_all_workers() utility (#30710)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30710\n\nWe need a backend-agnostic mechanism to do barrier-like operation before locally destroy RRef context and shutdown RPC Agent.\n\n- Sort worker names.\n- Elect the first name as the leader in the ordered worker names.\n- Followers reports therir intent to synchronize to the leader.\n- Leader also reports to itself, when `_wait_all_workers()` called.\n- If all workers report their intent to proceed, leader send the command to every one to proceed.\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork -- test_wait_all_workers\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_wait_all_workers$\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rref_leak\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rref_forward_chain\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_wait_all_workers\n\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_wait_all_workers$\n```\n\n# Stress runs\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\n# Debug\n\n```\nbuck test mode/dev-nosan caffe2/test:rpc_fork -- test_shutdown\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:dist_autograd_fork -- test_clean_context_during_backward\n\nbuck build mode/dev-nosan //caffe2/test:dist_autograd_fork\n\nbuck-out/gen/caffe2/test/dist_autograd_fork\\#binary.par -r test_clean_context_during_backward\n```\n\nhttps://our.intern.facebook.com/intern/testinfra/diagnostics/281475127895800.844424945328750.1575664368/\n\n```\nI1206 12:27:47.491420 185619 process_group_agent.cpp:211] Shutting down ProcessGroupAgent.\nI1206 12:27:47.493880 185630 process_group_agent.cpp:211] Shutting down ProcessGroupAgent.\nI1206 12:27:47.494526 185625 process_group_agent.cpp:211] Shutting down ProcessGroupAgent.\nI1206 12:27:47.495390 185636 process_group_agent.cpp:211] Shutting down ProcessGroupAgent.\nE1206 12:27:47.544198 185627 pair.cc:642] 1 --->>> 0, read ERROR: AsyncSocketException: Network error, type = Network error, errno = 104 (Connection reset by peer)\nE1206 12:27:47.544203 185633 pair.cc:642] 2 --->>> 0, read ERROR: AsyncSocketException: Network error, type = Network error, errno = 104 (Connection reset by peer)\nE1206 12:27:47.544210 185639 pair.cc:642] 3 --->>> 0, read ERROR: AsyncSocketException: Network error, type = Network error, errno = 104 (Connection reset by peer)\n```\nThis should mean the UDF in the request has been run, so Python proceeded and ran to `_agent.shutdown()`.\n\nWhile the RpcAgents on followers wanted to send back the response, but the leader has closed RPC.\n\nNeed to re-trigger \"pytorch_rpc-buck\" to reproduce the rare-seen issue.\n\nDifferential Revision: D18643137\n\nfbshipit-source-id: d669d4fc9ad65ed48bed1329a4eb1c32ba51323c", "pr_number": "30710", "files_changed": ["test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "3f0b330736": {"title": "corrected keyword argument name in docs for Tensor.scatter (#31617)", "body": "Summary:\nSee https://github.com/pytorch/pytorch/issues/31601\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31617\n\nDifferential Revision: D19268872\n\nPulled By: mruberry\n\nfbshipit-source-id: 52f0213f4aab991fd549b7623556a2ced61631a6", "pr_number": "31617", "files_changed": ["torch/_tensor_docs.py"], "labels": ["merged"]}, "fde94e7556": {"title": "Provide async mode for local autograd engine. (#31230)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31230\n\nA major issue with distributed autograd currently is that we block an\nRPC thread when we call Engine::execute_with_graph_task.\n\nTo resolve this issue, I've made modifications to the local autograd engine\nsuch that `execute_with_graph_task` returns a Future instead. The `execute()`\nmethods for Engine::execute() and DistEngine::execute() still wait() on this\nFuture which ensures there is no change in behavior yet.\n\nIn follow up PRs we can modify the distributed autograd engine to take\nadvantage of this Future.\n\nCloses #26359\nghstack-source-id: 96298057\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D18999709\n\nfbshipit-source-id: 388f54467fd2415a0acb7df17bd063aedc105229", "pr_number": "31230", "files_changed": ["test/test_utils.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_engine.h", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/utils/future.h"], "labels": ["merged"]}, "1ba1799a66": {"title": "C++ added 3rd arg of false to BatchNorm/InstanceNorm register_parameter \u2026 (#31873)", "body": "Summary:\nFix for issue https://github.com/pytorch/pytorch/issues/31680\nC++ BatchNorm & InstanceNorm attempt to register undefined tensors when affine is false.\n\nFixes https://github.com/pytorch/pytorch/issues/31680\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31873\n\nDifferential Revision: D19287087\n\nPulled By: yf225\n\nfbshipit-source-id: 0d57f10c49083386919b703d72b520a73a8e9e7f", "pr_number": "31873", "files_changed": ["torch/csrc/api/include/torch/nn/modules/batchnorm.h"], "labels": ["merged", "open source"]}, "ddff014b79": {"title": "fixed scale_factor calculation for uint8 tensor (#31778)", "body": "Summary:\nWhen calling the add_images() method on the tensorboard SummaryWriter with a uint8 NCHW tensor, the tensor is incorrectly scaled, resulting in overflow behavior. This leads to incorrect images being displayed in tensorboard.\n\nIssue: https://github.com/pytorch/pytorch/issues/31459\n\nLocal Testing (ran this code with and without the PR changes and printed scale_factor):\n\nimport torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\nx=torch.tensor([[[[1, 2, 3], [4, 5, 6]]]], dtype=torch.uint8)\nwriter.add_images(\"images\", x)\n\nBefore- scale_factor: 255, After- scale_factor: 1\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31778\n\nDifferential Revision: D19289189\n\nPulled By: anjali411\n\nfbshipit-source-id: 350a1650337244deae4fd8f8b7fb0e354ae6986b", "pr_number": "31778", "files_changed": ["test/test_tensorboard.py", "torch/utils/tensorboard/_utils.py"], "labels": ["merged"]}, "1f2b6d632a": {"title": "Refactor tests in pytorch's test/dist_autograd_test.py file (#31803)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31803\n\nRefactored the following fairly similar functions:\n  1. `test_context_cleanup_tensor_with_grad`\n  2. `test_context_cleanup_tensor_no_grad`\n  3. `test_context_cleanup_no_tensors`\nby creating a helper function `context_cleanup_test_helper` that can be invoked with the appropriate arguments.\n\nTest Plan: Verified by running tests.\n\nDifferential Revision: D19269246\n\nfbshipit-source-id: bfb42b078ad56b97ceeecf0d68b4169768c2c453", "pr_number": "31803", "files_changed": ["test/dist_autograd_test.py"], "labels": ["fb-exported", "merged"]}, "c65305e991": {"title": "Add a check method for custom type tensor (#31290)", "body": "Summary:\nFor backend integration, backend (e.g. Glow) needs to check the content of the tensor to determine whether it is a legit byte tensor or some special packed format. This provides a convenient interface for that.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31290\n\nReviewed By: jackm321, qizzzh\n\nDifferential Revision: D19069684\n\nPulled By: yinghai\n\nfbshipit-source-id: 63360fa2c4d32695fe9767a40027d446d63efdd4", "pr_number": "31290", "files_changed": ["aten/src/ATen/cpp_custom_type_hack.h"], "labels": ["merged"]}, "492ca46e71": {"title": "Fix androidTest - exclude host tests from it", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31522\n\nTest Plan: Imported from OSS\n\nReviewed By: dreiss\n\nDifferential Revision: D19200861\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: a6024f3013398f9e0d237e06c984a20493d42f11", "pr_number": "31522", "files_changed": ["android/pytorch_android/build.gradle"], "labels": ["merged"]}, "78cba90a8c": {"title": "Enable constant folding for Reshape (#31054)", "body": "Summary:\nEnabled constant folding for onnx::Reshape\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31054\n\nReviewed By: hl475\n\nDifferential Revision: D18946951\n\nPulled By: houseroad\n\nfbshipit-source-id: 499e8bf5fb091a94f7a27cbdf4311a23b1a6e3d3", "pr_number": "31054", "files_changed": ["test/onnx/test_onnx_opset.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/onnx/test_utility_funs.py", "torch/csrc/jit/passes/onnx/constant_fold.cpp"], "labels": ["jit", "merged", "open source"]}, "112196fdee": {"title": "Fix index put (#31552)", "body": "Summary:\nThis change is required for cases like:\nx[1:] = data or x[:3] = data\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31552\n\nReviewed By: hl475\n\nDifferential Revision: D19238815\n\nPulled By: houseroad\n\nfbshipit-source-id: 56c9837d86b341ea92b0a71d55034ce189d12e6c", "pr_number": "31552", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/csrc/jit/passes/onnx/prepare_inplace_ops_for_onnx.cpp"], "labels": ["jit", "merged", "open source"]}, "a9dae70bae": {"title": "Remove LibIRC logic from cmake. (#31152)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31152\n\nPer apaszke: I can't find any reasonable references to libIRC online, so\nI decided to remove this.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19262582\n\nPulled By: ezyang\n\nfbshipit-source-id: a1d47462427a3e0ca469062321d608e0badf8548", "pr_number": "31152", "files_changed": ["cmake/Modules/FindMKL.cmake"], "labels": ["merged"]}, "4ef9daf7b2": {"title": "Remove dead CAFFE2_LIBS variable (#31155)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31155\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19262584\n\nPulled By: ezyang\n\nfbshipit-source-id: 147ac5a9c36e813ea9a2f68b498880942d661be5", "pr_number": "31155", "files_changed": ["setup.py"], "labels": ["merged"]}, "58cffbff91": {"title": "Add missing TORCH_CUDA_API annotation to throw_nccl_error (#31157)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31157\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19262583\n\nPulled By: ezyang\n\nfbshipit-source-id: 8fb87b41ab53770329b38e1e2fe679fb868fee12", "pr_number": "31157", "files_changed": ["torch/csrc/cuda/nccl.h"], "labels": ["merged"]}, "5d80f63478": {"title": "no_grad, enable_grad: support for decorating generator functions (#31792)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/31497\n\nThis allows `torch.no_grad` and `torch.enable_grad` to be used as decorators for generator functions. In which case it disables/enables grad only inside the body of the generator and restores the context outside of the generator.\n\nhttps://github.com/pytorch/pytorch/issues/31497 doesn't include a complete reproducer but the included test with `torch.is_grad_enabled` show this is working where it failed before.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31792\n\nDifferential Revision: D19274971\n\nPulled By: albanD\n\nfbshipit-source-id: fde6d3fd95d76c8d324ad02db577213a4b68ccbe", "pr_number": "31792", "files_changed": ["test/test_autograd.py", "torch/autograd/grad_mode.py"], "labels": ["merged", "open source"]}, "34561dadcd": {"title": "Don't handle bias inside cudnn_convolution* (#31524)", "body": "Summary:\nCompared to cuDNN bias, PyTorch add has the following advantage:\n- faster, especially for backward (see: https://github.com/zasdfgbnm/things/blob/master/2019/conv-backward-profile.md)\n- handles 64bit indexing automatically\n- has less code, less maintenance effort\n\nngimel I submit this PR early so the CI could start building it. But I have not tested it locally yet (still waiting for compiling).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31524\n\nDifferential Revision: D19264244\n\nPulled By: ngimel\n\nfbshipit-source-id: cb483d378a6d8bce0a05c3643a796e544bd8e8f0", "pr_number": "31524", "files_changed": ["aten/src/ATen/native/ConvUtils.h", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cudnn/Conv.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "tools/autograd/derivatives.yaml"], "labels": ["merged", "open source"]}, "a561a8448b": {"title": "minor doc tweak to use mp.spawn in example (#30381)", "body": "Summary:\nPer pietern's comment in https://github.com/pytorch/pytorch/issues/30022, we can make this example launcher a bit simpler by using `torch.multiprocessing`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30381\n\nDifferential Revision: D19292080\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 018ace945601166ef3af05d8c3e69d900bd77c3b", "pr_number": "30381", "files_changed": ["docs/source/notes/distributed_autograd.rst"], "labels": ["merged"]}, "985fd970aa": {"title": "Enable BFloat16 support for Convolutions on ROCm (#30948)", "body": "Summary:\nThis PR adds bfloat16 support for convolutions on ROCm.\n\n- Intergrates MIOpen bfloat16 convolution support into PyTorch\n\n- Enables bfloat16 convolution for non-miopen paths, i.e THCUNN, native hip kernels\n\n- Enables bfloat16 type for probability distribution functions(this is included in this PR since conv unit tests use bfloat16 random number generators)\n\nNative cuda kernels for convolution and random functions will be compiled for CUDA as well.\n\niotamudelta bddppq\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30948\n\nDifferential Revision: D19274164\n\nPulled By: ezyang\n\nfbshipit-source-id: c0888a6ac72a2c5749b1ebb2195ac6f2209996be", "pr_number": "30948", "files_changed": ["aten/src/ATen/AccumulateType.h", "aten/src/ATen/Dispatch.h", "aten/src/ATen/miopen/Descriptors.cpp", "aten/src/ATen/miopen/Descriptors.h", "aten/src/ATen/miopen/Types.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu", "aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu", "aten/src/ATen/nn.yaml", "aten/src/ATen/nn_parse.py", "aten/src/THCUNN/SpatialConvolutionMM.cu", "aten/src/THCUNN/SpatialDepthwiseConvolution.cu", "aten/src/THCUNN/THCUNN.h", "aten/src/THCUNN/generic/SpatialConvolutionMM.cu", "aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu", "test/common_utils.py", "test/test_nn.py", "test/test_torch.py"], "labels": ["merged", "module: rocm", "open source"]}, "346a349111": {"title": "Update all instances of 1.4.0 -> 1.5.0 (#31785)", "body": "Summary:\nDone with:\n\n```\n\u276f sed -i 's/1\\.4\\.0/1.5.0/g' $(find -type f -not -path \"./third_party/*\")\n```\n\nThis was previously done in separate commits, but it would be beneficial to bump all included projects within this repository at the same time.\n\nOld bumps for reference:\n* [iOS]Update Cocoapods to 1.4.0: https://github.com/pytorch/pytorch/pull/30326\n* [android] Change nightly builds version to 1.4.0-SNAPSHOT: https://github.com/pytorch/pytorch/pull/27381\n* Roll master to 1.4.0: https://github.com/pytorch/pytorch/pull/27374\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31785\n\nDifferential Revision: D19277925\n\nPulled By: seemethere\n\nfbshipit-source-id: f72ad082f0566004858c9374879f4b1bee169f9c", "pr_number": "31785", "files_changed": [".circleci/scripts/binary_populate_env.sh", "android/README.md", "android/gradle.properties", "android/test_app/app/build.gradle", "ios/LibTorch.podspec", "version.txt"], "labels": ["merged"]}, "20c5dd59bd": {"title": "Add stub for transformer.py and MultiheadAttention Class. (#28396)", "body": "Summary:\nAdd stub for `transformer.py` and `class MultiheadAttention`. Add import for `transformer.py`  and `class MultiheadAttention` in `__init__.pyi.in`. I've tested the code hint in PyCharm and all works file.\nRelate issue: [https://github.com/pytorch/pytorch/issues/27842](https://github.com/pytorch/pytorch/issues/27842)\nezyang\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28396\n\nDifferential Revision: D19300287\n\nPulled By: ezyang\n\nfbshipit-source-id: 1a79d6518b5edd4643892c46a959108385c739ad", "pr_number": "28396", "files_changed": ["torch/nn/modules/__init__.pyi.in", "torch/nn/modules/activation.pyi.in", "torch/nn/modules/transformer.pyi.in"], "labels": ["merged", "open source"]}, "22044c6f7c": {"title": "Use TORCH_CHECK instead of AT_ASSERT in torch::cuda::gather() (#27456)", "body": "Summary:\nThe error message produced by AT_ASSERT() in gather() encouraged users to file a bug report (\"please report a bug to PyTorch...\"). The assertion should be a regular argument check since it can be triggered by passing tensors with different dimensionality, e.g. `torch.cuda.comm.gather([torch.rand(1, device='cuda'), torch.rand(1, 1, device='cuda')])`.\n\nSee: https://github.com/pytorch/pytorch/issues/26400\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27456\n\nDifferential Revision: D19300270\n\nPulled By: ezyang\n\nfbshipit-source-id: ec87d225e23445020b377521e0daccceb4748215", "pr_number": "27456", "files_changed": ["torch/csrc/cuda/comm.cpp"], "labels": ["merged", "module: cuda", "open source"]}, "809ee9d04c": {"title": "Enable personalized FC weight_init and sparse_emb weight_init (#31707)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31707\n\nChange the initialization value for FC weight init and sparse embedding lookup init.\n\nPrevious default initialization is uniform(-\\sqrt(1/input_dim), \\sqrt(1/input_dim)); Now pass into a flexible hyperparameter, say \\alpha into it, to change into uniform(-\\sqrt(\\alpha/input_dim), \\sqrt(\\alpha/input_dim));\n\nReviewed By: chonglinsun\n\nDifferential Revision: D18825615\n\nfbshipit-source-id: 4c5f2e07f2b3f5d642fd96d64dbf68892ebeb30b", "pr_number": "31707", "files_changed": ["caffe2/python/layers/fc.py", "caffe2/python/layers/fc_without_bias.py", "caffe2/python/layers/sparse_lookup.py"], "labels": ["fb-exported", "merged"]}, "3c7db5ccbc": {"title": "Don't unconditionally compile runJITCPPTests (#31236)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31236\n\nIt is not compiled on Windows\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19262581\n\nPulled By: ezyang\n\nfbshipit-source-id: 80bfa553333a946f00291aaca6ad26313caaa9e6", "pr_number": "31236", "files_changed": ["torch/CMakeLists.txt", "torch/csrc/jit/init.cpp"], "labels": ["jit", "merged"]}, "2f5eefe525": {"title": "Raise ValueError if CUDA device is specified without specifying the : (#29087)", "body": "Summary:\nFix for https://github.com/pytorch/pytorch/issues/19076\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29087\n\nDifferential Revision: D19298959\n\nPulled By: ezyang\n\nfbshipit-source-id: 878ea4840682012f07177d8d159a77c0e5afada6", "pr_number": "29087", "files_changed": ["test/test_cuda.py", "torch/serialization.py"], "labels": ["merged", "open source"]}, "4ee9c56218": {"title": "Support PyTorch ROCm CI on Ubuntu18.04 (#31886)", "body": "Summary:\nIn order to support Ubuntu18.04, some changes to the scripts are required.\n* install dependencies with -y flag\n* mark install noninteractive\n* install some required dependencies (gpg-agent, python3-distutils, libidn11)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31886\n\nDifferential Revision: D19300586\n\nPulled By: bddppq\n\nfbshipit-source-id: d7fb815a3845697ce63af191a5bc449d661ff1de", "pr_number": "31886", "files_changed": ["docker/caffe2/jenkins/common/install_clang.sh", "docker/caffe2/jenkins/common/install_python.sh", "docker/caffe2/jenkins/common/install_rocm.sh", "docker/caffe2/jenkins/ubuntu-rocm/Dockerfile"], "labels": ["merged", "module: rocm", "open source"]}, "5cc62f2913": {"title": "Ensure autograd callbacks are called only once for reentrant backward. (#31909)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31909\n\nhttps://github.com/pytorch/pytorch/pull/31230 introduced a bug where\nwe would end up calling `graph_task_post_processing` twice for reentrant\nbackward calls (once when we mark the future completed and then we we called\ngraph_task_post_processing in execute_with_graph_task).\n\nThis PR fixes the issues by verifying the future we return in that case is\ncompleted and we remove the call to graph_task_post_processing.\n\nIn addition to that I added a test that reproduced the problem and verified it\nis fixed by this PR.\nghstack-source-id: 96349102\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19296363\n\nfbshipit-source-id: dc01a4e95989709ad163bb0357b1d191ef5a4fb2", "pr_number": "31909", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/engine.cpp"], "labels": ["merged"]}, "0e5a6700cc": {"title": "Emit warning from deprecated torch function signatures (#31514)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/28430\n\nThe unpythonic signatures for functions such as `torch.addcdiv` are already seperated in [`deprecated.yaml`] and the signatures marked as deprecated in `PythonArgParser`. However, nothing was done with this information previously. So, this now emits a warning when the deprecated signatures are used.\n\nOne minor complication is that if all arguments are passed as keyword args then there is nothing to differentiate the deprecated overload. This can lead to false warnings being emitted. So, I've also modified `PythonArgParser` to prefer non-deprecated signatures.\n\n[`deprecated.yaml`]: https://github.com/pytorch/pytorch/blob/master/tools/autograd/deprecated.yaml\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31514\n\nDifferential Revision: D19298735\n\nPulled By: ezyang\n\nfbshipit-source-id: 03cb78af17658eaab9d577cd2497c6f413f07647", "pr_number": "31514", "files_changed": ["test/test_torch.py", "tools/autograd/gen_python_functions.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": ["merged", "open source", "triaged"]}, "bf8e1c0710": {"title": "Integrate async mode for autograd engine with distributed autograd. (#31508)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31508\n\nThis PR builds on top of https://github.com/pytorch/pytorch/pull/31230\nto ensure that distributed autograd doesn't block an RPC thread anymore during\nthe backward pass.\n\nI've also added a unit test where all ranks hammer rank 0 without about 60\nbackward calls (which would cause a deadlock earlier), but now such a test\npasses without any issues.\nghstack-source-id: 96345097\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19188749\n\nfbshipit-source-id: b21381b38175699afd0f9dce1ddc8ea6a220f589", "pr_number": "31508", "files_changed": ["test/dist_autograd_test.py", "test/dist_utils.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp"], "labels": ["merged"]}, "c888473b57": {"title": "Restructure docs organization and naming (#31849)", "body": "Summary:\n* Rename \u201cOther Languages\u201d \u2192 \u201cLanguage Bindings\u201d\n* Move the Community section to the bottom\n* Move \"Language Bindings\" above \"Python API\"\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31849\n\nDifferential Revision: D19290966\n\nPulled By: jlin27\n\nfbshipit-source-id: 30b579e032a9fb1636e4afc7bbbd85a2708f637d", "pr_number": "31849", "files_changed": ["docs/source/index.rst"], "labels": ["merged"]}, "74d69e296e": {"title": "Raise an error if torch.cat is given `out` as one of the input tensors (#30577)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30562 for both cpu and cuda.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30577\n\nDifferential Revision: D19298732\n\nPulled By: ezyang\n\nfbshipit-source-id: ea539c97493ee17d8f60b1134d100a44c8717578", "pr_number": "30577", "files_changed": ["aten/src/ATen/MemoryOverlap.cpp", "aten/src/ATen/MemoryOverlap.h", "aten/src/TH/generic/THTensor.cpp", "aten/src/THC/generic/THCTensorMath.cu", "test/test_torch.py"], "labels": ["merged", "open source", "triaged"]}, "3a2757c682": {"title": "Fix tracing for modules with List[Tensor] as output (#31343)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31343\n\nFix an issue in TorchScript tracing for modules with `c10::List<at::Tensor>` as an output. TensorList was not supported properly.\n\nTest Plan: unit tests\n\nReviewed By: wanchaol\n\nDifferential Revision: D18850722\n\nfbshipit-source-id: 87a223104d1361fe754d55deceeb1e8bbcad629b", "pr_number": "31343", "files_changed": ["test/test_jit.py", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h"], "labels": ["fb-exported", "jit", "merged"]}, "bb279c5c63": {"title": "named tensor max pooling support", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31669\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19240348\n\nPulled By: glaringlee\n\nfbshipit-source-id: 004387aa753e4e41afdede66647abbb0bcbd9808", "pr_number": "31669", "files_changed": ["aten/src/ATen/native/DilatedMaxPool2d.cpp", "aten/src/ATen/native/DilatedMaxPool3d.cpp", "aten/src/ATen/native/Pooling.cpp", "aten/src/ATen/native/cuda/DilatedMaxPool2d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_namedtensor.py"], "labels": ["merged"]}, "ca72df06ae": {"title": "disable __torch_function__ overides for operators in torch.functional (#30839)", "body": "Summary:\nFor now I'm just removing the decorators from all of the currently overridable functions in `torch.functional`. This means they are no longer overridable, however this should fix the benchmark regressions reported in https://github.com/pytorch/pytorch/issues/30831. Moving forward we'll be looking at reducing the overhead of the python-level override mechanism and failing that, re-implementing all of these operators in C++.\n\ncc hl475\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30839\n\nDifferential Revision: D18838848\n\nPulled By: ezyang\n\nfbshipit-source-id: 22b8015d7b2f7a947f1ebc9632c998e081b48ad8", "pr_number": "30839", "files_changed": ["test/test_overrides.py", "torch/functional.py"], "labels": ["merged", "open source"]}, "114562cf93": {"title": "For torch::from_blob() add clue when memory is non-owned. (#31222)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31222\n\n - When constructing torch::from_blob() in the case where the deleter is a nop, switch to using a nullptr context in the DataPtr (with a nop deleter)\n\n - No real extra memory/cpu requirements here, actually saves a minor alloc.\n\nWhy? Trying to get a signal that a Tensor might contain non-owned memory from\ntorch::from_blob(), by detecting the nullptr context.\nghstack-source-id: 96336078\n\nTest Plan:\nbuck test mode/dev caffe2/test/cpp/api/...\n   buck test mode/dev-nosan caffe2/test/...\n\nDifferential Revision: D18992119\n\nfbshipit-source-id: 4eea642f82d0858b57fdfc6995364a760c10567d", "pr_number": "31222", "files_changed": ["aten/src/ATen/templates/Functions.h", "test/cpp/api/tensor.cpp", "tools/autograd/templates/variable_factories.h"], "labels": ["merged"]}, "8a0503b355": {"title": "Run a non-quiet submodule update to prevent timeouts on Circle CI (#31900)", "body": "Summary:\nAs in title, this PR will disable the `--quiet` flag used in the CI as a workaround to a timeout hitting Mac OS CI.  Circle CI works by timing out when no text has been printed for 10 min.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31900\n\nDifferential Revision: D19302899\n\nPulled By: bwasti\n\nfbshipit-source-id: 145647da983ee06f40794bda1abd580ea45a0019", "pr_number": "31900", "files_changed": [".circleci/scripts/binary_checkout.sh"], "labels": ["merged"]}, "a730920a3d": {"title": "Make RRef leak detection always print a warning log (#31922)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31922\n\nFor better debugging, `test_rref_leak` failure in https://app.circleci.com/jobs/github/pytorch/pytorch/4135881, as per discussion in https://github.com/pytorch/pytorch/pull/31888.\n\nghstack-source-id: 96375261\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_wait_all_workers\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rref_leak\n```\n\n# Stress runs\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\nDifferential Revision: D19302814\n\nfbshipit-source-id: 51632aede98e01689f8bc0f266788a9b020daa15", "pr_number": "31922", "files_changed": ["test/rpc_test.py", "torch/csrc/distributed/rpc/rref_context.cpp"], "labels": ["merged"]}, "3c07eb33bb": {"title": "Better error for `torch::jit::load`ing a eager file (#31709)", "body": "Summary:\nThis adds a check to catch the case where someone `torch.save`s something then `torch::jit::load`s it in C++.\n\nRelevant for #31620\n](https://our.intern.facebook.com/intern/diff/19252172/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31709\n\nPulled By: driazati\n\nDifferential Revision: D19252172\n\nfbshipit-source-id: f2a9b4442647285418b2778306629b4ff77c15e5", "pr_number": "31709", "files_changed": ["test/cpp/jit/test_misc.cpp", "test/cpp/jit/tests.h", "test/cpp/jit/tests_setup.py", "test/cpp/jit/torch_python_test.cpp", "torch/csrc/jit/import.cpp"], "labels": ["jit", "merged"]}, "2d6a2c898c": {"title": "Support tensors with a storage offset in Java (#31584)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31584\n\nThese were returning incorrect data before.\n\nTest Plan: New unit test.\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D19221360\n\nfbshipit-source-id: b3f01de086857027f8e952a1c739f60814a57acd", "pr_number": "31584", "files_changed": ["android/pytorch_android/src/androidTest/java/org/pytorch/PytorchTestBase.java", "android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/test_asset.jit"], "labels": ["fb-exported", "merged"]}, "c21f89970f": {"title": "Remove c++14-conditional constexpr (#30916)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30916\n\nThese macros said \"make it constexpr if we're in C++14\". Since we're now always C++14, we can just say \"constexpr\" isntead.\nghstack-source-id: 96369584\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869635\n\nfbshipit-source-id: f41751e4e26fad6214ec3a98db2d961315fd73ff", "pr_number": "30916", "files_changed": ["c10/macros/Macros.h", "c10/test/util/string_view_test.cpp", "c10/util/Array.h", "c10/util/ArrayRef.h", "c10/util/reverse_iterator.h", "c10/util/string_view.h"], "labels": ["merged"]}, "0dca9c30ca": {"title": "constexpr typeid improvements (#31312)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31312\n\nghstack-source-id: 96369343\n\nTest Plan: unit tests\n\nDifferential Revision: D19087198\n\nfbshipit-source-id: 7f9a7169f11973759b9ecabcc755c211d34e2742", "pr_number": "31312", "files_changed": ["c10/util/typeid.h"], "labels": ["merged"]}, "ab60cca488": {"title": "Make c10::util::get_fully_qualified_type_name() backwards compatible with clang 4 (#31351)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31351\n\nClang 4 needs the c10:: namespace specifier on fully_qualified_type_name_impl() to work correctly.\n\nAlso, let's add an error message for people using clang 3 and earlier, we don't support those compilers anymore but before this PR, they got a crappy message.\nghstack-source-id: 96380163\n\nTest Plan: testinprod\n\nDifferential Revision: D19135587\n\nfbshipit-source-id: c206b56240b36e5c207fb2b69c389bb39f1e62aa", "pr_number": "31351", "files_changed": ["c10/util/C++17.h", "c10/util/TypeIndex.h"], "labels": ["merged"]}, "9116f02beb": {"title": "Rename TORCH_DCHECK to TORCH_INTERNAL_ASSERT_DEBUG_ONLY (#31917)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31917\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19301480\n\nPulled By: ezyang\n\nfbshipit-source-id: fcce8868733965b9fbd326b4ec273135759df377", "pr_number": "31917", "files_changed": ["c10/test/util/exception_test.cpp", "c10/util/Exception.h", "c10/util/intrusive_ptr.h"], "labels": ["merged"]}, "6664703842": {"title": "Implement backend-agnostic rpc._wait_all_workers() utility (#31888)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31888\n\nWe need a backend-agnostic mechanism to do barrier-like operation before locally destroy RRef context and shutdown RPC Agent.\n\n- Sort worker names.\n- Elect the first name as the leader in the ordered worker names.\n- Followers reports therir intent to synchronize to the leader.\n- Leader also reports to itself, when `_wait_all_workers()` called.\n- If all workers report their intent to proceed, leader send the command to every one to proceed.\nghstack-source-id: 96386210\n\nTest Plan:\n# Unit tests\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_wait_all_workers\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_rref_leak\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift\n\nbuck-out/gen/caffe2/test/rpc_fork\\#binary.par -r test_wait_all_workers\nbuck-out/gen/caffe2/test/rpc_fork_thrift\\#binary.par -r test_worker_id\n```\n\n# Stress runs\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_light_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_fork_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\n```\nbuck test mode/dev-nosan //caffe2/test:rpc_spawn_thrift -- test_stress_heavy_rpc --stress-runs 10\n```\n\nDifferential Revision: D19290954\n\nfbshipit-source-id: cdb22203c2f27b5e0d0ad5b2d3b279d438c22dcf", "pr_number": "31888", "files_changed": ["test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "ee817012b2": {"title": "Add more tests to the autograd wrt view and inplace (#31147)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31147\n\nThe goal here is to add more tests of the current behavior of the autograd to make sure no regressions are introduced when modifying it.\nDo let me know if you think of other corner cases I missed.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19301082\n\nPulled By: albanD\n\nfbshipit-source-id: 2cb07dcf99e56eb1f2c56a179796f2e6042d5a2d", "pr_number": "31147", "files_changed": ["test/test_autograd.py"], "labels": []}, "84dfa96f62": {"title": "Fix -Wundef warning in conversions.h", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31911\n\nTest Plan:\n* CI builds including GPU and OSS-build tests\n* The `defined(__HIP_DEVICE_COMPILE__) ` instance a few lines below is proof that this is a define/undef flag, not a define01 flag\n\nReviewed By: hlu1\n\nDifferential Revision: D19296560\n\nfbshipit-source-id: 1c45069aec534b0bf4a87751a74680675c985e06", "pr_number": "31911", "files_changed": ["caffe2/utils/conversions.h"], "labels": ["fb-exported", "merged"]}, "2a294aace6": {"title": "Remove memory ordering from LeftRight (#31026)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31026\n\nThis is error prone and probably wrong. Since we don't use LeftRight on the hot path anymore, let's remove this.\nghstack-source-id: 96369644\n\nTest Plan: none\n\nDifferential Revision: D18902165\n\nfbshipit-source-id: 7b9478cd7cc071f403d75da20c7c889c27248b5c", "pr_number": "31026", "files_changed": ["c10/util/LeftRight.h"], "labels": ["merged"]}, "f67851d69a": {"title": "Fix c10::util::get_fully_qualified_type_name for MSVC (#31313)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31313\n\nThis is a bugfix. The reason we couldn't enable the constexpr-ness for it before is that it was buggy,\nand without constexpr it crashed at runtime and not at compile time which seems to have passed our CI unfortunately...\nghstack-source-id: 96380160\n\nTest Plan: Now it works even when enabling constexpr for it\n\nDifferential Revision: D19087471\n\nfbshipit-source-id: 28be107389f4507d35d08eab4b089a405690529b", "pr_number": "31313", "files_changed": ["c10/test/util/TypeIndex_test.cpp", "c10/util/TypeIndex.h"], "labels": ["merged"]}, "f0072b3af5": {"title": "Remove C++11 compatibility from c10::optional (#30919)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30919\n\ndeletecode\nghstack-source-id: 96383227\n\nTest Plan: waitforsandcastle\n\nDifferential Revision: D18869641\n\nfbshipit-source-id: c08345d17a291cea3749af20473b6acddc78ab27", "pr_number": "30919", "files_changed": ["c10/util/Optional.h"], "labels": ["merged"]}, "c66ca74f03": {"title": "Add device debug info to CUDA build (#31929)", "body": "Summary:\nAlso print NVCC flags in the summary\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31929\n\nDifferential Revision: D19312079\n\nPulled By: ezyang\n\nfbshipit-source-id: cd20d5a385f61174c1907a9ad883c04de66ef037", "pr_number": "31929", "files_changed": ["CMakeLists.txt", "cmake/Summary.cmake"], "labels": ["merged", "open source"]}, "54777b1e73": {"title": "Avoid reference invalidation in cuda SpectralOps' plan_caches (#31861)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31412\n\nThe root cause is `plan_caches` being resized in one thread while another holds a reference to an existing `CuFFTParamsLRUCache` which then becomes invalidated.\n\nI was able to reproduce the crash very reliably without this fix applied and no longer see it. Being a race condition, it's hard to say for sure though.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31861\n\nDifferential Revision: D19312314\n\nPulled By: ezyang\n\nfbshipit-source-id: 06e4561128d503f2d70cdfe1982be0f3db2a8cf8", "pr_number": "31861", "files_changed": ["aten/src/ATen/native/cuda/SpectralOps.cu"], "labels": ["merged", "open source"]}, "620060cb0c": {"title": "Quantized H Tangent function (#31031)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31031\n\nThis activation will be needed for the LSTM implementation.\nAlso includes the QNNPack implementation.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18903453\n\nPulled By: z-a-f\n\nfbshipit-source-id: 0050b1cebb1ddb179b7ecbcb114fe70705070f67", "pr_number": "31031", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/src/tanh.c", "aten/src/ATen/native/quantized/cpu/qnnpack/test/tanh-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qtanh.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/test_quantized.py"], "labels": ["merged"]}, "462bfc7fe7": {"title": "docker hub image info (#31923)", "body": "Summary:\nresult: http://docker.pytorch.org/docker_hub.html\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31923\n\nDifferential Revision: D19316770\n\nPulled By: mingbowan\n\nfbshipit-source-id: 57f34d8983d26772bb0d310fa0a4085674c860e5", "pr_number": "31923", "files_changed": [".circleci/config.yml", ".circleci/ecr_gc_docker/Dockerfile", ".circleci/ecr_gc_docker/docker_hub.py", ".circleci/ecr_gc_docker/requirements.txt", ".circleci/validate-docker-version.py", ".circleci/verbatim-sources/docker_jobs.yml", ".circleci/verbatim-sources/workflows-ecr-gc.yml"], "labels": ["merged"]}, "c299cb05ef": {"title": "temporary fix for jit test backward compatibility issues", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31949\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19314763\n\nPulled By: albanD\n\nfbshipit-source-id: b5eff0ed53a371d260596ca85d914c8bddb0a8aa", "pr_number": "31949", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "1314f7f4f4": {"title": "Ensure the original grad_mode is restored during backward (#31884)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31884\n\nFix #31715\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19301076\n\nPulled By: albanD\n\nfbshipit-source-id: 2d20c01bfb6364fa96c8fe5aa5ce7ea39defa3ce", "pr_number": "31884", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/engine.cpp"], "labels": ["merged"]}, "8b4feff01d": {"title": "Use simd version for fp16 conversions (#31897)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31897\n\nPrevious version only use avx2. The _simd version uses avx512 if CPU is capable of that.\n\nTest Plan: Unitttest\n\nReviewed By: tracelogfb\n\nDifferential Revision: D19291499\n\nfbshipit-source-id: 3b1ee0ba756e5c9defbd5caf7f68982d9b2ca06c", "pr_number": "31897", "files_changed": ["caffe2/operators/half_float_ops.cc"], "labels": ["fb-exported", "merged"]}, "d2fdf140af": {"title": "Combine all the user inputs together and convert them to fp16 (#31898)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31898\n\nAtt\n\nReviewed By: tracelogfb\n\nDifferential Revision: D19291357\n\nfbshipit-source-id: 747ed5234ca042ceeaff2d094701ead7597ac3ee", "pr_number": "31898", "files_changed": ["caffe2/opt/custom/glow_net_transform.cc", "caffe2/opt/onnxifi_transformer.cc", "caffe2/opt/onnxifi_transformer.h", "caffe2/python/onnx/onnxifi.py", "caffe2/python/pybind_state.cc"], "labels": ["fb-exported", "merged"]}, "4f9d2f74e2": {"title": "Port softplus activation to Aten(CPU+CUDA) (#30504)", "body": "Summary:\nVitalyFedyunin, This PR is about port Softplus activation to Aten:\n**Test script:**\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\ndef _time():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.Softplus()\nif torch.cuda.is_available():\n    device = \"cuda\"\n    m = m.cuda()\n\n#warm up\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [100, 10000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.ones(128, n, device=device)\n    fwd_t = 0\n    bwd_t = 0\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\nTest Device: CPU: skx-8180, GPU: Tesla P40.\nPerfromance:\nBefore:\n```\nGPU:\ninput size(128, 100) forward time is 0.06 (ms); backwad avg time is 0.12 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.18 (ms).\nCPU:\ninput size(128, 100) forward time is 1.16 (ms); backwad avg time is 0.69 (ms).\ninput size(128, 10000) forward time is 60.19 (ms); backwad avg time is 31.86 (ms).\n```\nAfter:\n```\nGPU:\ninput size(128, 100) forward time is 0.05 (ms); backwad avg time is 0.11 (ms).\ninput size(128, 10000) forward time is 0.06 (ms); backwad avg time is 0.17 (ms).\nCPU:\ninput size(128, 100) forward time is 0.43 (ms); backwad avg time is 0.16 (ms).\ninput size(128, 10000) forward time is 1.65 (ms); backwad avg time is 0.83 (ms).\n```\n`OMP_NUM_THREADS=1:`\n```\nBefore:\ninput size(128, 100) forward time is 0.53 (ms); backwad avg time is 0.28 (ms).\ninput size(128, 10000) forward time is 51.33 (ms); backwad avg time is 25.48 (ms).\nAfter:\ninput size(128, 100) forward time is 0.44 (ms); backwad avg time is 0.16 (ms).\ninput size(128, 10000) forward time is 42.05 (ms); backwad avg time is 13.97 (ms).\n```\n\nFix https://github.com/pytorch/pytorch/issues/24633, https://github.com/pytorch/pytorch/issues/24634, https://github.com/pytorch/pytorch/issues/24766, https://github.com/pytorch/pytorch/issues/24767.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30504\n\nDifferential Revision: D19274913\n\nPulled By: ezyang\n\nfbshipit-source-id: 21b29e8459dcba5a040cc68333887b45a858328e", "pr_number": "30504", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/nn.yaml", "aten/src/THCUNN/CMakeLists.txt", "aten/src/THCUNN/SoftPlus.cu", "aten/src/THCUNN/generic/SoftPlus.cu", "aten/src/THCUNN/generic/THCUNN.h", "aten/src/THNN/generic/SoftPlus.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged", "open source"]}, "9ba6a768de": {"title": "Add op bitwise_or (#31559)", "body": "Summary:\nezyang ,  this PR add bitwise_or operator as https://github.com/pytorch/pytorch/pull/31104 .\nBenchmark script :\n```\nimport timeit\nimport torch\ntorch.manual_seed(1)\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__or__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a | b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\")', number=t))\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__ior__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a | b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n```\nDevice: **Tesla P100, skx-8180**\nCuda verison: **9.0.176**\n\nBefore:\n```\n__or__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.17616272252053022\ndevice: cpu, dtype: torch.uint8, 100000 times           0.17148233391344547\ndevice: cpu, dtype: torch.int16, 100000 times           0.17616403382271528\ndevice: cpu, dtype: torch.int32, 100000 times           0.17717823758721352\ndevice: cpu, dtype: torch.int64, 100000 times           0.1801931718364358\ndevice: cuda, dtype: torch.int8, 100000 times           1.270583058707416\ndevice: cuda, dtype: torch.uint8, 100000 times          1.2636413089931011\ndevice: cuda, dtype: torch.int16, 100000 times          1.2839747751131654\ndevice: cuda, dtype: torch.int32, 100000 times          1.2548385225236416\ndevice: cuda, dtype: torch.int64, 100000 times          1.2650810535997152\n__or__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.031136621721088886\ndevice: cpu, dtype: torch.uint8, 10000 times            0.030786747112870216\ndevice: cpu, dtype: torch.int16, 10000 times            0.02391665056347847\ndevice: cpu, dtype: torch.int32, 10000 times            0.024147341027855873\ndevice: cpu, dtype: torch.int64, 10000 times            0.024414129555225372\ndevice: cuda, dtype: torch.int8, 10000 times            0.12741921469569206\ndevice: cuda, dtype: torch.uint8, 10000 times           0.1249831635504961\ndevice: cuda, dtype: torch.int16, 10000 times           0.1283819805830717\ndevice: cuda, dtype: torch.int32, 10000 times           0.12591975275427103\ndevice: cuda, dtype: torch.int64, 10000 times           0.12655890546739101\n__ior__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.3908365070819855\ndevice: cpu, dtype: torch.uint8, 100000 times           0.38267823681235313\ndevice: cpu, dtype: torch.int16, 100000 times           0.38239253498613834\ndevice: cpu, dtype: torch.int32, 100000 times           0.3817988149821758\ndevice: cpu, dtype: torch.int64, 100000 times           0.3901665909215808\ndevice: cuda, dtype: torch.int8, 100000 times           1.4211318120360374\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4215159295126796\ndevice: cuda, dtype: torch.int16, 100000 times          1.4307750314474106\ndevice: cuda, dtype: torch.int32, 100000 times          1.4123614141717553\ndevice: cuda, dtype: torch.int64, 100000 times          1.4480243818834424\n__ior__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.06468924414366484\ndevice: cpu, dtype: torch.uint8, 10000 times            0.06442475505173206\ndevice: cpu, dtype: torch.int16, 10000 times            0.05267547257244587\ndevice: cpu, dtype: torch.int32, 10000 times            0.05286940559744835\ndevice: cpu, dtype: torch.int64, 10000 times            0.06211103219538927\ndevice: cuda, dtype: torch.int8, 10000 times            0.15332304500043392\ndevice: cuda, dtype: torch.uint8, 10000 times           0.15353196952492\ndevice: cuda, dtype: torch.int16, 10000 times           0.15300503931939602\ndevice: cuda, dtype: torch.int32, 10000 times           0.15274472255259752\ndevice: cuda, dtype: torch.int64, 10000 times           0.1512152962386608\n```\nAfter:\n```\n__or__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.2465507509186864\ndevice: cpu, dtype: torch.uint8, 100000 times           0.2472386620938778\ndevice: cpu, dtype: torch.int16, 100000 times           0.2469814233481884\ndevice: cpu, dtype: torch.int32, 100000 times           0.2535214088857174\ndevice: cpu, dtype: torch.int64, 100000 times           0.24855613708496094\ndevice: cuda, dtype: torch.int8, 100000 times           1.4351346511393785\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4434308474883437\ndevice: cuda, dtype: torch.int16, 100000 times          1.4520929995924234\ndevice: cuda, dtype: torch.int32, 100000 times          1.4456610176712275\ndevice: cuda, dtype: torch.int64, 100000 times          1.4580101007595658\n__or__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.029985425993800163\ndevice: cpu, dtype: torch.uint8, 10000 times            0.03024935908615589\ndevice: cpu, dtype: torch.int16, 10000 times            0.026356655173003674\ndevice: cpu, dtype: torch.int32, 10000 times            0.027377349324524403\ndevice: cpu, dtype: torch.int64, 10000 times            0.029163731262087822\ndevice: cuda, dtype: torch.int8, 10000 times            0.14540370367467403\ndevice: cuda, dtype: torch.uint8, 10000 times           0.1456305105239153\ndevice: cuda, dtype: torch.int16, 10000 times           0.1450125053524971\ndevice: cuda, dtype: torch.int32, 10000 times           0.1472016740590334\ndevice: cuda, dtype: torch.int64, 10000 times           0.14709716010838747\n__ior__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.27195510920137167\ndevice: cpu, dtype: torch.uint8, 100000 times           0.2692424338310957\ndevice: cpu, dtype: torch.int16, 100000 times           0.27726674638688564\ndevice: cpu, dtype: torch.int32, 100000 times           0.2815811652690172\ndevice: cpu, dtype: torch.int64, 100000 times           0.2852728571742773\ndevice: cuda, dtype: torch.int8, 100000 times           1.4743850827217102\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4766502184793353\ndevice: cuda, dtype: torch.int16, 100000 times          1.4774163831025362\ndevice: cuda, dtype: torch.int32, 100000 times          1.4749693805351853\ndevice: cuda, dtype: torch.int64, 100000 times          1.5772947426885366\n__ior__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.03614502027630806\ndevice: cpu, dtype: torch.uint8, 10000 times            0.03619729354977608\ndevice: cpu, dtype: torch.int16, 10000 times            0.0319912089034915\ndevice: cpu, dtype: torch.int32, 10000 times            0.03319283854216337\ndevice: cpu, dtype: torch.int64, 10000 times            0.0343862259760499\ndevice: cuda, dtype: torch.int8, 10000 times            0.1581476852297783\ndevice: cuda, dtype: torch.uint8, 10000 times           0.15974601730704308\ndevice: cuda, dtype: torch.int16, 10000 times           0.15957212820649147\ndevice: cuda, dtype: torch.int32, 10000 times           0.16002820804715157\ndevice: cuda, dtype: torch.int64, 10000 times           0.16129320487380028\n```\n\nFix  https://github.com/pytorch/pytorch/issues/24511, https://github.com/pytorch/pytorch/issues/24515, https://github.com/pytorch/pytorch/issues/24658, https://github.com/pytorch/pytorch/issues/24662.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31559\n\nDifferential Revision: D19315875\n\nPulled By: ezyang\n\nfbshipit-source-id: 4a3ca88fdafbeb796079687e676228111eb44aad", "pr_number": "31559", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/THC/THCTensorMathPairwise.cu", "aten/src/THC/THCTensorMathPointwise.cuh", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPairwise.h", "aten/src/THC/generic/THCTensorMathPointwise.cu", "aten/src/THC/generic/THCTensorMathPointwise.h", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "9a3cb1e859": {"title": "Move cauchy to Aten(CPU) (#31824)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/24684.\nBenchmark script :\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\n\ndef _time():\n    return time.time()\n\ndevice = \"cpu\"\n\n#warm up\nfor n in [10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=False, device=device)\n    for i in range(1000):\n        input.cauchy_()\n\nfor n in [1, 10, 100, 1000]:\n    fwd_t = 0\n    input = torch.randn(128, n, requires_grad=False, device=device)\n    for i in range(10000):\n        t1 = _time()\n        input.cauchy_()\n        t2 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n    fwd_avg = fwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.4f (ms).\" % (n, fwd_avg))\n```\nTest device: **skx-8180**.\nBefore:\n```\ninput size(128, 1) forward time is 0.0071 (ms).\ninput size(128, 10) forward time is 0.0596 (ms).\ninput size(128, 100) forward time is 0.5798 (ms).\ninput size(128, 1000) forward time is 5.8395 (ms).\n```\nAfter:\n```\ninput size(128, 1) forward time is 0.0070 (ms).\ninput size(128, 10) forward time is 0.0583 (ms).\ninput size(128, 100) forward time is 0.5714 (ms).\ninput size(128, 1000) forward time is 5.7674 (ms).\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31824\n\nDifferential Revision: D19314411\n\nPulled By: ezyang\n\nfbshipit-source-id: 58098546face3e5971b023f702cfe44ff1cccfbc", "pr_number": "31824", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorRandom.cpp", "aten/src/TH/generic/THTensorRandom.h"], "labels": ["merged", "open source"]}, "dedd16b418": {"title": "remove THConv code which never be used (#31879)", "body": "Summary:\nJust remove dead code in TH.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31879\n\nDifferential Revision: D19315818\n\nPulled By: ezyang\n\nfbshipit-source-id: dbeb2475e19e9ebf769df2649cc859c08d3d184d", "pr_number": "31879", "files_changed": ["aten/src/TH/CMakeLists.txt", "aten/src/TH/THTensor.h", "aten/src/TH/THTensorConv.cpp", "aten/src/TH/generic/THTensorConv.cpp", "aten/src/TH/generic/THTensorConv.h"], "labels": ["merged", "open source"]}, "8c59d48281": {"title": "Add doc previewing instructions (#31905)", "body": "Summary:\nStacked PRs\n * #31908 - Remove C++ docs contributing page\n * **#31905 - Add doc previewing instructions**\n\nThis adds some instructions on how to get started with Github pages you can show reviewers your documentation changes. Hopefully we can delete this eventually and build docs automatically on relevant PRs in CI.\n](https://our.intern.facebook.com/intern/diff/19296364/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31905\n\nPulled By: driazati\n\nDifferential Revision: D19296364\n\nfbshipit-source-id: df47fa1a8d7be029c3efcf6521298583ad9f7a95", "pr_number": "31905", "files_changed": ["CONTRIBUTING.md"], "labels": ["merged"]}, "09a22f3301": {"title": "Remove C++ docs contributing page (#31908)", "body": "Summary:\nStacked PRs\n * **#31908 - Remove C++ docs contributing page**\n * #31905 - Add doc previewing instructions\n\nWe should have 1 source of truth for contribution instructions (CONTRIBUTING.md).\nThis PR moves the instructions from the C++ doc pages there instead of having its\nown separate page.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31908\n\nPulled By: driazati\n\nDifferential Revision: D19296366\n\nfbshipit-source-id: c1daf004259342bd09e09dea3b80e34db47066ec", "pr_number": "31908", "files_changed": ["CONTRIBUTING.md", "docs/cpp/source/contributing.rst", "docs/cpp/source/index.rst"], "labels": ["merged"]}, "883fb5434a": {"title": "Use real argument names for Python functions (#29300)", "body": "Summary:\nThis hooks up `inspect` so that Python functions get their parameters\nnames attached instead of naming them `0, 1, 2, ...`. This also fixes\nissue #28537 where `ignore` functions were improperly typing `self`.\n](https://our.intern.facebook.com/intern/diff/19256434/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29300\n\nPulled By: driazati\n\nDifferential Revision: D19256434\n\nfbshipit-source-id: 6a1fe7bd0afab708b8439517798955d0abfeb44c", "pr_number": "29300", "files_changed": ["test/test_jit.py", "test/test_jit_py3.py", "torch/csrc/jit/script/python_sugared_value.cpp", "torch/jit/__init__.py", "torch/jit/_recursive.py", "torch/jit/annotations.py"], "labels": ["jit", "merged"]}, "5cc49ed45f": {"title": "Document `IValue` (#31904)", "body": "Summary:\nThis is a first pass attempt at documenting `IValue` to help with problems like in #17165. Most users are probably concerned with\n * how to make an `IValue` that matches the input type to their graph (most of the constructors are pretty self explanatory, so as long as they are in the docs I think its enough)\n * how to extract the results after running their graph (there is a small note on the behavior of `.toX()` based on confusions we've had in the past)\n\nPreview:\nhttps://driazati.github.io/pytorch_doc_previews/31904/api/structc10_1_1_i_value.html#exhale-struct-structc10-1-1-i-value\n\nThere are also some random CSS fixes to clean up the style.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31904\n\nPulled By: driazati\n\nDifferential Revision: D19318733\n\nfbshipit-source-id: b29dae3349d5a7ea5a3b8e09cd23f7ff8434edb4", "pr_number": "31904", "files_changed": ["aten/src/ATen/core/ivalue.h", "docs/cpp/source/Doxyfile", "docs/cpp/source/_static/cpp_theme.css", "docs/cpp/source/conf.py"], "labels": ["merged"]}, "319cc21108": {"title": "Add AliasDb API For Changing Aliasing (#31501)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31501\n\nWe have a number of places in our code base where we should be checking if it's safe to change the alias relationship between two sets of values. This PR adds an api to Alias Db to consolidate the logic, and refactors Constant Pooling and `CSE` to use the new api. Next steps: add api usage in peephole.cpp where applicable.\n\nHappy to bikeshed `AliasDb::safeToChangeAliasingRelationship`. Previously I suggested `AliasDb::safeToIntroduceAliasing`, however that's not quite accurate, because this API also handles when it is unsafe to remove aliasing.\n\nAlternate suggestions: `safeToChangeAliasing`, `validToChangeAliasing`, `validToChangeAliasingRelationship`\n\nRelated:  https://github.com/pytorch/pytorch/issues/28360\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19254413\n\nPulled By: eellison\n\nfbshipit-source-id: 17f7f52ad2d1526d303132767cbbb32f8189ae15", "pr_number": "31501", "files_changed": ["test/cpp/jit/test_alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.cpp", "torch/csrc/jit/passes/alias_analysis.h", "torch/csrc/jit/passes/common_subexpression_elimination.cpp", "torch/csrc/jit/passes/constant_pooling.cpp"], "labels": ["jit", "merged"]}, "8ecd3f783d": {"title": "check for object equality in constant pooling (#31800)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31800\n\nIf we know that two constants are the same object, we can ignore other constraints and pool them together. This fixes an issue introduced by the other PR where quantization relied on constant pooling happening for correctness.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19269499\n\nPulled By: eellison\n\nfbshipit-source-id: 9d4396125aa6899cb081863d463d4f024135cbf4", "pr_number": "31800", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "test/cpp/jit/test_constant_pooling.cpp", "test/test_jit.py", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/passes/constant_pooling.cpp"], "labels": ["jit", "merged"]}, "eb23171bce": {"title": "TensorIterator norm update (#31903)", "body": "Summary:\nspecial case for norm out where p == 2. Instead of calling `pow`,\nwe use multiplication as a faster code path.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31903\n\nDifferential Revision: D19312749\n\nPulled By: ngimel\n\nfbshipit-source-id: 73732b7b37a243a14438609784795b920271a0b5", "pr_number": "31903", "files_changed": ["aten/src/ATen/native/SharedReduceOps.h", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cuda/ReduceOpsKernel.cu", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/THC/THCTensorMathReduce.cuh", "aten/src/THC/generic/THCTensorMathReduce.cu"], "labels": ["merged", "open source"]}, "9e9ca6ec37": {"title": "add conversion functions to embedding tables (#31083)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31083\n\nadd (fp32/fp16)<->(int8 rowwise quantized fp32/fp16 scale biases)\n\nTest Plan:\nadded unit tests\nenhanced shape inference tests\n\nReviewed By: jspark1105\n\nDifferential Revision: D18920547\n\nfbshipit-source-id: 6b3d7cb93f9d1669ecf511817d73976177632891", "pr_number": "31083", "files_changed": ["caffe2/operators/fused_rowwise_8bit_conversion_ops.cc", "caffe2/operators/fused_rowwise_8bit_conversion_ops.h", "caffe2/perfkernels/fused_8bit_rowwise_conversion.cc", "caffe2/perfkernels/fused_8bit_rowwise_conversion.h", "caffe2/python/operator_test/shape_inference_test.py"], "labels": ["fb-exported", "merged"]}, "6d9a9e379d": {"title": "Fix segfault in caffe2 slice test (#31801)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31801\n\nTry to fix issue #30764\n\nTest Plan:\npython test/onnx/test_utility_funs.py TestUtilityFuns\n\nImported from OSS\n\nDifferential Revision: D19315046\n\nfbshipit-source-id: de3595969280e4ebe762cb098ff0891f8b5a9a90", "pr_number": "31801", "files_changed": ["caffe2/operators/quantized/int8_slice_op.h"], "labels": ["merged"]}, "0dbd5c0bfe": {"title": "Added torchvision tests as part of ORT tests (#31835)", "body": "Summary:\nAdded torchvision tests as part of ORT tests\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31835\n\nReviewed By: hl475\n\nDifferential Revision: D19278607\n\nPulled By: houseroad\n\nfbshipit-source-id: 18a6a85ce3019bcc9aee9517af1378964b585afd", "pr_number": "31835", "files_changed": [".jenkins/caffe2/test.sh", "test/onnx/test_pytorch_onnx_onnxruntime.py"], "labels": ["merged", "open source"]}, "8614860210": {"title": "Uniformly apply Windows logic in cpp_extensions everywhere (#31161)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31161\n\nPreviously, it wasn't necessary to specify `DT_NEEDED` in C++ extensions on Linux (aka pass `-l` flags) because all of the symbols would have already been loaded with `RTLD_GLOBAL`, so there wouldn't be any undefined symbols.  But when we switch to loading `_C` with `RTLD_LOCAL`, it's now necessary for all the C++ extensions to know what libraries to link with. The resulting code is clearer and more uniform, so it's wins all around.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19262578\n\nPulled By: ezyang\n\nfbshipit-source-id: a893cc96f2e9aad1c064a6de4f7ccf79257dec3f", "pr_number": "31161", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["merged"]}, "ddff4efa26": {"title": "Don't use RTLD_GLOBAL to load _C. (#31162)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31162\n\nThis should help us resolve a multitude of weird segfaults and crashes\nwhen PyTorch is imported along with other packages. Those would often\nhappen because libtorch symbols were exposed globally and could be used\nas a source of relocations in shared libraries loaded after libtorch.\n\nFixes #3059.\n\nSome of the subtleties in preparing this patch:\n\n* Getting ASAN to play ball was a pain in the ass. The basic problem is that when we load with `RTLD_LOCAL`, we now may load a library multiple times into the address space; this happens when we have custom C++ extensions. Since the libraries are usually identical, this is usually benign, but it is technically undefined behavior and UBSAN hates it. I sprayed a few ways of getting things to \"work\" correctly: I preload libstdc++ (so that it is seen consistently over all library loads) and added turned off vptr checks entirely. Another possibility is we should have a mode where we use RTLD_GLOBAL to load _C, which would be acceptable in environments where you're sure C++ lines up correctly. There's a long comment in the test script going into more detail about this.\n* Making some of our shared library dependencies load with `RTLD_LOCAL` breaks them. OpenMPI and MKL don't work; they play linker shenanigans to look up their symbols which doesn't work when loaded locally, and if we load a library with `RLTD_LOCAL` we aren't able to subsequently see it with `ctypes`. To solve this problem, we employ a clever device invented by apaszke: we create a dummy library `torch_global_deps` with dependencies on all of the libraries which need to be loaded globally, and then load that with `RTLD_GLOBAL`. As long as none of these libraries have C++ symbols, we can avoid confusion about C++ standard library.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: D19262579\n\nTest Plan: Imported from OSS\n\nPulled By: ezyang\n\nfbshipit-source-id: 06a48a5d2c9036aacd535f7e8a4de0e8fe1639f2", "pr_number": "31162", "files_changed": [".jenkins/pytorch/test.sh", "caffe2/CMakeLists.txt", "torch/__init__.py", "torch/_utils_internal.py", "torch/csrc/empty.c", "ubsan.supp"], "labels": ["merged"]}, "5c423cae72": {"title": "Add precision tests for CUDA half linspace+logspace (#31962)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31962\n\nI added precision tests for CUDA half, float, and double.\n\nThe precision for CUDA half seems bad, but I checked the numbers against\nprevious versions of pytorch. The output of CUDA Half linspace+logspace\nare exactly the same when compared with 1.2.0.\n\nTest Plan: - Run CI\n\nDifferential Revision: D19320182\n\nPulled By: zou3519\n\nfbshipit-source-id: 38d3d4dea2807875ed0b0ec2b93b19c10a289988", "pr_number": "31962", "files_changed": ["test/test_torch.py"], "labels": ["merged"]}, "5a76335aaa": {"title": "Move lshift to Aten (#31566)", "body": "Summary:\nVitalyFedyunin , this PR is about move lshift to Aten.\nBenchmark script :\n```\nimport timeit\nimport torch\ntorch.manual_seed(1)\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__lshift__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a << b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\")', number=t))\n        for dtype in ('torch.float32', 'torch.float64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a << b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randn({n}, dtype = {dtype}, device=\"{device}\"); b = torch.randn({n}, dtype = {dtype}, device=\"{device}\")', number=t))\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__ilshift__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a << b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n        for dtype in ('torch.float32', 'torch.float64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a << b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randn({n}, dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n```\nDevice: **Tesla P100, skx-8180**\nCuda verison: **9.0.176**\n\nBefore:\n```\n__lshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.31618343852460384\ndevice: cpu, dtype: torch.uint8, 100000 times           0.31258584931492805\ndevice: cpu, dtype: torch.int16, 100000 times           0.3140896391123533\ndevice: cpu, dtype: torch.int32, 100000 times           0.34389012958854437\ndevice: cpu, dtype: torch.int64, 100000 times           0.339566046372056\ndevice: cpu, dtype: torch.float32, 100000 times         0.4180623721331358\ndevice: cpu, dtype: torch.float64, 100000 times         0.4165227338671684\ndevice: cuda, dtype: torch.int8, 100000 times           1.7851383443921804\ndevice: cuda, dtype: torch.uint8, 100000 times          1.7842160519212484\ndevice: cuda, dtype: torch.int16, 100000 times          1.789359962567687\ndevice: cuda, dtype: torch.int32, 100000 times          1.7822618428617716\ndevice: cuda, dtype: torch.int64, 100000 times          1.7968465769663453\ndevice: cuda, dtype: torch.float32, 100000 times                1.8066061967983842\ndevice: cuda, dtype: torch.float64, 100000 times                1.8046843251213431\n__lshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.04618230368942022\ndevice: cpu, dtype: torch.uint8, 10000 times            0.04634759668260813\ndevice: cpu, dtype: torch.int16, 10000 times            0.040676115080714226\ndevice: cpu, dtype: torch.int32, 10000 times            0.04404774494469166\ndevice: cpu, dtype: torch.int64, 10000 times            0.04511771444231272\ndevice: cpu, dtype: torch.float32, 10000 times          0.6887832451611757\ndevice: cpu, dtype: torch.float64, 10000 times          0.5559549620375037\ndevice: cuda, dtype: torch.int8, 10000 times            0.17996764183044434\ndevice: cuda, dtype: torch.uint8, 10000 times           0.17970609478652477\ndevice: cuda, dtype: torch.int16, 10000 times           0.17873135022819042\ndevice: cuda, dtype: torch.int32, 10000 times           0.1781835313886404\ndevice: cuda, dtype: torch.int64, 10000 times           0.17846618220210075\ndevice: cuda, dtype: torch.float32, 10000 times         0.18056879844516516\ndevice: cuda, dtype: torch.float64, 10000 times         0.18132662680000067\n__ilshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.61110960226506\ndevice: cpu, dtype: torch.uint8, 100000 times           0.6333359787240624\ndevice: cpu, dtype: torch.int16, 100000 times           0.6345370784401894\ndevice: cpu, dtype: torch.int32, 100000 times           0.6470990972593427\ndevice: cpu, dtype: torch.int64, 100000 times           0.6587044578045607\ndevice: cpu, dtype: torch.float32, 100000 times         0.7269002720713615\ndevice: cpu, dtype: torch.float64, 100000 times         0.7217964073643088\ndevice: cuda, dtype: torch.int8, 100000 times           1.9880435159429908\ndevice: cuda, dtype: torch.uint8, 100000 times          1.986489498987794\ndevice: cuda, dtype: torch.int16, 100000 times          2.0059875370934606\ndevice: cuda, dtype: torch.int32, 100000 times          1.995262237265706\ndevice: cuda, dtype: torch.int64, 100000 times          1.9974954994395375\ndevice: cuda, dtype: torch.float32, 100000 times                2.00442770216614\ndevice: cuda, dtype: torch.float64, 100000 times                2.009664717130363\n__ilshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.08199594635516405\ndevice: cpu, dtype: torch.uint8, 10000 times            0.08096733782440424\ndevice: cpu, dtype: torch.int16, 10000 times            0.0734213450923562\ndevice: cpu, dtype: torch.int32, 10000 times            0.0769620593637228\ndevice: cpu, dtype: torch.int64, 10000 times            0.08650507684797049\ndevice: cpu, dtype: torch.float32, 10000 times          0.7196345143020153\ndevice: cpu, dtype: torch.float64, 10000 times          0.597336508333683\ndevice: cuda, dtype: torch.int8, 10000 times            0.19723015930503607\ndevice: cuda, dtype: torch.uint8, 10000 times           0.19754122477024794\ndevice: cuda, dtype: torch.int16, 10000 times           0.19710093270987272\ndevice: cuda, dtype: torch.int32, 10000 times           0.19611249305307865\ndevice: cuda, dtype: torch.int64, 10000 times           0.19750046730041504\ndevice: cuda, dtype: torch.float32, 10000 times         0.19680574722588062\ndevice: cuda, dtype: torch.float64, 10000 times         0.19689027685672045\n```\nAfter:\n```\n__lshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.3031281465664506\ndevice: cpu, dtype: torch.uint8, 100000 times           0.30772678554058075\ndevice: cpu, dtype: torch.int16, 100000 times           0.3088294789195061\ndevice: cpu, dtype: torch.int32, 100000 times           0.30907699652016163\ndevice: cpu, dtype: torch.int64, 100000 times           0.31315001379698515\ndevice: cpu, dtype: torch.float32, 100000 times         0.38823566399514675\ndevice: cpu, dtype: torch.float64, 100000 times         0.39300001971423626\ndevice: cuda, dtype: torch.int8, 100000 times           1.3225595457479358\ndevice: cuda, dtype: torch.uint8, 100000 times          1.31739442050457\ndevice: cuda, dtype: torch.int16, 100000 times          1.3198596313595772\ndevice: cuda, dtype: torch.int32, 100000 times          1.309600466862321\ndevice: cuda, dtype: torch.int64, 100000 times          1.3264533821493387\ndevice: cuda, dtype: torch.float32, 100000 times                1.3377520674839616\ndevice: cuda, dtype: torch.float64, 100000 times                1.3343619462102652\n__lshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.02718757465481758\ndevice: cpu, dtype: torch.uint8, 10000 times            0.02701799664646387\ndevice: cpu, dtype: torch.int16, 10000 times            0.025483975186944008\ndevice: cpu, dtype: torch.int32, 10000 times            0.025557605549693108\ndevice: cpu, dtype: torch.int64, 10000 times            0.026179466396570206\ndevice: cpu, dtype: torch.float32, 10000 times          0.0962932649999857\ndevice: cpu, dtype: torch.float64, 10000 times          0.1611471576616168\ndevice: cuda, dtype: torch.int8, 10000 times            0.13165222201496363\ndevice: cuda, dtype: torch.uint8, 10000 times           0.13358880020678043\ndevice: cuda, dtype: torch.int16, 10000 times           0.1342075066640973\ndevice: cuda, dtype: torch.int32, 10000 times           0.1328689968213439\ndevice: cuda, dtype: torch.int64, 10000 times           0.13336248509585857\ndevice: cuda, dtype: torch.float32, 10000 times         0.1345295710489154\ndevice: cuda, dtype: torch.float64, 10000 times         0.14084953162819147\n__ilshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.19080814253538847\ndevice: cpu, dtype: torch.uint8, 100000 times           0.18541878275573254\ndevice: cpu, dtype: torch.int16, 100000 times           0.19136024825274944\ndevice: cpu, dtype: torch.int32, 100000 times           0.1916898973286152\ndevice: cpu, dtype: torch.int64, 100000 times           0.1973192635923624\ndevice: cpu, dtype: torch.float32, 100000 times         0.2668355852365494\ndevice: cpu, dtype: torch.float64, 100000 times         0.24472137168049812\ndevice: cuda, dtype: torch.int8, 100000 times           1.3581306440755725\ndevice: cuda, dtype: torch.uint8, 100000 times          1.3522163443267345\ndevice: cuda, dtype: torch.int16, 100000 times          1.366145665757358\ndevice: cuda, dtype: torch.int32, 100000 times          1.3674909211695194\ndevice: cuda, dtype: torch.int64, 100000 times          1.3734915973618627\ndevice: cuda, dtype: torch.float32, 100000 times                1.3831533305346966\ndevice: cuda, dtype: torch.float64, 100000 times                1.396162535995245\n__ilshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.02847585454583168\ndevice: cpu, dtype: torch.uint8, 10000 times            0.02960751298815012\ndevice: cpu, dtype: torch.int16, 10000 times            0.028516249731183052\ndevice: cpu, dtype: torch.int32, 10000 times            0.02842544950544834\ndevice: cpu, dtype: torch.int64, 10000 times            0.029186096973717213\ndevice: cpu, dtype: torch.float32, 10000 times          0.0999628696590662\ndevice: cpu, dtype: torch.float64, 10000 times          0.16676222812384367\ndevice: cuda, dtype: torch.int8, 10000 times            0.13856443110853434\ndevice: cuda, dtype: torch.uint8, 10000 times           0.13766566663980484\ndevice: cuda, dtype: torch.int16, 10000 times           0.13652489613741636\ndevice: cuda, dtype: torch.int32, 10000 times           0.13678150344640017\ndevice: cuda, dtype: torch.int64, 10000 times           0.13749946560710669\ndevice: cuda, dtype: torch.float32, 10000 times         0.13879029918462038\ndevice: cuda, dtype: torch.float64, 10000 times         0.14587809145450592\n```\n\nFix https://github.com/pytorch/pytorch/issues/24510 #24514 https://github.com/pytorch/pytorch/issues/24657  https://github.com/pytorch/pytorch/issues/24661\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31566\n\nDifferential Revision: D19314251\n\nPulled By: ezyang\n\nfbshipit-source-id: 52df17b2c18ef1880374c6dbcf18fb1118086552", "pr_number": "31566", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/THC/THCTensorMathPairwise.cu", "aten/src/THC/THCTensorMathPointwise.cuh", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPairwise.h", "aten/src/THC/generic/THCTensorMathPointwise.cu"], "labels": ["merged", "open source"]}, "99b3f9cac4": {"title": "Move log_sigmoid to Aten(CPU) (#30958)", "body": "Summary:\nVitalyFedyunin, This PR is about port LogSigmoid activation to Aten:\nTest script:\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\n\ndef _time():\n    return time.time()\n\ndevice = \"cpu\"\nm = nn.LogSigmoid()\n#warm up\nfor n in [1, 10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.randn(128, n, device=device)\n    for i in range(1000):\n        output = m(input)\n        output.backward(grad_output)\n\nfor n in [1, 10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=True, device=device)\n    grad_output = torch.randn(128, n, device=device)\n    fwd_t = 0\n    bwd_t = 0\n    for i in range(10000):\n        t1 = _time()\n        output = m(input)\n        t2 = _time()\n        output.backward(grad_output)\n        t3 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n        bwd_t = bwd_t + (t3 - t2)\n    fwd_avg = fwd_t / 10000 * 1000\n    bwd_avg = bwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\n          % (n, fwd_avg, bwd_avg))\n```\n**Before:**\n```\ninput size(128, 1) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.10 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 100) forward time is 0.90 (ms); backwad avg time is 0.09 (ms).\ninput size(128, 1000) forward time is 9.04 (ms); backwad avg time is 0.87 (ms).\n```\n**After:**\n```\ninput size(128, 1) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 100) forward time is 0.04 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 1000) forward time is 0.28 (ms); backwad avg time is 0.07 (ms).\n```\n**OMP_NUM_THREADS=1:**\n```\nBefore:\ninput size(128, 1) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.10 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 100) forward time is 0.88 (ms); backwad avg time is 0.10 (ms).\ninput size(128, 1000) forward time is 8.72 (ms); backwad avg time is 0.81 (ms).\nAfter:\ninput size(128, 1) forward time is 0.01 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 10) forward time is 0.02 (ms); backwad avg time is 0.02 (ms).\ninput size(128, 100) forward time is 0.07 (ms); backwad avg time is 0.03 (ms).\ninput size(128, 1000) forward time is 0.63 (ms); backwad avg time is 0.15 (ms).\n```\n\nFix https://github.com/pytorch/pytorch/issues/24724, https://github.com/pytorch/pytorch/issues/24725.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30958\n\nDifferential Revision: D19275111\n\nPulled By: ezyang\n\nfbshipit-source-id: bbfe82e58fb27a4fb21c1914c6547a9050072e5c", "pr_number": "30958", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/THNN/generic/LogSigmoid.c", "aten/src/THNN/generic/THNN.h", "aten/src/THNN/init.cpp"], "labels": ["merged", "open source"]}, "e59e5ba5a3": {"title": "Move geometric to Aten(CPU) (#31878)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/24704.\nBenchmark script :\n```\nimport torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(0)\n\ndef _time():\n    return time.time()\n\ndevice = \"cpu\"\n\n#warm up\nfor n in [10, 100, 1000]:\n    input = torch.randn(128, n, requires_grad=False, device=device)\n    for i in range(1000):\n        input.geometric_(0.5)\n\nfor n in [1, 10, 100, 1000]:\n    fwd_t = 0\n    input = torch.randn(128, n, requires_grad=False, device=device)\n    for i in range(10000):\n        t1 = _time()\n        input.geometric_(0.5)\n        t2 = _time()\n        fwd_t = fwd_t + (t2 -t1)\n    fwd_avg = fwd_t / 10000 * 1000\n    print(\"input size(128, %d) forward time is %.4f (ms).\" % (n, fwd_avg))\n```\nTest device: **skx-8180**.\nBefore:\n```\ninput size(128, 1) forward time is 0.0092 (ms).\ninput size(128, 10) forward time is 0.0802 (ms).\ninput size(128, 100) forward time is 0.7994 (ms).\ninput size(128, 1000) forward time is 7.8403 (ms).\n```\nAfter:\n```\ninput size(128, 1) forward time is 0.0088 (ms).\ninput size(128, 10) forward time is 0.0781 (ms).\ninput size(128, 100) forward time is 0.7815 (ms).\ninput size(128, 1000) forward time is 7.7163 (ms).\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31878\n\nDifferential Revision: D19314510\n\nPulled By: ezyang\n\nfbshipit-source-id: 2d95bf9938c8becf280890acf9e37223ddd08a39", "pr_number": "31878", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/Distributions.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorRandom.cpp", "aten/src/TH/generic/THTensorRandom.h"], "labels": ["merged", "open source"]}, "26f552a3d1": {"title": "Javadoc changes (#31956)", "body": "Summary:\n- Add Javadoc url in index.rst\n- Delete no longer needed java rst files\n- Remove intersphinx extension from conf.oy\n- Remove javasphinx from docs/requirements.txt\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31956\n\nDifferential Revision: D19320493\n\nPulled By: jlin27\n\nfbshipit-source-id: cc76b2a2acbe2ecdabcd3339e1cc3182f0c906ae", "pr_number": "31956", "files_changed": ["docs/requirements.txt", "docs/source/conf.py", "docs/source/index.rst", "docs/source/org/pytorch/DType.rst", "docs/source/org/pytorch/IValue.rst", "docs/source/org/pytorch/Module.rst", "docs/source/org/pytorch/Tensor-Tensor_float32.rst", "docs/source/org/pytorch/Tensor-Tensor_float64.rst", "docs/source/org/pytorch/Tensor-Tensor_int32.rst", "docs/source/org/pytorch/Tensor-Tensor_int64.rst", "docs/source/org/pytorch/Tensor-Tensor_int8.rst", "docs/source/org/pytorch/Tensor-Tensor_uint8.rst", "docs/source/org/pytorch/Tensor.rst", "docs/source/org/pytorch/TensorImageUtils.rst", "docs/source/org/pytorch/package-index.rst", "docs/source/org/pytorch/torchvision/package-index.rst", "docs/source/packages.rst"], "labels": ["merged"]}, "bc68a8745f": {"title": "Spelling fix in transformer docs", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31973\n\nDifferential Revision: D19330660\n\nPulled By: zou3519\n\nfbshipit-source-id: 29ea1e790a34f0241cb7aba85110f087cdc069ba", "pr_number": "31973", "files_changed": ["torch/nn/modules/transformer.py"], "labels": ["merged", "open source"]}, "cfdfdf70d7": {"title": "remove JSON dumping dependency (#30724)", "body": "Summary:\nFix for https://github.com/pytorch/pytorch/issues/19420\n\nSo after actually writing a C++ JSON dumping class I figured that\na faster and cleaner way would be simply rewrite the Python without\nthe JSON module since the JSON that we need to output is so simple.\n\nFor now I decided to not touch the `parse_cpu_trace` function since\nonly changing `export_chrome_trace` shows a 4x speedup.\n\nHere's the script I used for benchmarking:\n``` python\nimport time\nimport torch\n\nx = torch.ones(2, 2)\n\nstart = time.time()\nwith torch.autograd.profiler.profile() as prof:\n  for _ in range(10000):\n    x * x\n\nfor i in range(50):\n  prof.export_chrome_trace(\"trace.json\")\n\nstop = time.time()\n\nprint(stop-start)\n```\nmaster branch (using json dump) -> 8.07515025138855\nnew branch (without json dump) ->  2.0943689346313477\n\nI checked the trace file generated in the [test](https://github.com/pytorch/pytorch/blob/master/test/test_autograd.py#L2659)\nand it does work fine.\n\nPlease let me know what you think.\n\nIf you still insist on the C++ version I can send a new patch soon enough.\n\nCC ezyang rgommers\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30724\n\nDifferential Revision: D19298955\n\nPulled By: ezyang\n\nfbshipit-source-id: b0d7324ea5f90884ab8a00dd272f3aa3d9bc0427", "pr_number": "30724", "files_changed": ["torch/autograd/profiler.py"], "labels": ["merged", "open source"]}, "1296e2d55e": {"title": "C++ API parity: isinf (#31099)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/31021, port the legacy binding method of `isinf` to C++ therefore support JIT\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31099\n\nDifferential Revision: D19314733\n\nPulled By: yf225\n\nfbshipit-source-id: 5725c51d19c33b4fddd0fc9e7034078580bd534e", "pr_number": "31099", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/native_functions.yaml", "test/cpp/api/functional.cpp", "torch/_torch_docs.py", "torch/functional.py"], "labels": ["merged", "module: cpp", "open source"]}, "67c1d930eb": {"title": "Lock graph_task before writing leaf_streams. (#31995)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31995\n\nFixes #31906.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19331259\n\nPulled By: ezyang\n\nfbshipit-source-id: 5d24bf3555e632211a9b6f8e50ff241603c18b3d", "pr_number": "31995", "files_changed": ["torch/csrc/autograd/engine.cpp"], "labels": ["merged"]}, "2968faf154": {"title": "Update doc about output_differentiability keyword in derivatives.yaml", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31925\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19303833\n\nPulled By: albanD\n\nfbshipit-source-id: 291a9f122720844a5f8386b22cf6abc66ae86e4d", "pr_number": "31925", "files_changed": ["tools/autograd/derivatives.yaml"], "labels": ["merged"]}, "67ff051ddd": {"title": "Remove temporary fix for torchbind in BC check (#31982)", "body": "Summary:\nRemove the patch\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31982\n\nReviewed By: hl475\n\nDifferential Revision: D19333205\n\nPulled By: houseroad\n\nfbshipit-source-id: 1d16fd31ede7266789141238520d47b762a7a340", "pr_number": "31982", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["merged"]}, "700d1c5cbc": {"title": "update CI script to take string docker image version (#31857)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31857\n\nAccording to mingbowan we will change to use string docker image\nversion because the tag is no longer an integer since we move the docker\nimage build job to circle CI:\nhttp://ossci-docker.s3-website.us-east-1.amazonaws.com/pytorch.html\n\nTest Plan: - with stacked PR\n\nDifferential Revision: D19282726\n\nPulled By: ljk53\n\nfbshipit-source-id: 7a12ae89a11cf15163b905734d50fed6dc98cb07", "pr_number": "31857", "files_changed": [".circleci/cimodel/data/caffe2_build_definitions.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/validate-docker-version.py"], "labels": ["merged"]}, "6abfa9ad8a": {"title": "Quantized H Tangent function (#31031)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31031\n\nThis activation will be needed for the LSTM implementation.\nAlso includes the QNNPack implementation.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19334280\n\nPulled By: z-a-f\n\nfbshipit-source-id: ae14399765a47afdf9b1e072d3967c24ff473e8d", "pr_number": "31031", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/src/tanh.c", "aten/src/ATen/native/quantized/cpu/qnnpack/test/tanh-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qtanh.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "test/test_quantized.py"], "labels": ["merged"]}, "62f93443e5": {"title": "Explain RPC behavior when using Tensor as arg or return value", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31968\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19321380\n\nPulled By: mrshenli\n\nfbshipit-source-id: e3431f1f02963cc8d8266a420ab03866106f26ac", "pr_number": "31968", "files_changed": ["docs/source/rpc.rst"], "labels": ["merged"]}, "4e84661139": {"title": "update llvmlite to 0.30.0 (#31858)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31858\n\nTrying to upgrade docker image but ran into the following error:\n\n```\nRunning test_nn ... [2020-01-04 18:05:12.537860]\nTraceback (most recent call last):\n  File \"test_nn.py\", line 45, in <module>\n    from common_cuda import TEST_CUDA, TEST_MULTIGPU, TEST_CUDNN, TEST_CUDNN_VERSION\n  File \"/var/lib/jenkins/workspace/test/common_cuda.py\", line 16, in <module>\n    import numba.cuda\n  File \"/opt/conda/lib/python3.6/site-packages/numba/__init__.py\", line 178, in <module>\n    _ensure_llvm()\n  File \"/opt/conda/lib/python3.6/site-packages/numba/__init__.py\", line 100, in _ensure_llvm\n    raise ImportError(msg)\nImportError: Numba requires at least version 0.30.0 of llvmlite.\nInstalled version is 0.28.0.\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19282923\n\nPulled By: ljk53\n\nfbshipit-source-id: bdeefbf4f6c0c97df622282f76e77eb1eadba436", "pr_number": "31858", "files_changed": [".circleci/docker/common/install_conda.sh"], "labels": []}, "8ea49e7a08": {"title": "add missing braces for format in rpc _to_worker_info (#31969)", "body": "Summary:\nThis was missing and resulted in the incorrect `name` passed into `_to_worker_info` not being printed out in the error message.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31969\n\nDifferential Revision: D19331927\n\nPulled By: rohan-varma\n\nfbshipit-source-id: e74d47daec3224c2d9b9da3c0a6404cfa67baf65", "pr_number": "31969", "files_changed": ["torch/distributed/rpc/api.py"], "labels": ["merged"]}, "b6f43afaca": {"title": "Fix tensordot allowing negative dims (#31954)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/31926\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31954\n\nDifferential Revision: D19331847\n\nPulled By: zou3519\n\nfbshipit-source-id: e30dd9517917c056a52be7d16f23247fe28f4e28", "pr_number": "31954", "files_changed": ["test/test_torch.py", "torch/functional.py"], "labels": ["merged", "open source"]}, "c6f41ae01b": {"title": "Fix and add more padding mode support for Conv (#31784)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/29712 #29668 , add arg checking, doc, and support for reflection and replication padding modes.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31784\n\nDifferential Revision: D19301974\n\nPulled By: ezyang\n\nfbshipit-source-id: a0ed4815c0c22e416b16e256bba04324e376b2f8", "pr_number": "31784", "files_changed": ["test/common_nn.py", "test/test_nn.py", "torch/nn/modules/conv.py", "torch/nn/modules/utils.py"], "labels": ["merged", "open source", "topic: bc-breaking"]}, "a201027e93": {"title": "Abstract atomic add calls (#31992)", "body": "Summary:\nInstead of a mixture of direct calls to library provided atomicAdd calls, such as float atomicAdd(float*, float) and calls provided internally, such as void atomicAdd(long*, long), abstract to one API void gpuAtomicAdd(T*, T) in THCAtomics.cuh for the PyTorch backend.\n\nThe advantage of this approach is that it allows us to more easily distinguish between capabiltiies of different platforms (and their versions). Additionally, the abstraction of void returning atomicAdds allows us to, in the future, support fast HW instructions on some platforms that will not return the previous value.\n\nCall sites that do not satisfy above conditions and are either highly platform specific (__half2 atomicAdd fast path in one operator) or require the return explicitly (some int atomicAdd invocations) are left untouched. The Caffe2 backend also remains untouched.\n\nWhile here, add a bunch of includes of THCAtomics.cuh that were missing before.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31992\n\nDifferential Revision: D19330220\n\nPulled By: ezyang\n\nfbshipit-source-id: d6ab73ec5168c77e328faeef6c6f48eefba00861", "pr_number": "31992", "files_changed": ["aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/GridSampler.cuh", "aten/src/ATen/native/cuda/KernelUtils.cuh", "aten/src/ATen/native/cuda/LossCTC.cu", "aten/src/ATen/native/cuda/ReflectionPad.cu", "aten/src/ATen/native/cuda/ReplicationPadding.cu", "aten/src/ATen/native/cuda/SortingRadixSelect.cuh", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/UpSample.cuh", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/THC/THCAtomics.cuh", "aten/src/THC/THCTensorIndex.cu", "aten/src/THC/THCTensorScatterGather.cu", "aten/src/THCUNN/SpatialClassNLLCriterion.cu", "docs/cpp/source/notes/tensor_basics.rst"], "labels": ["merged", "module: rocm", "open source"]}, "8098ae455c": {"title": "Move rshift to Aten (#31594)", "body": "Summary:\nVitalyFedyunin , this PR is about move rshift to Aten.\nBenchmark script :\n```\nimport timeit\nimport torch\ntorch.manual_seed(1)\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__rshift__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a >> b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\")', number=t))\n        for dtype in ('torch.float32', 'torch.float64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a >> b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randn({n}, dtype = {dtype}, device=\"{device}\"); b = torch.randn({n}, dtype = {dtype}, device=\"{device}\")', number=t))\n\nfor n, t in [(10, 100000),(1000, 10000)]:\n    print('__irshift__ (a.numel() == {}) for {} times'.format(n, t))\n    for device in ('cpu', 'cuda'):\n        for dtype in ('torch.int8', 'torch.uint8', 'torch.int16', 'torch.int32', 'torch.int64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a >> b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randint(0, 10, ({n},), dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n        for dtype in ('torch.float32', 'torch.float64'):\n            print(f'device: {device}, dtype: {dtype}, {t} times', end='\\t\\t')\n            print(timeit.timeit(f'a >> b\\nif \"{device}\" == \"cuda\": torch.cuda.synchronize()', setup=f'import torch; a = torch.randn({n}, dtype = {dtype}, device=\"{device}\"); b = torch.tensor(5, dtype = {dtype}, device=\"{device}\")', number=t))\n```\nDevice: **Tesla P100, skx-8180**\nCuda verison: **9.0.176**\n\nBefore:\n```\n__rshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.17183916084468365\ndevice: cpu, dtype: torch.uint8, 100000 times           0.16587729007005692\ndevice: cpu, dtype: torch.int16, 100000 times           0.16659130714833736\ndevice: cpu, dtype: torch.int32, 100000 times           0.17177579551935196\ndevice: cpu, dtype: torch.int64, 100000 times           0.17860156949609518\ndevice: cpu, dtype: torch.float32, 100000 times         0.23938780091702938\ndevice: cpu, dtype: torch.float64, 100000 times         0.22591270506381989\ndevice: cuda, dtype: torch.int8, 100000 times           1.2709560776129365\ndevice: cuda, dtype: torch.uint8, 100000 times          1.2692269310355186\ndevice: cuda, dtype: torch.int16, 100000 times          1.2785452520474792\ndevice: cuda, dtype: torch.int32, 100000 times          1.2733035255223513\ndevice: cuda, dtype: torch.int64, 100000 times          1.2785427365452051\ndevice: cuda, dtype: torch.float32, 100000 times                1.2980637094005942\ndevice: cuda, dtype: torch.float64, 100000 times                1.3062487514689565\n__rshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.03122080024331808\ndevice: cpu, dtype: torch.uint8, 10000 times            0.030290847644209862\ndevice: cpu, dtype: torch.int16, 10000 times            0.024531075730919838\ndevice: cpu, dtype: torch.int32, 10000 times            0.024743229150772095\ndevice: cpu, dtype: torch.int64, 10000 times            0.025563121773302555\ndevice: cpu, dtype: torch.float32, 10000 times          0.6707976600155234\ndevice: cpu, dtype: torch.float64, 10000 times          0.5344798369333148\ndevice: cuda, dtype: torch.int8, 10000 times            0.12768010422587395\ndevice: cuda, dtype: torch.uint8, 10000 times           0.12681372743099928\ndevice: cuda, dtype: torch.int16, 10000 times           0.12995595764368773\ndevice: cuda, dtype: torch.int32, 10000 times           0.12989260721951723\ndevice: cuda, dtype: torch.int64, 10000 times           0.12804713658988476\ndevice: cuda, dtype: torch.float32, 10000 times         0.13013121113181114\ndevice: cuda, dtype: torch.float64, 10000 times         0.1406280631199479\n__irshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.3805475188419223\ndevice: cpu, dtype: torch.uint8, 100000 times           0.36341007333248854\ndevice: cpu, dtype: torch.int16, 100000 times           0.36908434610813856\ndevice: cpu, dtype: torch.int32, 100000 times           0.3669992135837674\ndevice: cpu, dtype: torch.int64, 100000 times           0.37847711704671383\ndevice: cpu, dtype: torch.float32, 100000 times         0.4311870699748397\ndevice: cpu, dtype: torch.float64, 100000 times         0.44503832422196865\ndevice: cuda, dtype: torch.int8, 100000 times           1.4343859804794192\ndevice: cuda, dtype: torch.uint8, 100000 times          1.4298221375793219\ndevice: cuda, dtype: torch.int16, 100000 times          1.4460898758843541\ndevice: cuda, dtype: torch.int32, 100000 times          1.4518025070428848\ndevice: cuda, dtype: torch.int64, 100000 times          1.4456725595518947\ndevice: cuda, dtype: torch.float32, 100000 times                1.4610810624435544\ndevice: cuda, dtype: torch.float64, 100000 times                1.4736663019284606\n__irshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.05944254994392395\ndevice: cpu, dtype: torch.uint8, 10000 times            0.058085592463612556\ndevice: cpu, dtype: torch.int16, 10000 times            0.05094402376562357\ndevice: cpu, dtype: torch.int32, 10000 times            0.050842881202697754\ndevice: cpu, dtype: torch.int64, 10000 times            0.06223891582340002\ndevice: cpu, dtype: torch.float32, 10000 times          0.7006897022947669\ndevice: cpu, dtype: torch.float64, 10000 times          0.5614962242543697\ndevice: cuda, dtype: torch.int8, 10000 times            0.1461706068366766\ndevice: cuda, dtype: torch.uint8, 10000 times           0.14335164614021778\ndevice: cuda, dtype: torch.int16, 10000 times           0.1448021186515689\ndevice: cuda, dtype: torch.int32, 10000 times           0.14513055887073278\ndevice: cuda, dtype: torch.int64, 10000 times           0.1439579650759697\ndevice: cuda, dtype: torch.float32, 10000 times         0.14666561130434275\ndevice: cuda, dtype: torch.float64, 10000 times         0.1540807681158185\n```\nAfter:\n```\n_rshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.16366520430892706\ndevice: cpu, dtype: torch.uint8, 100000 times           0.16091545950621367\ndevice: cpu, dtype: torch.int16, 100000 times           0.1659633992239833\ndevice: cpu, dtype: torch.int32, 100000 times           0.1682385364547372\ndevice: cpu, dtype: torch.int64, 100000 times           0.17289020214229822\ndevice: cpu, dtype: torch.float32, 100000 times         0.24359441827982664\ndevice: cpu, dtype: torch.float64, 100000 times         0.21783945057541132\ndevice: cuda, dtype: torch.int8, 100000 times           1.2517220517620444\ndevice: cuda, dtype: torch.uint8, 100000 times          1.260181212797761\ndevice: cuda, dtype: torch.int16, 100000 times          1.2681935774162412\ndevice: cuda, dtype: torch.int32, 100000 times          1.2764465296640992\ndevice: cuda, dtype: torch.int64, 100000 times          1.294325228780508\ndevice: cuda, dtype: torch.float32, 100000 times                1.3062216322869062\ndevice: cuda, dtype: torch.float64, 100000 times                1.303224254399538\n__rshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.027045012451708317\ndevice: cpu, dtype: torch.uint8, 10000 times            0.026978280395269394\ndevice: cpu, dtype: torch.int16, 10000 times            0.025594274513423443\ndevice: cpu, dtype: torch.int32, 10000 times            0.02593063935637474\ndevice: cpu, dtype: torch.int64, 10000 times            0.02668109256774187\ndevice: cpu, dtype: torch.float32, 10000 times          0.09746317192912102\ndevice: cpu, dtype: torch.float64, 10000 times          0.1644029449671507\ndevice: cuda, dtype: torch.int8, 10000 times            0.12530914042145014\ndevice: cuda, dtype: torch.uint8, 10000 times           0.12615622486919165\ndevice: cuda, dtype: torch.int16, 10000 times           0.12741118855774403\ndevice: cuda, dtype: torch.int32, 10000 times           0.1284919548779726\ndevice: cuda, dtype: torch.int64, 10000 times           0.12974756956100464\ndevice: cuda, dtype: torch.float32, 10000 times         0.13044228963553905\ndevice: cuda, dtype: torch.float64, 10000 times         0.13918257877230644\n__irshift__ (a.numel() == 10) for 100000 times\ndevice: cpu, dtype: torch.int8, 100000 times            0.19456563983112574\ndevice: cpu, dtype: torch.uint8, 100000 times           0.190769555978477\ndevice: cpu, dtype: torch.int16, 100000 times           0.2002257639542222\ndevice: cpu, dtype: torch.int32, 100000 times           0.20456529594957829\ndevice: cpu, dtype: torch.int64, 100000 times           0.2043834924697876\ndevice: cpu, dtype: torch.float32, 100000 times         0.2832390898838639\ndevice: cpu, dtype: torch.float64, 100000 times         0.2582795573398471\ndevice: cuda, dtype: torch.int8, 100000 times           1.304957083426416\ndevice: cuda, dtype: torch.uint8, 100000 times          1.3216373259201646\ndevice: cuda, dtype: torch.int16, 100000 times          1.3238621400669217\ndevice: cuda, dtype: torch.int32, 100000 times          1.333009460940957\ndevice: cuda, dtype: torch.int64, 100000 times          1.3835567953065038\ndevice: cuda, dtype: torch.float32, 100000 times                1.4483617274090648\ndevice: cuda, dtype: torch.float64, 100000 times                1.4179155295714736\n__irshift__ (a.numel() == 1000) for 10000 times\ndevice: cpu, dtype: torch.int8, 10000 times             0.03196091763675213\ndevice: cpu, dtype: torch.uint8, 10000 times            0.03048650734126568\ndevice: cpu, dtype: torch.int16, 10000 times            0.03048624936491251\ndevice: cpu, dtype: torch.int32, 10000 times            0.030591044574975967\ndevice: cpu, dtype: torch.int64, 10000 times            0.031246556900441647\ndevice: cpu, dtype: torch.float32, 10000 times          0.10918692220002413\ndevice: cpu, dtype: torch.float64, 10000 times          0.18057993799448013\ndevice: cuda, dtype: torch.int8, 10000 times            0.13614848721772432\ndevice: cuda, dtype: torch.uint8, 10000 times           0.130373639985919\ndevice: cuda, dtype: torch.int16, 10000 times           0.1332557238638401\ndevice: cuda, dtype: torch.int32, 10000 times           0.1331850504502654\ndevice: cuda, dtype: torch.int64, 10000 times           0.1363008264452219\ndevice: cuda, dtype: torch.float32, 10000 times         0.1370363561436534\ndevice: cuda, dtype: torch.float64, 10000 times         0.1442740885540843\n```\nFix https://github.com/pytorch/pytorch/issues/24512 #24516  https://github.com/pytorch/pytorch/issues/24659  https://github.com/pytorch/pytorch/issues/24663\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31594\n\nDifferential Revision: D19346542\n\nPulled By: ezyang\n\nfbshipit-source-id: 37dd00b86898810b850cf4769c3af8aea6d4596b", "pr_number": "31594", "files_changed": ["aten/src/ATen/Declarations.cwrap", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/THC/THCTensorMathPairwise.cu", "aten/src/THC/THCTensorMathPointwise.cuh", "aten/src/THC/generic/THCTensorMathPairwise.cu", "aten/src/THC/generic/THCTensorMathPairwise.h", "aten/src/THC/generic/THCTensorMathPointwise.cu"], "labels": ["merged", "open source"]}, "f995ec2076": {"title": "Remove qconfig_dict in top level eager mode quantization API (#31972)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31972\n\nSince eager mode quantization requires many user modifications, we can't\nconsistently quantize a given model by just changing qconfig_dict, therefore\nthe top level `qconfig_dict` is not that useful.\nfixes: https://github.com/pytorch/pytorch/issues/31549\n\nTest Plan:\n.\n\nImported from OSS\n\nDifferential Revision: D19330691\n\nfbshipit-source-id: 8aee6e5249e0c14e8a363ac1a83836e88887cd7d", "pr_number": "31972", "files_changed": ["torch/quantization/quantize.py"], "labels": ["merged"]}, "ab5eb65e74": {"title": "gate torch_global_deps with BUILD_SHARED_LIBS flag (#32011)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32011\n\nRun into build problem with Ninja + code analysis build as follows:\n```\nThe install of the torch_global_deps target requires changing an RPATH from\nthe build tree, but this is not supported with the Ninja generator unless\non an ELF-based platform.\n```\n\nSeems we don't need build the target for static build mode?\n\nVerified code analyzer works with the patch.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19336818\n\nPulled By: ljk53\n\nfbshipit-source-id: 37f45a9392c45ce92c1df40d739b23954e50a13a", "pr_number": "32011", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["merged"]}, "03ff3eb94d": {"title": "skip TEST_DILL on Python2 (#32027)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32027\n\nThe test was added in #30985 for #28313. Seems the fix only works for\nPython3 but doesn't work on Python2. The current Python2 CI docker image\ndoesn't have `dill` module installed at all so it's not captured.\n\nI'm trying to build and push new CI docker image which has `dill` installed\nand I verified it's the latest version 0.3.1.1 but the fix doesn't seem\nto work and blocks me from upgrading image version. It works for Python3\ndocker image though...\n\nHere is a succeeded job with old image (no dill installed):\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/4192688\n\nHere is a failed job with new image (dill installed):\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/4192679\n\nThis PR bypasses the test for Py2 to unblock docker image change. We\ncan figure out a proper fix for Py2 later.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19341451\n\nPulled By: ljk53\n\nfbshipit-source-id: d5768de8cbaf1beba8911da76f4942b8f210f2d2", "pr_number": "32027", "files_changed": ["test/common_utils.py"], "labels": ["merged"]}, "16b8ca56b6": {"title": "update docker image version (#31848)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31848\n\nTrigger docker image build and bump up docker image version.\n\nTest Plan: - Check tag at: http://ossci-docker.s3-website.us-east-1.amazonaws.com/pytorch.html\n\nDifferential Revision: D19282725\n\nPulled By: ljk53\n\nfbshipit-source-id: a27b2831a92ff54d80ccbae0f18dadff0469254c", "pr_number": "31848", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-ecr-gc.yml", ".circleci/verbatim-sources/workflows-nightly-android-binary-builds.yml", ".circleci/verbatim-sources/workflows-pytorch-ge-config-tests.yml", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml"], "labels": ["merged"]}, "c5af0afdcb": {"title": "catch exceptions in ProcessGroupAgent::enqueueSend and report them. (#31023)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31023\n\nAdds support to catch exceptions in ProcessGroupAgent::enqueueSend and\nreport them in the future by marking the future as completed with an exception\nindicating the error. An example of when this could happen is if the receiving\nside aborts when the sender is sending the message, previously, we would hang\nuntil the timeout is hit, and the original exception would be lost.\nghstack-source-id: 96498386\n\nTest Plan: Added a relevant unit test: `test_sender_exceptions` in rpc_test.py\n\nDifferential Revision: D18901981\n\nfbshipit-source-id: 08de26936c4ad45b837219a247088cbea644c04c", "pr_number": "31023", "files_changed": ["test/dist_autograd_test.py", "test/dist_utils.py", "test/rpc_test.py", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h"], "labels": ["merged"]}, "28c1258f18": {"title": "Scale init for batch-norm and layer-norm (#31983)", "body": "Summary:\nPer discussion with Fei Tian, we need to add a `scale_init_value` to scale down the output of normalization such as batch-norm and layer-norm.\n\nCurrently we have `sparse_normalization_options` to normalize embedding pooling output. By default, scale = 1.0, we found it's better to set scale from 0.025 to 0.1 https://fb.quip.com/MiKUAibEaYhH\n\nBesides, I am removing the tags from normalizers because it makes more sense to calculate norm ops in distributed trainers, not ps.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31983\n\nTest Plan:\nTesting LN and BN after sum-pooling --\nbaseline f160348514\nLN: f160348609\nBN: f160348710\n\n{F226106518}\n\nLayer norm after sum-pooling fwd_net https://fburl.com/sa4j207n\nLayer norm after dot-prod fwd_net https://fburl.com/twggwyvb\n\n## Unit Tests\nTesting normalization after pooling\n```\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test_4 -- test_sparse_pooling_batch_normalization\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test_4 -- test_dense_sparse_pooling_batch_normalization\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test_4 -- test_sparse_pooling_layer_normalization\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test_4 -- test_dense_sparse_pooling_layer_normalization\n```\n\nTesting normalization after dot-prod\n```\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test -- test_last_layer_use_batch_norm\nbuck test caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test -- test_last_layer_use_layer_norm\n```\n\nDifferential Revision: D19277618\n\nPulled By: SilunWang\n\nfbshipit-source-id: ea323e33e3647ba55d2e808ef09d94ad7b45b934", "pr_number": "31983", "files_changed": ["caffe2/python/layers/batch_normalization.py", "caffe2/python/layers/layer_normalization.py", "caffe2/python/normalizer.py"], "labels": ["fb-exported", "merged"]}, "638e4ad8b9": {"title": "Updated function definition for torch.mode and torch.median in torch docs (#32003)", "body": "Summary:\nIssue: https://github.com/pytorch/pytorch/issues/32002\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32003\n\nDifferential Revision: D19334306\n\nPulled By: anjali411\n\nfbshipit-source-id: fe6a7cc7295b2d582a0b528f353ec64d9085e8c5", "pr_number": "32003", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged"]}, "b6cee03e29": {"title": "C++ tensor indexing: add Slice / TensorIndex (#30424)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30424\n\n`at::indexing::TensorIndex` is used for converting C++ tensor indices such as `{None, \"...\", Ellipsis, 0, true, {1, None, 2}, torch::tensor({1, 2})}` into its equivalent `std::vector<TensorIndex>`, so that further tensor indexing operations can be performed using the supplied indices.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18695902\n\nPulled By: yf225\n\nfbshipit-source-id: d73e14a411cdbec815866b02e75ffd71a9186e89", "pr_number": "30424", "files_changed": ["aten/src/ATen/native/Indexing.cpp", "aten/src/ATen/native/Indexing.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/TensorIndexing.cpp", "aten/src/ATen/native/TensorIndexing.h", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/Indexing.cu", "test/cpp/api/CMakeLists.txt", "test/cpp/api/tensor_indexing.cpp"], "labels": ["merged"]}, "20e5c90d82": {"title": "accept url query when rank or wolrd_size is specified (#32016)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32016\n\nThe previously logic will raise exception when there is query in url when rank or world_size is specified\nThe fix will parse the url and stitch rank and world_size into url.query and regenerate the url.\n\nTest Plan: f161291877\n\nDifferential Revision: D19337929\n\nfbshipit-source-id: 6bb3a07716dda5233553804000b706052ff18db8", "pr_number": "32016", "files_changed": ["torch/distributed/rendezvous.py"], "labels": ["fb-exported", "merged"]}, "927c2a02b0": {"title": "enable autograd profiler to work with RPC and RRef. (#31381)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31381\n\nThis PR adds support for being able to profile both sync and async RPCs, so that users can use the autograd profiler and be able to view metrics such as RPC latency and number of calls in the profiler output.\n\nThe way this is implemented is by using the existing `RecordFunction` class provided by the autograd profiler. We create a `RecordFunction` instance when sending an RPC, if autograd profiling is enabled. We also invoke the starting callbacks on this `RecordFunction` instance, this does things such as start the CPU timer.  This instance is then persisted across the lifetime of the RPC by attaching it to the `Future` created by the RPC. When the RPC is finished (i.e. when `future->markComplete()` is called), we run the `RecordFunction` instance's end callbacks, which among other things, stops the timer so that we get the correct RPC latency.\n\nThe `RecordFunction` and relevant callbacks in `profiler.cpp` are modified slightly to support running end callbacks from a different thread (which is needed since futures are marked as completed by a different thread than the main RPC thread). By default, the autograd profiler uses a `thread_local` list of `Events` and `thread_id`. However, since we'd like to run the `RecordFunction`'s callbacks from a different thread, we would like to access the list of `Events` created by the original thread. This is done by attaching the `thread_id` for the event to the `RecordFunction`, and then looking up the event with that thread in `all_event_lists` (see the changes in `profiler.cpp`). To ensure that the original behavior does not change in the profiler, this described behavior is only run when a user calls `setOverrideThreadId()` on the `RecordFunction` object.\nghstack-source-id: 96527291\n\nTest Plan: Added a unit test.\n\nDifferential Revision: D19053322\n\nfbshipit-source-id: 9a27a60c809fc4fdb16fa5d85085f3b6b21abfbb", "pr_number": "31381", "files_changed": ["test/rpc_test.py", "test/test_autograd.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler.cpp", "torch/csrc/autograd/profiler.h", "torch/csrc/autograd/record_function.cpp", "torch/csrc/autograd/record_function.h", "torch/csrc/distributed/autograd/utils.cpp", "torch/csrc/distributed/autograd/utils.h", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_functions.h", "torch/distributed/rpc/api.py", "torch/distributed/rpc/internal.py"], "labels": ["merged"]}, "46f32e136a": {"title": "Revert \"Support PyTorch ROCm CI on Ubuntu18.04 (#31886)\" (#31946)", "body": "Summary:\nThis reverts commit 4ee9c562188ae930cb2520cfce7805f55acaf968.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31946\n\nDifferential Revision: D19368391\n\nPulled By: bddppq\n\nfbshipit-source-id: 63d032a5256ff4da7247fb1092be314c5b133eb6", "pr_number": "31946", "files_changed": ["docker/caffe2/jenkins/common/install_clang.sh", "docker/caffe2/jenkins/common/install_python.sh", "docker/caffe2/jenkins/common/install_rocm.sh", "docker/caffe2/jenkins/ubuntu-rocm/Dockerfile"], "labels": ["merged", "module: rocm", "open source"]}, "14593f077f": {"title": "remove list specialization from ivalue (#30734)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30734\n\nWhat are specialized lists?\n\nThe IValues that hold List[int], List[Tensor], and List[AnythingElse] are different C++ types.\ne.g. List[int] has a std::vector<int> while List[AnythingElse] holds a std::vector<IValue>.\n\nWhy do we have specialized lists?\n\nWhen we first created the JIT we needed to bind the ATen C++ API which has std::vector<int>,\nstd::vector<Tensor> as inputs. The easiest way to match this API was to make our IValues contain\nthese same types. Conversion was just unwrapping the IValue, very easy and cheap.\n\nWhat is the problem with specialized lists?\n\nWe end up with significant special cases through the compiler. Other types like Dict are not\nspecialized. So in the Pickler, for instance, there is a single piece of logic to handle\ntheir serialization. For Lists, we end up with multiple cases. Furthermore, it doesn't\nmatch Python, leading to problems along translation boundaries. Our pickle serialization\nis slightly different than python, so it is harder to load objects from our IValue serialization\nas Python values.\n\nThey also make it harder to provide an easy-to-use user API. We'd like to match pybind11 for C++\nbindings to TorchScript. This would entail having a single torch::List class (untemplated)\nthat can be used to construct inputs. This is made much harder if the underlying ivalue needs\nto be different depending on the type inside the list. The ideal case would be to have a constructor like\n\n```\ntemplate<typename T>\nList(std::vector<T> foo);\n```\n\nIt would then set up the type tags correctly based on type T, without the need for passing tags.\n\nDo specialized lists improve perf?\n\nNot in a way we have been able to measure. Our major concern initially was having to translate\na std::vector<IValue> to std::vector<int> to call ATen functions. This was especially a concern\nfor aten::_convolution which takes a number of mostly-constant lists of integers. However,\nwhen we measure the effect of actually having to do this conversion for an aten::_convolution,\nit does not take measurable time (benchmark results below).\nThis is true even if you use a trivial convolution (e.g. 1x1x1), and comment out the actual convolution code.\n\nWhat are the issues removing them?\n\nThis PR removes list specialization but keeps the serialization format, and IValue APIs almost exactly\nthe same. The only visible change is that toTensorListRef and family have turned into toTensorVector\nbecause they now return by value a copy of the list as a vector.\n\nFurther PRs can then clean up the complexity issues that arose from speclization. This will likely\ninvolve removing the isTensorList/isIntList functions, and refactoring the code that used them to\nwork generically. At some point we will also change serialization to no longer write specialized\nlists in the pickle binary. This is forward incompatible, so will go in its own PR.\n\nBenchmark:\n```\nimport torch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\n\nclass MnistNet(nn.Module):\n    def __init__(self):\n        super(MnistNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 1, kernel_size=1)\n        self.conv2 = nn.Conv2d(1, 1, kernel_size=1)\n\n    def forward(self, x):\n        for i in range(10):\n            x = F.relu(self.conv1(x))\n            x = F.relu(self.conv2(x))\n        return x\n\nmodel = MnistNet()\nx = torch.rand(1, 1, 1, 1)\nr = torch.jit.trace(model, x )\nr(x)\nr(x)\nr(x)\nr(x)\nprint(torch.jit.last_executed_optimized_graph())\n\nwhile True:\n    b = time.time()\n    for i in range(100):\n        r(x)\n    e = time.time()\n    print(e - b)\n```\n\nResults (no observable difference):\n\n```\nBefore (actual conv)\n0.13251137733459473\n0.13260436058044434\n0.13276338577270508\n0.1327497959136963\n0.13250041007995605\n0.13270330429077148\n0.13290190696716309\n0.13265132904052734\n0.13274288177490234\n0.1326758861541748\n0.13253355026245117\n0.13254785537719727\n0.13260746002197266\n0.13285017013549805\n0.13264012336730957\n0.132490873336792\n0.13280034065246582\n0.13243484497070312\n0.1325232982635498\n0.1326127052307129\n0.13264131546020508\n0.13274383544921875\n0.13298296928405762\n0.1326909065246582\n-------------------\nAfter (actual conv)\n0.13127517700195312\n0.13150334358215332\n0.13092470169067383\n0.13102364540100098\n0.13134360313415527\n0.13155555725097656\n0.13314104080200195\n0.13151955604553223\n0.13160037994384766\n0.1315293312072754\n0.13137340545654297\n0.13148093223571777\n0.131455659866333\n0.1327371597290039\n0.13134026527404785\n0.13152337074279785\n0.13151192665100098\n0.13165974617004395\n0.13403725624084473\n0.13251852989196777\n0.13135504722595215\n0.1315624713897705\n0.1317615509033203\n0.1314380168914795\n0.13157200813293457\n--------------------\n\nThe following replace the convolution operator with a no-op, to show\nthat even if the conv op was made faster, then we still would not see\na difference:\n\nBefore (fake conv)\n0.0069539546966552734\n0.0069522857666015625\n0.007120847702026367\n0.007344722747802734\n0.007689952850341797\n0.007932662963867188\n0.00761723518371582\n0.007501363754272461\n0.007532835006713867\n0.007141828536987305\n0.007174253463745117\n0.007114410400390625\n0.007071495056152344\n------------------\nAfter (fake conv)\n0.007458209991455078\n0.007337093353271484\n0.007268190383911133\n0.007313251495361328\n0.007306575775146484\n0.007468700408935547\n0.0073091983795166016\n0.007308483123779297\n0.007538318634033203\n0.007356882095336914\n0.007464170455932617\n0.007372140884399414\n```\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18814702\n\nPulled By: zdevito\n\nfbshipit-source-id: 0371c73b63068fdc12f24b801371ea90f23531a6", "pr_number": "30734", "files_changed": ["aten/src/ATen/core/Dict_inl.h", "aten/src/ATen/core/List.h", "aten/src/ATen/core/List_inl.h", "aten/src/ATen/core/List_test.cpp", "aten/src/ATen/core/boxing/boxing.h", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/jit_type.h", "caffe2/core/operator.h", "caffe2/operators/experimental/c10/cpu/filler_cpu.cc", "test/cpp/jit/test_ivalue.cpp", "test/cpp/jit/test_misc.cpp", "torch/csrc/jit/constants.cpp", "torch/csrc/jit/graph_executor.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/register_mobile_ops.cpp", "torch/csrc/jit/passes/peephole.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/passes/utils/check_alias_annotation.cpp", "torch/csrc/jit/pickler.cpp", "torch/csrc/jit/pickler.h", "torch/csrc/jit/pybind_utils.h", "torch/csrc/jit/register_c10_ops.cpp", "torch/csrc/jit/register_prim_ops.cpp", "torch/csrc/jit/register_special_ops.cpp", "torch/csrc/jit/script/function_schema_parser.cpp", "torch/csrc/jit/tracer.cpp", "torch/csrc/jit/tracer.h", "torch/csrc/jit/unpickler.cpp", "torch/csrc/jit/unpickler.h", "torch/csrc/jit/vararg_functions.h"], "labels": ["jit", "merged"]}, "77c2c78e01": {"title": "Fix typographical error in torch.triu docstring (#32067)", "body": "Summary:\nbelow --> above\n\nFixes https://github.com/pytorch/pytorch/issues/32032\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32067\n\nDifferential Revision: D19355788\n\nPulled By: zou3519\n\nfbshipit-source-id: dc7a2538a78cd11e72d47ad923ef50599a5a87e2", "pr_number": "32067", "files_changed": ["torch/_torch_docs.py"], "labels": ["merged", "open source"]}, "e74a215ade": {"title": "Changed clip_grad_norm_ total_norm calculation (#32020)", "body": "Summary:\nRedefines the computation of the total_norm to increase performance as shown in https://github.com/pytorch/pytorch/issues/31474.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32020\n\nDifferential Revision: D19353309\n\nPulled By: ngimel\n\nfbshipit-source-id: bf7530dcd39f56614a211b5f21445864d4f2e875", "pr_number": "32020", "files_changed": ["torch/nn/utils/clip_grad.py"], "labels": ["merged", "open source"]}, "4002fec509": {"title": "Display NVCC version in CI for convenience to look at", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32069\n\nDifferential Revision: D19372943\n\nPulled By: ezyang\n\nfbshipit-source-id: c78e5779d4139e42df1f235db65d8c0399ffa1a2", "pr_number": "32069", "files_changed": [".jenkins/pytorch/build.sh"], "labels": ["merged", "open source"]}, "9a4219eb39": {"title": "Install complete set of headers for ROCm build (#32076)", "body": "Summary:\nThis PR adds a more complete list of pytorch header files to be installed at build time. It also fixes one instance of including a header from local src directory instead of installed directory.\nA more complete set of headers enable other modules to correctly work with pyTorch built for ROCm.\n\ncc: ezyang bddppq iotamudelta\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32076\n\nDifferential Revision: D19372933\n\nPulled By: ezyang\n\nfbshipit-source-id: 3b5f3241c001fa05ea448c359a706ce9a8214aa0", "pr_number": "32076", "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h", "setup.py"], "labels": ["merged", "open source"]}, "8e93159fb6": {"title": "CUDA 8 cleanup (#32013)", "body": "Summary:\nCUDA 8 is no longer supported\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32013\n\nDifferential Revision: D19372963\n\nPulled By: ezyang\n\nfbshipit-source-id: e584d7d5d5908933221ea4400234b3e6e7c32e7a", "pr_number": "32013", "files_changed": [".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/docker/common/install_conda.sh", ".circleci/verbatim-sources/workflows-docker-builder.yml", ".jenkins/caffe2/build.sh", ".jenkins/pytorch/multigpu-test.sh", ".jenkins/pytorch/test.sh", "docker/caffe2/ubuntu-16.04-cuda8-cudnn6-all-options/Dockerfile", "docker/caffe2/ubuntu-16.04-cuda8-cudnn7-all-options/Dockerfile"], "labels": ["merged", "open source"]}, "695c4f1bab": {"title": "Fix a typo in function name: liner -> linear", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32068\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19373360\n\nPulled By: nairbv\n\nfbshipit-source-id: 7696300b5c1dbcd7991fda3311d68807b2960982", "pr_number": "32068", "files_changed": ["aten/src/ATen/native/Normalization.cpp"], "labels": ["merged"]}, "5988d36f58": {"title": "Fix cumprod error for tensors with zero elements (#32070)", "body": "Summary:\nCurrently cumprod crashes for tensors with non-empty dimensions but with zero elements, which could happen when some dimension is zero. This commit fixes the error by checking both dim() and numel() in cumprod backward\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32070\n\nDifferential Revision: D19373200\n\nPulled By: ezyang\n\nfbshipit-source-id: d8ecde33f3330b40a7c611f6faa3b1d707ef2a9a", "pr_number": "32070", "files_changed": ["test/test_torch.py", "tools/autograd/templates/Functions.cpp"], "labels": ["merge-this-please", "merged"]}, "a3dd44653f": {"title": "Fix typo in config script to re-enable libtorch build and test in macOS CI (#32072)", "body": "Summary:\nCurrently, libtorch build and test are not running in macOS CI. This PR fixes the issue.\n\n**Test Plan:**\nCheck that libtorch build and test are running again in macOS CI.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32072\n\nDifferential Revision: D19373615\n\nPulled By: yf225\n\nfbshipit-source-id: 28686ef5895358a2b60db46b1946f21c58c6a18e", "pr_number": "32072", "files_changed": [".jenkins/pytorch/common.sh", ".jenkins/pytorch/macos-test.sh"], "labels": ["merged"]}, "1f34801460": {"title": "More robust mangling (#31978)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31978\n\nCurrently we keep a `mangleIndex_` that's intenral to compilation unit and\njust increment the index when we found the original name is mangled, this doesn't\nguarantee the new name is not defined.\nThis PR fixes the problem by querying whether the new name is defined or not.\nfixes: https://github.com/pytorch/pytorch/issues/31268\n\nTest Plan:\nfixes the issue\n\nImported from OSS\n\nDifferential Revision: D19350535\n\nfbshipit-source-id: fe3262b2838d4208ab72e2cd4a5970b3a792ae86", "pr_number": "31978", "files_changed": ["test/test_jit.py", "torch/csrc/jit/script/compiler.cpp"], "labels": ["jit", "merged"]}, "d97413eb7a": {"title": "Change python/cpp docs CI to use a CPU-only image (#32102)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32102\n\nPreviously, the docs CI depended on our CUDA xenial py3 build. This\nmeant that the turnaround time to get signal for docs was very slow\n(I've seen builds that go as much as 3 hours).\n\nFortunately, the docs CI do not (and should not!) rely on CUDA. This\nPR changes it so that the docs CI runs on a CPU-only machine.\n\nFixes #29995\n\nTest Plan:\n- Check CI status on this PR by reading logs for the python and cpp docs\nbuilds.\n- I built the docs locally, once for CPU, and once for CUDA, and\nverified (via diff) that the pages were exactly the same)\n\nDifferential Revision: D19374078\n\nPulled By: zou3519\n\nfbshipit-source-id: 3eb36f692c3c0632d2543d3439c822d51a87b809", "pr_number": "32102", "files_changed": [".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml"], "labels": ["merged"]}, "701ca68882": {"title": "Docs entry for the `is_quantized`", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32075\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19353861\n\nPulled By: z-a-f\n\nfbshipit-source-id: 4249216ac9a4af354a251c62181d65bc14cbfd3e", "pr_number": "32075", "files_changed": ["docs/source/tensors.rst", "torch/_tensor_docs.py"], "labels": ["merged"]}, "f003008d6e": {"title": "Allow TCPStore to pick a port to bind to. (#31674)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31674\n\nThe motivation of this PR was to fix the problem where we would see\n\"Address already in use\" issues for TCPStoreTest due to port conflicts. To\nresolve this:\n\n1. We can now pass in port 0 for TCPStore and retrieve the port it actually\nbound to using a new getPort() API.\n2. Added a `wait` flag to TCPStore constructor indicating whether or not it\nshould wait for workers (defaults to true).\n3. Made `waitForWorkers` a public API to ensure that we can construct TCPStore\nwithout waiting and wait for workers separately. This helps in TCPStoreTest to\nensure we can retrieve the port and pass it to the client stores.\nghstack-source-id: 96486845\n\nTest Plan: waitforbuildbot\n\nDifferential Revision: D19240947\n\nfbshipit-source-id: 7b1d1cb2730209fac788764845f1dbbe73d75d9b", "pr_number": "31674", "files_changed": ["torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/TCPStore.hpp", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": ["merged"]}, "470c496eb2": {"title": "use cholesky_inverse to compute precision matrix (#32092)", "body": "Summary:\nResolves a long-standing TODO. :D\n\nI also fix the docs of lowrank_mvn which is raised at [forum](https://discuss.pytorch.org/t/lowrankmultivariatenormal-example-raises-valueerror/65381).\n\ncc vishwakftw\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32092\n\nDifferential Revision: D19373912\n\nPulled By: ezyang\n\nfbshipit-source-id: b13129d7c30e87c6f8a6ced86601762a3f5c5624", "pr_number": "32092", "files_changed": ["torch/distributions/lowrank_multivariate_normal.py", "torch/distributions/multivariate_normal.py"], "labels": ["merge-this-please", "merged", "open source"]}, "a472f0201f": {"title": "Added support for Dim operation in ONNX export (#31928)", "body": "Summary:\nWhile ONNX does not currently directly support the Dim operation on a\ntensor, we can provide the same functionality with two ONNX operations.\nThis allows us to support Dim for all opsets. It may be adventageous to\nadd support for Dim into a future ONNX opset, and use that for more\nefficient code.\nWhile testing dim op found that there is an issue with empty blocks\nwithing if statements. Modified graph generation to prevent generation\nof empty if blocks.\n\nFixes https://github.com/pytorch/pytorch/issues/27569\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31928\n\nReviewed By: hl475\n\nDifferential Revision: D19376602\n\nPulled By: houseroad\n\nfbshipit-source-id: 111682b058a5341f5cca6c1a950c83ae412a4c6c", "pr_number": "31928", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/onnx/expect/TestOperators.test_dim.expect", "test/onnx/test_operators.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/jit/init.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_conditionals.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_conditionals.h", "torch/onnx/symbolic_opset9.py", "torch/onnx/utils.py"], "labels": ["jit", "merged", "open source", "triaged"]}, "26621d101f": {"title": "remove simple .data from torch/nn", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31482\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19303185\n\nPulled By: albanD\n\nfbshipit-source-id: 610eae096bab24a7b9f651b9af2e3ecd19df55b0", "pr_number": "31482", "files_changed": ["torch/autograd/gradcheck.py", "torch/testing/__init__.py"], "labels": ["merged"]}, "c036fbdc5c": {"title": "remove .data from torch/jit", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31480\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19303244\n\nPulled By: albanD\n\nfbshipit-source-id: ec66b32353f2f9b16072185ecde3ae8abbe09a35", "pr_number": "31480", "files_changed": ["torch/jit/__init__.py"], "labels": ["jit", "merged"]}, "77c78b7d28": {"title": "remove .data from torch/nn doc", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31481\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19303242\n\nPulled By: albanD\n\nfbshipit-source-id: 4f650df9e9e302a299175967bcc6e30a5099fa2a", "pr_number": "31481", "files_changed": ["torch/nn/functional.py", "torch/nn/modules/module.py"], "labels": ["merged"]}, "8d472bab6b": {"title": "Make torch.backends.mkldnn usable without import", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32055\n\nDifferential Revision: D19373220\n\nPulled By: ezyang\n\nfbshipit-source-id: 50ab3ff70fc893c81123419c4d3cf2e3e48a0a93", "pr_number": "32055", "files_changed": ["torch/__init__.py"], "labels": ["merge-this-please", "merged", "open source"]}, "5f1a881cb8": {"title": "Add private user tensor type IDs for experimentation. (#31830)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31830\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19330312\n\nPulled By: ezyang\n\nfbshipit-source-id: fe2e53e732e946088e983ec45fed2393436f0517", "pr_number": "31830", "files_changed": ["c10/core/TensorTypeId.h"], "labels": ["merged"]}, "dbd737158b": {"title": "support torch script call over rpc (#30063)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30063\n\nThis diff makes following changes:\n1. Providing a new set of python rpc privated APIs, they can accept an annotated TorchScript call and this call can be serialized, deserialized and executed in C++ without GIL. These privated APIs will be binded to JIT in the future, and they are different from public APIs as future JIT binded private APIs will be able to accept qualified_name, not callables. These private APIs are subject to be deprecated once JIT supports torch script function to be a JIT type.\n\nAlso, these APIs require torch script function to be defined and annotated by users in python land, it can not be script class/module constructor or class/module methods.\n\n2. This diff also allows public rpc APIs to accept an annotated TorchScript call and execute code path that above private APIs ran on. Therefore if users invoke an annotated TorchScript call over RPC, this call can be serialized, deserialized and executed in C++ without GIL as well.\n\n3. The above private APIs call a newly defined C++ function to make rpc torch script call to be serialized, deserialized and executed in C++ land. This C++ function returns an ivalue::Future. so that in follow up diff this C++ function can be called when these privated APIs are binded to JIT.\n\n4. script_call.cpp/.h and request_callback_impl.cpp files are refactored accordingly so that torch script call and builtin call can share same message type and codes.\n\n5. refactored deserializeResponse() and added a new utility to deserizalize response to IValue\n\nghstack-source-id: 96638829\n\nTest Plan: unit test\n\nDifferential Revision: D18482934\n\nfbshipit-source-id: bd82a0d820c47a8e45b2e7c616eca06573f7d7ea", "pr_number": "30063", "files_changed": ["test/dist_autograd_test.py", "test/rpc_test.py", "tools/build_variables.py", "torch/CMakeLists.txt", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.h", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.cpp", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/script_call.cpp", "torch/csrc/distributed/rpc/script_call.h", "torch/csrc/distributed/rpc/script_functions.cpp", "torch/csrc/distributed/rpc/script_functions.h", "torch/csrc/distributed/rpc/script_remote_call.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h", "torch/distributed/rpc/__init__.py", "torch/distributed/rpc/api.py"], "labels": ["merged"]}, "1487582ba7": {"title": "Switch important CI from CUDA 9 to 10.1 (#31951)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/31427\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31951\n\nDifferential Revision: D19393566\n\nPulled By: ezyang\n\nfbshipit-source-id: 06f9637791494a453d3fbef765840dc9f9805196", "pr_number": "31951", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/scripts/should_run_job.py", ".circleci/verbatim-sources/job-specs-custom.yml", ".circleci/verbatim-sources/workflows-ecr-gc.yml", ".circleci/verbatim-sources/workflows-nightly-android-binary-builds.yml", ".circleci/verbatim-sources/workflows-pytorch-ge-config-tests.yml", ".circleci/verbatim-sources/workflows-pytorch-mobile-builds.yml", ".jenkins/pytorch/build.sh", ".jenkins/pytorch/common.sh", ".jenkins/pytorch/multigpu-test.sh", ".jenkins/pytorch/test.sh"], "labels": ["merged", "open source"]}, "fa60e1150d": {"title": "Fix tensor^tensor derivative for 0 base entries", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32062\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19394259\n\nPulled By: agolynski\n\nfbshipit-source-id: 836525e03573af838511ad5b4cc87ec2c1536a5e", "pr_number": "32062", "files_changed": ["test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/templates/Functions.cpp"], "labels": ["merged"]}, "b783a75aa3": {"title": "Fix scalar^tensor derivative for scalars that are zero", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32063\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19394258\n\nPulled By: agolynski\n\nfbshipit-source-id: 3eed0f9cc1b8c677c6948c927d007044be67fe7f", "pr_number": "32063", "files_changed": ["test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/templates/Functions.cpp"], "labels": ["merged"]}, "0664c6bbfd": {"title": "Add ccls cache to gitignore (#31437)", "body": "Summary:\n`ccls` [puts a cache](https://github.com/MaskRay/ccls/wiki/Customization#cachedirectory) in the working directory by default, this PR adds it to gitignore so git doesn't pick it up\n](https://our.intern.facebook.com/intern/diff/19165007/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31437\n\nPulled By: driazati\n\nDifferential Revision: D19165007\n\nfbshipit-source-id: 41012eb0ece2df60b8566d7929710b154c38ee66", "pr_number": "31437", "files_changed": [".gitignore"], "labels": ["merged"]}, "61e509b992": {"title": "Skip un-runnable tests (#31965)", "body": "Summary:\n`test_init_ops` calls `orthogonal_` which fails without lapack (this test was just missing a skip condition)\n\nThe cpp tests would fail with a `undefined symbol` error if run with `BUILD_TESTS=0`, so this PR skips them if that flag is `0`\n](https://our.intern.facebook.com/intern/diff/19320064/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31965\n\nPulled By: driazati\n\nDifferential Revision: D19320064\n\nfbshipit-source-id: d1dcd36714107688ded25a414e8969abe026bd03", "pr_number": "31965", "files_changed": ["test/jit/test_unsupported_ops.py", "test/jit/unsupported_ops.py", "test/test_jit.py", "torch/CMakeLists.txt", "torch/csrc/jit/init.cpp"], "labels": ["jit", "merged"]}, "b0ac425dc4": {"title": "Emit warning from deprecated torch function signatures (#32009)", "body": "Summary:\nContinuation of https://github.com/pytorch/pytorch/issues/31514, fixes https://github.com/pytorch/pytorch/issues/28430\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32009\n\nTest Plan:\nI verified that the deprecation warnings only occur once on a relevant workflow. Built with:\n\n```\nbuck build mode/opt //vision/fair/detectron2/tools:train_net\n```\n\nRan with:\n\n```\nDETECTRON2_ENV_MODULE=detectron2.fb.env ~/local/train_net.par --config-file configs/quick_schedules/retinanet_R_50_FPN_instant_test.yaml --num-gpus 1 SOLVER.IMS_PER_BATCH 2\n```\n\nInspected log:\n\n```\n[01/14 07:28:13 d2.engine.train_loop]: Starting training from iteration 0\nbuck-out/opt/gen/caffe2/generate-code=python_variable_methods.cpp/python_variable_methods.cpp:1299: UserWarning: This overload of add is deprecated:\nadd(Number alpha, Tensor other)\nConsider using one of the following signatures instead:\nadd(Tensor other, Number alpha)\nbuck-out/opt/gen/caffe2/generate-code=python_variable_methods.cpp/python_variable_methods.cpp:1334: UserWarning: This overload of add_ is deprecated:\nadd_(Number alpha, Tensor other)\nConsider using one of the following signatures instead:\nadd_(Tensor other, Number alpha)\n[01/14 07:28:25 d2.utils.events]: eta: 0:00:10  iter: 19  total_loss: 1.699  loss_cls: 1.185  loss_box_reg: 0.501  time: 0.5020  data_time: 0.0224  lr: 0.000100  max_mem: 3722M\n[01/14 07:28:35 fvcore.common.checkpoint]: Saving checkpoint to ./output/model_final.pth\n```\n\nDifferential Revision: D19373523\n\nPulled By: ezyang\n\nfbshipit-source-id: 75756de129645501f43ecc4e3bf8cc0f78c40b90", "pr_number": "32009", "files_changed": ["test/common_utils.py", "test/test_torch.py", "tools/autograd/gen_python_functions.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": ["merged", "open source"]}, "2bb9dbeffa": {"title": "omit constexpr with nvcc on clang (#32149)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32149\n\nThis is an attempt at clarifying some of the preprocessor boolean logic that was getting more and more complicated. The previous logic used constexpr with nvcc on clang; which we were getting compiler failures on in ovrsource with mode/linux/* (based on platform007).\n\nTest Plan:\novrsource xplat/caffe2 compiles\nfbsource sandcastle green\n\nDifferential Revision: D19385409\n\nfbshipit-source-id: 60a02bae9854388b87510afdd927709673a6c313", "pr_number": "32149", "files_changed": ["c10/util/TypeIndex.h"], "labels": ["fb-exported", "merged"]}, "4a26bb9b18": {"title": "Suppress pip logs (#31912)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31912\n\n### Summary\n\nClean up the logs from pip-install.\n\n### Test Plan\n\n- Don't break the iOS simulator build\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19395526\n\nPulled By: xta0\n\nfbshipit-source-id: a638a209cab801ce90c8615e7ea030b1ab0939f3", "pr_number": "31912", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs-custom.yml"], "labels": ["merged"]}}